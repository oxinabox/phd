% This file was created with JabRef 2.10.
% Encoding: Cp1252


@Inproceedings{White2015BOWgen,
  Title                    = {Generating Bags of Words from the Sums of their Word Embeddings},
  Author                   = {White, Lyndon and Togneri,Roberto and Liu,Wei and Bennamoun, Mohammed},
  Booktitle                = {17th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing)},
  Year                     = {2016},

  Abstract                 = {Many methods have been proposed to generate sentence vector representations, such as recursive neural networks, latent distributed memory models, and the simple sum of word embeddings (SOWE). However, very few methods demonstrate the ability to reverse the process -- recovering sentences from sentence embeddings. Amongst the many sentence embeddings, SOWE has been shown to maintain semantic meaning, so in this paper we introduce a method for moving from the SOWE representations back to the bag of words (BOW) for the original sentences. This is a part way step towards recovering the whole sentence and has useful theoretical and practical applications of its own. This is done using a greedy algorithm to convert the vector to a bag of words. To our knowledge this is the first such work. It demonstrates qualitatively the ability to recreate the words from a large corpus based on its sentence embeddings.

As well as practical applications for allowing classical information retrieval methods to be combined with more recent methods using the sums of word embeddings, the success of this method has theoretical implications on the degree of information maintained by the sum of embeddings representation. This lends some credence to the consideration of the SOWE as a dimensionality reduced, and meaning enhanced, data manifold for the bag of words. },
  Timestamp                = {2013.08.15}
}

@Inproceedings{White2016a,
  Title                    = {Modelling Sentence Generation from Sum of Word Embedding Vectors as a Mixed Integer Programming Problem},
  Author                   = {White, Lyndon and Togneri,Roberto and Liu,Wei and Bennamoun, Mohammed},
  Booktitle                = {IEEE International Conference on Data Mining: High Dimensional Data Mining Workshop (ICDM: HDM)},
  Year                     = {2016},

  Abstract                 = {Converting a sentence to a meaningful vector representation
has uses in many NLP tasks, however very few methods allow that
representation to be restored to a human readable sentence. Being able
to generate sentences from the vector representations demonstrates the
level of information maintained by the embedding representation â€“ in this
case a simple sum of word embeddings. We introduce such a method
for moving from this vector representation back to the original sentences.
This is done using a two stage process; first a greedy algorithm is utilised
to convert the vector to a bag of words, and second a simple probabilistic
language model is used to order the words to get back the sentence.
To the best of our knowledge this is the first work to demonstrate
quantitatively the ability to reproduce text from a large corpus based
directly on its sentence embeddings},
  Doi                      = {10.1109/ICDMW.2016.0113},
  Owner                    = {20361362},
  Timestamp                = {2017.02.28},
  Url                      = {http://white.ucc.asn.au/publications/White2016SOWE2Sent.pdf}
}

@Inproceedings{White2015SentVecMeaning,
  Title                    = {How Well Sentence Embeddings Capture Meaning},
  Author                   = {White, Lyndon and Togneri, Roberto and Liu, Wei and Bennamoun, Mohammed},
  Booktitle                = {Proceedings of the 20th Australasian Document Computing Symposium},
  Year                     = {2015},
  Pages                    = {9:1--9:8},
  Publisher                = {ACM},
  Series                   = {ADCS '15},

  Abstract                 = {Several approaches for embedding a sentence into a vector space have been developed. However, it is unclear to what extent the sentence's position in the vector space reflects its semantic meaning, rather than other factors such as syntactic structure. Depending on the model used for the embeddings this will vary -- different models are suited for different down-stream applications. For applications such as machine translation and automated summarization, it is highly desirable to have semantic meaning encoded in the embedding. We consider this to be the quality of semantic localization for the model -- how well the sentences' meanings coincides with their embedding's position in vector space. Currently the semantic localization is assessed indirectly through practical benchmarks for specific applications. 

In this paper, we ground the semantic localization problem through a semantic classification task. The task is to classify sentences according to their meaning. A SVM with a linear kernel is used to perform the classification using the sentence vectors as its input. The sentences from subsets of two corpora, the Microsoft Research Paraphrase corpus and the Opinosis corpus, were partitioned according to their semantic equivalence. These partitions give the target classes for the classification task. Several existing models, including URAE, PV--DM and PV--DBOW, were assessed against a bag of words benchmark},
  Acmid                    = {2838932},
  Articleno                = {9},
  Doi                      = {10.1145/2838931.2838932},
  ISBN                     = {978-1-4503-4040-3},
  Keywords                 = {Semantic vector space representations, semantic consistency evaluation, sentence embeddings, word embeddings},
  Location                 = {Parramatta, NSW, Australia},
  Numpages                 = {8},
  Owner                    = {20361362},
  Timestamp                = {2015.11.19},
  Url                      = {http://doi.acm.org/10.1145/2838931.2838932}
}

@Inproceedings{7376619,
  Title                    = {Transistor Sizing Using Particle Swarm Optimisation},
  Author                   = {L. White and L. While and B. Deeks and F. Boussaid},
  Booktitle                = {2015 IEEE Symposium Series on Computational Intelligence},
  Year                     = {2015},
  Month                    = {Dec},
  Pages                    = {259-266},

  Abstract                 = {We describe an application of particle swarm optimisation to the problem of determining the optimal sizing of transistors in an integrated circuit. The algorithm minimises the total area of silicon utilised by a given circuit, whilst maintaining the propagation delay of the circuit within a hard limit. It assesses designs using the well-known circuit simulation engine SPICE, making allowance for the inability of SPICE to assess poorly-designed circuits within a reasonable timeframe. Experiments on three different types of circuits demonstrate that the algorithm is able to derive excellent designs for a range of problem instances, including several problems where the Monte Carlo method is unable to find any feasible solutions at all.},
  Doi                      = {10.1109/SSCI.2015.46},
  Keywords                 = {MOSFET;Monte Carlo methods;elemental semiconductors;particle swarm optimisation;silicon;Monte Carlo method;Si;circuit simulation engine SPICE;integrated circuit;optimal transistor sizing;particle swarm optimisation;propagation delay;silicon;Algorithm design and analysis;Birds;Integrated circuit modeling;Optimization;Propagation delay;SPICE;Transistors},
  Owner                    = {20361362},
  Timestamp                = {2017.05.03}
}

@comment{jabref-meta: databaseType:biblatex;}

@comment{jabref-meta: groupsversion:3;}

@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:to_read\;0\;;
1 ExplicitGroup:KeyPapers\;0\;;
1 ExplicitGroup:Automatically created groups\;2\;;
2 KeywordGroup:Amr\;0\;keywords\;Amr\;0\;0\;;
2 KeywordGroup:Competition\;0\;keywords\;Competition\;0\;0\;;
2 KeywordGroup:Computational complexity\\\;learning (artificial intell
igence)\\\;neural nets\\\;speech recognition\\\;4-gram model\\\;comput
ational complexity\\\;english broadcast news speech recognition task\\
\;hash-based implementation\\\;large scale neural network language mod
el training\\\;maximum entropy model\\\;training data\\\;word error ra
te\\\;artificial neural networks\\\;computational complexity\\\;comput
ational modeling\\\;data models\\\;entropy\\\;training\\\;training dat
a\;0\;keywords\;Computational complexity\\\;learning (artificial intel
ligence)\\\;neural nets\\\;speech recognition\\\;4-gram model\\\;compu
tational complexity\\\;english broadcast news speech recognition task\
\\;hash-based implementation\\\;large scale neural network language mo
del training\\\;maximum entropy model\\\;training data\\\;word error r
ate\\\;artificial neural networks\\\;computational complexity\\\;compu
tational modeling\\\;data models\\\;entropy\\\;training\\\;training da
ta\;0\;0\;;
2 KeywordGroup:Computer vision\\\;estimation theory\\\;image segmentat
ion\\\;nonparametric statistics\\\;pattern clustering\\\;smoothing met
hods\\\;nadaraya-watson estimator\\\;algorithm performance\\\;analysis
 resolution\\\;arbitrarily shaped cluster delineation\\\;color images\
\\;complex multimodal feature space\\\;computational module\\\;converg
ence\\\;density function\\\;density modes detection\\\;discontinuity-p
reserving image smoothing\\\;discrete data\\\;gray-level images\\\;ima
ge segmentation\\\;kernel regression\\\;location estimation\\\;low-lev
el vision algorithms\\\;mean shift\\\;nearest stationary point\\\;nonp
arametric technique\\\;pattern recognition procedure\\\;recursive mean
 shift procedure\\\;robust m-estimators\\\;robust feature space analys
is\\\;user-set parameter\\\;convergence\\\;density functional theory\\
\;image analysis\\\;image color analysis\\\;image resolution\\\;image 
segmentation\\\;kernel\\\;pattern recognition\\\;robustness\\\;smoothi
ng methods\;0\;keywords\;Computer vision\\\;estimation theory\\\;image
 segmentation\\\;nonparametric statistics\\\;pattern clustering\\\;smo
othing methods\\\;nadaraya-watson estimator\\\;algorithm performance\\
\;analysis resolution\\\;arbitrarily shaped cluster delineation\\\;col
or images\\\;complex multimodal feature space\\\;computational module\
\\;convergence\\\;density function\\\;density modes detection\\\;disco
ntinuity-preserving image smoothing\\\;discrete data\\\;gray-level ima
ges\\\;image segmentation\\\;kernel regression\\\;location estimation\
\\;low-level vision algorithms\\\;mean shift\\\;nearest stationary poi
nt\\\;nonparametric technique\\\;pattern recognition procedure\\\;recu
rsive mean shift procedure\\\;robust m-estimators\\\;robust feature sp
ace analysis\\\;user-set parameter\\\;convergence\\\;density functiona
l theory\\\;image analysis\\\;image color analysis\\\;image resolution
\\\;image segmentation\\\;kernel\\\;pattern recognition\\\;robustness\
\\;smoothing methods\;0\;0\;;
2 KeywordGroup:Corpora\;0\;keywords\;Corpora\;0\;0\;;
2 KeywordGroup:Dataset\;0\;keywords\;Dataset\;0\;0\;;
2 KeywordGroup:Delay effects\\\;delay systems\\\;filtering\\\;gaussian
 noise\\\;nonlinear equations\\\;nonlinear systems\\\;riccati equation
s\\\;robustness\\\;sufficient conditions\\\;uncertain systems\;0\;keyw
ords\;Delay effects\\\;delay systems\\\;filtering\\\;gaussian noise\\\
;nonlinear equations\\\;nonlinear systems\\\;riccati equations\\\;robu
stness\\\;sufficient conditions\\\;uncertain systems\;0\;0\;;
2 KeywordGroup:Discrete-time systems\;0\;keywords\;Discrete-time syste
ms\;0\;0\;;
2 KeywordGroup:Embeddings\;0\;keywords\;Embeddings\;0\;0\;;
2 KeywordGroup:Entity linking\;0\;keywords\;Entity linking\;0\;0\;;
2 KeywordGroup:Filtering\;0\;keywords\;Filtering\;0\;0\;;
2 KeywordGroup:Gaussian processes\\\;acoustic signal processing\\\;hid
den markov models\\\;mixture models\\\;neural nets\\\;speech recogniti
on\\\;asr\\\;dnn\\\;gmm\\\;gaussian mixture models\\\;hmm\\\;acoustic 
features\\\;acoustic modeling\\\;acoustic models\\\;automatic speech r
ecognition\\\;burgeoning area\\\;deep learning\\\;deep neural networks
\\\;hidden markov models\\\;high-level symbolic inputs\\\;human speech
 production\\\;intermediate acoustic feature sequences\\\;low-level sp
eech waveforms\\\;parametric speech generation\\\;statistical parametr
ic approach\\\;acoustic signal detection\\\;gaussian mixture models\\\
;hidden markov models\\\;speech processing\\\;speech recognition\\\;sp
eech synthesis\\\;vocoders\;0\;keywords\;Gaussian processes\\\;acousti
c signal processing\\\;hidden markov models\\\;mixture models\\\;neura
l nets\\\;speech recognition\\\;asr\\\;dnn\\\;gmm\\\;gaussian mixture 
models\\\;hmm\\\;acoustic features\\\;acoustic modeling\\\;acoustic mo
dels\\\;automatic speech recognition\\\;burgeoning area\\\;deep learni
ng\\\;deep neural networks\\\;hidden markov models\\\;high-level symbo
lic inputs\\\;human speech production\\\;intermediate acoustic feature
 sequences\\\;low-level speech waveforms\\\;parametric speech generati
on\\\;statistical parametric approach\\\;acoustic signal detection\\\;
gaussian mixture models\\\;hidden markov models\\\;speech processing\\
\;speech recognition\\\;speech synthesis\\\;vocoders\;0\;0\;;
2 KeywordGroup:Hidden markov models\\\;speech processing\\\;baseline s
ystem\\\;extractive speech summarization\\\;nongenerative probabilisti
c framework\\\;rhetorical information\\\;rhetorical-state hidden marko
v models\\\;automatic speech recognition\\\;data mining\\\;decoding\\\
;feature extraction\\\;hidden markov models\\\;humans\\\;natural langu
ages\\\;support vector machine classification\\\;support vector machin
es\\\;text recognition\\\;hidden markov models\\\;rhetorical informati
on\\\;speech features\\\;spoken document summarization\;0\;keywords\;H
idden markov models\\\;speech processing\\\;baseline system\\\;extract
ive speech summarization\\\;nongenerative probabilistic framework\\\;r
hetorical information\\\;rhetorical-state hidden markov models\\\;auto
matic speech recognition\\\;data mining\\\;decoding\\\;feature extract
ion\\\;hidden markov models\\\;humans\\\;natural languages\\\;support 
vector machine classification\\\;support vector machines\\\;text recog
nition\\\;hidden markov models\\\;rhetorical information\\\;speech fea
tures\\\;spoken document summarization\;0\;0\;;
2 KeywordGroup:H8 filtering\;0\;keywords\;H8 filtering\;0\;0\;;
2 KeywordGroup:H8 optimisation\\\;kalman filters\\\;riccati equations\
\\;difference equations\\\;discrete time filters\\\;error analysis\\\;
filtering theory\\\;game theory\\\;noise\\\;parameter estimation\\\;si
gnal processing\\\;kalman filter\\\;amplification\\\;difference riccat
i equation\\\;discrete h8 filter design\\\;estimation error signals\\\
;exogenous inputs\\\;finite energy signals\\\;finite-horizon discrete 
h8 filter\\\;game theory\\\;hostile noise signals\\\;linear quadratic 
game\\\;modified wiener filter\\\;system initial condition\\\;unknown 
statistics\\\;estimation error\\\;filtering theory\\\;game theory\\\;h
 infinity control\\\;noise measurement\\\;nonlinear filters\\\;riccati
 equations\\\;signal design\\\;signal processing\\\;statistics\;0\;key
words\;H8 optimisation\\\;kalman filters\\\;riccati equations\\\;diffe
rence equations\\\;discrete time filters\\\;error analysis\\\;filterin
g theory\\\;game theory\\\;noise\\\;parameter estimation\\\;signal pro
cessing\\\;kalman filter\\\;amplification\\\;difference riccati equati
on\\\;discrete h8 filter design\\\;estimation error signals\\\;exogeno
us inputs\\\;finite energy signals\\\;finite-horizon discrete h8 filte
r\\\;game theory\\\;hostile noise signals\\\;linear quadratic game\\\;
modified wiener filter\\\;system initial condition\\\;unknown statisti
cs\\\;estimation error\\\;filtering theory\\\;game theory\\\;h infinit
y control\\\;noise measurement\\\;nonlinear filters\\\;riccati equatio
ns\\\;signal design\\\;signal processing\\\;statistics\;0\;0\;;
2 KeywordGroup:Information extraction\;0\;keywords\;Information extrac
tion\;0\;0\;;
2 KeywordGroup:Machine learning\\\; reasoning\\\; recursive networks\;
0\;keywords\;Machine learning\\\; reasoning\\\; recursive networks\;0\
;0\;;
2 KeywordGroup:Markov processes\\\;speech recognition\\\;balls-in-urns
 system\\\;coin-tossing\\\;discrete markov chains\\\;ergodic models\\\
;hidden markov models\\\;hidden states\\\;left-right models\\\;probabi
listic function\\\;speech recognition\\\;distortion\\\;hidden markov m
odels\\\;mathematical model\\\;multiple signal classification\\\;signa
l processing\\\;speech recognition\\\;statistical analysis\\\;stochast
ic processes\\\;temperature measurement\\\;tutorial\;0\;keywords\;Mark
ov processes\\\;speech recognition\\\;balls-in-urns system\\\;coin-tos
sing\\\;discrete markov chains\\\;ergodic models\\\;hidden markov mode
ls\\\;hidden states\\\;left-right models\\\;probabilistic function\\\;
speech recognition\\\;distortion\\\;hidden markov models\\\;mathematic
al model\\\;multiple signal classification\\\;signal processing\\\;spe
ech recognition\\\;statistical analysis\\\;stochastic processes\\\;tem
perature measurement\\\;tutorial\;0\;0\;;
2 KeywordGroup:Max-entropy\;0\;keywords\;Max-entropy\;0\;0\;;
2 KeywordGroup:Mt\;0\;keywords\;Mt\;0\;0\;;
2 KeywordGroup:Named entity recognition\;0\;keywords\;Named entity rec
ognition\;0\;0\;;
2 KeywordGroup:Nlp\;0\;keywords\;Nlp\;0\;0\;;
2 KeywordGroup:Nonlinear systems\;0\;keywords\;Nonlinear systems\;0\;0
\;;
2 KeywordGroup:Optimisation\;0\;keywords\;Optimisation\;0\;0\;;
2 KeywordGroup:Paraphrasing\;0\;keywords\;Paraphrasing\;0\;0\;;
2 KeywordGroup:Phrase-embeddings\;0\;keywords\;Phrase-embeddings\;0\;0
\;;
2 KeywordGroup:Rae\;0\;keywords\;Rae\;0\;0\;;
2 KeywordGroup:Rcm\;0\;keywords\;Rcm\;0\;0\;;
2 KeywordGroup:Rnn\;0\;keywords\;Rnn\;0\;0\;;
2 KeywordGroup:Robust filtering\;0\;keywords\;Robust filtering\;0\;0\;
;
2 KeywordGroup:Rvnn\;0\;keywords\;Rvnn\;0\;0\;;
2 KeywordGroup:Sentiment analysis\\\; product review\\\; neural networ
k language model\\\; semi-supervised learning\;0\;keywords\;Sentiment 
analysis\\\; product review\\\; neural network language model\\\; semi
-supervised learning\;0\;0\;;
2 KeywordGroup:Software\;0\;keywords\;Software\;0\;0\;;
2 KeywordGroup:Summarization\;0\;keywords\;Summarization\;0\;0\;;
2 KeywordGroup:Terminology\;0\;keywords\;Terminology\;0\;0\;;
2 KeywordGroup:Tools\;0\;keywords\;Tools\;0\;0\;;
2 KeywordGroup:Uncertain systems\;0\;keywords\;Uncertain systems\;0\;0
\;;
2 KeywordGroup:Urae\;0\;keywords\;Urae\;0\;0\;;
2 KeywordGroup:Word-embeddings\;0\;keywords\;Word-embeddings\;0\;0\;;
2 KeywordGroup:Word-sense-disambiguation\;0\;keywords\;Word-sense-disa
mbiguation\;0\;0\;;
2 KeywordGroup:Wordnet\;0\;keywords\;Wordnet\;0\;0\;;
}

