#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.5cm
\topmargin 2.5cm
\rightmargin 1.5cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
For 
\begin_inset Formula $y$
\end_inset

 a class identifier
\end_layout

\begin_layout Standard
\begin_inset Formula $P(Y=y\mid X=x)=\frac{e^{[Wx+b]_{y}}}{\sum_{\forall i}e^{[Wx+b]_{i}}}$
\end_inset


\end_layout

\begin_layout Standard
Breaking these terms down we have:
\end_layout

\begin_layout Standard
\begin_inset Formula $P(Y=y\mid X=x)=\frac{e^{[Wx]_{i}}\,e^{b{}_{y}}}{\sum_{\forall i}e^{[Wx]_{i}}\,e^{b{}_{i}}}$
\end_inset


\end_layout

\begin_layout Standard
-
\end_layout

\begin_layout Standard
One can see that 
\begin_inset Formula $e^{[b]_{i}}$
\end_inset

 does not depend on the value of 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
It is analogous to the prior probability, the bias we have towards each
 element without observing the condition.
\end_layout

\begin_layout Standard
i.e.
 
\begin_inset Formula $Q(Y=i)=e^{[b]_{i}}=e^{b_{i}}$
\end_inset


\end_layout

\begin_layout Standard
-
\end_layout

\begin_layout Standard
Consider 
\begin_inset Formula $e^{[Wx]_{i}}$
\end_inset

.
\end_layout

\begin_layout Standard
The key component is 
\begin_inset Formula $[Wx]_{i}$
\end_inset


\end_layout

\begin_layout Standard
If we consider this, for each column 
\begin_inset Formula $y$
\end_inset

,
\end_layout

\begin_layout Standard
then we have 
\begin_inset Formula $[Wx]_{i}=\sum_{\forall j}W_{j,i}\,x_{j}=W_{i}^{\prime}x$
\end_inset

.
\end_layout

\begin_layout Standard
i.e given one is considering the case with 
\begin_inset Formula $Y=i$
\end_inset

, then 
\end_layout

\begin_layout Standard
considering the row vector 
\begin_inset Formula $W{}_{i}^{\prime}$
\end_inset

 is a weighting map for features possessed by 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
i.e.
 
\begin_inset Formula $R(X=x\mid Y=i)=e^{[Wx]_{i}}=e^{W_{i}^{\prime}x}$
\end_inset


\end_layout

\begin_layout Standard
-
\end_layout

\begin_layout Standard
Then reformulating the original statement we have:
\end_layout

\begin_layout Standard
\begin_inset Formula $P(Y=y\mid X=x)=\frac{R(X=x\mid Y=y)\,Q(Y=y)}{\sum_{\forall i}R(X=x\mid Y=i)\,Q(Y=i)}$
\end_inset


\end_layout

\begin_layout Standard
-
\end_layout

\begin_layout Standard
Contrast this to Bayes' Theorem
\end_layout

\begin_layout Standard
\begin_inset Formula $P(Y=y\mid X=x)=\frac{P(X=x\mid Y=y)\,P(Y=y)}{\sum_{\forall i}P(X=x\mid Y=i)\,P(Y=i)}$
\end_inset


\end_layout

\begin_layout Standard
-
\end_layout

\begin_layout Subsection
Application to NLP
\end_layout

\begin_layout Standard
Consider the Trigram Neural Probabilistic Language Model.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename D:/phd/documents_prepared/Books/Neural Representations of Natural Language/Book/figs/chapterwordrepr/trigram-neural-language-model.pdf

\end_inset


\end_layout

\begin_layout Standard
The final layer is 
\begin_inset Formula $s_{max}(VZ_{2}+k)$
\end_inset

.
\end_layout

\begin_layout Standard
By considering each the output of softmax element-wise
\end_layout

\begin_layout Standard
it can be reformulated as
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename D:/phd/documents_prepared/Books/Neural Representations of Natural Language/Book/figs/chapterwordrepr/trigram-adv-neural-language-model.pdf

\end_inset


\end_layout

\begin_layout Standard
With the final layer being defined as 
\begin_inset Formula $\dfrac{\exp\left(V_{w_{i}}^{\prime}Z_{2}+k_{i}\right)_{i=0}^{i=N}}{\sum_{\forall j}exp\left(V_{j}^{\prime}Z_{2}+k_{j}\right)}$
\end_inset

.
\end_layout

\begin_layout Standard
In this formulation, we have 
\begin_inset Formula $V_{w_{i}}^{\prime}$
\end_inset

 as the output embedding for 
\begin_inset Formula $w_{i}$
\end_inset

.
\end_layout

\begin_layout Standard
As we have 
\begin_inset Formula $C_{w_{i}}$
\end_inset

 as its input embedding.
\end_layout

\begin_layout Standard
-
\end_layout

\begin_layout Standard
It is also clear when we look at the Bayes-like reformulation:
\end_layout

\begin_layout Standard
\begin_inset Formula $P(Y=y\mid X=x)=\frac{R(X=x\mid Y=y)\,Q(Y=y)}{\sum_{\forall i}R(X=x\mid Y=i)\,Q(Y=i)}$
\end_inset


\end_layout

\begin_layout Standard
i.e.
 
\begin_inset Formula $P(w_{i}\mid w_{i-1},w_{i-2})=\frac{R(Z_{2}\mid W_{i}=w_{i})\,Q(W_{i}=w_{i})}{\sum_{\forall j}R(Z_{2}\mid W_{i}=w_{j})\,Q(W_{i}=w_{j})}$
\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $Q(Y=y)=e^{k}$
\end_inset


\end_layout

\begin_layout Standard
that this is linked to the Unigram word probabilities.
 
\begin_inset Formula $P(Y=y)$
\end_inset


\end_layout

\begin_layout Standard
such that one might suggest a good value for 
\begin_inset Formula $k$
\end_inset

 to be given by the log unigram probabilities.
\end_layout

\begin_layout Standard
\begin_inset Formula $k=\left(\log P(W_{i}=w_{j})\right)_{j=1}^{j=N}$
\end_inset

.
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $\mathbb{E}(R(Z\mid W_{i})=1$
\end_inset

 then this would be optimal.
\end_layout

\begin_layout Standard
That condition is equivalent to if 
\begin_inset Formula $\mathbb{E}(VZ_{2})=0$
\end_inset


\end_layout

\begin_layout Standard
Given the 
\begin_inset Formula $V$
\end_inset

 is initialized as a zero mean Gaussian,
\end_layout

\begin_layout Standard
this condition is at least initially true.
\end_layout

\begin_layout Standard
see 
\begin_inset CommandInset href
LatexCommand href
target "https://stats.stackexchange.com/questions/21568/softmax-regression-bias-and-prior-probabilities-for-unequal-classes/292467#292467"

\end_inset


\end_layout

\end_body
\end_document
