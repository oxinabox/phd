#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.5cm
\topmargin 2.5cm
\rightmargin 1.5cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
For 
\begin_inset Formula $y$
\end_inset

 a class identifier
\end_layout

\begin_layout Standard
\begin_inset Formula $P(Y=y\mid X=x)=\frac{e^{[Wx+b]_{y}}}{\sum_{\forall i}e^{[Wx+b]_{i}}}$
\end_inset


\end_layout

\begin_layout Standard
Breaking these terms down we have:
\end_layout

\begin_layout Standard
\begin_inset Formula $P(Y=y\mid X=x)=\frac{e^{[Wx]_{i}}\,e^{b_{y}}}{\sum_{\forall i}e^{[Wx]_{i}}\,e^{b_{i}}}$
\end_inset


\end_layout

\begin_layout Standard
-
\end_layout

\begin_layout Standard
One can see that 
\begin_inset Formula $e^{[b]_{i}}$
\end_inset

 does not depend on the value of 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
It is analogous to the prior probability, the bias we have towards each
 element without observing the condition.
\end_layout

\begin_layout Standard
i.e.
 
\begin_inset Formula $Q(Y=i)=e^{[b]_{i}}=e^{b_{i}}$
\end_inset


\end_layout

\begin_layout Standard
-
\end_layout

\begin_layout Standard
Consider 
\begin_inset Formula $e^{[Wx]_{i}}$
\end_inset

.
\end_layout

\begin_layout Standard
The key component is 
\begin_inset Formula $[Wx]_{i}$
\end_inset


\end_layout

\begin_layout Standard
If we consider this, for each column 
\begin_inset Formula $y$
\end_inset

,
\end_layout

\begin_layout Standard
then we have 
\begin_inset Formula $[Wx]_{i}=\sum_{\forall j}W_{j,i}\,x_{j}=W_{i}^{\prime}x$
\end_inset

.
\end_layout

\begin_layout Standard
i.e given one is considering the case with 
\begin_inset Formula $Y=i$
\end_inset

, then 
\end_layout

\begin_layout Standard
considering the row vector 
\begin_inset Formula $W_{i}^{\prime}$
\end_inset

 is a weighting map for features possessed by 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
i.e.
 
\begin_inset Formula $R(X=x\mid Y=i)=e^{[Wx]_{i}}=e^{W_{i}^{\prime}x}$
\end_inset


\end_layout

\begin_layout Standard
-
\end_layout

\begin_layout Standard
Then reformulating the original statement we have:
\end_layout

\begin_layout Standard
\begin_inset Formula $P(Y=y\mid X=x)=\frac{R(X=x\mid Y=y)\,Q(Y=y)}{\sum_{\forall i}R(X=x\mid Y=i)\,Q(Y=i)}$
\end_inset


\end_layout

\begin_layout Standard
-
\end_layout

\begin_layout Standard
Contrast this to Bayes' Theorem
\end_layout

\begin_layout Standard
\begin_inset Formula $P(Y=y\mid X=x)=\frac{P(X=x\mid Y=y)\,P(Y=y)}{\sum_{\forall i}P(X=x\mid Y=i)\,P(Y=i)}$
\end_inset


\end_layout

\begin_layout Standard
-
\end_layout

\begin_layout Subsection
Application to NLP
\end_layout

\begin_layout Standard
Consider the Trigram Neural Probabilistic Language Model.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename D:/phd/documents_prepared/Books/Neural Representations of Natural Language/Book/figs/chapterwordrepr/trigram-neural-language-model.pdf

\end_inset


\end_layout

\begin_layout Standard
The final layer is 
\begin_inset Formula $s_{max}(Vz_{2}+k)$
\end_inset

.
\end_layout

\begin_layout Standard
By considering each the output of softmax element-wise
\end_layout

\begin_layout Standard
it can be reformulated as
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename D:/phd/documents_prepared/Books/Neural Representations of Natural Language/Book/figs/chapterwordrepr/trigram-adv-neural-language-model.pdf

\end_inset


\end_layout

\begin_layout Standard
With the final layer being defined as 
\begin_inset Formula $\dfrac{\exp\left(V_{w_{i}}^{T}z_{2}+k_{i}\right)_{i=0}^{i=N}}{\sum_{\forall j}exp\left(V_{j}^{T}z_{2}+k_{j}\right)}$
\end_inset

.
\end_layout

\begin_layout Standard
In this formulation, we have 
\begin_inset Formula $V_{w_{i}}^{T}$
\end_inset

 as the output embedding for 
\begin_inset Formula $w_{i}$
\end_inset

.
\end_layout

\begin_layout Standard
As we have 
\begin_inset Formula $C_{w_{i}}$
\end_inset

 as its input embedding.
\end_layout

\begin_layout Standard
Further, 
\begin_inset Formula $V_{w_{i}}^{T}z_{2}$
\end_inset

 can be seen as computing the dot product between the output embedding for
 
\begin_inset Formula $w_{i}$
\end_inset

 and the hidden layer represenation of 
\begin_inset Formula $w_{i-1}$
\end_inset

 and 
\begin_inset Formula $w_{i-2}$
\end_inset

 in the form of 
\begin_inset Formula $z_{2}$
\end_inset

.
 
\end_layout

\begin_layout Standard
-
\end_layout

\begin_layout Standard
It is also clear when we look at the Bayes-like reformulation:
\end_layout

\begin_layout Standard
\begin_inset Formula $P(Y=y\mid X=x)=\frac{R(X=x\mid Y=y)\,Q(Y=y)}{\sum_{\forall i}R(X=x\mid Y=i)\,Q(Y=i)}$
\end_inset


\end_layout

\begin_layout Standard
i.e.
 
\begin_inset Formula $P(w_{i}\mid w_{i-1},w_{i-2})=\frac{R(Z=z_{2}\mid W_{i}=w_{i})\,Q(W_{i}=w_{i})}{\sum_{\forall j}R(Z=z_{2}\mid W_{i}=w_{j})\,Q(W_{i}=w_{j})}$
\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $Q(Y=y)=e^{k}$
\end_inset


\end_layout

\begin_layout Standard
that this is linked to the Unigram word probabilities.
 
\begin_inset Formula $P(Y=y)$
\end_inset


\end_layout

\begin_layout Standard
such that one might suggest a good value for 
\begin_inset Formula $k$
\end_inset

 to be given by the log unigram probabilities.
\end_layout

\begin_layout Standard
\begin_inset Formula $k=\left(\log P(W_{i}=w_{j})\right)_{j=1}^{j=N}$
\end_inset

.
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $\mathbb{E}(R(Z\mid W_{i})=1$
\end_inset

 then this would be optimal.
\end_layout

\begin_layout Standard
That condition is equivalent to if 
\begin_inset Formula $\mathbb{E}(VZ_{2})=0$
\end_inset


\end_layout

\begin_layout Standard
Given the 
\begin_inset Formula $V$
\end_inset

 is initialized as a zero mean Gaussian,
\end_layout

\begin_layout Standard
this condition is at least initially true.
\end_layout

\begin_layout Standard
see 
\begin_inset CommandInset href
LatexCommand href
target "https://stats.stackexchange.com/questions/21568/softmax-regression-bias-and-prior-probabilities-for-unequal-classes/292467#292467"

\end_inset


\end_layout

\begin_layout Section
An incredibly gentle introduction to hierarchical softmax
\end_layout

\begin_layout Subsection
First consider a binary tree with just 1 layer and 2 leaves
\end_layout

\begin_layout Standard
We are at the initial root which we call 
\begin_inset Formula $n_{0}$
\end_inset

, and we can go to either node 
\begin_inset Formula $n_{00}$
\end_inset

 or node 
\begin_inset Formula $n_{01}$
\end_inset

, based on the input from the layer below which we will call 
\begin_inset Formula $z$
\end_inset

.
\end_layout

\begin_layout Standard
Here we write 
\begin_inset Formula $n_{01}$
\end_inset

 to represent the event of the first nonroot node being the branch given
 by following the 0th branch, vs the 
\begin_inset Formula $n_{01}$
\end_inset

 being to follow the first branch.
 
\end_layout

\begin_layout Standard
We are naming the root node as a notation convenience so we can talk about
 the decision made at 
\begin_inset Formula $n_{0}$
\end_inset

 – 
\begin_inset Formula $P(n_{0})=1$
\end_inset

.
\end_layout

\begin_layout Standard
We want to know the probability of the next node being the 0th (i.e.
 
\begin_inset Formula $P(n_{0}\mid z)$
\end_inset

 ) or the first (i.e.
 
\begin_inset Formula $P(n_{01}\mid z)$
\end_inset

).
\end_layout

\begin_layout Standard
We could represent the decision with a softmax, with two outputs.
\end_layout

\begin_layout Standard
However, since it is a binary decision, we don't need a softmax, we can
 just use a sigmoid.
\end_layout

\begin_layout Standard
\begin_inset Formula $P(n_{01}\mid z)=P(n_{0}\mid z)-1$
\end_inset


\end_layout

\begin_layout Standard
So the weight matrix for a sigmoid layer has a number of columns given by
 the number of outputs.
\end_layout

\begin_layout Standard
As there is only one output, it is just a vector.
\end_layout

\begin_layout Standard
We will call this vector 
\begin_inset Formula $V_{0}^{\prime}$
\end_inset

 – the decision associated with node 
\begin_inset Formula $n_{0}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P(n_{00}\mid z)=\sigma(V_{0}^{\prime}z)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P(n_{01}\mid z)=1-\sigma(V_{0}^{\prime}z)$
\end_inset


\end_layout

\begin_layout Standard
Note that the sigmoid function: 
\begin_inset Formula $1-\sigma(x)=\sigma(-x)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $1-\sigma(x)=\frac{e^{x}+1}{e^{x}+1}-\frac{e^{x}}{e^{x}+1}=\dfrac{1}{e^{x}+1}\frac{e^{-x}}{e^{-x}}=\dfrac{e^{-x}}{1+e^{-1}}=\sigma(-x)$
\end_inset


\end_layout

\begin_layout Standard
So we have:
\end_layout

\begin_layout Standard
\begin_inset Formula $P(n_{01}\mid z)=\sigma(-V_{0}^{\prime}z)$
\end_inset


\end_layout

\begin_layout Subsection
Now consider 2 layers with 3 leaves
\end_layout

\begin_layout Standard
Consider the tree with nodes: 
\begin_inset Formula $n_{0}$
\end_inset

, 
\begin_inset Formula $n_{00}$
\end_inset

,
\begin_inset Formula $n_{000}$
\end_inset

, 
\begin_inset Formula $n_{001}$
\end_inset

,
\begin_inset Formula $n_{01}$
\end_inset


\end_layout

\begin_layout Standard
From earlier we still have:
\end_layout

\begin_layout Standard
\begin_inset Formula $P(n_{00}\mid z)=\sigma(V_{0}^{\prime}z)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P(n_{01}\mid z)=\sigma(-V_{0}^{\prime}z)$
\end_inset


\end_layout

\begin_layout Standard
Now to find 
\begin_inset Formula $P(n_{000}\mid z)$
\end_inset


\end_layout

\begin_layout Standard
We need another decision at node 
\begin_inset Formula $n_{00}$
\end_inset


\end_layout

\begin_layout Standard
The decision at 
\begin_inset Formula $n_{00}$
\end_inset

 as to if 
\begin_inset Formula $n_{000}$
\end_inset

 or 
\begin_inset Formula $n_{001}$
\end_inset

 is made, assuming we have reached 
\begin_inset Formula $n_{00}$
\end_inset

already.
\end_layout

\begin_layout Standard
So the decision is defined by 
\begin_inset Formula $P(n_{000}\mid z,\:n_{oo})$
\end_inset

,
\end_layout

\begin_layout Standard
which we can then use the conditional probability chain rule to recombine:
 
\begin_inset Formula $P(n_{000}\mid z)=P(n_{000}\mid z,\:n_{oo})\,P(n_{00}\mid z)\,$
\end_inset


\end_layout

\begin_layout Standard
Again we can use a sigmoid.
\end_layout

\begin_layout Standard
\begin_inset Formula $P(n_{000}\mid z,n_{00})=\sigma(V_{00}^{\prime}z)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P(n_{001}\mid z,n_{00})=\sigma(-V_{00}^{\prime}z)$
\end_inset


\end_layout

\begin_layout Standard
Very similar to before.
\end_layout

\begin_layout Standard
Our 3 leaf nodes final probabilities are given by:
\end_layout

\begin_layout Standard
\begin_inset Formula $P(n_{01}\mid z)=\sigma(-V_{0}^{\prime}z)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P(n_{000}\mid z)=\sigma(V_{00}^{\prime}z)\sigma(V_{0}^{\prime}z)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P(n_{001}\mid z)=\sigma(-V_{00}^{\prime}z)\sigma(V_{0}^{\prime}z)$
\end_inset


\end_layout

\begin_layout Subsection
Continue this logic
\end_layout

\begin_layout Standard
Using this system,
\end_layout

\begin_layout Standard
we know for a node encoded at position 
\begin_inset Formula $[t_{1},t_{2},t_{3},...,t_{L}]$
\end_inset

 e.g.
 
\begin_inset Formula $[0,1,0,...,1]$
\end_inset

.
\end_layout

\begin_layout Standard
It's probability can be found as: 
\begin_inset Formula $P(n_{0t_{1}...,t_{L}}\mid z)=P(n_{0t_{1}...,t_{L}}\mid z,\,n_{0t_{1}...,t_{L-1}})\,P(n_{0t_{1}...,t_{L-1}}\mid z)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P(n_{0,t_{1}}\mid z)=\sigma\left((-1)^{t_{1}}V_{0}^{\prime}z\right)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P(n_{0,t_{1},t_{2}}\mid z,\,n_{0,t_{1}})=\sigma\left((-1)^{t_{2}}V_{0t_{1}}^{\prime}z\right)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P(n_{0,t_{1},...,t_{i}}\mid z,\,n_{0,t_{1}...,t_{i-1}})=\sigma\left((-1)^{t_{i}}V_{0,t_{1}...,t_{i-1}}^{\prime}z\right)$
\end_inset


\end_layout

\begin_layout Standard
Applying the conditional probability chain rule to get:
\end_layout

\begin_layout Standard
\begin_inset Formula $P(n_{0,t_{1},...,t_{i}}\mid z,)=\prod_{i=1}^{i=L}\sigma\left((-1)^{t_{i}}V_{0,t_{1}...,t_{i-1}}^{\prime}z\right)$
\end_inset


\end_layout

\begin_layout Subsubsection
Formulation
\end_layout

\begin_layout Standard
He we have decides that the 0th branch represents the positive choice.
\end_layout

\begin_layout Standard
It is equivalent to have the 1th branch repressent the positive choice
\end_layout

\begin_layout Standard
\begin_inset Formula $P(n_{0,t_{1},...,t_{i}}\mid z,)=\prod_{i=1}^{i=L}\sigma\left((-1)^{t_{i}+1}V_{0,t_{1}...,t_{i-1}}^{\prime}z\right)$
\end_inset


\end_layout

\begin_layout Standard
Or to allow it to vary per node.
\end_layout

\begin_layout Standard
As in the formulation of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{mikolovSkip}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Using 
\begin_inset Formula $ch(n)$
\end_inset

 to represent an arbitrary child node of the node 
\begin_inset Formula $n$
\end_inset


\end_layout

\begin_layout Standard
and using an indicator function 
\begin_inset Formula $\left\llbracket a=b\right\rrbracket =\begin{cases}
1 & a=b\\
-1 & a\ne b
\end{cases}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $P(n_{0,t_{1},...,t_{i}}\mid z,)=\prod_{i=1}^{i=L}\sigma\left(\left\llbracket n_{t_{i}}=ch(n_{t_{i-1}})\right\rrbracket V_{0,t_{1}...,t_{i-1}}^{\prime}z\right)$
\end_inset


\end_layout

\begin_layout Subsubsection
Combining multiplications
\end_layout

\begin_layout Standard
If one wants to find both 
\begin_inset Formula $V_{00}^{\prime}z$
\end_inset

 and 
\begin_inset Formula $V_{0}^{\prime}z$
\end_inset

,
\end_layout

\begin_layout Standard
then this can be done using matrices simultaneously (Taking advantage of
 optimized matrix multiplication routines)
\end_layout

\begin_layout Standard
Using 
\begin_inset Formula $V_{0}^{\prime}$
\end_inset

 and 
\begin_inset Formula $V_{00}^{\prime}$
\end_inset

 as row vectors, and 
\begin_inset Formula $z$
\end_inset

 as a column vector
\end_layout

\begin_layout Standard
\begin_inset Formula $\left[\begin{array}{c}
V_{0}^{\prime}\\
V_{00}^{\prime}
\end{array}\right]z=\left[\begin{array}{c}
V_{0}^{\prime}\cdot z\\
V_{00}^{\prime}\cdot z
\end{array}\right]$
\end_inset


\end_layout

\begin_layout Standard
Thus the whole product for all of the decisions can be written as 
\begin_inset Formula $V^{\prime}z$
\end_inset

.
\end_layout

\begin_layout Standard
The problem then becomes indexing the relevant node rows.
\end_layout

\begin_layout Standard
Which is a not at all trivial problem in packing tree nodes elements into
 a matrix.
\end_layout

\begin_layout Standard
And might be better handled via actually using a tree structure.
\end_layout

\end_body
\end_document
