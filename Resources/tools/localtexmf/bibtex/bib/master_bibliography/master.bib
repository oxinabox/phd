% Encoding: UTF-8


@Inproceedings{rehurek_lrec,
  Title                    = {{Software Framework for Topic Modelling with Large Corpora}},
  Author                   = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
  Booktitle                = {{Proceedings of the LREC 2010 Workshop on New
 Challenges for NLP Frameworks}},
  Year                     = {2010},

  Address                  = {Valletta, Malta},
  Annote                   = {This is the preferred citation for Gensim.},
  Month                    = May,
  Note                     = {\url{http://is.muni.cz/publication/884893/en}},
  Pages                    = {45--50},
  Publisher                = {ELRA},

  Day                      = {22},
  Language                 = {English},
  Owner                    = {20361362},
  Timestamp                = {2015.07.12},
  Url                      = {https://github.com/piskvorky/gensim}
}

@Inproceedings{agirre2006,
  Title                    = {Evaluating and optimizing the parameters of an unsupervised graph-based WSD algorithm},
  Author                   = {Agirre, Eneko and Mart{\'\i}nez, David and De Lacalle, Oier L{\'o}pez and Soroa, Aitor},
  Booktitle                = {Proceedings of the first workshop on graph based methods for natural language processing},
  Year                     = {2006},
  Annote                   = {For $H_{w}=\{h_{w,1},h_{w,2},...\}$ the set of induced senses for
the word $w$ \\

And for $S_{w}=\{s_{w,1},s_{w,2},...\}$the set of standard senses
of the word $w$ \\

A mapping matrix $$M_{w}=\{P(s_{w,j}|h_{w,i})\}$$ for $h_{w,i}\in H_{w}$
and $s_{w,j}\in S_{w}$ is constructed by using the Unsupervised
WSD on a a labelled corpus, and by calculating $P(s_{w,j}|h_{w,i})$
though counting how often a word ($w$) annotated with sense $s_{w,j}$
is assigned to the induced sense $h_{w,i}$ (and applying defintion
of conditional probability) -- that is to say Hard (winner takes all)
descsions are made during the unsupervised WSD, and then counted.

With this mapping Mapping matrix defined we can transform

$\bar{h}_{w}=[\bar{h}_{w1},\bar{h}_{w_{2}},...]$ the set of scores
of induced senses for a the word $w$

in to $\bar{s}_{w}=[\bar{s}_{w1},\bar{s}_{w_{2}},...]$

by $\bar{s}_{w}=M\bar{h}_{w}$

As the corpus for evaluation they used the Semeval 2 english lexical sample (S2LS).
Using the training split for Mapping, and the training+test for inducting the senses with hyperlex.



This is the method used by SemEval 2007 \cite{SemEval2007WSIandWSD} for evaluating WSI methods on WSD

Sidenote: Begona Altuna, who I met at CICLing 2016, has authored a paper with several of the authors of this paper, and is in the same lab.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {89--96},

  Keywords                 = {WSD},
  Owner                    = {20361362},
  Timestamp                = {2016.08.10},
  Url                      = {https://www.researchgate.net/profile/David_Martinez14/publication/216591548_Evaluating_and_optimizing_the_parameters_of_an_unsupervised_graph-based_WSD_algorithm/links/0fcfd508f21b273d0e000000.pdf}
}

@Inproceedings{SemEval2007WSIandWSD,
  Title                    = {Semeval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems},
  Author                   = {Agirre, Eneko and Soroa, Aitor},
  Booktitle                = {Proceedings of the 4th International Workshop on Semantic Evaluations},
  Year                     = {2007},

  Address                  = {Stroudsburg, PA, USA},
  Annote                   = {Note: uses the same corpus as SemEval-2007 task 17 :English lexical
sample subtask", which has OntoNotes senses.},
  Pages                    = {7--12},
  Publisher                = {Association for Computational Linguistics},
  Series                   = {SemEval '07},

  Acmid                    = {1621476},
  Location                 = {Prague, Czech Republic},
  Numpages                 = {6},
  Owner                    = {20361362},
  Timestamp                = {2016.08.10},
  Url                      = {http://dl.acm.org/citation.cfm?id=1621474.1621476}
}

@Article{aherne1998bhattacharyya,
  Title                    = {The Bhattacharyya metric as an absolute similarity measure for frequency coded data},
  Author                   = {Aherne, Frank J and Thacker, Neil A and Rockett, Peter I},
  Journal                  = {Kybernetika},
  Year                     = {1998},

  Annote                   = {This is about a a distance measure for differentiating distributions},
  Number                   = {4},
  Pages                    = {363--368},
  Volume                   = {34},

  Owner                    = {20361362},
  Publisher                = {Institute of Information Theory and Automation AS CR},
  Timestamp                = {2015.07.29},
  Url                      = {http://www.tina-vision.net/docs/memos/1997-001.pdf}
}

@Article{androutsopoulos2010survey,
  Title                    = {A survey of paraphrasing and textual entailment methods},
  Author                   = {Androutsopoulos, Ion and Malakasiotis, Prodromos},
  Journal                  = {Journal of Artificial Intelligence Research},
  Year                     = {2010},

  Annote                   = {Long Journal Paper.

Gives bidirectional entailment defintion of semantic equivelence / paraphrase.},
  Pages                    = {135--187},

  Abstract                 = {Paraphrasing methods recognize, generate, or extract phrases, sentences, or longer natural lan-
guage expressions that convey almost the same information. Textual entailment methods, on the
other hand, recognize, generate, or extract pairs of natural language expressions, such that a human
who reads (and trusts) the first element of a pair would most likely infer that the other element is
also true. Paraphrasing can be seen as bidirectional textual entailment and methods from the two
areas are often similar. Both kinds of methods are useful, at least in principle, in a wide range of
natural language processing applications, including question answering, summarization, text gener-
ation, and machine translation. We summarize key ideas from the two areas by considering in turn
recognition, generation, and extraction methods, also pointing to prominent articles and resources.},
  Keywords                 = {paraphrasing},
  Owner                    = {20361362},
  Timestamp                = {2015.08.03},
  Url                      = {https://www.jair.org/media/2985/live-2985-5001-jair.pdf}
}

@Electronic{SCOWL,
  Title                    = {SCOWL (Spell Checker Orientated Word Lists)},
  Annote                   = {This is citing for the word lists in included in linux distributions.},
  Author                   = {Kevub Atkinson},
  Year                     = {2011},

  Owner                    = {20361362},
  Timestamp                = {2015.07.14}
}

@Inproceedings{1988CNN,
  Title                    = {An artificial neural network for spatio-temporal bipolar patterns: Application to phoneme classification},
  Author                   = {Atlas, Les E and Homma, Toshiteru and Marks II, Robert J},
  Booktitle                = {Proc. Neural Information Processing Systems (NIPS)},
  Year                     = {1988},
  Annote                   = {The actual first CNN paper, prior to Lecunn's work},
  Pages                    = {31},

  Owner                    = {20361362},
  Timestamp                = {2015.09.09},
  Url                      = {http://goo.gl/4OApDV}
}

@Article{Balikas,
  Title                    = {Learning language-independent sentence representations for multi-lingual, multi-document summarization},
  Author                   = {Balikas, Georgios and Amini, Massih-Reza},

  Annote                   = {Denoising auto encoder, parallel corpus based sentence representations.
extractive summaristation.

Roughtly take a sentence vector for a sentence, in several languages (repires parallel corpus),
concattenate. 
Use a denoising autoencoeder to get a representation.

Use standard sumarriation of vectorised sentences stuff to make a extactive summary},

  Keywords                 = {summarization, vector-based_summarization},
  Owner                    = {20361362},
  Timestamp                = {2015.10.21},
  Url                      = {ama.liglab.fr/~amini/Publis/MultiLingualTxtSumm_CAp15.pdf}
}

@Manual{AMRspec,
  Title                    = {Abstract meaning representation (AMR) specification},
  Author                   = {Banarescu, Laura and Bonial, Claire and Cai, Shu and Georgescu, Madalina and Griffitt, Kira and Hermjakob, Ulf and Knight, Kevin and Koehn, Philipp and Palmer, Martha and Schneider, Nathan},
  Organization             = {The University of Southern California: Information Sciences Institute},

  Annote                   = {AMR is complicated, yo.},
  Comment                  = {This is a live resource, it will be updated periodically.},
  Keywords                 = {AMR},
  Owner                    = {20361362},
  Timestamp                = {2015.04.24},
  Url                      = {https://github.com/amrisi/amr-guidelines/blob/master/amr.md}
}

@Article{Banarescu13abstractmeaning,
  author    = {Laura Banarescu and Claire Bonial and Shu Cai and Madalina Georgescu and Kira Griffitt and Ulf Hermjakob and Kevin Knight and Philipp Koehn and Martha Palmer and Nathan Schneider},
  title     = {Abstract Meaning Representation for Sembanking},
  year      = {2013},
  url       = {http://www.isi.edu/natural-language/amr/a.pdf},
  annote    = {AMR is a graph based description of a sentences meaning. Very different to a phrase vector. Every sentence with the same maning will have the same AMR (Abstract Meaning Representation). Eg: (d / destroy-01 :arg0 (b / boy) :arg1 (r / room)) the destruction of the room by the boy ... the boy’s destruction of the room ... The boy destroyed the room. AMR seeks to create a large annoted corpus of sentences. The annotations describe the Meaning of the sentence. In a Propbank like way.},
  groups    = {KeyPapers},
  owner     = {20361362},
  timestamp = {2015.04.18},
}

@Article{barron2013plagiarism,
  Title                    = {Plagiarism meets paraphrasing: Insights for the next generation in automatic plagiarism detection},
  Author                   = {Barr{\'o}n-Cede{\~n}o, Alberto and Vila, Marta and Mart{\'\i}, M Ant{\`o}nia and Rosso, Paolo},
  Journal                  = {Computational Linguistics},
  Year                     = {2013},

  Annote                   = {Discusses the creation of the Paraphrase for plagerism corpus P4P},
  Number                   = {4},
  Pages                    = {917--947},
  Volume                   = {39},

  Keywords                 = {corpora},
  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2015.06.24},
  Url                      = {http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00153}
}

@Article{AdaGrams,
  Title                    = {Breaking Sticks and Ambiguities with Adaptive Skip-gram},
  Author                   = {Sergey Bartunov and
 Dmitry Kondrashkin and
 Anton Osokin and
 Dmitry P. Vetrov},
  Journal                  = {CoRR},
  Year                     = {2015},

  Annote                   = {Honestly, pretty hard to understand.
Its a baysian method.
It models each word as having some number of senses (the number being given by a dirchlet process). 
And then uses stocastic variational Expectation Maximistation
to learn all parameters.
Think fitting a GMM.},
  Volume                   = {abs/1502.07257},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/BartunovKOV15},
  Keywords                 = {wsi, wsd},
  Owner                    = {20361362},
  Timestamp                = {2014.03.03},
  Url                      = {http://arxiv.org/pdf/1502.07257v2.pdf}
}

@Inproceedings{basile-caputo-semeraro:2014:Coling,
  Title                    = {An Enhanced Lesk Word Sense Disambiguation Algorithm through a Distributional Semantic Model},
  Author                   = {Basile, Pierpaolo and Caputo, Annalina and Semeraro, Giovanni},
  Booktitle                = {Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
  Year                     = {2014},

  Address                  = {Dublin, Ireland},
  Annote                   = {Enhances Lesk using LSA (Not word2vec).
Use LSA on a large corpus to get word emeddings.
Expand the gloss, by contattendated the Glosses of related words (similar to extended Lesk)
Sum the embeddings in the expanded Gloss, and the Embeddings in the Context.
Find the cosine distence.between them
Shortest distence, is returned .ord sense},
  Month                    = {August},
  Pages                    = {1591--1600},
  Publisher                = {Dublin City University and Association for Computational Linguistics},

  Abstract                 = {This paper describes a new Word Sense Disambiguation (WSD) algorithm which extends two
well-known variations of the Lesk WSD method. Given a word and its context, Lesk algorithm
exploits the idea of maximum number of shared words (maximum overlaps) between the context
of a word and each definition of its senses (gloss) in order to select the proper meaning. The main
contribution of our approach relies on the use of a word similarity function defined on a distribu-
tional semantic space to compute the gloss-context overlap. As sense inventory we adopt Babel-
Net, a large multilingual semantic network built exploiting both WordNet and Wikipedia. Besides
linguistic knowledge, BabelNet also represents encyclopedic concepts coming from Wikipedia.
The evaluation performed on SemEval-2013 Multilingual Word Sense Disambiguation shows
that our algorithm goes beyond the most frequent sense baseline and the simplified version of the
Lesk algorithm. Moreover, when compared with the other participants in SemEval-2013 task,
our approach is able to outperform the best system for English.},
  Keywords                 = {word-sense-disambiguation},
  Owner                    = {20361362},
  Timestamp                = {2015.07.17},
  Url                      = {http://www.aclweb.org/anthology/C14-1151}
}

@Book{BellmanDynamicProgramming,
  Title                    = {Dynamic Programming},
  Author                   = {R.R. Bellman},
  Publisher                = {Printon University Press},
  Year                     = {1972},
  Edition                  = {6th},
  Note                     = {Orignial Published in 1957},

  Owner                    = {20361362},
  Timestamp                = {2015.09.15},
  Url                      = {http://goo.gl/mHiXxD}
}

@Article{NPLM,
  Title                    = {A Neural Probabilistic Language Model},
  Author                   = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
  Journal                  = {The Journal of Machine Learning Research},
  Year                     = {2003},

  Annote                   = {The Paper that started this all,NNLM NN LM. Language Models},
  Pages                    = {137--186},

  Booktitle                = {Innovations in Machine Learning},
  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2015.09.11},
  Url                      = {http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf}
}

@Article{Julia,
  Title                    = {{J}ulia: A Fresh Approach to Numerical Computing},
  Author                   = {Jeff Bezanson and Alan Edelman and Stefan Karpinski and Viral B. Shah},
  Year                     = {2014},

  Abstract                 = {The Julia programming language is gaining enormous popularity. Julia was designed to be easy and fast. Most importantly, Julia shatters deeply established notions widely held in the applied community:

1. High-level, dynamic code has to be slow by some sort of law of nature.
2. It is sensible to prototype in one language and then recode in another language.
3. There are parts of a system for the programmer, and other parts best left untouched as they are built by the experts.

Julia began with a deep understanding of the needs of the scientific programmer and the needs of the computer in mind. Bridging cultures that have often been distant, Julia combines expertise from computer science and computational science creating a new approach to scientific computing. This note introduces the programmer to the language and the underlying design theory. It invites the reader to rethink the fundamental foundations of numerical computing systems.

In particular, there is the fascinating dance between specialization and abstraction. Specialization allows for custom treatment. We can pick just the right algorithm for the right circumstance and this can happen at runtime based on argument types (code selection via multiple dispatch). Abstraction recognizes what remains the same after differences are stripped away and ignored as irrelevant. The recognition of abstraction allows for code reuse (generic programming). A simple idea that yields incredible power. The Julia design facilitates this interplay in many explicit and subtle ways for machine performance and, most importantly, human convenience.},
  Eprint                   = {1411.1607},
  Eprintclass              = {cs.MS},
  Eprinttype               = {arXiv},
  Keywords                 = {tools},
  Owner                    = {20361362},
  Timestamp                = {2015.12.18},
  Url                      = {http://arxiv.org/abs/1411.1607}
}

@Book{bird2009natural,
  Title                    = {Natural language processing with Python},
  Author                   = {Bird, Steven and Klein, Ewan and Loper, Edward},
  Publisher                = {" O'Reilly Media, Inc."},
  Year                     = {2009},
  Annote                   = {This is the prefered citation for NLTK},

  Keywords                 = {software, tools},
  Owner                    = {20361362},
  Timestamp                = {2015.07.12},
  Url                      = {http://www.nltk.org/}
}

@Article{blei2003latent,
  Title                    = {Latent dirichlet allocation},
  Author                   = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  Journal                  = {the Journal of machine Learning research},
  Year                     = {2003},
  Pages                    = {993--1022},
  Volume                   = {3},

  Owner                    = {20361362},
  Publisher                = {JMLR. org},
  Timestamp                = {2015.07.12},
  Url                      = {https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf}
}

@Article{borko1963automatic,
  Title                    = {Automatic document classification},
  Author                   = {Borko, Harold and Bernick, Myrna},
  Journal                  = {Journal of the ACM (JACM)},
  Year                     = {1963},

  Annote                   = {Ancient article on document classification},
  Number                   = {2},
  Pages                    = {151--162},
  Volume                   = {10},

  Owner                    = {20361362},
  Publisher                = {ACM},
  Timestamp                = {2015.09.02},
  Url                      = {http://delivery.acm.org/10.1145/330000/321165/p151-borko.pdf?ip=130.95.149.38&id=321165&acc=ACTIVE%20SERVICE&key=65D80644F295BC0D.05ACB6DAA5D75F76.4D4702B0C3E38B35.4D4702B0C3E38B35&CFID=709398561&CFTOKEN=93369711&__acm__=1441184806_42491bd6a71163d771438c4c00eb8888}
}

@Article{Bottou2014,
  Title                    = {From machine learning to machine reasoning},
  Author                   = {Bottou, Léon},
  Journal                  = {Machine Learning},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {133-149},
  Volume                   = {94},

  Doi                      = {10.1007/s10994-013-5335-x},
  ISSN                     = {0885-6125},
  Keywords                 = {Machine learning; Reasoning; Recursive networks},
  Language                 = {English},
  Owner                    = {20361362},
  Publisher                = {Springer US},
  Timestamp                = {2015.05.27},
  Url                      = {http://dx.doi.org/10.1007/s10994-013-5335-x}
}

@Article{RvNNLogicalSemantics,
  Title                    = {Recursive Neural Networks for Learning Logical Semantics},
  Author                   = {Bowman, Samuel R and Potts, Christopher and Manning, Christopher D},
  Journal                  = {arXiv preprint arXiv:1406.1827},
  Year                     = {2014},

  Annote                   = {Train RvNN to work on logical structures.
Also trained recursive neural tensor networks (RvNTN) to do the same.
Found RvNTN to be better, and indeedd to work well in all cases.

Taking two sentences, decomposing them,
then at the top using a softmax classifier to deterrmine entailment, reverse entailment, equivalence, alternation, negation, cover or independence (else), between them.

It is noted that the RvNN fails to suitably separate long different expressions in embedded space.
The RvNTN does much better.},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.22},
  Url                      = {http://arxiv.org/pdf/1406.1827v1.pdf}
}

@Article{Bowman2015SmoothGeneration,
  Title                    = {Generating Sentences from a Continuous Space},
  Author                   = {Bowman, Samuel R and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M and Jozefowicz, Rafal and Bengio, Samy},
  Journal                  = {International Conference on Learning Representations (ICLR) Workshop},
  Year                     = {2016},

  Annote                   = {Focus is on generating smooth sentences, using RNN-Language models.

Preprint, under review for ICLR 2016},

  Owner                    = {20361362},
  Timestamp                = {2015.11.26},
  Url                      = {http://arxiv.org/pdf/1511.06349.pdf}
}

@Book{brickell1970differentiable,
  Title                    = {Differentiable Manifolds: An Introduction},
  Author                   = {Brickell, F. and Clark, R.S.},
  Publisher                = {Van Nostrand Reinhold},
  Year                     = {1970},
  Annote                   = {(In Science Library at 516.36 , 1970 DIF)

Good Book.
First few pages are a super consise intro to topology.
THen gets into Differentiable manifolds,
includding Grassmann Manifold.

Concludes with Lie Groups},
  Series                   = {The New University Mathematics Series},

  ISBN                     = {9780442010515},
  Keywords                 = {topology},
  Owner                    = {20361362},
  Timestamp                = {2016.08.18},
  Url                      = {https://books.google.com.au/books?id=25EJNQEACAAJ}
}

@Inproceedings{phrasaltranslationtool,
  Title                    = {Phrasal: a toolkit for statistical machine translation with facilities for extraction and incorporation of arbitrary model features},
  Author                   = {Cer, Daniel and Galley, Michel and Jurafsky, Daniel and Manning, Christopher D},
  Booktitle                = {Proceedings of the NAACL HLT 2010 Demonstration Session},
  Year                     = {2010},
  Annote                   = {Quiet likely THE engine for phrase-based translation.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {9--12},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://mt-archive.info/NAACL-HLT-2010-Cer-2.pdf}
}

@Inproceedings{cer2010best,
  Title                    = {The best lexical metric for phrase-based statistical MT system optimization},
  Author                   = {Cer, Daniel and Manning, Christopher D and Jurafsky, Daniel},
  Booktitle                = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  Year                     = {2010},
  Annote                   = {A comparason of the alterntives the BLEU},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {555--563},

  Keywords                 = {machine translation},
  Owner                    = {20361362},
  Timestamp                = {2015.10.26},
  Url                      = {https://web.stanford.edu/~jurafsky/N10-1080.pdf}
}

@Article{APS:2745308,
  Title                    = {Contexts of antonymous adjectives},
  Author                   = {Charles,Walter G. and Miller,George A.},
  Journal                  = {Applied Psycholinguistics},
  Year                     = {1989},

  Annote                   = {Suggests that you can not substitute adjectived with their antonyms an get a normal setence, but does not evaluate with a suitablely sized test set to draw strong conclusions.},
  Month                    = {9},
  Pages                    = {357--375},
  Volume                   = {10},

  Abstract                 = { ABSTRACT ABSTRACTThe method of sorting is used to compare sets of adjectival contexts. Contexts of directly antonymous adjectives are found to be highly discriminable, both with sentential and phrasal contexts. These results are used to argue that words with different meanings normally appear in discriminably different contexts, and that the cue for learning to associate direct antonyms is not their substitutability, but rather their relatively frequent co-occurrence in the same sentence. },
  Doi                      = {10.1017/S0142716400008675},
  ISSN                     = {1469-1817},
  Issue                    = {03},
  Numpages                 = {19},
  Owner                    = {20361362},
  Timestamp                = {2015.07.24},
  Url                      = {http://journals.cambridge.org/article_S0142716400008675}
}

@Article{7122294,
  Title                    = {Sentence Compression for Aspect-Based Sentiment Analysis},
  Author                   = {Wanxiang Che and Yanyan Zhao and Honglei Guo and Zhong Su and Ting Liu},
  Journal                  = {Audio, Speech, and Language Processing, IEEE/ACM Transactions on},
  Year                     = {2015},

  Annote                   = {ZA method by which they sterip non-senitiment relevant facts from the document},
  Month                    = {Dec},
  Number                   = {12},
  Pages                    = {2111-2124},
  Volume                   = {23},

  Abstract                 = {Sentiment analysis, which addresses the computational treatment of opinion, sentiment, and subjectivity in text, has received considerable attention in recent years. In contrast to the traditional coarse-grained sentiment analysis tasks, such as document-level sentiment classification, we are interested in the fine-grained aspect-based sentiment analysis that aims to identify aspects that users comment on and these aspects' polarities. Aspect-based sentiment analysis relies heavily on syntactic features. However, the reviews that this task focuses on are natural and spontaneous, thus posing a challenge to syntactic parsers. In this paper, we address this problem by proposing a framework of adding a sentiment sentence compression (Sent_Comp) step before performing the aspect-based sentiment analysis. Different from the previous sentence compression model for common news sentences, Sent_Comp seeks to remove the sentiment-unnecessary information for sentiment analysis, thereby compressing a complicated sentiment sentence into one that is shorter and easier to parse. We apply a discriminative conditional random field model, with certain special features, to automatically compress sentiment sentences. Using the Chinese corpora of four product domains, Sent_Comp significantly improves the performance of the aspect-based sentiment analysis. The features proposed for Sent_Comp, especially the potential semantic features, are useful for sentiment sentence compression.},
  Doi                      = {10.1109/TASLP.2015.2443982},
  ISSN                     = {2329-9290},
  Keywords                 = {data compression;grammars;pattern classification;random processes;text analysis;Chinese corpora;Internet;Sent_Comp;discriminative conditional random field model;document-level sentiment classification;fine-grained aspect-based sentiment analysis;sentiment sentence compression;syntactic parsers;Analytical models;Feature extraction;Semantics;Sentiment analysis;Speech processing;Syntactics;Aspect-based sentiment analysis;potential semantic features;sentence compression;sentiment analysis},
  Owner                    = {20361362},
  Timestamp                = {2015.11.23},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7122294}
}

@Inproceedings{chen1996empirical,
  Title                    = {An empirical study of smoothing techniques for language modeling},
  Author                   = {Chen, Stanley F and Goodman, Joshua},
  Booktitle                = {Proceedings of the 34th annual meeting on Association for Computational Linguistics},
  Year                     = {1996},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {310--318},

  Owner                    = {20361362},
  Timestamp                = {2015.10.20},
  Url                      = {http://www.speech.sri.com/projects/srilm/manpages/pdfs/chen-goodman-tr-10-98.pdf}
}

@Inproceedings{Chen2014,
  Title                    = {A Unified Model for Word Sense Representation and Disambiguation.},
  Author                   = {Chen, Xinxiong and Liu, Zhiyuan and Sun, Maosong},
  Booktitle                = {EMNLP},
  Year                     = {2014},
  Annote                   = {Requires WordNet, plus an unlabelled corpus.
Three major steps.

First learn word vectors using skipgrams.
Then for each word sense in WordNet, consider each word in the gloss with cosine similarity to the word of less than threshold hyperparameter. Average those to get the initital word sense vector.

Then:
For each sensence in the corpus
Calculate a context vectors initially as the average of all word vectors in the sentence.
For each word (Either from left to right, or in order of least word senses to most),
evaluate the cosine similarity of each the word sense to the current contect vector.
If the closest vector wins by greater than a given threshold (over the second closest), 
then replace the word vector within the context vector with the word sense vector.
From this word senses are determined for all words in corpus.

Then
Modify objective of skipgram to predict both context words, and if in the previous step the senses were determined beyond the threshhold (senses are encoded as a integer, based on WordNet index).},
  Organization             = {Citeseer},
  Pages                    = {1025--1035},

  Owner                    = {20361362},
  Timestamp                = {2016.04.19},
  Url                      = {http://emnlp2014.org/papers/pdf/EMNLP2014110.pdf}
}

@Unpublished{chenSpokenDialogueThesisProposal,
  Title                    = {Thesis Proposal: Unsupervised Learning and Modeling of Knowledge and Intent for Spoken Dialogue Systems},
  Author                   = {Yun-Nung (Vivian) Chen},

  Annote                   = {A PhD thesis Proposal. (Or is it the whole thesis? It is huge!) Focus is on Acquiring Knowledge from spoken word.},
  Month                    = {April},
  Year                     = {2015},

  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.cs.cmu.edu/~yvchen/doc/dissertation.pdf}
}

@Inproceedings{collobert2008unified,
  Title                    = {A unified architecture for natural language processing: Deep neural networks with multitask learning},
  Author                   = {Collobert, Ronan and Weston, Jason},
  Booktitle                = {Proceedings of the 25th international conference on Machine learning},
  Year                     = {2008},
  Organization             = {ACM},
  Pages                    = {160--167},

  Keywords                 = {em},
  Owner                    = {20361362},
  Timestamp                = {2015.07.27},
  Url                      = {http://www.thespermwhale.com/jaseweston/papers/unified_nlp.pdf}
}

@Inproceedings{colorni1991distributed,
  Title                    = {Distributed optimization by ant colonies},
  Author                   = {Colorni, Alberto and Dorigo, Marco and Maniezzo, Vittorio and others},
  Booktitle                = {Proceedings of the first European conference on artificial life},
  Year                     = {1991},
  Organization             = {Paris, France},
  Pages                    = {134--142},
  Volume                   = {142},

  Owner                    = {20361362},
  Timestamp                = {2015.09.15},
  Url                      = {https://svn-d1.mpi-inf.mpg.de/AG1/MultiCoreLab/papers/DorigoManiezzoColorni91%20-%20Ant%20Colonies.pdf}
}

@Article{meanshift,
  Title                    = {Mean shift: a robust approach toward feature space analysis},
  Author                   = {Comaniciu, D. and Meer, P.},
  Journal                  = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  Year                     = {2002},

  Annote                   = {is in sklearn \url{http://scikit-learn.org/dev/auto_examples/cluster/plot_mean_shift.html}},
  Month                    = {May},
  Number                   = {5},
  Pages                    = {603-619},
  Volume                   = {24},

  Abstract                 = {A general non-parametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure: the mean shift. For discrete data, we prove the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density. The relation of the mean shift procedure to the Nadaraya-Watson estimator from kernel regression and the robust M-estimators; of location is also established. Algorithms for two low-level vision tasks discontinuity-preserving smoothing and image segmentation - are described as applications. In these algorithms, the only user-set parameter is the resolution of the analysis, and either gray-level or color images are accepted as input. Extensive experimental results illustrate their excellent performance},
  Doi                      = {10.1109/34.1000236},
  ISSN                     = {0162-8828},
  Keywords                 = {computer vision;estimation theory;image segmentation;nonparametric statistics;pattern clustering;smoothing methods;Nadaraya-Watson estimator;algorithm performance;analysis resolution;arbitrarily shaped cluster delineation;color images;complex multimodal feature space;computational module;convergence;density function;density modes detection;discontinuity-preserving image smoothing;discrete data;gray-level images;image segmentation;kernel regression;location estimation;low-level vision algorithms;mean shift;nearest stationary point;nonparametric technique;pattern recognition procedure;recursive mean shift procedure;robust M-estimators;robust feature space analysis;user-set parameter;Convergence;Density functional theory;Image analysis;Image color analysis;Image resolution;Image segmentation;Kernel;Pattern recognition;Robustness;Smoothing methods},
  Owner                    = {20361362},
  Timestamp                = {2015.05.28},
  Url                      = {ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1000236}
}

@Inproceedings{conway1998algorithmicpluralisation,
  Title                    = {An algorithmic approach to english pluralization},
  Author                   = {Conway, DM},
  Booktitle                = {Proceedings of the Second Annual Perl Conference},
  Year                     = {1998},
  Annote                   = {A heuristic based appraoch to pluralising word.},

  Owner                    = {20361362},
  Timestamp                = {2015.06.25}
}

@Inproceedings{Daiber:2013:IEA:2506182.2506198,
  Title                    = {Improving Efficiency and Accuracy in Multilingual Entity Extraction},
  Author                   = {Daiber, Joachim and Jakob, Max and Hokamp, Chris and Mendes, Pablo N.},
  Booktitle                = {Proceedings of the 9th International Conference on Semantic Systems},
  Year                     = {2013},

  Address                  = {New York, NY, USA},
  Annote                   = {DBpedia},
  Pages                    = {121--124},
  Publisher                = {ACM},
  Series                   = {I-SEMANTICS '13},

  Acmid                    = {2506198},
  Doi                      = {10.1145/2506182.2506198},
  ISBN                     = {978-1-4503-1972-0},
  Keywords                 = {entity linking, information extraction, named entity recognition},
  Location                 = {Graz, Austria},
  Numpages                 = {4},
  Owner                    = {20361362},
  Timestamp                = {2015.06.10},
  Url                      = {http://doi.acm.org/10.1145/2506182.2506198}
}

@Misc{BlogMaxLikeMaxEnt,
  Title                    = {maximum-likelihood-and-entropy},

  Annote                   = {Uncitable Blog, but nicely written into to max entropy and the relationship between -ve log likelyhood, entropy rate (true entropy), and Kullback-Leiber diverence (relitive entropy, between true and estimate) 

Likelyhood is the joint pdf for all the events ($x_i$) occuring, parameterised by $\theta$, we assume that events are IID.

Arg Max the likelyhood of the emperical samples, to get $\theta$
Equivelently can Argmin the -ve log of the likelyhood.

$\theta_0$ is the true parameter
$\hat{\theta}$ is out estimated parameteri

"mean negative log-likelihood converges to the differential entropy under the true distribution plus the Kullback-Leibler divergence between the true distribution and the distribution we guess at"},
  Author                   = {David Darmon},
  HowPublished             = {blog},

  Keywords                 = {max-entropy},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://thirdorderscientist.org/homoclinic-orbit/2013/4/1/maximum-likelihood-and-entropy}
}

@Article{das2014frame,
  Title                    = {Frame-semantic parsing},
  Author                   = {Das, Dipanjan and Chen, Desai and Martins, Andr{\'e} FT and Schneider, Nathan and Smith, Noah A},
  Journal                  = {Computational Linguistics},
  Year                     = {2014},

  Annote                   = {See Eratum (under Note in BibTeX).},
  Note                     = {Has Erratum: \url{http://www.mitpressjournals.org/doi/pdfplus/10.1162/COLI_x_00205}
 
Erratum
The authors of the article ”Frame-Semantic P
arsing” and a graduate student discovered
that in rows 7 and 8 of Table 8, at inference time for argument identification with
gold frames, the described model included gold spans along with the candidate set
of automatic spans (elaborated in Section 6.1), thus creating an oracle, and artificially
bloating the precision, recall, and F
1 metrics. The revised metrics are:
Naive decoding: Precision=78.65 Recall=72.85 Fscore=75.64 (row 7)
Beam search decoding: Precision=80.40 Recall=72.84 Fscore=76.43 (row 8)
This unintended artifact also changes the
interpretation of Table 9. The reported
results there should be interpreted as an oracle comparison of various inference
methods, that uses both automatically extr
acted candidate spans as well as gold spans
for argument identification.
None of the other results in the article are affected by this error.},
  Number                   = {1},
  Pages                    = {9--56},
  Volume                   = {40},

  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00163}
}

@Article{DeMulder2015,
  Title                    = {A survey on the application of recurrent neural networks to statistical language modeling},
  Author                   = {De Mulder, Wim and Bethard, Steven and Moens, Marie-Francine},
  Journal                  = {Computer Speech \& Language},
  Year                     = {2015},

  Annote                   = {Review article on RNNs
Takes of Language modeling, ie next word predition.

Covers variation on RNNs.},
  Number                   = {1},
  Pages                    = {61--98},
  Volume                   = {30},

  Owner                    = {20361362},
  Publisher                = {Elsevier},
  Timestamp                = {2015.08.26},
  Url                      = {http://ac.els-cdn.com/S088523081400093X/1-s2.0-S088523081400093X-main.pdf?_tid=e8cb0744-4ba0-11e5-8c3c-00000aacb35d&acdnat=1440559197_e56fe4786343f5c77482cbcc845e1bc7}
}

@Article{de2012pattern,
  Title                    = {Pattern for python},
  Author                   = {De Smedt, Tom and Daelemans, Walter},
  Journal                  = {The Journal of Machine Learning Research},
  Year                     = {2012},

  Annote                   = {This is the paper for the patterns project.
barely comments on the features for pluralistation etc

Pluralisation part of the agorithm comes from \cite{conway1998algorithmicpluralisation}},
  Number                   = {1},
  Pages                    = {2063--2067},
  Volume                   = {13},

  Keywords                 = {tools, NLP},
  Owner                    = {20361362},
  Publisher                = {JMLR. org},
  Timestamp                = {2015.06.25},
  Url                      = {http://jmlr.csail.mit.edu/papers/volume13/desmedt12a/desmedt12a.pdf}
}

@Article{dernoncourt2012designing,
  Title                    = {Designing an intelligent dialogue system for serious games.},
  Author                   = {Dernoncourt, F.},
  Journal                  = {RJC EIAH’2012},
  Year                     = {2012},

  Annote                   = {A classical approch to a NLP/NLU chatterbot.

I vaguely know the author online.},
  Pages                    = {33},

  Owner                    = {20361362},
  Timestamp                = {2015.04.22},
  Url                      = {http://www.francky.me/doc/RJCEIAH2012_articlereviewed-20120412en.pdf}
}

@Inproceedings{Dinu2014CompositionalGeneration,
  Title                    = {How to make words with vectors: Phrase generation in distributional semantics},
  Author                   = {Dinu, Georgiana and Baroni, Marco},
  Booktitle                = {Proceedings of ACL},
  Year                     = {2014},
  Annote                   = {\textcite{Dinu2014CompositionalGeneration} extends the models described by \textcite{zanzotto2010estimating, Guevara2010} for generation. The composition is described as the sum of pair of linear transformations of the input vectors ($\u_1$ and $\u_2$) to get a output vector $\p$. 
The input and target output vectors are found using single value decomposition on the word (for input) and phrase (for output) co-occurrence matrix for the training data. The composition function is given by: $$f_{comp}\::\:\mathbb{R}^d\times\mathbb{R}^d \to \mathbb{R}^d\::\: (\u_1, \u_2)\mapsto W_1\u_2+W_2\u_2$$
Likewise, decomposition can be described by: $$f_{decomp}\::\:\mathbb{R}^d\to \mathbb{R}^d\times\mathbb{R}^d \::\: \p \mapsto (W^\prime_1\p, W^\prime_2\p)$$.
During training linear transformation matrices $W_1$,$W_2$ and $W_1^\prime$,$W_2^\prime$ can be solved for using least squares regression.
While it theoretically generalises to whole sentences, by recursive application of the composition or decomposition functions, Dinu and Baroni's work is quantitatively assessed only for very short phrases.},
  Pages                    = {624--633},

  Owner                    = {20361362},
  Timestamp                = {2015.11.13},
  Url                      = {http://clic.cimec.unitn.it/marco/publications/acl2014/dinu-baroni-generation-acl2014.pdf}
}

@Inproceedings{msrParapharaCorpus,
  Title                    = {Automatically Constructing a Corpus of Sentential Paraphrases},
  Author                   = {William B. Dolan and Chris Brockett},
  Booktitle                = {Third International Workshop on Paraphrasing (IWP2005)},
  Year                     = {2005},
  Annote                   = {This is the paper about how the MSRP corpus was creasted.
MSRPC},
  Publisher                = {Asia Federation of Natural Language Processing},

  Abstract                 = {<p>An obstacle to research in automatic paraphrase identification and generation
 is the lack of large-scale, publiclyavailable labeled corpora of sentential
 paraphrases. This paper describes the creation of the recently-released
 MicrosoftResearch Paraphrase Corpus, which contains 5801 sentence pairs, each
 hand-labeled with a binary judgment as to whether the pair constitutes a
 paraphrase. The corpus was created using heuristic extraction techniques in
 conjunction with an SVM-based classifier to select likely sentence-level
 paraphrases from a large corpus of topicclustered news data. These pairs were
 then submitted to human judges, who confirmed that 67{\%} were in fact semantically
 equivalent. In addition to describing the corpus itself, we explore a number of
 issues that arose in defining guidelines for the human raters.</p>},
  Keywords                 = {paraphrasing, corpora},
  Owner                    = {20361362},
  Timestamp                = {2015.07.15},
  Url                      = {http://research.microsoft.com/pubs/101076/I05-5002[1].pdf}
}

@Article{donahue2014long,
  Title                    = {Long-term recurrent convolutional networks for visual recognition and description},
  Author                   = {Donahue, Jeff and Hendricks, Lisa Anne and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
  Journal                  = {arXiv preprint arXiv:1411.4389},
  Year                     = {2014},

  Keywords                 = {image description},
  Owner                    = {20361362},
  Timestamp                = {2015.12.17},
  Url                      = {http://arxiv.org/pdf/1411.4389.pdf}
}

@Article{AdaGrad,
  Title                    = {Adaptive subgradient methods for online learning and stochastic optimization},
  Author                   = {Duchi, John and Hazan, Elad and Singer, Yoram},
  Journal                  = {The Journal of Machine Learning Research},
  Year                     = {2011},

  Annote                   = {People are super keen on this method.

This paper is huge, and full of math i need to look up.

Read these instead: http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf
Adagrad provides a perfeature learning rate, where rare features have a higher learning rate.


what adagrad descr4ibes as $x$ and $x_{t+1}$, would be a row/column of $W$ in a neural network (I think)},
  Pages                    = {2121--2159},
  Volume                   = {12},

  Keywords                 = {optimisation},
  Owner                    = {20361362},
  Publisher                = {JMLR. org},
  Timestamp                = {2015.04.23},
  Url                      = {http://www.magicbroom.info/Papers/DuchiHaSi10.pdf}
}

@Article{LIBLIBEAR,
  Title                    = {{LIBLINEAR}: A Library for Large Linear Classification},
  Author                   = {Rong-En Fan and Kai-Wei Chang and Cho-Jui Hsieh and Xiang-Rui Wang and Chih-Jen Lin},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2008},

  Annote                   = {This is the prefered citation for Liblinear (SVM)},
  Pages                    = {1871--1874},
  Volume                   = {9},

  Owner                    = {20361362},
  Timestamp                = {2015.09.08}
}

@Incollection{farhadi2010every,
  Title                    = {Every picture tells a story: Generating sentences from images},
  Author                   = {Farhadi, Ali and Hejrati, Mohsen and Sadeghi, Mohammad Amin and Young, Peter and Rashtchian, Cyrus and Hockenmaier, Julia and Forsyth, David},
  Booktitle                = {Computer Vision--ECCV 2010},
  Publisher                = {Springer},
  Year                     = {2010},
  Pages                    = {15--29},

  Keywords                 = {image description},
  Owner                    = {20361362},
  Timestamp                = {2015.09.09},
  Url                      = {https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf}
}

@Article{fellbaum1990english,
  Title                    = {English verbs as a semantic net},
  Author                   = {Fellbaum, Christiane},
  Journal                  = {International Journal of Lexicography},
  Year                     = {1990},

  Annote                   = {Verbs in Wordnet
Interstingly has differenbt author to other wordnet papers},
  Number                   = {4},
  Pages                    = {278--301},
  Volume                   = {3},

  Keywords                 = {wordnet},
  Owner                    = {20361362},
  Publisher                = {Oxford Univ Press},
  Timestamp                = {2015.07.16},
  Url                      = {http://ijl.oxfordjournals.org/content/3/4/278.full.pdf}
}

@Inproceedings{WordSim353,
  Title                    = {Placing search in context: The concept revisited},
  Author                   = {Finkelstein, Lev and Gabrilovich, Evgeniy and Matias, Yossi and Rivlin, Ehud and Solan, Zach and Wolfman, Gadi and Ruppin, Eytan},
  Booktitle                = {Proceedings of the 10th international conference on World Wide Web},
  Year                     = {2001},
  Annote                   = {Prefered citation for wordsim353},
  Organization             = {ACM},
  Pages                    = {406--414},

  Keywords                 = {corpora},
  Owner                    = {20361362},
  Timestamp                = {2016.06.14},
  Url                      = {http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/}
}

@Inproceedings{JARMparser,
  Title                    = {A Discriminative Graph-Based Parser for the Abstract Meaning Representation},
  Author                   = {Flanigan, Jeffrey and Thomson, Sam and Carbonell, Jaime and Dyer, Chris and Smith, Noah A.},
  Booktitle                = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  Year                     = {2014},

  Address                  = {Baltimore, Maryland},
  Annote                   = {Carbonell, Jaime and Dyer, Chris and Smith, Noah A. are the supervisors of Flanigan, Jeffrey 

Parses sentences into AMR, using graph techniques.
Stuff being done in this is pretty neat.


Evaluation is two steps:
 - Mapping words to concept nodes, done with a linear feature scoring
 - Mapping Relationships to edges (or rather fdereminging when realtiuonship edges exist), done with a maximum spanning subgraph alorigthm which maxised the scrores from a different linear features scoring.


Both feature scorers much be trained.
For the word to concept mapping a "Automatic Aligner" is used to know which concepts (in AMR space), align to which words. That is to say the auytomatic aligner is used to determine the ground truth from the training data (of sentecnec paired wit their AMR rep) to which the concept mapping is trained to replicate.},
  Month                    = {June},
  Pages                    = {1426--1436},
  Publisher                = {Association for Computational Linguistics},

  Abstract                 = {We present a novel per-dimension learning rate method for
gradient descent called ADADELTA. The method dynami-
cally adapts over time using only first order information and
has minimal computational overhead beyond vanilla stochas-
tic gradient descent. The method requires no manual tuning of
a learning rate and appears robust to noisy gradient informa-
tion, different model architecture choices, various data modal-
ities and selection of hyperparameters. We show promising
results compared to other methods on the MNIST digit clas-
sification task using a single machine and on a large scale
voice dataset in a distributed cluster environment},
  Keywords                 = {AMR},
  Owner                    = {20361362},
  Timestamp                = {2015.04.22},
  Url                      = {http://www.cs.cmu.edu/~jmflanig/flanigan+etal.acl2014.pdf}
}

@Article{francis1979brown,
  Title                    = {Brown corpus manual},
  Author                   = {Francis, W Nelson and Kucera, Henry},
  Journal                  = {Brown University},
  Year                     = {1979},

  Abstract                 = {This Manual was first published in 1964, when the Standard Sample of Present-Day American English (the Brown Corpus) was first made available. *) A revised edition was issued in 1971, principally to incorporate information about the text turned up in seven years of use. The present revision is more extensive, since it includes information about recently prepared versions of the Corpus, notably the «tagged» text completed at Brown University in 1979. Two complete proofreadings of the Corpus have resulted in corrections of two kinds: errors in the preparation of the original tape, which have been silently corrected in recently issued copies, and further typographical errors and anomalies in the underlying text, which have been recorded in the descriptions of individual samples on pages 33-176. (Most of these were listed on corrigenda sheets which have been enclosed with recently issued copies of the Manual and incorporated in this web-version.)},
  Keywords                 = {corpora},
  Owner                    = {20361362},
  Timestamp                = {2015.11.26},
  Url                      = {http://clu.uni.no/icame/brown/bcm.html}
}

@Article{frey2007clustering,
  Title                    = {Clustering by passing messages between data points},
  Author                   = {Frey, Brendan J and Dueck, Delbert},
  Journal                  = {Science},
  Year                     = {2007},

  Annote                   = {Quiet possibly the best clustering algorithm out.},
  Number                   = {5814},
  Pages                    = {972--976},
  Volume                   = {315},

  Owner                    = {20361362},
  Publisher                = {American Association for the Advancement of Science},
  Timestamp                = {2015.05.28},
  Url                      = {http://dlib.net.pku.edu.cn/qmeiCourse/files/AffinityPropagation.pdf}
}

@Inproceedings{4518770,
  Title                    = {Rhetorical-State Hidden Markov Models for extractive speech summarization},
  Author                   = {Fung, P. and Chan, R.H.Y. and Zhang, J.J.},
  Booktitle                = {Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on},
  Year                     = {2008},
  Month                    = {March},
  Pages                    = {4957-4960},

  Abstract                 = {We propose an extractive summarization system with a novel non-generative probabilistic framework for speech summarization. One of the most underutilized features in extractive summarization is rhetorical information - semantically cohesive units that are hidden in spoken documents. We propose Rhetorical-State Hidden Markov Models (RSHMMs) to automatically decode this underlying structure in speech. We show that RSHMMs give a 71.69% ROUGE-L F-measure, a 5.69% absolute increase in lecture speech summarization performance compared to the baseline system without using RSHMM. It equally outperforms the baseline system with additional discourse features, showing that our RSHMM is a more refined improvement on the conventional discourse feature.},
  Doi                      = {10.1109/ICASSP.2008.4518770},
  ISSN                     = {1520-6149},
  Keywords                 = {hidden Markov models;speech processing;baseline system;extractive speech summarization;nongenerative probabilistic framework;rhetorical information;rhetorical-state hidden Markov models;Automatic speech recognition;Data mining;Decoding;Feature extraction;Hidden Markov models;Humans;Natural languages;Support vector machine classification;Support vector machines;Text recognition;hidden Markov models;rhetorical information;speech features;spoken document summarization},
  Owner                    = {20361362},
  Timestamp                = {2015.05.28}
}

@Inproceedings{Fung:2003:COC:1119312.1119315,
  Title                    = {Combining Optimal Clustering and Hidden Markov Models for Extractive Summarization},
  Author                   = {Fung, Pascale and Ngai, Grace and Cheung, Chi-Shun},
  Booktitle                = {Proceedings of the ACL 2003 Workshop on Multilingual Summarization and Question Answering - Volume 12},
  Year                     = {2003},

  Address                  = {Stroudsburg, PA, USA},
  Pages                    = {21--28},
  Publisher                = {Association for Computational Linguistics},
  Series                   = {MultiSumQA '03},

  Acmid                    = {1119315},
  Doi                      = {10.3115/1119312.1119315},
  Location                 = {Sapporo, Japan},
  Numpages                 = {8},
  Owner                    = {20361362},
  Timestamp                = {2015.05.28},
  Url                      = {http://dx.doi.org/10.3115/1119312.1119315}
}

@Inproceedings{ganesan2010opinosis,
  Title                    = {Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions},
  Author                   = {Ganesan, Kavita and Zhai, ChengXiang and Han, Jiawei},
  Booktitle                = {Proceedings of the 23rd International Conference on Computational Linguistics},
  Year                     = {2010},
  Annote                   = {This is the citation for the extractive sumaraistion dataset used by \cite{KaagebExtractiveSummaristation}},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {340--348},

  Keywords                 = {corpora, summarization},
  Owner                    = {20361362},
  Timestamp                = {2015.06.05},
  Url                      = {http://kavita-ganesan.com/opinosis-opinion-dataset}
}

@Article{gao2014learning,
  Title                    = {Learning continuous phrase representations for translation modeling},
  Author                   = {Gao, Jianfeng and He, Xiaodong and Yih, Wen-tau and Deng, Li},
  Journal                  = {Proc. of ACL. Association for Computational Linguistics, June},
  Year                     = {2014},

  Annote                   = {Input is bag of words, where each bag is a phrase.
Translation by projecting to a common embedding space.

The training method is based on L-BFGS.

Notebly they do not use the Cosign Similarity:
Quote: "In our experiments, we compare dot product and the cosine 
similarity functions and find that the former works better for nonlinear multilayer neural networks, and the latter works 
better for linear neural networks"},

  Keywords                 = {MT},
  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://research.microsoft.com/pubs/211749/nn4smt.acl.v9.pdf}
}

@Inproceedings{Germann2001,
  Title                    = {Fast decoding and optimal decoding for machine translation},
  Author                   = {Germann, Ulrich and Jahr, Michael and Knight, Kevin and Marcu, Daniel and Yamada, Kenji},
  Booktitle                = {Proceedings of the 39th Annual Meeting on Association for Computational Linguistics},
  Year                     = {2001},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {228--235},

  Keywords                 = {word-ordering, machine-translation},
  Owner                    = {20361362},
  Timestamp                = {2016.02.04},
  Url                      = {http://www.dtic.mil/dtic/tr/fulltext/u2/a459945.pdf}
}

@Article{gershmanphrase,
  Title                    = {Phrase similarity in humans and machines},
  Author                   = {Gershman, Samuel J and Tenenbaum, Joshua B},
  Journal                  = {Proceedings of the 37th Annual Conference of the Cognitive Science Society},
  Year                     = {2015},

  Annote                   = {It is compairing similarity rank in embedding space to a ranking given by people},

  Owner                    = {20361362},
  Timestamp                = {2015.06.23},
  Url                      = {http://web.mit.edu/sjgershm/www/GershmanTenenbaum15.pdf}
}

@Incollection{gilmour2005understanding,
  Title                    = {Understanding the pheromone system within ant colony optimization},
  Author                   = {Gilmour, Stephen and Dras, Mark},
  Booktitle                = {AI 2005: Advances in Artificial Intelligence},
  Publisher                = {Springer},
  Year                     = {2005},
  Pages                    = {786--789},

  Owner                    = {20361362},
  Timestamp                = {2015.10.19},
  Url                      = {http://web.science.mq.edu.au/~madras/papers/ai05.pdf}
}

@Inproceedings{goller1996BPstructure,
  Title                    = {Learning task-dependent distributed representations by backpropagation through structure},
  Author                   = {Goller, Christoph and Kuchler, Andreas},
  Booktitle                = {Neural Networks, 1996., IEEE International Conference on},
  Year                     = {1996},
  Annote                   = {This is not a fun paper to read or understand.
It is however cruicial for the understanding of Socher2011

It deals with how to apply Back-propergation Through Structure (
BPTS) to Directed Acyclic Graphs (DAG) (notables including trees).
Which is like Back-propergation Though Time.},
  Organization             = {IEEE},
  Pages                    = {347--352},
  Volume                   = {1},

  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.1968&rep=rep1&type=pdf}
}

@Inproceedings{gong2011dynamic,
  Title                    = {Dynamic manifold warping for view invariant action recognition},
  Author                   = {Gong, Dian and Medioni, Gerard},
  Booktitle                = {2011 International Conference on Computer Vision},
  Year                     = {2011},
  Organization             = {IEEE},
  Pages                    = {571--578},

  Keywords                 = {computer-vision},
  Owner                    = {20361362},
  Timestamp                = {2016.08.22},
  Url                      = {http://www-scf.usc.edu/~diangong/index_files/ICCV2011.pdf}
}

@Incollection{gonzalez2012graph,
  Title                    = {A graph-based method to improve wordnet domains},
  Author                   = {Gonz{\'a}lez, Aitor and Rigau, German and Castillo, Mauro},
  Booktitle                = {Computational Linguistics and Intelligent Text Processing},
  Publisher                = {Springer},
  Year                     = {2012},
  Annote                   = {This is the paper for exented wordnet domains.},
  Pages                    = {17--28},

  Keywords                 = {wordnet},
  Owner                    = {20361362},
  Timestamp                = {2015.07.16},
  Url                      = {http://adimen.si.ehu.es/~rigau/publications/cicling12-gcr.pdf}
}

@Article{DBLP:journals/corr/GravesWD14,
  Title                    = {Neural Turing Machines},
  Author                   = {Alex Graves and
 Greg Wayne and
 Ivo Danihelka},
  Journal                  = {CoRR},
  Year                     = {2014},

  Annote                   = {Not well typeset. Figures miss-placed -- check they are lining up with text before over thinking it.

Several experiments are using a Neural network which control a memory bank.
Notably every part of the system is differntable allowing graident desent to be used.

Head "Head nearons" that have read and a write weight matrix to contol how they interact with the memory store.},
  Volume                   = {abs/1410.5401},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/GravesWD14},
  Owner                    = {20361362},
  Timestamp                = {2015.04.11},
  Url                      = {http://arxiv.org/pdf/1410.5401v2.pdf}
}

@Article{Grebla,
  Title                    = {Multi-Dimensional OFDMA Scheduling in a Wireless Network with Relay Nodes},
  Author                   = {Grebla, Reuven Cohen Guy},

  Annote                   = {Has a heuristic solution for sparse multidimentional multichoice knapsack problems.},

  Owner                    = {20361362},
  Timestamp                = {2015.11.06},
  Url                      = {http://www.cs.technion.ac.il/~rcohen/PAPERS/mult-OFDMA.pdf}
}

@Article{Grefenstette2014,
  Title                    = {A Deep Architecture for Semantic Parsing},
  Author                   = {Grefenstette, Edward and Blunsom, Phil and de Freitas, Nando and Hermann, Karl Moritz},
  Journal                  = {arXiv preprint arXiv:1404.7296},
  Year                     = {2014},

  Annote                   = {A neural network approch that takes in raw prhases, and converst it to ontologu specific queires.(SO more knowledge base querying).
Apparently without partsing.

It does use word embeddigns

I have not read this paper in deatil},

  Abstract                 = {Many successful approaches to semantic
parsing build on top of the syntactic anal-
ysis of text, and make use of distribu-
tional representations or statistical mod-
els to match parses to ontology-specific
queries. This paper presents a novel deep
learning architecture which provides a se-
mantic parsing system through the union
of two neural models of language se-
mantics. It allows for the generation of
ontology-specific queries from natural lan-
guage statements and questions without
the need for parsing, which makes it es-
pecially suitable to grammatically mal-
formed or syntactically atypical text, such
as tweets, as well as permitting the devel-
opment of semantic parsers for resource-
poor languages.},
  Keywords                 = {phrase-embeddings},
  Owner                    = {20361362},
  Timestamp                = {2015.04.18},
  Url                      = {http://yoavartzi.com/sp14/pub/gbfh-sp14-2014.pdf}
}

@Article{Gudmundsson1999,
  Title                    = {A fast approximation algorithm for TSP with neighborhoods},
  Author                   = {Gudmundsson, Joachim and Levcopoulos, Christos},
  Journal                  = {Nord. J. Comput.},
  Year                     = {1999},
  Number                   = {4},
  Pages                    = {469},
  Volume                   = {6},

  Owner                    = {20361362},
  Timestamp                = {2016.02.25},
  Url                      = {http://rp-www.cs.usyd.edu.au/~joachim/Papers/J2.pdf}
}

@Inproceedings{Guevara2010,
  Title                    = {A regression model of adjective-noun compositionality in distributional semantics},
  Author                   = {Guevara, Emiliano},
  Booktitle                = {Proceedings of the 2010 Workshop on Geometrical Models of Natural Language Semantics},
  Year                     = {2010},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {33--37},

  Owner                    = {20361362},
  Timestamp                = {2015.11.19},
  Url                      = {http://www.aclweb.org/anthology/W10-28#page=43}
}

@Article{Gupta2010,
  Title                    = {A survey of text summarization extractive techniques},
  Author                   = {Gupta, Vishal and Lehal, Gurpreet Singh},
  Journal                  = {Journal of Emerging Technologies in Web Intelligence},
  Year                     = {2010},

  Annote                   = {(Its not not published in a preditory journal, I checked. JETWI is a real thing and Academy Publisher too. It doesn't look nice)
Seems like quiet a nice review of text sumarisation.
focus on extractive, but does define abstractive..},
  Number                   = {3},
  Pages                    = {258--268},
  Volume                   = {2},

  Owner                    = {20361362},
  Timestamp                = {2015.04.17},
  Url                      = {http://learnpunjabi.org/pdf/survey-paper.pdf}
}

@Inproceedings{Harabagiu:2006:NCC:1597538.1597659,
  Title                    = {Negation, Contrast and Contradiction in Text Processing},
  Author                   = {Harabagiu, Sanda and Hickl, Andrew and Lacatusu, Finley},
  Booktitle                = {Proceedings of the 21st National Conference on Artificial Intelligence - Volume 1},
  Year                     = {2006},
  Annote                   = {Cool.About Negation/Contradiction and Contrast (as two sperate tasks)
Negation/Contradoiction is the opposite of textual entilment,contrast is "X, but Y" etc.


They use a pile of features and some graph stuff (to find antonyms) to do the dections.
They had human annotators manually},
  Pages                    = {755--762},
  Publisher                = {AAAI Press},
  Series                   = {AAAI'06},

  Acmid                    = {1597659},
  ISBN                     = {978-1-57735-281-5},
  Location                 = {Boston, Massachusetts},
  Numpages                 = {8},
  Owner                    = {20361362},
  Timestamp                = {2015.08.04},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.7652&rep=rep1&type=pdf}
}

@Article{hennig2010methods,
  Title                    = {Methods for merging Gaussian mixture components},
  Author                   = {Hennig, Christian},
  Journal                  = {Advances in data analysis and classification},
  Year                     = {2010},

  Annote                   = {DEMP},
  Number                   = {1},
  Pages                    = {3--34},
  Volume                   = {4},

  Abstract                 = {The problem of merging Gaussian mixture components is discussed in
situations where a Gaussian mixture is fitted but the mixture components are not
separated enough from each other to interpret them as “clusters”. The problem of
merging Gaussian mixtures is not statistically identifiable, therefore merging algo-
rithms have to be based on subjective cluster concepts. Cluster concepts based on
unimodality and misclassification probabilities (“patterns”) are distinguished. Several
different hierarchical merging methods are proposed for different cluster concepts,
based on the ridgeline analysis of modality of Gaussian mixtures, the dip test, the
Bhattacharyya dissimilarity, a direct estimator of misclassification and the strength of
predicting pairwise cluster memberships. The methods are compared by a simulation
study and application to two real datasets. A new visualisation method of the separation
of Gaussian mixture components, the ordered posterior plot, is also introduced.},
  Keywords                 = {model merging, clustering},
  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2016.06.27},
  Url                      = {http://download.springer.com/static/pdf/26/art%253A10.1007%252Fs11634-010-0058-3.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11634-010-0058-3&token2=exp=1467010407~acl=%2Fstatic%2Fpdf%2F26%2Fart%25253A10.1007%25252Fs11634-010-0058-3.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1007%252Fs11634-010-0058-3*~hmac=fe04e8ccd01691d56a798ab15d25fec16e57323cedaeb5a7a2ec9d33ed4b2032}
}

@Article{DBLP:journals/corr/HermannB13,
  Title                    = {A Simple Model for Learning Multilingual Compositional Semantics},
  Author                   = {Karl Moritz Hermann and
 Phil Blunsom},
  Journal                  = {CoRR},
  Year                     = {2013},

  Annote                   = {Good Stuff Sentece embeddings via parallel corpora},
  Volume                   = {abs/1312.6173},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/HermannB13},
  Owner                    = {20361362},
  Timestamp                = {2015.08.04},
  Url                      = {http://arxiv.org/abs/1312.6173}
}

@Inproceedings{hermann-blunsom:2013:ACL2013,
  Title                    = {{The Role of Syntax in Vector Space Models of Compositional Semantics}},
  Author                   = {Hermann, Karl Moritz and Blunsom, Phil},
  Booktitle                = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  Year                     = {2013},

  Address                  = {Sofia, Bulgaria},
  Month                    = {August},
  Pages                    = {894--904},
  Publisher                = {Association for Computational Linguistics},

  Owner                    = {20361362},
  Timestamp                = {2015.08.31},
  Url                      = {http://www.karlmoritz.com/_media/hermannblunsom_acl2013.pdf}
}

@Article{hochreiter1997long,
  Title                    = {Long short-term memory},
  Author                   = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  Journal                  = {Neural computation},
  Year                     = {1997},
  Number                   = {8},
  Pages                    = {1735--1780},
  Volume                   = {9},

  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2016.02.09},
  Url                      = {http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf}
}

@Inproceedings{Horvat2014,
  Title                    = {A Graph-Based Approach to String Regeneration.},
  Author                   = {Horvat, Matic and Byrne, William},
  Booktitle                = {EACL},
  Year                     = {2014},
  Annote                   = {Generate ordering word order bow


I was sure someone would have thought to do this before.
Rewriting the bag of words ordering problem as some form of TSP problem.
There were a few notions around it going back to 1999ish, but they tend to be mixed in with other machine translation concerns.

but there is a 2014 paper that is almost exactly what I have done.
(It came out of the first authors master's thesis).

The difference between there method and mine are 2:
 1 - They go to a true closed TSP, where as I go to a Open TSP with known start and beginning vexticies.
 2 - They feed that to a TSP solver, where as I process it further and give it to a MIPs solver

They translate the problem to a Generalised Asymetric TSP (GATSP), then feed that to a Traveling Salesman solver.
I translate the problem to an variation on the GATSP, where the tour does not begin where it starts,
then translate that problem in to a Mixed Integer Programming (MIPS) problem, which I feed to a MIPS solver.

My variation of the GATSP, can be turned back into a normal closed GATSP, by adding a zero cost edge between the END vertex and the START vertex. This does potentially change the performance characteristics, (though not in a big O kinda way).

Under the hood of the Traveling Salesman Solver is a optimized MIPs solver (possibly some other things to, but I know there is a MIPs solver from looking at their website).},
  Pages                    = {85--95},

  Owner                    = {20361362},
  Timestamp                = {2016.02.05},
  Url                      = {https://www.cl.cam.ac.uk/~mh693/files/eacl2014_paper_prepublish.pdf}
}

@Article{hotelling1933analysis,
  Title                    = {Analysis of a complex of statistical variables into principal components.},
  Author                   = {Hotelling, Harold},
  Journal                  = {Journal of educational psychology},
  Year                     = {1933},

  Annote                   = {The original PCA paper.},
  Number                   = {6},
  Pages                    = {417},
  Volume                   = {24},

  Owner                    = {20361362},
  Publisher                = {Warwick \& York},
  Timestamp                = {2015.07.10},
  Url                      = {http://ovidsp.tx.ovid.com.ezproxy.library.uwa.edu.au/sp-3.16.0a/ovidweb.cgi?WebLinkFrameset=1&S=KLGPFPKHCMDDAGDLNCKKHFDCNILNAA00&returnUrl=ovidweb.cgi%3fMain%2bSearch%2bPage%3d1%26S%3dKLGPFPKHCMDDAGDLNCKKHFDCNILNAA00&directlink=http%3a%2f%2fgraphics.tx.ovid.com%2fovftpdfs%2fFPDDNCDCHFDLCM00%2ffs046%2fovft%2flive%2fgv023%2f00004760%2f00004760-193309000-00003.pdf&filename=Analysis+of+a+complex+of+statistical+variables+into+principal+components.&navigation_links=NavLinks.S.sh.22.1&link_from=S.sh.22|1&pdf_key=FPDDNCDCHFDLCM00&pdf_index=/fs046/ovft/live/gv023/00004760/00004760-193309000-00003&D=yrovft&link_set=S.sh.22|1|sl_10|resultSet|S.sh.22.23|0}
}

@InProceedings{Huang2012,
  author        = {Huang, Eric H and Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
  title         = {Improving word representations via global context and multiple word prototypes},
  booktitle     = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1},
  year          = {2012},
  organization  = {Association for Computational Linguistics},
  pages         = {873--882},
  url           = {http://www.aclweb.org/anthology/P12-1092},
  __markedentry = {[20361362:]},
  annote        = {First learn (single prototype) word embeddings, using a neural network trained to predict next word. This network architecture is itself novel. It is split to have a local subnetwork and a global subnetwork. The local subnetwork uses as its input a concatenated window of words embeddings prior to the word to be predicted. The global subnetwork uses as its input a weighted average of all word vectors in the document (or alternativelty a large window), concatenated with the wordvector for the word immediately prior to the word to be predicted (i.e. the last word embedding concatenated in to the local subnetwork's input vector). These single prototype word vectors are used to create multiprototype word vectors by clustering the contexts of each work instance. Then for each word, sum up word vectors of its context words, then cluster the context sums using spherical K-means -- Huang et al set K to 10 i.e there to be 10 senses for all words. Then relabel each word in the corpus with the cluster id that it's context belongs to -- to get a sense label. Finally retrain on the relabelled words, to get word sense embeddings.},
  groups        = {to_read},
  keywords      = {WSD, word-embeddings},
  owner         = {20361362},
  timestamp     = {2016.06.01},
}

@Book{huddleston2002cambridge,
  Title                    = {The Cambridge Grammar of the English Language},
  Author                   = {Huddleston, R.D. and Pullum, G.K.},
  Publisher                = {Cambridge University Press},
  Year                     = {2002},
  Series                   = {Cambridge textbooks in linguistics},

  ISBN                     = {9780521431460},
  Lccn                     = {20025630},
  Owner                    = {20361362},
  Timestamp                = {2015.07.23},
  Url                      = {https://books.google.com.au/books?id=2yoQhHikxE8C}
}

@Article{hutchinson2013tensor,
  Title                    = {Tensor deep stacking networks},
  Author                   = {Hutchinson, Brian and Deng, Li and Yu, Dong},
  Journal                  = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  Year                     = {2013},
  Number                   = {8},
  Pages                    = {1944--1957},
  Volume                   = {35},

  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2015.09.15}
}

@Inproceedings{iacobacci2015sensembed,
  Title                    = {SensEmbed: learning sense embeddings for word and relational similarity},
  Author                   = {Iacobacci, Ignacio and Pilehvar, Mohammad Taher and Navigli, Roberto},
  Booktitle                = {Proceedings of ACL},
  Year                     = {2015},
  Annote                   = {Direct supervised apprach.
First Babelfly is used to label the Corpus, this is then used to train CBOW (The nonskip-gram word2vec method).
Rather than the labels being words, the labels are word senses.
Babelfly is independent preexisting, nonneural network WSD software.

We could actually use the sense embeddings and language model output by SenseEmbed to perform WSD;
it is just bypassing out alignment step. Iacobacci et al do not attempt this},
  Pages                    = {95--105},

  Owner                    = {20361362},
  Timestamp                = {2016.04.19},
  Url                      = {http://anthology.aclweb.org/P/P15/P15-1010.pdf}
}

@Inproceedings{iyyer2014neural,
  Title                    = {A neural network for factoid question answering over paragraphs},
  Author                   = {Iyyer, Mohit and Boyd-Graber, Jordan and Claudino, Leonardo and Socher, Richard and {Daum{\'e} III}, Hal},
  Booktitle                = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  Year                     = {2014},
  Pages                    = {633--644},

  Owner                    = {20361362},
  Timestamp                = {2015.08.31},
  Url                      = {https://cs.umd.edu/~miyyer/pubs/2014_qb_rnn.pdf}
}

@Inproceedings{iyyer2014generating,
  Title                    = {Generating Sentences from Semantic Vector Space Representations},
  Author                   = {Iyyer, Mohit and Boyd-Graber, Jordan and {Daum{\'e} III}, Hal},
  Booktitle                = {NIPS Workshop on Learning Semantics},
  Year                     = {2014},
  Annote                   = {Uses a variation of Unfolding RAE,
uses Dependecy Parse Trees, rather than thre Constituency-tree used in the original.
 Calls it a DT-RAE Also makes use of a different weight matrix for each of the relations eg NN NSUB CCOMP.


The work of \textcite{iyyer2014generating} is based on an extension of the unfolding recursive autoencoder \textcite{SocherEtAl2011:PoolRAE}. Recursive neural networks are jointly trained for both composing the sentence's words into a vector, and for decomposing that vector into words. They learn, the mapping network weights, and fine tunee the word embeddings during training, and produce the sentence embeddings through the central bottleneck of the recursive auto-encoder. This composition and decomposition is done by reusing a composition neural network at each vertex the dependency tree structure with different weight matrices for each dependency relation. Multiple inputs to the same vertex are handled by summing after multiplication by the weight matrix. 
Decomposition is handled similarly, following the dependency structure from the root (sentence) out to the leaves (words), with a neural network reused at each branch, with decomposition matrices. It is further augmented to encode the order of identically related siblings, through the addition of a sibling weight matrix. The total network is trained based on its accuracy for to reproduce its input word vectors, through back-propagation. Through just using the top decomposing half of the network, from the sentence embedding up, it can be used to regenerate the words. This method was demonstrated quantitatively on five examples, shown in \Cref{results}. There are not currently any large scale quantitative results published for the use of Iyyer et. al.'s technique.

 
%$$g_{comp}\:: (u_i)^N_{i=1} \mapsto \sigma(\sum_{i=1}^N W_i\u_i+\tilde{b})$$
%$$g_{decomp} \p \mapsto (\sigma(W^\prime_i\p+\tilde{b}^\prime+W_{sib}\u^\prime_j)^N_{i=1}$$
%$\u^\prime_j$ is $g_{decomp}(p)_j$ if $j$ is the left most sibling of $i$ with the same dependancy relation to the parent as $i$ if one exists, or the zero vector otherwith
%Here $W_i$ and $W^\prime_i$ vary not by order, but by the dependency tree part of speech of the $i$th element.
%$\tilde{b}$ and $\tilde{b}^\prime$ are the biases.
%$\sigma$ is a nonlinear activation function, such as the sigmoid or the hyperbolic tangent functions. It is this nonlinearity that prevents the values being solved by any kind of least squares.
 
%Theoretically, even without the extensions the unfolding recursive auto-encoder could be used in the same way to generate sentences, however this has not been shown.},

  Keywords                 = {phrase-embeddings},
  Owner                    = {20361362},
  Timestamp                = {2015.06.30},
  Url                      = {http://cs.umd.edu/~miyyer/pubs/2014_nips_generation.pdf}
}

@Article{2016arXiv160404661J,
  author        = {{Ji}, S. and {Satish}, N. and {Li}, S. and {Dubey}, P.},
  title         = {{Parallelizing Word2Vec in Shared and Distributed Memory}},
  year          = {2016},
  month         = apr,
  eprint        = {1604.04661},
  url           = {http://arxiv.org/pdf/1604.04661v1.pdf},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2016arXiv160404661J},
  annote        = {Interestding discussion of paralising word2vec wrt negitive sampling. convert vector dot products to matrix products. and some other tequnieues.h},
  archiveprefix = {arXiv},
  groups        = {to_read},
  journal       = {ArXiv e-prints},
  keywords      = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Computation and Language, Statistics - Machine Learning},
  owner         = {20361362},
  primaryclass  = {cs.DC},
  timestamp     = {2016.06.01},
}

@Inproceedings{judge2006questionbank,
  Title                    = {Questionbank: Creating a corpus of parse-annotated questions},
  Author                   = {Judge, John and Cahill, Aoife and Van Genabith, Josef},
  Booktitle                = {Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics},
  Year                     = {2006},
  Annote                   = {About the QuestionBank corpus -- 4000 tree parsed sentences -- all of which are questions.

Apparently this works really well as training data, and that models trained with it are still good at parsing normal sentences.
In someway related to ATIS

download from \url{http://www.computing.dcu.ie/~jjudge/qtreebank/}},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {497--504},

  Keywords                 = {corpora},
  Owner                    = {20361362},
  Timestamp                = {2015.06.10},
  Url                      = {http://doras.dcu.ie/15271/1/judge_et_al_06.pdf}
}

@Inproceedings{jurgens2012semeval,
  Title                    = {Semeval-2012 task 2: Measuring degrees of relational similarity},
  Author                   = {Jurgens, David A and Turney, Peter D and Mohammad, Saif M and Holyoak, Keith J},
  Booktitle                = {Proceedings of the Sixth International Workshop on Semantic Evaluation},
  Year                     = {2012},
  Annote                   = {Described the semantic relations analogy task},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {356--364},

  Owner                    = {20361362},
  Timestamp                = {2015.09.04},
  Url                      = {http://ixa2.si.ehu.eus/starsem/proc/pdf/STARSEM-SEMEVAL047.pdf}
}

@Inproceedings{kaageback2015neural,
  Title                    = {Neural context embeddings for automatic discovery of word senses},
  Author                   = {K{\aa}geb{\"a}ck, Mikael and Johansson, Fredrik and Johansson, Richard and Dubhashi, Devdatt},
  Booktitle                = {Proceedings of NAACL-HLT},
  Year                     = {2015},
  Annote                   = {Does word sense induction (WSI).
combine the word-vectors of the context of the word, and a temportal and semantic factors, to produce a Instance Context Embedding (ICE).
Use kmeans clustering on the ICE, to induce a cluster for each sense.

Perform Word Sense Disabiguagation on a given word instance, by calculating it's ICE, then selecting the sense cluster it is most similar too.},
  Pages                    = {25--32},

  Keywords                 = {word-embeddings},
  Owner                    = {20361362},
  Timestamp                = {2015.06.30},
  Url                      = {http://www.aclweb.org/anthology/W/W15/W15-1504.pdf}
}

@InProceedings{KaagebExtractiveSummaristation,
  author    = {K{\aa}geb{\"a}ck, Mikael and Mogren, Olof and Tahmasebi, Nina and Dubhashi, Devdatt},
  title     = {Extractive summarization using continuous vector space models},
  booktitle = {Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)@ EACL},
  year      = {2014},
  pages     = {31--39},
  url       = {http://www.aclweb.org/anthology/W14-1504},
  annote    = {Focus in on using word and phrase embeedings to do multidocument extractive summary. Quote: "To the best of our knowledge, continuous vector space models have not previously been used in summarization tasks" Has a nice summery of the medthods used to produce word embeddings.\ Discusses two methos for phrase embeedings: Unfolder Recussive Autoencers (Socher) And simple addition (apparently done by Mikolov) It seems to really get the purpose of \cite{mikolovSkip} wrong though, saying the opposite of what that paper said. Usign the Opinosis dataset, which might be good to look into, it contaiend human gnerated summerys. Assessess accuracy by compairing various word overlap between output summaries and the gold standard summaries. Which seems dodgy for me -- but is apparently a normal method called ROUGE.},
  groups    = {KeyPapers},
  keywords  = {RvNN, URAE, summarization, vector-based_summarization},
  owner     = {20361362},
  timestamp = {2015.04.17},
}

@Article{Kalchbrenner2014,
  Title                    = {A convolutional neural network for modelling sentences},
  Author                   = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
  Journal                  = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
  Year                     = {2014},
  Volume                   = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},

  Keywords                 = {phrase-embeddings},
  Owner                    = {20361362},
  Timestamp                = {2015.08.31},
  Url                      = {http://nal.co/papers/Kalchbrenner_DCNN_ACL14}
}

@Book{karp1972reducibility,
  Title                    = {Reducibility among combinatorial problems},
  Author                   = {Karp, Richard M},
  Publisher                = {Springer},
  Year                     = {1972},
  Annote                   = {This si the big theing with 21 NP-HArd problems.
It is a very short "book:"

when it says Knapsack (\#18) it means subset sum.},

  Keywords                 = {knapsack},
  Owner                    = {20361362},
  Timestamp                = {2015.11.05},
  Url                      = {http://www.cs.berkeley.edu/~luca/cs172/karp.pdf}
}

@Article{karrasch2014introduction,
  Title                    = {An Introduction to Grassmann Manifolds and their Matrix Representation},
  Author                   = {Karrasch, Daniel},
  Year                     = {2014},

  Owner                    = {20361362},
  Timestamp                = {2016.08.05},
  Url                      = {http://www.zfm.ethz.ch/~karrasch/Intro_Grassmann.pdf}
}

@Article{katz1987estimation,
  Title                    = {Estimation of probabilities from sparse data for the language model component of a speech recognizer},
  Author                   = {Katz, Slava M},
  Journal                  = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
  Year                     = {1987},
  Number                   = {3},
  Pages                    = {400--401},
  Volume                   = {35},

  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2015.10.20},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.129.7219&rep=rep1&type=pdf}
}

@Inbook{Kilgarriff2004,
  Title                    = {How Dominant Is the Commonest Sense of a Word?},
  Author                   = {Kilgarriff, Adam},
  Editor                   = {Sojka, Petr
and Kope{\v{c}}ek, Ivan
and Pala, Karel},
  Pages                    = {103--111},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2004},

  Address                  = {Berlin, Heidelberg},

  Booktitle                = {Text, Speech and Dialogue: 7th International Conference, TSD 2004, Brno, Czech Republic, September 8-11, 2004. Proceedings},
  Doi                      = {10.1007/978-3-540-30120-2_14},
  ISBN                     = {978-3-540-30120-2},
  Owner                    = {20361362},
  Timestamp                = {2016.10.14},
  Url                      = {https://pdfs.semanticscholar.org/a271/edc22c4eb649a92d5e66a5ff2f0359968bb7.pdf}
}

@Article{DBLP:journals/corr/Kim14f,
  Title                    = {Convolutional Neural Networks for Sentence Classification},
  Author                   = {Yoon Kim},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1408.5882},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/Kim14f},
  Owner                    = {20361362},
  Timestamp                = {2015.08.04},
  Url                      = {http://arxiv.org/abs/1408.5882}
}

@Article{kingma2013auto,
  Title                    = {Auto-encoding variational bayes},
  Author                   = {Kingma, Diederik P and Welling, Max},
  Journal                  = {arXiv preprint arXiv:1312.6114},
  Year                     = {2013},

  Owner                    = {20361362},
  Timestamp                = {2016.02.09},
  Url                      = {http://arxiv.org/pdf/1312.6114}
}

@Inproceedings{kingsbury2002treebank,
  Title                    = {From TreeBank to PropBank.},
  Author                   = {Kingsbury, Paul and Palmer, Martha},
  Booktitle                = {LREC},
  Year                     = {2002},
  Annote                   = {About lableing the penn treebank with propositional struction.So that each sentence has a number o predicated with positional arguments.


This paper has interesting footnotes.},
  Organization             = {Citeseer},

  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.180.5642&rep=rep1&type=pdf}
}

@Article{DBLP:journals/corr/KirosZSZTUF15,
  Title                    = {Skip-Thought Vectors},
  Author                   = {Ryan Kiros and
 Yukun Zhu and
 Ruslan Salakhutdinov and
 Richard S. Zemel and
 Antonio Torralba and
 Raquel Urtasun and
 Sanja Fidler},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1506.06726},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/KirosZSZTUF15},
  Owner                    = {20361362},
  Timestamp                = {2016.06.01},
  Url                      = {http://arxiv.org/pdf/1506.06726v1.pdf}
}

@Inproceedings{kneser1995improved,
  Title                    = {Improved backing-off for m-gram language modeling},
  Author                   = {Kneser, Reinhard and Ney, Hermann},
  Booktitle                = {Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on},
  Year                     = {1995},
  Annote                   = {This is the Kneser Ney Smoothing paper.
It was the state of the art that Bengio beat with the PPNLM in 2010},
  Organization             = {IEEE},
  Pages                    = {181--184},
  Volume                   = {1},

  Owner                    = {20361362},
  Timestamp                = {2015.11.23},
  Url                      = {http://www-i6.informatik.rwth-aachen.de/publications/download/951/Kneser-ICASSP-1995.pdf}
}

@Article{Knight1999,
  Title                    = {Decoding complexity in word-replacement translation models},
  Author                   = {Knight, Kevin},
  Journal                  = {Computational Linguistics},
  Year                     = {1999},
  Number                   = {4},
  Pages                    = {607--615},
  Volume                   = {25},

  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2016.02.05},
  Url                      = {http://www.aclweb.org/anthology/J99-4005}
}

@Article{Kolmogorov2009,
  Title                    = {Blossom V: a new implementation of a minimum cost perfect matching algorithm},
  Author                   = {Kolmogorov, Vladimir},
  Journal                  = {Mathematical Programming Computation},
  Year                     = {2009},
  Number                   = {1},
  Pages                    = {43--67},
  Volume                   = {1},

  __markedentry            = {[20361362:6]},
  Abstract                 = {We describe a new implementation of the Edmonds’s algorithm for
computing a perfect matching of minimum cost, to which we refer as
Blossom V.},
  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2016.07.13},
  Url                      = {http://mpc.zib.de/index.php/MPC/article/viewFile/11/4}
}

@Article{kongbayesian,
  Title                    = {Bayesian Optimization of Text Representations},
  Author                   = {Kong, Dani Yogatama Lingpeng and Smith, Noah A},
  Journal                  = {Conference on Empirical Methods in Natural Language Processing},
  Year                     = {2015},

  Annote                   = {Uses n-grams as input to logistic regression,
plus baysian methods for tuning,

to perform many NLP tasks,
including sentiment analysis.

With performance almost as good as state of the art neural network methods.
and without hyper parameters.},

  Owner                    = {20361362},
  Timestamp                = {2015.09.02},
  Url                      = {www.cs.cmu.edu/~lingpenk/paper/emnlp15.pdf}
}

@Article{Kong20082672,
  Title                    = {A new ant colony optimization algorithm for the multidimensional Knapsack problem },
  Author                   = {Min Kong and Peng Tian and Yucheng Kao},
  Journal                  = {Computers \& Operations Research },
  Year                     = {2008},
  Note                     = {Queues in Practice },
  Number                   = {8},
  Pages                    = {2672 - 2683},
  Volume                   = {35},

  Abstract                 = {The paper proposes a new ant colony optimization (ACO) approach, called binary ant system (BAS), to multidimensional Knapsack problem (MKP). Different from other ACO-based algorithms applied to MKP, \{BAS\} uses a pheromone laying method specially designed for the binary solution structure, and allows the generation of infeasible solutions in the solution construction procedure. A problem specific repair operator is incorporated to repair the infeasible solutions generated in every iteration. Pheromone update rule is designed in such a way that pheromone on the paths can be directly regarded as selecting probability. To avoid premature convergence, the pheromone re-initialization and different pheromone intensification strategy depending on the convergence status of the algorithm are incorporated. Experimental results show the advantages of \{BAS\} over other ACO-based approaches for the benchmark problems selected from \{OR\} library. },
  Doi                      = {http://dx.doi.org/10.1016/j.cor.2006.12.029},
  ISSN                     = {0305-0548},
  Keywords                 = {Ant colony optimization},
  Owner                    = {20361362},
  Timestamp                = {2015.10.16},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0305054806003236}
}

@Inproceedings{kremer2014substitutes,
  Title                    = {What substitutes tell us -- analysis of an ``all-words'' lexical substitution corpus},
  Author                   = {Kremer, Gerhard and Erk, Katrin and Pad{\'o}, Sebastian and Thater, Stefan},
  Booktitle                = {Proceedings of EACL},
  Year                     = {2014},
  Annote                   = {This is about the CoInCo (Concepts in Context) corpus.
It was created by crowd sourcing word substitutions for a portion of the MASC},

  Keywords                 = {corpora},
  Owner                    = {20361362},
  Timestamp                = {2015.07.20},
  Url                      = {http://www.aclweb.org/anthology/E14-1057}
}

@Article{kulik2010there,
  Title                    = {There is no EPTAS for two-dimensional knapsack},
  Author                   = {Kulik, Ariel and Shachnai, Hadas},
  Journal                  = {Information Processing Letters},
  Year                     = {2010},
  Number                   = {16},
  Pages                    = {707--710},
  Volume                   = {110},

  Keywords                 = {knapsack},
  Owner                    = {20361362},
  Publisher                = {Elsevier},
  Timestamp                = {2015.11.09},
  Url                      = {http://www.researchgate.net/profile/Ariel_Kulik/publication/220115358_There_is_no_EPTAS_for_two-dimensional_knapsack/links/0f31753961d659735f000000.pdf}
}

@Book{larson1995knowledge,
  Title                    = {Knowledge of Meaning: An Introduction to Semantic Theory},
  Author                   = {Larson, R.K. and Segal, G.},
  Publisher                = {MIT Press},
  Year                     = {1995},
  Annote                   = {Presents Semantics.
In terms of T Theory, in the section i read (chapter 2)

From this the defintions of synonym aimplication and converse can be derived.
I could cite it for those definions.

The presentation that T Theory explains how people learn langiuage is not relevant to my work.
Only that T Theory cna describe language.},
  Series                   = {A Bradford book},

  ISBN                     = {9780262121934},
  Lccn                     = {95005324},
  Owner                    = {20361362},
  Timestamp                = {2015.08.03},
  Url                      = {https://books.google.com.au/books?id=DcKyQgAACAAJ}
}

@InProceedings{le2014distributed,
  author    = {Le, Quoc and Mikolov, Tomas},
  title     = {Distributed Representations of Sentences and Documents},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
  year      = {2014},
  pages     = {1188--1196},
  url       = {http://jmlr.org/proceedings/papers/v32/le14.pdf},
  annote    = {An actual Paragraph vector approach. commonly called doc2vec PV-DM: As per learning Word Representation, but along side word repressentaion provide a paragraph vector as an extra word. This provides more context to the learning. And that extra context required identifies the paragraph. PV-DBOW: I don't fully understand how this works. It seem they train parageraph vectors by asking htme ot predict what words the paragraph contains.},
  groups    = {KeyPapers},
  keywords  = {phrase-embeddings},
  owner     = {20361362},
  timestamp = {2015.06.10},
}

@Incollection{lecun2012efficient,
  Title                    = {Efficient backprop},
  Author                   = {LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  Booktitle                = {Neural networks: Tricks of the trade},
  Publisher                = {Springer},
  Year                     = {2012},
  Annote                   = {Practical advice for training nearula netowrks},
  Pages                    = {9--48},

  Owner                    = {20361362},
  Timestamp                = {2016.10.08}
}

@Article{LeCunCNN,
  Title                    = {Gradient-based learning applied to document recognition},
  Author                   = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  Journal                  = {Proceedings of the IEEE},
  Year                     = {1998},
  Number                   = {11},
  Pages                    = {2278--2324},
  Volume                   = {86},

  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2015.09.09},
  Url                      = {http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf}
}

@Inproceedings{leguizamon1999new,
  Title                    = {A new version of ant system for subset problems},
  Author                   = {Leguizamon, Guillermo and Michalewicz, Zbigniew},
  Booktitle                = {Proceedings of the 1999 Congress on Evolutionary Computation},
  Year                     = {1999},
  Annote                   = {Fairly obvious what I am doing basically. Not 100% sure on the N_max stuff.

I think I need to look at appximated subset sum},
  Organization             = {Piscataway, New Jersey, USA},
  Pages                    = {1458--1464},
  Volume                   = {2},

  Keywords                 = {ant colony, subset sum, graph search},
  Owner                    = {20361362},
  Timestamp                = {2015.10.16},
  Url                      = {https://cs.adelaide.edu.au/users/zbyszek/Papers/as4.pdf}
}

@Inproceedings{liang2006alignment,
  Title                    = {Alignment by agreement},
  Author                   = {Liang, Percy and Taskar, Ben and Klein, Dan},
  Booktitle                = {Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics},
  Year                     = {2006},
  Annote                   = {Word Alignment seems to be the problem of how to align sentence in two languages with each word maps to another word in the other sentence.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {104--111},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://repository.upenn.edu/cgi/viewcontent.cgi?article=1569&context=cis_papers}
}

@Inproceedings{lin2011class,
  Title                    = {A class of submodular functions for document summarization},
  Author                   = {Lin, Hui and Bilmes, Jeff},
  Booktitle                = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1},
  Year                     = {2011},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {510--520},

  Keywords                 = {summarization, vector-based_summarization},
  Owner                    = {20361362},
  Timestamp                = {2015.06.05},
  Url                      = {http://melodi.ee.washington.edu/~hlin/papers/lin-acl11-summ.pdf}
}

@Article{7078992,
  Title                    = {Deep Learning for Acoustic Modeling in Parametric Speech Generation: A systematic review of existing techniques and future trends},
  Author                   = {Zhen-Hua Ling and Shi-Yin Kang and Heiga Zen and Senior, A. and Schuster, M. and Xiao-Jun Qian and Meng, H.M. and Li Deng},
  Journal                  = {Signal Processing Magazine, IEEE},
  Year                     = {2015},

  Annote                   = {Review article.How we got from text to speech (Via HMMs).
Also talks about voice conversion (changing peoples voices to other peoples voices, sayign the same words)

Covers the istory of using HMM and GMMs.


Also covers a hughe background.Interestign stuff like considfering RBMs are PoE and as GMMs.Also covers Contional RBMS},
  Month                    = {May},
  Number                   = {3},
  Pages                    = {35-52},
  Volume                   = {32},

  Abstract                 = {Hidden Markov models (HMMs) and Gaussian mixture models (GMMs) are the two most common types of acoustic models used in statistical parametric approaches for generating low-level speech waveforms from high-level symbolic inputs via intermediate acoustic feature sequences. However, these models have their limitations in representing complex, nonlinear relationships between the speech generation inputs and the acoustic features. Inspired by the intrinsically hierarchical process of human speech production and by the successful application of deep neural networks (DNNs) to automatic speech recognition (ASR), deep learning techniques have also been applied successfully to speech generation, as reported in recent literature. This article systematically reviews these emerging speech generation approaches, with the dual goal of helping readers gain a better understanding of the existing techniques as well as stimulating new work in the burgeoning area of deep learning for parametric speech generation.},
  Doi                      = {10.1109/MSP.2014.2359987},
  ISSN                     = {1053-5888},
  Keywords                 = {Gaussian processes;acoustic signal processing;hidden Markov models;mixture models;neural nets;speech recognition;ASR;DNN;GMM;Gaussian mixture models;HMM;acoustic features;acoustic modeling;acoustic models;automatic speech recognition;burgeoning area;deep learning;deep neural networks;hidden Markov models;high-level symbolic inputs;human speech production;intermediate acoustic feature sequences;low-level speech waveforms;parametric speech generation;statistical parametric approach;Acoustic signal detection;Gaussian mixture models;Hidden Markov models;Speech processing;Speech recognition;Speech synthesis;Vocoders},
  Owner                    = {20361362},
  Timestamp                = {2015.05.22}
}

@Article{liu2015combining,
  Title                    = {Combining Relevance Language Modeling and Clarity Measure for Extractive Speech Summarization},
  Author                   = {Liu, Shih-Hung and Chen, Kuan-Yu and Chen, Berlin and Wang, Hsin-Min and Yen, Hsu-Chun and Hsu, Wen-Lian},
  Journal                  = {Audio, Speech, and Language Processing, IEEE/ACM Transactions on},
  Year                     = {2015},

  Annote                   = {Using Language models, and the cross entropy between the sentence LM and the document LM (each of which are a prob distribution on the next work, give nthe previous),
To find the most relevant sentences.

It is run on Speech data. (rather than simply text.)},
  Number                   = {6},
  Pages                    = {957--969},
  Volume                   = {23},

  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2015.05.04},
  Url                      = {http://www.iis.sinica.edu.tw/papers/whm/18321-F.pdf}
}

@Article{jump,
  Title                    = {Computing in operations research using Julia},
  Author                   = {Lubin, Miles and Dunning, Iain},
  Journal                  = {INFORMS Journal on Computing},
  Year                     = {2015},
  Number                   = {2},
  Pages                    = {238--248},
  Volume                   = {27},

  Owner                    = {20361362},
  Publisher                = {INFORMS},
  Timestamp                = {2016.01.26},
  Url                      = {http://arxiv.org/pdf/1312.1431.pdf}
}

@Article{PLN847,
  Title                    = {Paraphrase Concept and Typology. A Linguistically Based and Computationally Oriented Approach},
  Author                   = {Marta Vila y M. Antònia Martí y Horacio Rodríguez},
  Journal                  = {Procesamiento del Lenguaje Natural},
  Year                     = {2010},

  Annote                   = {This one defines the typeology, not the corpus sing the typology},
  Number                   = {0},
  Pages                    = {83--90},
  Volume                   = {46},

  Abstract                 = {In this paper, we present a critical analysis of the state of the art in the definition and typologies of paraphrasing. This analysis shows that there exists no characterization of paraphrasing that is comprehensive, linguistically based and computationally tractable at the same time. The following sets out to define and delimit the concept on the basis of the propositional content. We present a general, inclusive and computationally oriented typology of the linguistic mechanisms that give rise to form variations between paraphrase pairs.},
  ISSN                     = {1989-7553},
  Owner                    = {20361362},
  Timestamp                = {2015.06.24},
  Url                      = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/847}
}

@Article{magazine1984note,
  Title                    = {A note on approximation schemes for multidimensional knapsack problems},
  Author                   = {Magazine, Michael J and Chern, Maw-Sheng},
  Journal                  = {Mathematics of Operations Research},
  Year                     = {1984},
  Number                   = {2},
  Pages                    = {244--247},
  Volume                   = {9},

  Keywords                 = {knapsack},
  Owner                    = {20361362},
  Publisher                = {INFORMS},
  Timestamp                = {2015.11.09},
  Url                      = {http://www.jstor.org/stable/pdf/3689249.pdf?acceptTC=true}
}

@Inproceedings{Malouf2002,
  Title                    = {A comparison of algorithms for maximum entropy parameter estimation},
  Author                   = {Malouf, Robert},
  Booktitle                = {proceedings of the 6th conference on Natural language learning-Volume 20},
  Year                     = {2002},
  Annote                   = {This is not useful.
It is a review of different algorithms for maximum entropy estimation,but does not explain how the more compelx ones work.
Does present some data on the comparative speed etc.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {1--7},

  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://www.coli.uni-saarland.de/groups/HU/HUwm4/Slides/malouf.pdf}
}

@Incollection{Manne2012,
  Title                    = {Extraction Based Automatic Text Summarization System with HMM Tagger},
  Author                   = {Manne, Suneetha and Shaik Mohd., ZaheerParvez and Sameen Fatima, S.},
  Booktitle                = {Proceedings of the International Conference on Information Systems Design and Intelligent Applications 2012 (INDIA 2012) held in Visakhapatnam, India, January 2012},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2012},
  Editor                   = {Satapathy, SureshChandra and Avadhani, P.S. and Abraham, Ajith},
  Pages                    = {421-428},
  Series                   = {Advances in Intelligent and Soft Computing},
  Volume                   = {132},

  Doi                      = {10.1007/978-3-642-27443-5_48},
  ISBN                     = {978-3-642-27442-8},
  Language                 = {English},
  Owner                    = {20361362},
  Timestamp                = {2015.06.08},
  Url                      = {http://dx.doi.org/10.1007/978-3-642-27443-5_48}
}

@Book{manning1999foundations,
  Title                    = {Foundations of Statistical Natural Language Processing},
  Author                   = {Manning, C.D. and Sch{\"u}tze, H.},
  Publisher                = {MIT Press},
  Year                     = {1999},
  Annote                   = {Big book, good reference},

  ISBN                     = {9780262133609},
  Lccn                     = {99021137},
  Owner                    = {20361362},
  Timestamp                = {2015.07.17},
  Url                      = {https://books.google.com.au/books?id=YiFDxbEX3SUC}
}

@Article{Mao2014GenerateImageDesc,
  Title                    = {Explain Images with Multimodal Recurrent Neural Networks},
  Author                   = {Junhua Mao and
 Wei Xu and
 Yi Yang and
 Jiang Wang and
 Alan L. Yuille},
  Journal                  = {CoRR},
  Year                     = {2014},

  Annote                   = {Uses multimodel RNN (m-RNN), 
to combine language modelling with image description.},
  Volume                   = {abs/1410.1090},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/MaoXYWY14},
  Keywords                 = {image description},
  Owner                    = {20361362},
  Timestamp                = {2016.01.01},
  Url                      = {http://arxiv.org/abs/1410.1090}
}

@Electronic{treebank3,
  Title                    = {Treebank-3 LDC99T42},

  Address                  = {Philadelphia},
  Annote                   = {This is the dataset everyone uses. The Penn Treebank.

There is a subset of this in nltk.corpus.treebank

There is a mistakenly unprotected copy of the full set in http://trac.csail.mit.edu/workbench-projects/browser/trunk/edu.upenn.treebank-3?rev=748},
  Author                   = {Marcus, Mitchell},
  HowPublished             = {corpus},
  Organization             = {Linguistic Data Consortium},
  Year                     = {1999},

  Owner                    = {20361362},
  Timestamp                = {2015.05.21}
}

@Article{BuildingPennTreebank,
  Title                    = {Building a large annotated corpus of English: The Penn Treebank},
  Author                   = {Marcus, Mitchell P and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
  Journal                  = {Computational linguistics},
  Year                     = {1993},

  Annote                   = {This is aboiut how the Penn Treebank was built.

Of intest to me right now is what the Penn Treebank POS tagset is.},
  Number                   = {2},
  Pages                    = {313--330},
  Volume                   = {19},

  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2015.07.14},
  Url                      = {http://repository.upenn.edu/cgi/viewcontent.cgi?article=1246&context=cis_reports}
}

@Article{de2008finding,
  Title                    = {Finding Contradictions in Text},
  Author                   = {de Marneffe, Marie-Catherine and Rafferty, Anna N and Manning, Christopher D},
  Journal                  = {ACL-08: HLT},
  Year                     = {2008},

  Annote                   = {Link to their datasets was broken but can now be found at: \url{http://nlp.stanford.edu/projects/contradiction/}

Classes contracticions into several catogoies:

Easy:
 - Antonym
 - Negation
 - Numeric

Hard:
 - Factive
 - Structure
 - Lexical
 - World Knowledge},
  Pages                    = {1039},

  Owner                    = {20361362},
  Publisher                = {Citeseer},
  Timestamp                = {2015.08.04},
  Url                      = {http://www-nlp.stanford.edu/pubs/contradiction-acl08.pdf}
}

@Article{maron1961automatic,
  Title                    = {Automatic indexing: an experimental inquiry},
  Author                   = {Maron, Melvin Earl},
  Journal                  = {Journal of the ACM (JACM)},
  Year                     = {1961},

  Annote                   = {Ancient paper on topic classification},
  Number                   = {3},
  Pages                    = {404--417},
  Volume                   = {8},

  Owner                    = {20361362},
  Publisher                = {ACM},
  Timestamp                = {2015.09.02},
  Url                      = {sci2s.ugr.es/keel/pdf/algorithm/classification-algorithm/Maron1961.pdf}
}

@Article{10.2307/1263890,
  Author                   = {George V. Maverick},
  Journal                  = {International Journal of American Linguistics},
  Year                     = {1969},
  Number                   = {1},
  Pages                    = {71-75},
  Volume                   = {35},

  ISSN                     = {00207071, 15457001},
  Owner                    = {20361362},
  Publisher                = {The University of Chicago Press},
  Reviewed-author          = {Henry Kucera, W. Nelson Francis},
  Timestamp                = {2015.11.26},
  Url                      = {http://www.jstor.org/stable/1263890}
}

@Article{McKevitt1992,
  Title                    = {Approaches to natural language discourse processing},
  Author                   = {Mc Kevitt, Paul and Partridge, Derek and Wilks, Yorick},
  Journal                  = {Artificial Intelligence Review},
  Year                     = {1992},
  Number                   = {4},
  Pages                    = {333--364},
  Volume                   = {6},

  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2015.04.18},
  Url                      = {http://www.paulmckevitt.com/pubs/airenlp.pdf}
}

@Article{mccarthy2009english,
  Title                    = {The English lexical substitution task},
  Author                   = {McCarthy, Diana and Navigli, Roberto},
  Journal                  = {Language resources and evaluation},
  Year                     = {2009},

  Annote                   = {This is the discussion of the SemEval task of substituting the correct word.
It is a word sense disabmiguation problem.

The SemEval 2007 task defintion exists, online. Apparently there is a 2002 paper discussing the idea.},
  Number                   = {2},
  Pages                    = {139--159},
  Volume                   = {43},

  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2015.07.15},
  Url                      = {http://wwwusers.di.uniroma1.it/~navigli/pubs/LRE_2009_McCarthy_Navigli.pdf}
}

@Article{meindl2012analysis,
  Title                    = {Analysis of commercial and free and open source solvers for linear optimization problems},
  Author                   = {Meindl, Bernhard and Templ, Matthias},
  Journal                  = {Eurostat and Statistics Netherlands},
  Year                     = {2012},

  Annote                   = {Compairs GLPK and Gurobi and a number of other solves to find which is fastest},

  Owner                    = {20361362},
  Timestamp                = {2016.01.26},
  Url                      = {http://neon.vb.cbs.nl/cascprivate/..%5Ccasc%5CESSNet2%5Cdeliverable_solverstudy.pdf}
}

@Inproceedings{mihalcea2004senseval,
  Title                    = {The Senseval-3 English lexical sample task},
  Author                   = {Mihalcea, Rada and Chklovski, Timothy Anatolievich and Kilgarriff, Adam},
  Year                     = {2004},
  Organization             = {Association for Computational Linguistics},

  Owner                    = {20361362},
  Timestamp                = {2016.10.17},
  Url                      = {http://www.aclweb.org/anthology/W04-0807}
}

@Inproceedings{mihalcea2006corpus,
  Title                    = {Corpus-based and knowledge-based measures of text semantic similarity},
  Author                   = {Mihalcea, Rada and Corley, Courtney and Strapparava, Carlo},
  Booktitle                = {AAAI},
  Year                     = {2006},
  Annote                   = {Combining various measures of word semantics to try to get sentence semantics.},
  Pages                    = {775--780},
  Volume                   = {6},

  Owner                    = {20361362},
  Timestamp                = {2015.07.31},
  Url                      = {http://www.aaai.org/Papers/AAAI/2006/AAAI06-123.pdf}
}

@Article{mikolov2013efficient,
  Title                    = {Efficient estimation of word representations in vector space},
  Author                   = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  Journal                  = {arXiv:1301.3781},
  Year                     = {2013},

  Annote                   = {word2vec skipgram

Compares several methods for finding single word vector repressentations.

A "You shall know a word by the company it keeps" type paper.

the paper for CBOW and skip nGram},

  Owner                    = {20361362},
  Timestamp                = {2015.04.15},
  Url                      = {http://arxiv.org/pdf/1301.3781v3}
}

@Inproceedings{6163930,
  Title                    = {Strategies for training large scale neural network language models},
  Author                   = {Mikolov, T. and Deoras, A. and Povey, D. and Burget, L. and Cernocky, J.},
  Booktitle                = {Automatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on},
  Year                     = {2011},
  Annote                   = {This is a discussion of the Class based maximum entropy model.
With RNNs
I have not read it in much detail.},
  Month                    = {Dec},
  Pages                    = {196-201},

  Abstract                 = {We describe how to effectively train neural network based language models on large data sets. Fast convergence during training and better overall performance is observed when the training data are sorted by their relevance. We introduce hash-based implementation of a maximum entropy model, that can be trained as a part of the neural network model. This leads to significant reduction of computational complexity. We achieved around 10% relative reduction of word error rate on English Broadcast News speech recognition task, against large 4-gram model trained on 400M tokens.},
  Doi                      = {10.1109/ASRU.2011.6163930},
  Keywords                 = {computational complexity;learning (artificial intelligence);neural nets;speech recognition;4-gram model;computational complexity;english broadcast news speech recognition task;hash-based implementation;large scale neural network language model training;maximum entropy model;training data;word error rate;Artificial neural networks;Computational complexity;Computational modeling;Data models;Entropy;Training;Training data},
  Owner                    = {20361362},
  Timestamp                = {2015.06.23},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6163930}
}

@Inproceedings{mikolov2011RnnLM,
  Title                    = {Extensions of recurrent neural network language model},
  Author                   = {Mikolov, Tomas and Kombrink, Stefan and Burget, Lukas and Cernocky, Jan H and Khudanpur, Sanjeev},
  Booktitle                = {Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on},
  Year                     = {2011},
  Annote                   = {Probably first RNN language model (Recurrent Neural net).
Input is One Hot,
Output is a proability distribution over classes of the next word,
(from which a uni

Output is Probabilituy distribution over some number of word classes,
where words are assigned to classes based on frequencuy (so each class containes the same number of probaility, but the ones containing low frewency words, contain more different words). ()Eg The might be in a class of its own).

Then uses a Probility distribution within the class to choose the word.

This hgive speedup in training (ast a small loss of accurasy)},
  Organization             = {IEEE},
  Pages                    = {5528--5531},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5947611}
}

@InProceedings{mikolovSkip,
  author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  title     = {Distributed representations of words and phrases and their compositionality},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2013},
  pages     = {3111--3119},
  url       = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf},
  annote    = {Most of the paper focuses on local structure for relationships, ands is similar ot \cite{mikolov2013linguisticsubstructures} This phrasal section of the work is not for per sentence but rather small phrases that are reused, so called "idiomatic phrases" Eg "Austrian Airlines", "New York Times" As I understand the discussion of phrases mostly focuses on having this short phrases preidentified (Eg by a tokenizer, NER, or via staticical cooccurance (that last one what how they did it), and treating them as one phrase.},
  groups    = {KeyPapers},
  owner     = {20361362},
  timestamp = {2015.04.24},
}

@InProceedings{mikolov2013linguisticsubstructures,
  author    = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  title     = {Linguistic Regularities in Continuous Space Word Representations.},
  booktitle = {HLT-NAACL},
  year      = {2013},
  pages     = {746--751},
  url       = {http://www.msr-waypoint.com/en-us/um/people/gzweig/Pubs/NAACL2013Regularities.pdf},
  annote    = {notes the existence of linear substructures in single-word embeddings for example: v("king")-v("man")+v("woman") has nearest neighbor v("queen") Where nearest neighber was defined as the cosign similarity. Experiements were carried out on the use of vector offsets to solve the: A is to B as C is to ? sytle question. Best 40% correct. (Not good enough for use for resythisis) *May* have been overly difficult since a closely related word might end up nearer. Eg: Australia is to Canberra as USA is to ? May have answered New York, instead of Washington -- Washinton and New York both being quiet close (for example) in Vector space. So having overly dense vocabulary *may* have made the task harder. (Need to check up on Cosign Simliarity)},
  groups    = {KeyPapers},
  owner     = {20361362},
  timestamp = {2015.04.15},
}

@Inproceedings{mikolov2012contextRNNLM,
  Title                    = {Context dependent recurrent neural network language model.},
  Author                   = {Mikolov, Tomas and Zweig, Geoffrey},
  Booktitle                = {SLT},
  Year                     = {2012},
  Annote                   = {Exention to RNN Language model which track the 
Topic of the document using LDA

This is the paper that does this. It seems that it was naturally extened for Le and Mikolov's paper on PV-DM and PV-DBOW},
  Pages                    = {234--239},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://www.msr-waypoint.com/en-us/um/people/gzweig/Pubs/SLT2012.pdf}
}

@Article{miller1995wordnet,
  Title                    = {WordNet: a lexical database for English},
  Author                   = {Miller, George A},
  Journal                  = {Communications of the ACM},
  Year                     = {1995},

  Annote                   = {This is the preferred citation for WordNet},
  Number                   = {11},
  Pages                    = {39--41},
  Volume                   = {38},

  Keywords                 = {wordnet},
  Owner                    = {20361362},
  Publisher                = {ACM},
  Timestamp                = {2015.07.14},
  Url                      = {http://mooo.cf/20140731/Reference/1995%20-%20Miller%20-%20WordNet%20a%20lexical%20database%20for%20English.pdf}
}

@Article{WORDNET5miller1990nouns,
  Title                    = {Nouns in WordNet: a lexical inheritance system},
  Author                   = {Miller, George A},
  Journal                  = {International journal of Lexicography},
  Year                     = {1990},

  Annote                   = {POne of the 5 key papers on wordnet.I might like ot cite this for the defnions of noun synonyms.},
  Number                   = {4},
  Pages                    = {245--264},
  Volume                   = {3},

  Keywords                 = {wordnet},
  Owner                    = {20361362},
  Publisher                = {Oxford Univ Press},
  Timestamp                = {2015.07.14},
  Url                      = {http://ijl.oxfordjournals.org/content/3/4/245.full.pdf}
}

@Article{miller1991contextualsemantics,
  Title                    = {Contextual correlates of semantic similarity},
  Author                   = {Miller, George A and Charles, Walter G},
  Journal                  = {Language and cognitive processes},
  Year                     = {1991},

  Annote                   = {I had to find this in *print*
For reference it ins at code P401.9 in the 3rd floor of reid.},
  Number                   = {1},
  Pages                    = {1--28},
  Volume                   = {6},

  File                     = {:..\\annotated_documents\\miller1991contextualsemantics.pdf:PDF},
  Owner                    = {20361362},
  Publisher                = {Taylor \& Francis},
  Timestamp                = {2015.07.23}
}

@Inproceedings{Mitchell2008,
  Title                    = {Vector-based Models of Semantic Composition.},
  Author                   = {Mitchell, Jeff and Lapata, Mirella},
  Booktitle                = {ACL},
  Year                     = {2008},
  Pages                    = {236--244},

  Abstract                 = {This paper proposes a framework for repre-
senting the meaning of phrases and sentences
in vector space. Central to our approach is
vector composition which we operationalize
in terms of additive and multiplicative func-
tions. Under this framework, we introduce a
wide range of composition models which we
evaluate empirically on a sentence similarity
task. Experimental results demonstrate that
the multiplicative models are superior to the
additive alternatives when compared against
human judgments},
  Owner                    = {20361362},
  Timestamp                = {2015.11.19},
  Url                      = {https://www.aclweb.org/anthology/P/P08/P08-1028.pdf}
}

@Article{2016arXiv160603821M,
  author        = {{Monroe}, W. and {Goodman}, N.~D. and {Potts}, C.},
  title         = {{Learning to Generate Compositional Color Descriptions}},
  year          = {2016},
  month         = jun,
  eprint        = {1606.03821},
  url           = {https://arxiv.org/pdf/1606.03821v2.pdf},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2016arXiv160603821M},
  annote        = {Uses LSTM to to learn to output a language model for describing a color, when input a color. Has a fancy fourier-basis color represention, but I don't think much of that as compaired to any other option from \url{http://stats.stackexchange.com/questions/218407/}. It is kinda similar to the shifted gaussian sum reprention there. The introductory paragraph is something I've been wanting to say. It is really well written: "Color descriptions represent a microcosm of grounded language semantics. Basic color terms like “red” and “blue” provide a rich set of semantic building blocks in a continuous meaning space; in addition, people employ compositional color descriptions to express meanings not covered by basic terms, such as “greenish blue” or “the color of the rust on my aunt’s old Chevrolet” (Berlin and Kay, 1991). The production of color language is essential for referring expression generation (Krahmer and Van Deemter, 2012) and image captioning (Kulkarni et al., 2011; Mitchell et al., 2012), among other grounded language generation problems"},
  archiveprefix = {arXiv},
  journal       = {ArXiv e-prints},
  keywords      = {color},
  owner         = {20361362},
  primaryclass  = {cs.CL},
  timestamp     = {2016.11.16},
}

@Inproceedings{morin2005hierarchical,
  Title                    = {Hierarchical probabilistic neural network language model},
  Author                   = {Morin, Frederic and Bengio, Yoshua},
  Booktitle                = {Proceedings of the international workshop on artificial intelligence and statistics},
  Year                     = {2005},
  Annote                   = {Hierarchical Softmax.},
  Organization             = {Citeseer},
  Pages                    = {246--252},

  Owner                    = {20361362},
  Timestamp                = {2015.06.18},
  Url                      = {http://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf}
}

@Article{moro2015semeval,
  Title                    = {SemEval-2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking},
  Author                   = {Moro, Andrea and Navigli, Roberto},
  Journal                  = {Proceedings of SemEval-2015},
  Year                     = {2015},

  Annote                   = {BabelNet word sense disambiguation.
Nothing beat Most Frequent Sense.},

  Keywords                 = {competition, word-sense-disambiguation},
  Owner                    = {20361362},
  Timestamp                = {2015.07.17},
  Url                      = {http://www.aclweb.org/anthology/S15-2049}
}

@Inproceedings{CRPITV165P53-59,
  Title                    = { A Video-oriented Knowledge Collection and Accumulation System with Associated Multimedia Resource Integration },
  Author                   = {Nakano, R. and Kiyoki, Y.},
  Booktitle                = { 11th Asia-Pacific Conference on Conceptual Modelling (APCCM 2015) },
  Year                     = { 2015 },

  Address                  = {Sydney, Australia},
  Annote                   = {This a example of a papar from the conference I am submitting to},
  Editor                   = { Saeki, M. and Kohler, H. },
  Pages                    = { 53-59 },
  Publisher                = {ACS},
  Series                   = {CRPIT},
  Volume                   = { 165 },

  Owner                    = {20361362},
  Timestamp                = {2015.07.09},
  Url                      = { http://crpit.com/confpapers/CRPITV165Nakano.pdf }
}

@Article{nation2006large,
  Title                    = {How large a vocabulary is needed for reading and listening?},
  Author                   = {Nation, I},
  Journal                  = {Canadian Modern Language Review},
  Year                     = {2006},

  Annote                   = {Never more than about 10,000},
  Number                   = {1},
  Pages                    = {59--82},
  Volume                   = {63},

  Owner                    = {20361362},
  Publisher                = {University of Toronto Press},
  Timestamp                = {2016.02.09},
  Url                      = {http://muse.jhu.edu/journals/canadian_modern_language_review/v063/63.1nation.html}
}

@Inproceedings{navigli2013semeval,
  Title                    = {Semeval-2013 task 12: Multilingual word sense disambiguation},
  Author                   = {Navigli, Roberto and Jurgens, David and Vannella, Daniele},
  Booktitle                = {Second Joint Conference on Lexical and Computational Semantics (* SEM)},
  Year                     = {2013},
  Pages                    = {222--231},
  Volume                   = {2},

  Abstract                 = {Word sense diambiguation competion.
We see only very small gains over the base case of using most common word (MFS = Most Frequent Sense).

Best on ENglish wordnet was F1 score of 0.685 vs MFS of 0.665},
  Keywords                 = {word-sense-disambiguation, competition},
  Owner                    = {20361362},
  Timestamp                = {2015.07.17},
  Url                      = {http://wwwusers.di.uniroma1.it/~navigli/pubs/Semeval_2013_Navigli_etal.pdf}
}

@Inproceedings{Navigli:2007:STC:1621474.1621480,
  Title                    = {SemEval-2007 Task 07: Coarse-grained English All-words Task},
  Author                   = {Navigli, Roberto and Litkowski, Kenneth C. and Hargraves, Orin},
  Booktitle                = {Proceedings of the 4th International Workshop on Semantic Evaluations},
  Year                     = {2007},

  Address                  = {Stroudsburg, PA, USA},
  Pages                    = {30--35},
  Publisher                = {Association for Computational Linguistics},
  Series                   = {SemEval '07},

  Acmid                    = {1621480},
  Location                 = {Prague, Czech Republic},
  Numpages                 = {6},
  Owner                    = {20361362},
  Timestamp                = {2016.10.13},
  Url                      = {http://dl.acm.org/citation.cfm?id=1621474.1621480}
}

@Inproceedings{navigli2010babelnet,
  Title                    = {BabelNet: Building a very large multilingual semantic network},
  Author                   = {Navigli, Roberto and Ponzetto, Simone Paolo},
  Booktitle                = {Proceedings of the 48th annual meeting of the association for computational linguistics},
  Year                     = {2010},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {216--225},

  Owner                    = {20361362},
  Timestamp                = {2015.07.17},
  Url                      = {http://www.anthology.aclweb.org/P/P10/P10-1023.pdf}
}

@Article{neelakantan2015efficient,
  Title                    = {Efficient non-parametric estimation of multiple embeddings per word in vector space},
  Author                   = {Neelakantan, Arvind and Shankar, Jeevan and Passos, Alexandre and McCallum, Andrew},
  Journal                  = {arXiv preprint arXiv:1504.06654},
  Year                     = {2015},

  Annote                   = {Two vector tables, Vs(word, sense_id), Vg(word)
v_context=sum(Vg,context_words)
sense_t = arg max(Similarity(v_context, Vs(word, sense), over all senses)
update Vg(word, sense_t)
also update Vg(word).

most similar to mine idea.},

  Abstract                 = {There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly all this work, however, assumes a single vector per word type—ignoring poly semy and thus jeopardizing their usefulness for downstream tasks. 

 We presentan extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on acorpus of nearly 1 billion tokens in less than 6 hours.},
  Keywords                 = {WSI, WSD},
  Owner                    = {20361362},
  Timestamp                = {2016.05.30},
  Url                      = {http://arxiv.org/pdf/1504.06654.pdf}
}

@Inproceedings{Ngiam2011,
  Title                    = {On optimization methods for deep learning},
  Author                   = {Ngiam, Jiquan and Coates, Adam and Lahiri, Ahbik and Prochnow, Bobby and Le, Quoc V and Ng, Andrew Y},
  Booktitle                = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  Year                     = {2011},
  Annote                   = {Discusses why we shouldn't use SGD for deep leanrning adsn should use somes uaw BFGS or CG},
  Pages                    = {265--272},

  Abstract                 = {The predominant methodology in training
deep learning advocates the use of stochastic
gradient descent methods (SGDs). Despite
its ease of implementation, SGDs are diffi-
cult to tune and parallelize. These problems
make it challenging to develop, debug and
scale up deep learning algorithms with SGDs.
In this paper, we show that more sophisti-
cated off-the-shelf optimization methods such
as Limited memory BFGS (L-BFGS) and
Conjugate gradient (CG) with line search
can significantly simplify and speed up the
process of pretraining deep algorithms. In
our experiments, the difference between L-
BFGS/CG and SGDs are more pronounced
if we consider algorithmic extensions (e.g.,
sparsity regularization) and hardware ex-
tensions (e.g., GPUs or computer clusters).
Our experiments with distributed optimiza-
tion support the use of L-BFGS with locally
connected networks and convolutional neural
networks. Using L-BFGS, our convolutional
network model achieves 0.69% on the stan-
dard MNIST dataset. This is a state-of-the-
art result on MNIST among algorithms that
do not use distortions or pretraining.},
  Owner                    = {20361362},
  Timestamp                = {2015.05.19},
  Url                      = {http://www.icml-2011.org/papers/210_icmlpaper.pdf}
}

@Inproceedings{Nguyen2015a,
  Title                    = {Tensor-Variate Restricted Boltzmann Machines.},
  Author                   = {Nguyen, Tu Dinh and Tran, Truyen and Phung, Dinh Q and Venkatesh, Svetha},
  Booktitle                = {AAAI},
  Year                     = {2015},
  Pages                    = {2887--2893},

  Owner                    = {20361362},
  Timestamp                = {2016.03.18}
}

@Book{WebBookBackprop,
  Title                    = {Neural Networks and Deep Learning},
  Author                   = {Nielsen, Michael A},
  Publisher                = {Determination Press},
  Year                     = {2014},
  Annote                   = {By far best description of backpropergation I have seen.
It is a incomplete work, available online under creative commons.},

  Owner                    = {20361362},
  Timestamp                = {2015.05.17},
  Url                      = {http://neuralnetworksanddeeplearning.com/chap2.html}
}

@Article{nocedal1980updating,
  Title                    = {Updating quasi-Newton matrices with limited storage},
  Author                   = {Nocedal, Jorge},
  Journal                  = {Mathematics of computation},
  Year                     = {1980},

  Annote                   = {L-BFGS paper},
  Number                   = {151},
  Pages                    = {773--782},
  Volume                   = {35},

  Keywords                 = {optimisation},
  Owner                    = {20361362},
  Timestamp                = {2016.10.24},
  Url                      = {http://www.ii.uib.no/~lennart/drgrad/Nocedal1980.pdf}
}

@Article{DBLP:journals/corr/OordDZSVGKSK16,
  Title                    = {WaveNet: {A} Generative Model for Raw Audio},
  Author                   = {A{\"{a}}ron van den Oord and
 Sander Dieleman and
 Heiga Zen and
 Karen Simonyan and
 Oriol Vinyals and
 Alex Graves and
 Nal Kalchbrenner and
 Andrew W. Senior and
 Koray Kavukcuoglu},
  Journal                  = {CoRR},
  Year                     = {2016},
  Volume                   = {abs/1609.03499},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/OordDZSVGKSK16},
  Owner                    = {20361362},
  Timestamp                = {Mon, 03 Oct 2016 17:51:10 +0200},
  Url                      = {http://arxiv.org/abs/1609.03499}
}

@Article{DBLP:journals/corr/PaineKCZRHH16,
  Title                    = {Fast Wavenet Generation Algorithm},
  Author                   = {Tom Le Paine and
 Pooya Khorrami and
 Shiyu Chang and
 Yang Zhang and
 Prajit Ramachandran and
 Mark A. Hasegawa{-}Johnson and
 Thomas S. Huang},
  Journal                  = {CoRR},
  Year                     = {2016},
  Volume                   = {abs/1611.09482},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/PaineKCZRHH16},
  Owner                    = {20361362},
  Timestamp                = {Thu, 01 Dec 2016 19:32:08 +0100},
  Url                      = {http://arxiv.org/abs/1611.09482}
}

@Inproceedings{pang2005seeing,
  Title                    = {Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales},
  Author                   = {Pang, Bo and Lee, Lillian},
  Booktitle                = {Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics},
  Year                     = {2005},
  Annote                   = {This eventually becomes the Sentiment Treebank. it is the movie reviews from \cite{SocherEtAl2011:RAE}},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {115--124},

  Keywords                 = {corpora},
  Owner                    = {20361362},
  Timestamp                = {2015.09.01},
  Url                      = {http://ssli.ee.washington.edu/conferences/ACL2005/ACL/pdf/ACL15.pdf}
}

@Inproceedings{pantel2002WSI,
  Title                    = {Discovering word senses from text},
  Author                   = {Pantel, Patrick and Lin, Dekang},
  Booktitle                = {Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining},
  Year                     = {2002},
  Annote                   = {A nonneural network clustering based approach.

Every word instance is transformed into a feature vector.
Patel and Lin call the features contexts, but we will avoid that term for clarity.
the word instance wine in the sentence "Fine diners only sip wine, never gulp"
exhibits the the verb-object feature "sip \_\_\_\_"

These are then clustered using a novel variation on heirachical clustering.


They compare there word sense clusters to Wordnet's senses, by clustering wordnet synsets using Hypernym distence.
This seems rather dubious.},
  Organization             = {ACM},
  Pages                    = {613--619},

  Abstract                 = {Inventories of manually compiled dictionaries usually serve as a source for word senses. However, they often include many rare senses while missing corpus/domain-specific senses. We present a clustering algorithm called CBC (Clustering By Committee) that automatically discovers word senses from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the cluster. We proceed by assigning words to their most similar clusters. After assigning an element to a cluster, we remove their overlapping features from the element. This allows CBC to discover the less frequent senses of a word and to avoid discovering duplicate senses. Each cluster that a word belongs to represents one of its senses. We also present an evaluation methodology for automatically measuring the precision and recall of discovered senses.},
  Owner                    = {20361362},
  Timestamp                = {2016.08.11},
  Url                      = {https://www.semanticscholar.org/paper/Discovering-word-senses-from-text-Pantel-Lin/01bd55608de56aa92696318b47f8db53ce5b0bc4/pdf}
}

@Inproceedings{Papineni2002,
  Title                    = {BLEU: a method for automatic evaluation of machine translation},
  Author                   = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  Booktitle                = {Proceedings of the 40th annual meeting on association for computational linguistics},
  Year                     = {2002},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {311--318},

  Owner                    = {20361362},
  Timestamp                = {2015.11.20},
  Url                      = {http://aclweb.org/anthology/P/P02/P02-1040.pdf}
}

@Article{Park,
  Title                    = {Expressing an Image Stream with a Sequence of Natural Sentences},
  Author                   = {Park, Cesc Chunseong and Kim, Gunhee},

  Annote                   = {Matches text segments from blogs against the images from same post.},

  Owner                    = {20361362},
  Timestamp                = {2015.11.17},
  Url                      = {http://www.cs.cmu.edu/~gunhee/publish/nips15_stream2text.pdf}
}

@Inproceedings{passonneau2012masc,
  Title                    = {The MASC word sense sentence corpus},
  Author                   = {Passonneau, Rebecca J and Baker, Collin and Fellbaum, Christiane and Ide, Nancy},
  Booktitle                = {Proceedings of LREC},
  Year                     = {2012},
  Annote                   = {This is the preferred citation for MASC (Manually Annotated Subcorpus of the Open American National Corpus},

  Owner                    = {20361362},
  Timestamp                = {2015.07.20}
}

@Article{scikit-learn,
  Title                    = {Scikit-learn: Machine Learning in {P}ython},
  Author                   = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
 and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
 and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
 Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2011},

  Annote                   = {This is the prefered citation for Sklearn},
  Pages                    = {2825--2830},
  Volume                   = {12},

  Keywords                 = {tools},
  Owner                    = {20361362},
  Timestamp                = {2015.09.08}
}

@Inproceedings{pennington2014glove,
  Title                    = {GloVe: Global Vectors for Word Representation},
  Author                   = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  Booktitle                = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014)},
  Year                     = {2014},
  Pages                    = {1532--1543},

  Owner                    = {20361362},
  Timestamp                = {2015.12.15},
  Url                      = {http://www.aclweb.org/anthology/D14-1162}
}

@Inproceedings{pinto2007upv,
  Title                    = {UPV-SI: Word sense induction using self term expansion},
  Author                   = {Pinto, David and Rosso, Paolo and Jim{\'e}nez-Salazar, H{\'e}ctor},
  Booktitle                = {Proceedings of the 4th International Workshop on Semantic Evaluations},
  Year                     = {2007},
  Annote                   = {An extension to Simplified Lesk, and to Simplified Extended Lesk.
This varient expands the Context by adding the 100 most similar words to content word in the context.
The expanded context is then compared with the words in the definitions (Simplified Lesk), or in definitions plus hypernym defintions (Simplified Extended Lesk),


The similarity is determined by using a distributional thesaurus, which they construct by extracting relations using a depency parse of a corpus.
Which they use as features, for each word, and apply feature reduction.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {430--433},

  Abstract                 = {We explore the contribution of distributional information for purely knowledge-based word
sense disambiguation. Specifically, we use a distributional thesaurus, computed from a large
parsed corpus, for lexical expansion of context and sense information. This bridges the lexical
gap that is seen as the major obstacle for word overlap–based approaches. We apply this
mechanism to two traditional knowledge-based methods and show that distributional informa-
tion significantly improves disambiguation results across several data sets. This improvement
exceeds the state of the art for disambiguation without sense frequency information—a situation
which is especially encountered with new domains or languages for which no sense-annotated
corpus is available.},
  Keywords                 = {WSD},
  Owner                    = {20361362},
  Timestamp                = {2016.08.11},
  Url                      = {https://www.ukp.tu-darmstadt.de/fileadmin/user_upload/Group_UKP/publikationen/2012/MillerBiemannZeschGurevych_COLING2012_WSD.pdf}
}

@Article{Pollack199077,
  Title                    = {Recursive distributed representations },
  Author                   = {Jordan B. Pollack},
  Journal                  = {Artificial Intelligence },
  Year                     = {1990},
  Number                   = {1â€“2},
  Pages                    = {77 - 105},
  Volume                   = {46},

  Abstract                 = {A longstanding difficulty for connectionist modeling has been how to represent variable-sized recursive data structures, such as trees and lists, in fixed-width patterns. This paper presents a connectionist architecture which automatically develops compact distributed representations for such compositional structures, as well as efficient accessing mechanisms for them. Patterns which stand for the internal nodes of fixed-valence trees are devised through the recursive use of backpropagation on three-layer auto-associative encoder networks. The resulting representations are novel, in that they combine apparently immiscible aspects of features, pointers, and symbol structures. They form a bridge between the data structures necessary for high-level cognitive tasks and the associative, pattern recognition machinery provided by neural networks.},
  Doi                      = {http://dx.doi.org/10.1016/0004-3702(90)90005-K},
  ISSN                     = {0004-3702},
  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.sciencedirect.com/science/article/pii/000437029090005K}
}

@Article{pourdamghanialigning,
  Title                    = {Aligning English Strings with Abstract Meaning Representation Graphs},
  Author                   = {Pourdamghani, Nima and Gao, Yang and Hermjakob, Ulf and Knight, Kevin},

  Annote                   = {Seaks to align English strings with words in the AMR.

(Also speaks in introduction about how it would be good just to use a generative model between AMR and Strings, but we don't got one for Strings<-> Graphs).

Has a preprocessing precedure to linearise the AMR, depths first.},

  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.isi.edu/natural-language/mt/amr_eng_align.pdf}
}

@Book{press2007numerical,
  Title                    = {Numerical Recipes 3rd Edition: The Art of Scientific Computing},
  Author                   = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
  Publisher                = {Cambridge University Press},
  Year                     = {2007},

  Address                  = {New York, NY, USA},
  Edition                  = {3},

  ISBN                     = {0521880688, 9780521880688},
  Owner                    = {20361362},
  Timestamp                = {2016.10.21}
}

@Inproceedings{Qiu2014,
  Title                    = {Learning Word Representation Considering Proximity and Ambiguity.},
  Author                   = {Qiu, Lin and Cao, Yong and Nie, Zaiqing and Yu, Yong and Rui, Yong},
  Booktitle                = {AAAI},
  Year                     = {2014},
  Annote                   = {Adds an additional "proximity weighting" factors to Skip-Grams/CBOWS.
Also relabelled/works with words as Word+Part Of Speech.
So handles POS ambigurity.
Does not do sense ambigurity (beyond POS).},
  Pages                    = {1572--1578},

  Keywords                 = {POS, word-embeddings},
  Owner                    = {20361362},
  Timestamp                = {2016.05.31},
  Url                      = {http://msr-waypoint.com/en-us/um/people/znie/embedding_aaai2014.pdf}
}

@Article{hmmsSpeechTuit,
  Title                    = {A tutorial on hidden Markov models and selected applications in speech recognition},
  Author                   = {Rabiner, L.},
  Journal                  = {Proceedings of the IEEE},
  Year                     = {1989},

  Annote                   = {A very good introduction to HMMs for speech},
  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {257-286},
  Volume                   = {77},

  Abstract                 = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described},
  Doi                      = {10.1109/5.18626},
  ISSN                     = {0018-9219},
  Keywords                 = {Markov processes;speech recognition;balls-in-urns system;coin-tossing;discrete Markov chains;ergodic models;hidden Markov models;hidden states;left-right models;probabilistic function;speech recognition;Distortion;Hidden Markov models;Mathematical model;Multiple signal classification;Signal processing;Speech recognition;Statistical analysis;Stochastic processes;Temperature measurement;Tutorial},
  Owner                    = {20361362},
  Timestamp                = {2015.06.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=18626}
}

@Book{radford2004english,
  Title                    = {English syntax: An introduction},
  Author                   = {Radford, Andrew},
  Publisher                = {Cambridge University Press},
  Year                     = {2004},
  Annote                   = {This is a good book for knowing about Auxiliary verbs.},

  Owner                    = {20361362},
  Timestamp                = {2015.07.14},
  Url                      = {http://faculty.ksu.edu.sa/mmazen/Documents/English_Syntax__Radford-%20up%20%20to%20p%2070.pdf}
}

@Book{radford2004minimalist,
  Title                    = {Minimalist Syntax: Exploring the Structure of English},
  Author                   = {Radford, A.},
  Publisher                = {Cambridge University Press},
  Year                     = {2004},
  Series                   = {Cambridge Textbooks in Linguistics},

  ISBN                     = {9780521542746},
  Lccn                     = {2003055385},
  Owner                    = {20361362},
  Timestamp                = {2015.07.24},
  Url                      = {https://books.google.com.au/books?id=y5VJLP-NW1gC}
}

@Article{rei2014looking,
  Title                    = {Looking for Hyponyms in Vector Space},
  Author                   = {Rei, Marek and Briscoe, Ted},
  Journal                  = {CoNLL-2014},
  Year                     = {2014},

  Annote                   = {considers using word embeddings to generate hyponyms.
It uses a similarity metrics to find similar words, which are thus likely hyponyms.},
  Pages                    = {68},

  Owner                    = {20361362},
  Timestamp                = {2015.06.05},
  Url                      = {http://www.aclweb.org/anthology/W/W14/W14-16.pdf#page=78}
}

@InProceedings{Reisinger2010,
  author        = {Reisinger, Joseph and Mooney, Raymond J},
  title         = {Multi-prototype vector-space models of word meaning},
  booktitle     = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  year          = {2010},
  organization  = {Association for Computational Linguistics},
  pages         = {109--117},
  url           = {http://www.anthology.aclweb.org/N/N10/N10-1013.pdf},
  __markedentry = {[20361362:6]},
  annote        = {The original word emebedding WSD paper. Older than word2vec. Words instance features tdf features or $\chi^2$ features detected in the context around the word instance. These are clustered, using a mixture model (von Mises-Fisher distributions). Thus giving word senses clusters. Nice quote: "Word meaning violates the triangle inequality when viewed at the level of word types, posing a problem for vector-space models (Tversky and Gati, 1982)"},
  groups        = {to_read},
  keywords      = {WSD, word-embeddings},
  owner         = {20361362},
  timestamp     = {2016.06.01},
}

@Article{RitterPosition,
  Title                    = {Leveraging Preposition Ambiguity to Assess Compositional Distributional Models of Semantics},
  Author                   = {Ritter, Samuel and Long, Cotie and Paperno, Denis and Baroni, Marco and Botvinick, Matthew and Goldberg, Adele},
  Journal                  = {The Fourth Joint Conference on Lexical and Computational Semantics},
  Year                     = {2015},

  Annote                   = {This is the paper where they assess phrase embeddins by seeing if they can be classified as to wheich kind ogf spacial stituation they describe.},

  Owner                    = {20361362},
  Timestamp                = {2015.09.04},
  Url                      = {http://clic.cimec.unitn.it/marco/publications/ritter-etal-prepositions-starsem-2015.pdf}
}

@Article{rong2014word2vec,
  Title                    = {word2vec Parameter Learning Explained},
  Author                   = {Rong, Xin},
  Journal                  = {arXiv preprint arXiv:1411.2738},
  Year                     = {2014},

  Owner                    = {20361362},
  Timestamp                = {2015.06.19},
  Url                      = {http://arxiv.org/pdf/1411.2738.pdf}
}

@Techreport{1961Perceptron,
  Title                    = {Principles of neurodynamics. perceptrons and the theory of brain mechanisms},
  Author                   = {Rosenblatt, Frank},
  Institution              = {DTIC Document},
  Year                     = {1961},

  Owner                    = {20361362},
  Timestamp                = {2015.09.09}
}

@Article{rosenfeld2000two,
  Title                    = {Two decades of statistical language modeling: Where do we go from here?},
  Author                   = {Rosenfeld, Ronald},
  Journal                  = {Proceedings of the IEEE},
  Year                     = {2000},

  Annote                   = {Reivw of Statistical Lnaguage modelling,},
  Number                   = {8},
  Pages                    = {1270--1278},
  Volume                   = {88},

  Owner                    = {20361362},
  Publisher                = {Institute of Electrical and Electronics Engineers},
  Timestamp                = {2016.10.24},
  Url                      = {http://repository.cmu.edu/cgi/viewcontent.cgi?article=2320&context=compsci}
}

@Article{backprop,
  Title                    = {Learning representations by back-propagating errors},
  Author                   = {Rumelhart, David E and Hintont, Geoffrey E and Williams, Ronald J},
  Journal                  = {Nature},
  Year                     = {1986},
  Pages                    = {9},
  Volume                   = {323},

  Owner                    = {20361362},
  Timestamp                = {2015.09.09},
  Url                      = {http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf}
}

@Article{DBLP:journals/corr/RusuRDSKKPH16,
  Title                    = {Progressive Neural Networks},
  Author                   = {Andrei A. Rusu and
 Neil C. Rabinowitz and
 Guillaume Desjardins and
 Hubert Soyer and
 James Kirkpatrick and
 Koray Kavukcuoglu and
 Razvan Pascanu and
 Raia Hadsell},
  Journal                  = {CoRR},
  Year                     = {2016},
  Volume                   = {abs/1606.04671},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/RusuRDSKKPH16},
  Owner                    = {20361362},
  Timestamp                = {Fri, 01 Jul 2016 17:39:49 +0200},
  Url                      = {http://arxiv.org/abs/1606.04671}
}

@Inproceedings{saarikoski2006building,
  Title                    = {Building an optimal WSD ensemble using per-word selection of best system},
  Author                   = {Saarikoski, Harri MT and Legrand, Steve},
  Booktitle                = {Iberoamerican Congress on Pattern Recognition},
  Year                     = {2006},
  Organization             = {Springer},
  Pages                    = {864--872},

  Owner                    = {20361362},
  Timestamp                = {2016.10.14},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.221.2996&rep=rep1&type=pdf#page=140}
}

@Inproceedings{saarikoski2006defining,
  Title                    = {Defining classifier regions for WSD ensembles using word space features},
  Author                   = {Saarikoski, Harri MT and Legrand, Steve and Gelbukh, Alexander},
  Booktitle                = {Mexican International Conference on Artificial Intelligence},
  Year                     = {2006},
  Organization             = {Springer},
  Pages                    = {855--867},

  Owner                    = {20361362},
  Timestamp                = {2016.10.14},
  Url                      = {http://www.gelbukh.com/CV/Publications/2006/MICAI-2006-WSD.pdf}
}

@Inproceedings{2009DBM,
  Title                    = {Deep boltzmann machines},
  Author                   = {Salakhutdinov, Ruslan and Hinton, Geoffrey E},
  Booktitle                = {International Conference on Artificial Intelligence and Statistics},
  Year                     = {2009},
  Annote                   = {Has a Boltzman Machine Learning procedure nicely defined.


In terms of Fantasy Particles and Sample
Boltsman machiens have 3 weight matrixes: visible-visible, visible-hidden, and vhidden-hidden. (Unlioke RBM which has the first and last set to zero).

Fiocus is however on deep boltman machines (not just plan regular ones).

"... unlike deep belief networks, the approxi-
mate inference procedure, in addition to an initial bottom-
up pass, can incorporate top-down feedback, allowing deep
Boltzmann machines to better propagate uncertainty about,
and hence deal more robustly with, ambiguous inputs."


Shows ghow to use a modifed RBM for pretraining},
  Pages                    = {448--455},

  Owner                    = {20361362},
  Timestamp                = {2015.04.17},
  Url                      = {http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS09_SalakhutdinovH.pdf}
}

@Inproceedings{CRPITV165P43-52,
  Title                    = { A Semi-supervised Topic-based User Model for Web Information Visualization },
  Author                   = { Saleheen, S. and Lai, W. },
  Booktitle                = { 11th Asia-Pacific Conference on Conceptual Modelling (APCCM 2015) },
  Year                     = { 2015 },

  Address                  = { Sydney, Australia },
  Annote                   = {A paper from conference I am submitting to},
  Editor                   = { Saeki, M. and Kohler, H. },
  Pages                    = { 43-52 },
  Publisher                = {ACS},
  Series                   = {CRPIT},
  Volume                   = { 165 },

  Owner                    = {20361362},
  Timestamp                = {2015.07.09},
  Url                      = { http://crpit.com/confpapers/CRPITV165Saleheen.pdf }
}

@Article{Penn_POS_Taggening,
  Title                    = {Part-of-speech tagging guidelines for the Penn Treebank Project (3rd revision)},
  Author                   = {Santorini, Beatrice},
  Year                     = {1990},

  Owner                    = {20361362},
  Timestamp                = {2015.07.14},
  Url                      = {http://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&context=cis_reports}
}

@Article{2016arXiv160408633S,
  Title                    = {{Word Ordering Without Syntax}},
  Author                   = {{Schmaltz}, A. and {Rush}, A.~M. and {Shieber}, S.~M.},
  Journal                  = {ArXiv e-prints},
  Year                     = {2016},

  Annote                   = {Use a LSTM as a language model, then use beam search to search it, to do word ordering.},
  Month                    = apr,

  Adsnote                  = {Provided by the SAO/NASA Astrophysics Data System},
  Adsurl                   = {http://adsabs.harvard.edu/abs/2016arXiv160408633S},
  Archiveprefix            = {arXiv},
  Eprint                   = {1604.08633},
  Keywords                 = {word ordering},
  Owner                    = {20361362},
  Primaryclass             = {cs.CL},
  Timestamp                = {2016.06.07},
  Url                      = {https://arxiv.org/pdf/1604.08633v1.pdf}
}

@Article{564201,
  Title                    = {Game theory approach to discrete H infin; filter design},
  Author                   = {Xuemin Shen and Li Deng},
  Journal                  = {Signal Processing, IEEE Transactions on},
  Year                     = {1997},
  Month                    = {Apr},
  Number                   = {4},
  Pages                    = {1092-1095},
  Volume                   = {45},

  Abstract                 = {A finite-horizon discrete H8 filter design with a linear quadratic (LQ) game approach is presented. The exogenous inputs composed of the “hostile” noise signals and system initial condition are assumed to be finite energy signals with unknown statistics. The design criterion is to minimize the worst possible amplification of the estimation error signals in terms of the exogenous inputs, which is different from the classical minimum variance estimation error criterion for the modified Wiener or Kalman filter design. The approach can show how far the estimation error can be reduced under an existence condition on the solution to a corresponding Riccati equation. A numerical example is given to compare the performance of the H8 filter with that of the conventional Kalman filter},
  Doi                      = {10.1109/78.564201},
  ISSN                     = {1053-587X},
  Keywords                 = {H8 optimisation;Kalman filters;Riccati equations;difference equations;discrete time filters;error analysis;filtering theory;game theory;noise;parameter estimation;signal processing;Kalman filter;amplification;difference Riccati equation;discrete H8 filter design;estimation error signals;exogenous inputs;finite energy signals;finite-horizon discrete H8 filter;game theory;hostile noise signals;linear quadratic game;modified Wiener filter;system initial condition;unknown statistics;Estimation error;Filtering theory;Game theory;H infinity control;Noise measurement;Nonlinear filters;Riccati equations;Signal design;Signal processing;Statistics},
  Owner                    = {20361362},
  Timestamp                = {2015.05.05}
}

@Article{shenHinftySpeachEnhancement,
  Title                    = {H8 Filtering for Speech Ehancement},
  Author                   = {Shen, Xuemin and Deng, Li and Yasmin, Anisa},

  Annote                   = {I feel there must be a similar paper to this that has actually been published.

It is woth noting the state matrix, is [x_{n}, x_{n-1}, x_{n-2}]
and the state transition matrix is just the shift of these plus scaling each.
The Observation matrix, simply picks out the current.

This is a pretty great idea, and you see it in many places.},

  Abstract                 = {In this paper, a new approach based on the H8
filtering is presented
for speech enhancement. This approach differs from the trad
itional
modified Wiener/Kalman filtering approach in the following two as-
pects: 1) no a priori knowledge of the noise statistics is required;
instead the noise signals are only assumed to have finite energy; 2)
the estimation criterion for the filter design is to minimize the worst
possible amplification of the estimation error signal in terms of the
modeling errors and additive noises. Since most additive noises in
speech are not Gaussian, this approach is highly robust and is more
appropriate in practical speech enhancement. The global signal-to-
noise ratio (SNR), time domain speech representation and listening
evaluations are used to verify the performance of the H8 filtering
algorithm. Experimented results show that the filtering performance
is better than other speech enhancement approaches in the
literature under similar experimental conditions.},
  Owner                    = {20361362},
  Timestamp                = {2015.05.07},
  Url                      = {http://research.microsoft.com/pubs/194586/ShenDengYasmin1996.pdf}
}

@Book{BookOptimalStateEstimation,
  Title                    = {Optimal state estimation: Kalman, H infinity, and nonlinear approaches},
  Author                   = {Simon, Dan},
  Publisher                = {John Wiley \& Sons},
  Year                     = {2006},
  Annote                   = {Nice book.
In the science library.

Covers most things on Kalman, EKF, UKF, H infinity, and particle filters.},

  Owner                    = {20361362},
  Timestamp                = {2015.05.05}
}

@Article{SimonsFeatureArticleHInfinity,
  Title                    = {From Here to Infinity},
  Author                   = {Dan Simon},
  Journal                  = {Embedded Systems Programming},
  Year                     = {2001},

  Annote                   = {Nice.
Written in Feature Article Style.

Some of the math formatting is messed up.

Similar to the description of the filter from his book: \cite{BookOptimalStateEstimation}.

The last section in the article talks about how to handle poor knowledge of the system function.},
  Note                     = {Is a Feature article},

  Keywords                 = {filtering},
  Owner                    = {20361362},
  Timestamp                = {2015.05.05},
  Url                      = {http://academic.csuohio.edu/simond/courses/eec641/hinfinity.pdf}
}

@Article{slama2015accurate,
  Title                    = {Accurate 3D action recognition using learning on the Grassmann manifold},
  Author                   = {Slama, Rim and Wannous, Hazem and Daoudi, Mohamed and Srivastava, Anuj},
  Journal                  = {Pattern Recognition},
  Year                     = {2015},
  Number                   = {2},
  Pages                    = {556--567},
  Volume                   = {48},

  Owner                    = {20361362},
  Publisher                = {Elsevier},
  Timestamp                = {2016.08.22}
}

@PhdThesis{socher2014recursive,
  author    = {Socher, Richard},
  title     = {Recursive Deep Learning for Natural Language Processing and Computer Vision},
  year      = {2014},
  url       = {http://nlp.stanford.edu/~socherr/thesis.pdf},
  annote    = {This thesis sumerises most of socher's papers. see also: http://lxmls.it.pt/2014/socher-lxmls.pdf Currently up to chapter 3.},
  file      = {:..\\annotated_documents\\socher_thesis.pdf:PDF},
  groups    = {KeyPapers},
  keywords  = {RvNN},
  owner     = {20361362},
  school    = {Stanford University},
  timestamp = {2015.04.14},
}

@Incollection{SocherEtAl2013:CVG,
  Title                    = {Parsing With Compositional Vector Grammars},
  Author                   = {Richard Socher and John Bauer and Christopher D. Manning and Andrew Y. Ng},
  Booktitle                = {ACL},
  Year                     = {2013},
  Annote                   = {This is the improved RvNN parser that combines it with Grammar,
This is what is in the Stanford Parser.},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.socher.org/uploads/Main/SocherBauerManningNg_ACL2013.pdf}
}

@Incollection{Socher2013TensorReasoning,
  Title                    = {Reasoning With Neural Tensor Networks For Knowledge Base Completion},
  Author                   = {Richard Socher and Danqi Chen and Christopher D. Manning and Andrew Y. Ng},
  Booktitle                = {Advances in Neural Information Processing Systems 26},
  Year                     = {2013},
  Annote                   = {Simarlar to mikolov2013linguisticsubstructures
Instead of , A is to B as C is to D (as in Mikolov et al)

This reasons in triples of a knowledge base,
trying to create more triples.

Going form a large number of (A,R,B) where R is a relationship between A and B.
To having even more of these.},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.15},
  Url                      = {http://nlp.stanford.edu/~socherr/SocherChenManningNg_NIPS2013.pdf}
}

@Inproceedings{Socher2010,
  Title                    = {Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora},
  Author                   = {Socher, Richard and Fei-Fei, Li},
  Booktitle                = {Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on},
  Year                     = {2010},
  Annote                   = {This is heavily inspired by machine translation.
Image is mapped to Visual Words, which are then moved to a common vector space that both visual words and textural words are mapped to .

Does Not Use Word Embedding, I think??},
  Organization             = {IEEE},
  Pages                    = {966--973},

  Owner                    = {20361362},
  Timestamp                = {2015.04.23},
  Url                      = {http://www.socher.org/uploads/Main/SocherFeiFei_CVPR2010.pdf}
}

@Incollection{SocherEtAl2011:PoolRAE,
  Title                    = {Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection},
  Author                   = {Richard Socher and Eric H. Huang and Jeffrey Pennington and Andrew Y. Ng and Christopher D. Manning},
  Booktitle                = {{Advances in Neural Information Processing Systems 24}},
  Year                     = {2011},

  Owner                    = {20361362},
  Timestamp                = {2015.04.24},
  Url                      = {http://www.socher.org/uploads/Main/SocherHuangPenningtonNgManning_NIPS2011.pdf}
}

@Inproceedings{SocherMVRNN,
  Title                    = {Semantic compositionality through recursive matrix-vector spaces},
  Author                   = {Socher, Richard and Huval, Brody and Manning, Christopher D and Ng, Andrew Y},
  Booktitle                = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
  Year                     = {2012},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {1201--1211},

  Owner                    = {20361362},
  Timestamp                = {2015.09.09},
  Url                      = {http://www.aclweb.org/anthology/D12-1110}
}

@Article{socherDTRNN,
  Title                    = {Grounded compositional semantics for finding and describing images with sentences},
  Author                   = {Socher, Richard and Karpathy, Andrej and Le, Quoc V and Manning, Christopher D and Ng, Andrew Y},
  Journal                  = {Transactions of the Association for Computational Linguistics},
  Year                     = {2014},
  Pages                    = {207--218},
  Volume                   = {2},

  Keywords                 = {image description},
  Owner                    = {20361362},
  Timestamp                = {2015.09.09},
  Url                      = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/download/325/45}
}

@Inproceedings{Socher2011ParsingPhrases,
  Title                    = {Parsing natural scenes and natural language with recursive neural networks},
  Author                   = {Socher, Richard and Lin, Cliff C and Manning, Chris and Ng, Andrew Y},
  Booktitle                = {Proceedings of the 28th international conference on machine learning (ICML-11)},
  Year                     = {2011},
  Annote                   = {Defines a tree of recurrsively applied neural networks for parsing.

Each network has:
2 split inputs, $c_i$, $c_j$ each 1 hot word vector
each goto $L$: the word embeeding matrix, with the output then concatenated which goes
to $W$: the hidden layer 

output of $W$ is $p_{i,j}$ -- the merged word vector
which also goes to two places:
To $W_{score}$ -- the score of how good the merge was
and To $W_{label}$ -- the softmaxed Part of Speech (POS) tagging layer


The paper describes how the parse trees are selected.
It also (poorly) decribes how the learning is carried out (to understand will need to chase up references).

-----
This set of slides: http://lxmls.it.pt/2014/socher-lxmls.pdf
Describes all things quiet well},
  Pages                    = {129--136},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://nlp.stanford.edu/pubs/SocherLinNgManning_ICML2011.pdf}
}

@Inproceedings{socher2010PhraseEmbedding,
  Title                    = {Learning continuous phrase representations and syntactic parsing with recursive neural networks},
  Author                   = {Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
  Booktitle                = {Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop},
  Year                     = {2010},
  Annote                   = {Sorcher and Mannings earlier work.
Predecessor to \cite{socher2010PhraseEmbedding} Likely a easier paper to understand that due to its earlier nature.


Embeeds Phrases via Parse of speach 


Does Assess Nerest Neighbour Phrases, which is interesting.
This is done by Embeeding a chunk of Wall Street Journal, then finding nearest neighbours on using a undsiclosed metric (Possibly equclidean)

Claims: "parser can accurately recover tree structures using only the distributed phrase representations. Furthermore, it provides semantic information even" But I can't see how.},
  Pages                    = {1--9},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://wuawua.googlecode.com/files/Learning%20Continuous%20Phrase%20Representations%20and%20Syntactic%20Parsing%20with%20Recursive%20Neural%20Networks.pdf}
}

@Inproceedings{SocherEtAl2011:RAE,
  Title                    = {Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions},
  Author                   = {Richard Socher and Jeffrey Pennington and Eric H. Huang and Andrew Y. Ng and Christopher D. Manning},
  Booktitle                = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  Year                     = {2011},
  Annote                   = {Similar to the RvNN, the Recursive Auto Encoder applies an Auto Encoder while generating the tree, in the place of the neural network shown in[fig:The-Neural-Net]. Roughly replacing the Max-Margin derived scoring matrix with a reconstruction error.

The tree selected is selecting much the say way as for the RvNN -- by merge resulting in best reconstruction.

It is noted that if the left child (for example) is a node made by merging 5 children and the right a node by merging only 2, then it is more important to correctly reconstruct the left child than the right. The weighting function in the RAE in that paper is adjusted as such. 

It can be extended in a similar way to the RvNN by adding additional trained outputs at each node.

It is not purely greedy, on simply training the to minimize the error of the merge at each node. Like the RvNN, it uses BPTS, and attempt to minimize the sum of all errors across the tree. Unlike the RvNN, there is no target structure -- it is free to use what ever structure minimizes total error.},

  Keywords                 = {RAE},
  Owner                    = {20361362},
  Timestamp                = {2015.04.24},
  Url                      = {http://www.socher.org/uploads/Main/SocherPenningtonHuangNgManning_EMNLP2011.pdf}
}

@Inproceedings{RvNTN,
  Title                    = {Recursive deep models for semantic compositionality over a sentiment treebank},
  Author                   = {Socher, Richard and Perelygin, Alex and Wu, Jean Y and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  Booktitle                = {Proceedings of the conference on empirical methods in natural language processing (EMNLP)},
  Year                     = {2013},
  Annote                   = {This is the preferece cisation for the sentiment treeba nk},
  Organization             = {Citeseer},
  Pages                    = {1642},
  Volume                   = {1631},

  Owner                    = {20361362},
  Timestamp                = {2015.09.01},
  Url                      = {nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf}
}

@Article{Song2016,
  Title                    = {Word Embeddings, Sense Embeddings and their Application to Word Sense Induction},
  Author                   = {Song, Linfeng},
  Year                     = {2016},

  Annote                   = {Very long (41 pages) review article on WSI using word emebddings.
Solid summary. 
Also presents own new method, which I haven't read into closely yet.},

  Keywords                 = {WSI; WSD;},
  Owner                    = {20361362},
  Timestamp                = {2016.05.30},
  Url                      = {https://www.cs.rochester.edu/~lsong10/papers/area.pdf}
}

@Techreport{export:226586,
  Title                    = {Unsupervised Learning of Word Semantic Embedding using the Deep Structured
 Semantic Model},
  Author                   = {Xinying Song and Xiaodong He and Jianfeng Gao and Li Deng},
  Year                     = {2014},
  Annote                   = {A method for learning Word Embeddings, using DNNs.

Bag of words based.
Embed the whole bag of words, in a document.
Use cosine similarity to embedding of bag of wards in query.},
  Month                    = {August},
  Number                   = {MSR-TR-2014-109},

  Owner                    = {20361362},
  Publisher                = {Microsoft Research},
  Timestamp                = {2015.05.26},
  Url                      = {http://research.microsoft.com/apps/pubs/default.aspx?id=226586}
}

@Inproceedings{Taghipour2015,
  Title                    = {Semi-Supervised Word Sense Disambiguation Using Word Embeddings in General and Specific Domains},
  Author                   = {Taghipour, Kaveh and Ng, Hwee Tou},
  Booktitle                = {The 2015 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  Year                     = {2015},
  Annote                   = {Uses word emebeddings as a feature for WSD},
  Pages                    = {314--323},

  Keywords                 = {WSD, word embeddings},
  Owner                    = {20361362},
  Timestamp                = {2016.04.19},
  Url                      = {http://www.aclweb.org/anthology/N15-1035}
}

@Article{CulturomicOpenProblems,
  Title                    = {Visions and open challenges for a knowledge-based culturomics},
  Author                   = {Tahmasebi, Nina and Borin, Lars and Capannini, Gabriele and Dubhashi, Devdatt and Exner, Peter and Forsberg, Markus and Gossen, Gerhard and Johansson, Fredrik D and Johansson, Richard and K{\aa}geb{\"a}ck, Mikael and others},
  Journal                  = {International Journal on Digital Libraries},
  Year                     = {2015},

  Annote                   = {A collection of opentasks and motivations for the field of culturomics,
which involves doign large text processing, taking into acount development of culture over time.},
  Number                   = {2-4},
  Pages                    = {169--187},
  Volume                   = {15},

  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2015.04.17},
  Url                      = {http://link.springer.com/article/10.1007/s00799-015-0139-1/fulltext.html}
}

@Article{TarasovGenerateAbstractSummary,
  Title                    = {Natural Language Generation Paraphrasing and Summarization
 of User Reviews with Recurrent
 Neural Networks},
  Author                   = {D. S. Tarasov},
  Year                     = {n.d.},

  Annote                   = {Makes a lot more senese if read inconcert with Mao et al's paper.

I don't know that this is it actually published anywhere with peer review.

Combining language modeling with the Summarisation learning task.
It doesn't use sentence vectors, though the authors indicate how it could.},

  Keywords                 = {summarization, vector-based_summarization},
  Owner                    = {20361362},
  Timestamp                = {2015.09.13},
  Url                      = {http://www.dialog-21.ru/digests/dialog2015/materials/pdf/TarasovDS2.pdf}
}

@Inproceedings{Taskar2004,
  Title                    = {Max-Margin Parsing.},
  Author                   = {Taskar, Ben and Klein, Dan and Collins, Michael and Koller, Daphne and Manning, Christopher D},
  Booktitle                = {EMNLP},
  Year                     = {2004},
  Annote                   = {This is NOT a good paper to read to help undersstand Socher2011.
It is a predecessor to it, but not direct enough.
It does not example the max-margin method well (but reference a baper that does)},
  Number                   = {1.1},
  Organization             = {Citeseer},
  Pages                    = {3},
  Volume                   = {1},

  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://ai.stanford.edu/~koller/Papers/Taskar+al:EMNLP04.pdf}
}

@Inbook{tengi1998design,
  Title                    = {WordNet: an electronic lexical database, The MIT Press, Cambridge, Massachusetts},
  Author                   = {Tengi, Randee I},
  Editor                   = {Fellbaum, Christiane (r{\'e}d.)},
  Pages                    = {105},
  Year                     = {1998},
  Annote                   = {Book chapter about how wordnet works.
This is the source for things like wordnet only contains lemmas.
Kind of. Uses the phrase "base word form"},

  Chapter                  = {Design and implementation of the WordNet lexical database and searching software},
  Journal                  = {Fellbaum, Christiane (r{\'e}d.), WordNet: an electronic lexical database, The MIT Press, Cambridge, Massachusetts},
  Owner                    = {20361362},
  Timestamp                = {2015.07.27}
}

@Inproceedings{tian2014probabilistic,
  Title                    = {A Probabilistic Model for Learning Multi-Prototype Word Embeddings.},
  Author                   = {Tian, Fei and Dai, Hanjun and Bian, Jiang and Gao, Bin and Zhang, Rui and Chen, Enhong and Liu, Tie-Yan},
  Booktitle                = {COLING},
  Year                     = {2014},
  Pages                    = {151--160},

  Owner                    = {20361362},
  Timestamp                = {2016.10.17}
}

@Article{tibshirani2005cluster,
  Title                    = {Cluster validation by prediction strength},
  Author                   = {Tibshirani, Robert and Walther, Guenther},
  Journal                  = {Journal of Computational and Graphical Statistics},
  Year                     = {2005},
  Number                   = {3},
  Pages                    = {511--528},
  Volume                   = {14},

  Keywords                 = {clustering},
  Owner                    = {20361362},
  Publisher                = {Taylor \& Francis},
  Timestamp                = {2016.06.28},
  Url                      = {http://www.tandfonline.com/doi/pdf/10.1198/106186005X59243}
}

@Inproceedings{Tillmann2000,
  Title                    = {Word re-ordering and DP-based search in statistical machine translation},
  Author                   = {Tillmann, Christoph and Ney, Hermann},
  Booktitle                = {Proceedings of the 18th conference on Computational linguistics-Volume 2},
  Year                     = {2000},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {850--856},

  Keywords                 = {word-ordering, machine-translation},
  Owner                    = {20361362},
  Timestamp                = {2016.02.04},
  Url                      = {http://www.aclweb.org/anthology/C00-2123}
}

@Article{Tomalin,
  Title                    = {Word Ordering with Phrase-Based Grammars},
  Author                   = {Tomalin, Marcus and Byrne, William},

  Owner                    = {20361362},
  Timestamp                = {2016.02.16},
  Url                      = {http://mi.eng.cam.ac.uk/~wjb31/ppubs/EACL2014.pdf}
}

@Inproceedings{StandfordPOSTagger,
  Title                    = {Feature-rich part-of-speech tagging with a cyclic dependency network},
  Author                   = {Toutanova, Kristina and Klein, Dan and Manning, Christopher D and Singer, Yoram},
  Booktitle                = {Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1},
  Year                     = {2003},
  Annote                   = {This is the preferred citation for the Standford POS tagger},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {173--180},

  Owner                    = {20361362},
  Timestamp                = {2015.07.14},
  Url                      = {ilpubs.stanford.edu:8090/603/1/2003-43.pdf}
}

@Inproceedings{turaga2008statistical,
  Title                    = {Statistical analysis on Stiefel and Grassmann manifolds with applications in computer vision},
  Author                   = {Turaga, Pavan and Veeraraghavan, Ashok and Chellappa, Rama},
  Booktitle                = {Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on},
  Year                     = {2008},
  Organization             = {IEEE},
  Pages                    = {1--8},

  Abstract                 = {Many applications in computer vision and pattern recog-
nition involve drawing inferences on certain manifold-
valued parameters. In order to develop accurate inference
algorithms on these manifolds we need to a) understand the
geometric structure of these manifolds b) derive appropri-
ate distance measures and c) develop probability distribu-
tion functions (pdf) and estimation techniques that are con-
sistent with the geometric structure of these manifolds. In
this paper, we consider two related manifolds - the Stiefel
manifold and the Grassmann manifold, which arise natu-
rally in several vision applications such as spatio-temporal
modeling, affine invariant shape analysis, image matching
and learning theory. We show how accurate statistical char-
acterization that reflects the geometry of these manifolds
allows us to design efficient algorithms that compare fa-
vorably to the state of the art in these very different ap-
plications. In particular, we describe appropriate distance
measures and parametric and non-parametric density esti-
mators on these manifolds. These methods are then used
to learn class conditional densities for applications such as
activity recognition, video based face recognition and shape
classification},
  Keywords                 = {computer vision},
  Owner                    = {20361362},
  Timestamp                = {2016.08.18},
  Url                      = {http://www.public.asu.edu/~pturaga/papers/Manifolds.pdf}
}

@Inproceedings{turpin2012attempt,
  Title                    = {An attempt to measure the quality of questions in question time of the Australian Federal Parliament},
  Author                   = {Turpin, Andrew},
  Booktitle                = {Proceedings of the Seventeenth Australasian Document Computing Symposium},
  Year                     = {2012},
  Annote                   = {This is mostly interesting because it is associated with a Hansard subset.
The Hansard can be downloaded yourself -- it is a very nicely formatted xml dataset labeled with all the desirable meta data like speaker name and party.
Apprently though the Hansard changed format in 2011.

this also features a question time subcorpus.},
  Organization             = {ACM},
  Pages                    = {96--103},

  Keywords                 = {corpora},
  Owner                    = {20361362},
  Timestamp                = {2015.06.15},
  Url                      = {http://people.eng.unimelb.edu.au/aturpin/qtCorpus/adcs2012.pdf}
}

@Book{tymoczko1995sweet,
  Title                    = {Sweet Reason: A Field Guide to Modern Logic},
  Author                   = {Tymoczko, T. and Henle, J. and Henle, J.M.},
  Publisher                = {Key College},
  Year                     = {1995},
  Annote                   = {Includes the discussion of Buffello buffello},
  Series                   = {Textbooks in Mathematical Sciences},

  ISBN                     = {9780387989303},
  Lccn                     = {99043387},
  Owner                    = {20361362},
  Timestamp                = {2016.02.11},
  Url                      = {https://books.google.com.au/books?id=LQnsSuvP9dAC}
}

@Article{veronis2004hyperlex,
  Title                    = {Hyperlex: lexical cartography for information retrieval},
  Author                   = {V{\'e}ronis, Jean},
  Journal                  = {Computer Speech \& Language},
  Year                     = {2004},
  Number                   = {3},
  Pages                    = {223--252},
  Volume                   = {18},

  Keywords                 = {WSD, graph},
  Owner                    = {20361362},
  Publisher                = {Elsevier},
  Timestamp                = {2016.08.11},
  Url                      = {http://sites.univ-provence.fr/~veronis/pdf/2004-hyperlex-CSL.pdf}
}

@Inproceedings{veronis1998study,
  Title                    = {A study of polysemy judgements and inter-annotator agreement},
  Author                   = {V{\'e}ronis, Jean},
  Booktitle                = {Programme and advanced papers of the Senseval workshop},
  Year                     = {1998},
  Pages                    = {2--4},

  Owner                    = {20361362},
  Timestamp                = {2016.10.19},
  Url                      = {http://sites.univ-provence.fr/veronis/pdf/1998senseval.pdf}
}

@Article{Waibel1989,
  Title                    = {Phoneme recognition using time-delay neural networks},
  Author                   = {Waibel, Alexander and Hanazawa, Toshiyuki and Hinton, Geoffrey and Shikano, Kiyohiro and Lang, Kevin J},
  Journal                  = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
  Year                     = {1989},
  Number                   = {3},
  Pages                    = {328--339},
  Volume                   = {37},

  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2015.09.11},
  Url                      = {http://www.cs.toronto.edu/~fritz/absps/waibelTDNN.pdf}
}

@Article{Wang2015,
  Title                    = {Semantic Expansion using Word Embedding Clustering and Convolutional Neural Network for Improving Short Text Classification },
  Author                   = {Peng Wang and Bo Xu and Jiaming Xu and Guanhua Tian and Cheng-Lin Liu and Hongwei Hao},
  Journal                  = {Neurocomputing },
  Year                     = {2015},
  Pages                    = { - },

  Abstract                 = {Abstract Text classification can help users to effectively handle and exploit useful information hidden in large-scale documents. However, the sparsity of data and the semantic sensitivity to context often hinder the classification performance of short texts. In order to overcome the weakness, we propose a unified framework to expand short texts based on word embedding clustering and convolutional neural network (CNN). Empirically, the semantically related words are usually close to each other in embedding spaces. Thus, we first discover semantic cliques via fast clustering. Then, by using additive composition over word embeddings from context with variable window width, the representations of multi-scale semantic units11 Semantic units are defined as n-grams which have dominant meaning of text. With n varying, multi-scale contextual information can be exploited. in short texts are computed. In embedding spaces, the restricted nearest word embeddings (NWEs)22 In order to prevent outliers, a Euclidean distance threshold is preset between semantic cliques and semantic units, which is used as restricted condition. of the semantic units are chosen to constitute expanded matrices, where the semantic cliques are used as supervision information. Finally, for a short text, the projected matrix33 The projected matrix is obtained by table looking up, which encodes Unigram level features. and expanded matrices are combined and fed into \{CNN\} in parallel. Experimental results on two open benchmarks validate the effectiveness of the proposed method.},
  Doi                      = {http://dx.doi.org/10.1016/j.neucom.2015.09.096},
  ISSN                     = {0925-2312},
  Keywords                 = {Short Text},
  Owner                    = {20361362},
  Timestamp                = {2015.10.21},
  Url                      = {http://ac.els-cdn.com/S0925231215014502/1-s2.0-S0925231215014502-main.pdf?_tid=30e8e386-77be-11e5-bafa-00000aacb35f&acdnat=1445409625_60fc1cf534425eb6d5d0282d1e94a6f7}
}

@Incollection{Wang2014,
  Title                    = {Word Vector Modeling for Sentiment Analysis of Product Reviews},
  Author                   = {Wang, Yuan and Li, Zhaohui and Liu, Jie and He, Zhicheng and Huang, Yalou and Li, Dong},
  Booktitle                = {Natural Language Processing and Chinese Computing},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2014},
  Editor                   = {Zong, Chengqing and Nie, Jian-Yun and Zhao, Dongyan and Feng, Yansong},
  Pages                    = {168-180},
  Series                   = {Communications in Computer and Information Science},
  Volume                   = {496},

  Doi                      = {10.1007/978-3-662-45924-9_16},
  ISBN                     = {978-3-662-45923-2},
  Keywords                 = {Sentiment Analysis; Product Review; Neural Network Language Model; Semi-supervised Learning},
  Language                 = {English},
  Owner                    = {20361362},
  Timestamp                = {2015.06.19},
  Url                      = {http://dx.doi.org/10.1007/978-3-662-45924-9_16}
}

@Article{webber2006short,
  Title                    = {A short introduction to the Penn Discourse Tree Bank},
  Author                   = {Webber, Bonnie and Joshi, Aravind and Miltsakaki, Eleni and Prasad, Rashmi and Dinesh, Nikhil and Lee, Alan and Forbes, Katherine},
  Journal                  = {COPENHAGEN STUDIES IN LANGUAGE},
  Year                     = {2006},

  Annote                   = {Explains why we have binary trees.


base-NPs 
CCG combinational Categorical Grammar},
  Pages                    = {9},
  Volume                   = {32},

  Keywords                 = {corpora},
  Owner                    = {20361362},
  Publisher                = {Samfundslitteratur},
  Timestamp                = {2015.05.08},
  Url                      = {http://sydney.edu.au/engineering/it/~dvadas1/papers/vadas07_np_data_penn_treebank.pdf}
}

@Article{MemoryNN,
  Title                    = {Memory Networks},
  Author                   = {Jason Weston and
 Sumit Chopra and
 Antoine Bordes},
  Journal                  = {CoRR},
  Year                     = {2014},

  Annote                   = {A generalised model for a memory network.
Application in question answering.
Controls its own knowlege base with neurons.

Interestingly has a leaned tokeniser (segementor as it is called in text) as part of its write operation.

Example of expiment:
"Joe went to the kitchen. Fred went to the kitchen. Joe picked u
p the milk.
Joe travelled to the office. Joe left the milk. Joe went to the b
athroom.
Where is the milk now?
A: office
Where is Joe?
A: bathroom
Where was Joe before the office?
A: kitchen"},
  Volume                   = {abs/1410.3916},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/WestonCB14},
  Owner                    = {20361362},
  Timestamp                = {2015.04.12},
  Url                      = {http://arxiv.org/abs/1410.3916}
}

@Article{SOWEgen,
  Title                    = {A Two Step Process for Generating Sentences from the Sums of their Embeddings},
  Author                   = {White, Lyndon and Togneri,Roberto and Liu,Wei and Bennamoun, Mohammed},
  Year                     = {2016},
  Volume                   = {3 (March)},

  Abstract                 = {Converting a sentence to a meaningful vector representation has uses in many NLP tasks, however very few methods allow that representation to be restored to a human readable sentence. Being able to generate sentences from the vector representations is expected to open up many new applications. We introduce such a method for moving from sum of word embedding representations back to the original sentences. This is done using a greedy algorithm to convert the vector to a bag of words. We then show how the bag of words can be ordered using simple probabilistic language models to get back the sentence. To our knowledge this is the first work to demonstrate qualitatively the ability to reproduce text from a large corpus based on its sentence embeddings. 
As well as practical applications for sentence generation, the success of this method has theoretical implications on the degree of information maintained by the sum of embeddings representation. },
  Timestamp                = {2014.03.04}
}

@Inproceedings{White2015BOWgen,
  Title                    = {Generating Bags of Words from the Sums of their Word Embeddings},
  Author                   = {White, Lyndon and Togneri,Roberto and Liu,Wei and Bennamoun, Mohammed},
  Booktitle                = {17th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing)},
  Year                     = {2016},

  Abstract                 = {Many methods have been proposed to generate sentence vector representations, such as recursive neural networks, latent distributed memory models, and the simple sum of word embeddings (SOWE). However, very few methods demonstrate the ability to reverse the process -- recovering sentences from sentence embeddings. Amongst the many sentence embeddings, SOWE has been shown to maintain semantic meaning, so in this paper we introduce a method for moving from the SOWE representations back to the bag of words (BOW) for the original sentences. This is a part way step towards recovering the whole sentence and has useful theoretical and practical applications of its own. This is done using a greedy algorithm to convert the vector to a bag of words. To our knowledge this is the first such work. It demonstrates qualitatively the ability to recreate the words from a large corpus based on its sentence embeddings.

As well as practical applications for allowing classical information retrieval methods to be combined with more recent methods using the sums of word embeddings, the success of this method has theoretical implications on the degree of information maintained by the sum of embeddings representation. This lends some credence to the consideration of the SOWE as a dimensionality reduced, and meaning enhanced, data manifold for the bag of words. },
  Timestamp                = {2013.08.15}
}

@Inproceedings{White2015SentVecMeaning,
  Title                    = {How Well Sentence Embeddings Capture Meaning},
  Author                   = {White, Lyndon and Togneri, Roberto and Liu, Wei and Bennamoun, Mohammed},
  Booktitle                = {Proceedings of the 20th Australasian Document Computing Symposium},
  Year                     = {2015},
  Pages                    = {9:1--9:8},
  Publisher                = {ACM},
  Series                   = {ADCS '15},

  Abstract                 = {Several approaches for embedding a sentence into a vector space have been developed. However, it is unclear to what extent the sentence's position in the vector space reflects its semantic meaning, rather than other factors such as syntactic structure. Depending on the model used for the embeddings this will vary -- different models are suited for different down-stream applications. For applications such as machine translation and automated summarization, it is highly desirable to have semantic meaning encoded in the embedding. We consider this to be the quality of semantic localization for the model -- how well the sentences' meanings coincides with their embedding's position in vector space. Currently the semantic localization is assessed indirectly through practical benchmarks for specific applications. 

In this paper, we ground the semantic localization problem through a semantic classification task. The task is to classify sentences according to their meaning. A SVM with a linear kernel is used to perform the classification using the sentence vectors as its input. The sentences from subsets of two corpora, the Microsoft Research Paraphrase corpus and the Opinosis corpus, were partitioned according to their semantic equivalence. These partitions give the target classes for the classification task. Several existing models, including URAE, PV--DM and PV--DBOW, were assessed against a bag of words benchmark},
  Acmid                    = {2838932},
  Articleno                = {9},
  Doi                      = {10.1145/2838931.2838932},
  ISBN                     = {978-1-4503-4040-3},
  Keywords                 = {Semantic vector space representations, semantic consistency evaluation, sentence embeddings, word embeddings},
  Location                 = {Parramatta, NSW, Australia},
  Numpages                 = {8},
  Owner                    = {20361362},
  Timestamp                = {2015.11.19},
  Url                      = {http://doi.acm.org/10.1145/2838931.2838932}
}

@Article{raey,
  Title                    = {Annotating Expressions of Opinions and Emotions in Language},
  Author                   = {Wiebe, Janyce and Wilson, Theresa and Cardie, Claire},
  Journal                  = {Language Resources and Evaluation},
  Year                     = {2005},

  Annote                   = {Preferred citation for use of MPQA},
  Number                   = {2-3},
  Pages                    = {165-210},
  Volume                   = {39},

  Doi                      = {10.1007/s10579-005-7880-9},
  ISSN                     = {1574-020X},
  Keywords                 = {corpora},
  Language                 = {English},
  Owner                    = {20361362},
  Publisher                = {Kluwer Academic Publishers},
  Timestamp                = {2015.09.15},
  Url                      = {http://dx.doi.org/10.1007/s10579-005-7880-9}
}

@Article{wieting2015towards,
  Title                    = {Towards Universal Paraphrastic Sentence Embeddings},
  Author                   = {Wieting, John and Bansal, Mohit and Gimpel, Kevin and Livescu, Karen},
  Journal                  = {International Conference on Learning Representations (ICLR)},
  Year                     = {2016},

  Keywords                 = {sentence embeddings},
  Owner                    = {20361362},
  Timestamp                = {2016.06.09},
  Url                      = {http://ttic.uchicago.edu/~wieting/wieting2016ICLR.pdf}
}

@Article{RNC:RNC234,
  Title                    = {Robust filtering for a class of discrete-time uncertain nonlinear systems: An H8 approach},
  Author                   = {Xie, Lihua and De Souza, Carlos E. and Wang, Youyi},
  Journal                  = {International Journal of Robust and Nonlinear Control},
  Year                     = {1996},
  Number                   = {4},
  Pages                    = {297--312},
  Volume                   = {6},

  Abstract                 = {This paper deals with the H8 filtering problem for a class of discrete-time nonlinear systems with or without real time-varying parameter uncertainty and unknown initial state. For the case when there is no parametric uncertainty in the system, we are concerned with designing a nonlinear H8 filter such that the induced l2 norm of the mapping from the noise signal to the estimation error is within a specified bound. It is shown that this problem can be solved via one Riccati equation. We also consider the design of nonlinear filters which guarantee a prescribed H8 performance in the presence of parametric uncertainties. In this situation, a solution is obtained in terms of two Riccati equations.},
  Doi                      = {10.1002/(SICI)1099-1239(199605)6:4<297::AID-RNC234>3.0.CO;2-V},
  ISSN                     = {1099-1239},
  Keywords                 = {robust filtering, H8 filtering, uncertain systems, discrete-time systems, nonlinear systems},
  Owner                    = {20361362},
  Publisher                = {Wiley Subscription Services, Inc., A Wiley Company},
  Timestamp                = {2015.05.07},
  Url                      = {http://dx.doi.org/10.1002/(SICI)1099-1239(199605)6:4<297::AID-RNC234>3.0.CO;2-V}
}

@Article{1159121,
  Title                    = {Robust H infin; filtering for a class of discrete-time uncertain nonlinear systems with state delay},
  Author                   = {Shengyuan Xu},
  Journal                  = {Circuits and Systems I: Fundamental Theory and Applications, IEEE Transactions on},
  Year                     = {2002},

  Annote                   = {Robust $H/infty$ filters are of interst to me.
This however covers the more general case of Robut H \inftyu filters with time lag in state.},
  Month                    = {Dec},
  Number                   = {12},
  Pages                    = {1853-1859},
  Volume                   = {49},

  Doi                      = {10.1109/TCSI.2002.805736},
  ISSN                     = {1057-7122},
  Keywords                 = {Delay effects;Delay systems;Filtering;Gaussian noise;Nonlinear equations;Nonlinear systems;Riccati equations;Robustness;Sufficient conditions;Uncertain systems},
  Owner                    = {20361362},
  Timestamp                = {2015.05.07}
}

@Inproceedings{SemHuff,
  Title                    = {Optimize Hierarchical Softmax with Word Similarity Knowledge},
  Author                   = {Yang, Zhixuan and Ruan, Chong and Li, Caihua and Hu, Junfeng},
  Booktitle                = {17th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing)},
  Year                     = {2016},
  Annote                   = {username: cicling2016
password: hatem@konya

Reoroganise huffman tree, only within level.
So the unigram probability still determines what level a thing is on, but its semantic similarity determines where it is.
Putting similar things next to each other

This restructuring can bring heirachical softmax up to similar performance to negative sampling.

Possibly issues is that is seems like an NP hard problem to reorganise the tree.

Also only shown on chinese, which could mean the data was cherry picked.},

  Owner                    = {20361362},
  Timestamp                = {2016.07.08},
  Url                      = {http://www.cicling.org/2016/intranet/papers/paper_21.pdf}
}

@Article{yin2014exploration,
  author    = {Yin, Wenpeng and Sch{\"u}tze, Hinrich},
  title     = {An Exploration of Embeddings for Generalized Phrases},
  year      = {2014},
  pages     = {41},
  url       = {https://www.aclweb.org/anthology/P/P14/P14-3006.pdf},
  annote    = {Conserides various kinsd sof short phrase embeddings. This is the paper I am going to Cite for saying that terminology is inconsistent.},
  groups    = {KeyPapers},
  journal   = {ACL 2014},
  keywords  = {embeddings, terminology},
  owner     = {20361362},
  timestamp = {2015.06.24},
}

@Article{Yin2015,
  Title                    = {Learning Word Meta-Embeddings by Using Ensembles of Embedding Sets},
  Author                   = {Wenpeng Yin and Hinrich Schütze},
  Year                     = {2015},

  Annote                   = {considers using multiple word embeddings, via ensembles.

They uses SVD (~= PCA) to cut down dimentions.

Includes Giant table of evaluation of various word embeddings. and of ensembles of them
Including on Milokovs A is to B as C is to _ test.
In both syntactic and semeantic quality,

Really though ensembles only yeild magiunal improvement over word2vec.

Apparently they have reased the embeddings publically.},
  Month                    = aug,

  Abstract                 = {Word embeddings -- distributed representations for words -- in deep learning are beneficial for many tasks in Natural Language Processing (NLP). However, different embedding sets vary greatly in quality and characteristics of the captured semantics. Instead of relying on a more advanced algorithm for embedding learning, this paper proposes an ensemble approach of combining different public embedding sets with the aim of learning meta-embeddings. Experiments on word similarity and analogy tasks and on part-of-speech (POS) tagging show better performance of meta-embeddings compared to individual embedding sets. One advantage of meta-embeddings is that they have increased coverage of the vocabulary. We will release our meta-embeddings publicly.},
  Comments                 = {10 pages, 6 figures},
  Eprint                   = {1508.04257},
  Oai2identifier           = {1508.04257},
  Owner                    = {20361362},
  Timestamp                = {2015.09.01}
}

@Article{yogatamaextractive,
  Title                    = {Extractive Summarization by Maximizing Semantic Volume},
  Author                   = {Yogatama, Dani and Liu, Fei and Smith, Noah A},
  Journal                  = {Conference on Empirical Methods in Natural Language Processing},
  Year                     = {2015},

  Keywords                 = {summarization, vector-based_summarization},
  Owner                    = {20361362},
  Timestamp                = {2015.09.02},
  Url                      = {http://www.cs.cmu.edu/~dyogatam/papers/yogatama+liu+smith.emnlp2015.pdf}
}

@Article{younes1989parametric,
  Title                    = {Parametric inference for imperfectly observed Gibbsian fields},
  Author                   = {Younes, Laurent},
  Journal                  = {Probability theory and related fields},
  Year                     = {1989},

  Annote                   = {Part of the reason why \cite{2009DBM} became possible.
Not certasin why though.

A very heavy mathematical paper.
I have not got through it all yet.},
  Number                   = {4},
  Pages                    = {625--645},
  Volume                   = {82},

  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2015.04.17},
  Url                      = {http://www.researchgate.net/profile/Laurent_Younes/publication/227121666_Parametric_Inference_for_imperfectly_observed_Gibbsian_fields/links/00b7d5268355a8fdb7000000.pdf}
}

@Article{yu2013deep,
  Title                    = {The deep tensor neural network with applications to large vocabulary speech recognition},
  Author                   = {Yu, Dong and Deng, Li and Seide, Frank},
  Journal                  = {Audio, Speech, and Language Processing, IEEE Transactions on},
  Year                     = {2013},
  Number                   = {2},
  Pages                    = {388--396},
  Volume                   = {21},

  Keywords                 = {audio},
  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2015.09.15},
  Url                      = {http://www.msr-waypoint.net/pubs/177443/DTNN-TASLP2012-Proof.pdf}
}

@Article{TACL15CompVector,
  Title                    = {Learning Composition Models for Phrase Embeddings},
  Author                   = {Yu, Mo and Dredze, Mark},
  Journal                  = {Transactions of the Association for Computational Linguistics},
  Year                     = {2015},

  Annote                   = {New compositional vector sentence embeeding

Can be seen as simplifying Tensor RNNs to be diagonal matrixes,
or broadeding weighted sum to to elementwise weighted some.},
  Pages                    = {227--242},
  Volume                   = {3},

  Abstract                 = {Lexical embeddings can serve as useful representations for words for a variety of NLP tasks, but learning embeddings for phrases can be challenging. While separate embeddings are learned for each word, this is infeasible for every phrase. We construct phrase embeddings by learning how to compose word embeddings using features that capture phrase structure and context. We propose efficient unsupervised and task-specific learning objectives that scale our model to large datasets. We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths. We make the implementation of our model and the datasets available for general use.},
  ISSN                     = {2307-387X},
  Owner                    = {20361362},
  Timestamp                = {2016.01.18},
  Url                      = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/586}
}

@Inproceedings{yu2014improving,
  Title                    = {Improving lexical embeddings with semantic knowledge},
  Author                   = {Yu, Mo and Dredze, Mark},
  Booktitle                = {Association for Computational Linguistics (ACL)},
  Year                     = {2014},
  Annote                   = {Purpoise: to show that using suplimentary semantic iformation can improve word embeddings

Approach taken: Multimodal training of some so form, using second objective of making ssynomymn embeddings localised. Extends word2vec

Evaluation 3 task: Sematnic Nearness to synomnyms, Predicting human judgemnet, and language modeling

How to apply to may work: Combining this extension of word2vec onto doc2vec could imporve phrase semantics

Agortihms: is calleds Relation Constrained Model (RCM)

Further reading: the Paraphases Database},
  Pages                    = {545--550},

  Keywords                 = {RCM, word-embeddings},
  Owner                    = {20361362},
  Timestamp                = {2015.06.30},
  Url                      = {www.aclweb.org/anthology/P/P14/P14-2089.pdf}
}

@Inproceedings{zanzotto2010estimating,
  Title                    = {Estimating linear models for compositional distributional semantics},
  Author                   = {Zanzotto, Fabio Massimo and Korkontzelos, Ioannis and Fallucchi, Francesca and Manandhar, Suresh},
  Booktitle                = {Proceedings of the 23rd International Conference on Computational Linguistics},
  Year                     = {2010},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {1263--1271},

  Owner                    = {20361362},
  Timestamp                = {2015.11.19},
  Url                      = {https://art.torvergata.it/retrieve/handle/2108/32520/38263/2010_COLING_ZanzottoKorkontzelosFallucchiManandhar.pdf}
}

@Article{DBLP:journals/corr/abs-1212-5701,
  Title                    = {{ADADELTA:} An Adaptive Learning Rate Method},
  Author                   = {Matthew D. Zeiler},
  Journal                  = {CoRR},
  Year                     = {2012},

  Annote                   = {Improved over AdaGrad by not depending on a set global learning rate hyper parameter. As well as not having the gradient always decrease overtime.},
  Volume                   = {abs/1212.5701},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1212-5701},
  Keywords                 = {optimisation},
  Owner                    = {20361362},
  Timestamp                = {2015.05.14},
  Url                      = {http://arxiv.org/abs/1212.5701}
}

@Inproceedings{zhang2014BRAE,
  Title                    = {Bilingually-constrained Phrase Embeddings for Machine Translation},
  Author                   = {Jiajun Zhang and Shujie Liu and Mu Li and Ming Zhou and Chengqing Zong},
  Year                     = {2014},
  Publisher                = {ACL},

  Abstract                 = {<p>We propose Bilingually-constrained Recursive Auto-encoders (BRAE) to learn
 phrase embeddings (compact vector representations for phrases), which can
 distinguish the phrases in different semantic meanings. The BRAE is trained with
 the objective to minimize the semantic distance of translation equivalents and
 maximize the semantic distance of nontranslation pairs. The learned model can
 embed any phrase semantically in two languages and can transform semantic space
 in one language to the other. We evaluate the BRAE on two end-to-end SMT tasks
 (phrase table pruning and translation hypotheses reranking) which need to measure
 semantic similarity between a source phrase and its translation candidates.
 Extensive experiments show that the BRAE is spectacularly successful in these two
 tasks.</p>},
  Owner                    = {20361362},
  Timestamp                = {2015.09.09},
  Url                      = {http://anthology.aclweb.org/P/P14/P14-1011.pdf}
}

@Inproceedings{zhang2014chinese,
  Title                    = {Chinese poetry generation with recurrent neural networks},
  Author                   = {Zhang, Xingxing and Lapata, Mirella},
  Booktitle                = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  Year                     = {2014},
  Pages                    = {670--680},

  Abstract                 = {We propose a model for Chinese poem
generation based on recurrent neural net-
works which we argue is ideally suited to
capturing poetic content and form. Our
generator
jointly
performs content selec-
tion (“what to say”) and surface realization
(“how to say”) by learning representations
of individual characters, and their com-
binations into one or more lines as well
as how these mutually reinforce and con-
strain each other. Poem lines are gener-
ated incrementally by taking into account
the entire history of what has been gen-
erated so far rather than the limited hori-
zon imposed by the previous line or lexical
n
-grams. Experimental results show that
our model outperforms competitive Chi-
nese poetry generation systems using both
automatic and manual evaluation methods.},
  Keywords                 = {RNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://emnlp2014.org/papers/pdf/EMNLP2014074.pdf}
}

@Article{DBLP:journals/corr/ZhangL15,
  Title                    = {Text Understanding from Scratch},
  Author                   = {Xiang Zhang and
 Yann LeCun},
  Journal                  = {CoRR},
  Year                     = {2015},

  Annote                   = {This is the paper where embeddings are learn from the character level.
Via convolutoional neural network with many layers.},
  Volume                   = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/ZhangL15},
  Keywords                 = {phrase-embeddings},
  Owner                    = {20361362},
  Timestamp                = {2015.08.31},
  Url                      = {http://arxiv.org/abs/1502.01710}
}

@Article{ZhangWordOrderSyntax,
  Title                    = {Discriminative Syntax-based Word Ordering for Text Generation},
  Author                   = {Zhang, Yue and Clark, Stephen},
  Journal                  = {Comput. Linguist.},
  Year                     = {2015},
  Month                    = sep,
  Number                   = {3},
  Pages                    = {503--538},
  Volume                   = {41},

  Acmid                    = {2847427},
  Address                  = {Cambridge, MA, USA},
  Doi                      = {10.1162/COLI_a_00229},
  ISSN                     = {0891-2017},
  Issue_date               = {September 2015},
  Keywords                 = {word ordering},
  Numpages                 = {36},
  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2016.06.07},
  Url                      = {http://delivery.acm.org/10.1145/2850000/2847427/coli_a_00229.pdf?ip=130.95.149.38&id=2847427&acc=PUBLIC&key=65D80644F295BC0D.05ACB6DAA5D75F76.4D4702B0C3E38B35.4D4702B0C3E38B35&CFID=626603773&CFTOKEN=69370806&__acm__=1465292119_1dd1318ddcf7f6ed98f5d82aa72b92ea}
}

@Inproceedings{moviebook,
  Title                    = {Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books},
  Author                   = {Yukun Zhu and Ryan Kiros and Richard Zemel and Ruslan Salakhutdinov and Raquel Urtasun and Antonio Torralba and Sanja Fidler},
  Booktitle                = {arXiv preprint arXiv:1506.06724},
  Year                     = {2015},
  Annote                   = {This is the preferred citatin for},

  Owner                    = {20361362},
  Timestamp                = {2016.02.09},
  Url                      = {http://arxiv.org/abs/1506.06724}
}

@Book{17003720060101,
  Title                    = {Semi-Supervised Learning.},
  Author                   = {Zien, Alexander and Schölkopf, Bernhard and Chapelle, Olivier},
  Publisher                = {The MIT Press},
  Year                     = {2006},
  Series                   = {Adaptive Computation and Machine Learning},

  Abstract                 = {In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research.Semi-Supervised Learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assum},
  ISBN                     = {9780262033589},
  Owner                    = {20361362},
  Timestamp                = {2016.10.19},
  Url                      = {http://search.ebscohost.com.ezproxy.library.uwa.edu.au/login.aspx?direct=true&db=nlebk&AN=170037&site=ehost-live}
}

@Book{zipf1949human,
  Title                    = {Human behavior and the principle of least effort: an introduction to human ecology},
  Author                   = {Zipf, G.K.},
  Publisher                = {Addison-Wesley Press},
  Year                     = {1949},

  Lccn                     = {49007787},
  Owner                    = {20361362},
  Timestamp                = {2016.10.13},
  Url                      = {https://books.google.com.au/books?id=1tx9AAAAIAAJ}
}

@Article{zipf1945meaning,
  Title                    = {The meaning-frequency relationship of words},
  Author                   = {Zipf, George Kingsley},
  Journal                  = {The Journal of general psychology},
  Year                     = {1945},
  Number                   = {2},
  Pages                    = {251--256},
  Volume                   = {33},

  Owner                    = {20361362},
  Publisher                = {Taylor \& Francis},
  Timestamp                = {2016.10.04},
  Url                      = {http://search.proquest.com/docview/1290515471?accountid=14681}
}

@Inproceedings{zou2013bilingual,
  Title                    = {Bilingual Word Embeddings for Phrase-Based Machine Translation.},
  Author                   = {Zou, Will Y and Socher, Richard and Cer, Daniel M and Manning, Christopher D},
  Booktitle                = {EMNLP},
  Year                     = {2013},
  Annote                   = {Basic idea is to create bilingual word embeddings, (from unsupervised data??)
then use them with a standard phrasal translator.


Makes use of /cite{liang2006alignment} to align the Embeddings.
Normal Embedding is done 
which are then},
  Pages                    = {1393--1398},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://nlp.stanford.edu/pubs/emnlp2013_ZouSocherCerManning.pdf}
}

@Article{zhang2002shape,
  author    = {Zhang, Dengsheng and Lu, Guojun},
  title     = {Shape-based image retrieval using generic Fourier descriptor},
  year      = {2002},
  volume    = {17},
  number    = {10},
  pages     = {825--848},
  url       = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.6312&rep=rep1&type=pdf},
  annote    = {Very details about ways of representing shapes. Supposed to be what inspired \cite{2016arXiv160603821M}.Possibly a good souce to read about representations including angle.},
  journal   = {Signal Processing: Image Communication},
  owner     = {20361362},
  publisher = {Elsevier},
  timestamp = {2017.01.13},
}

@Article{mcmahan2015bayesian,
  author    = {McMahan, Brian and Stone, Matthew},
  title     = {A Bayesian model of grounded color semantics},
  year      = {2015},
  volume    = {3},
  pages     = {103--115},
  url       = {https://tacl.cs.columbia.edu/ojs/index.php/tacl/article/download/276/114},
  annote    = {Basically an approximation to the histogram of color decritions response over the HSV space, using fuzzy squares, fitted using MCMC. There are 829 color descriptions, and for each point, each one is assigned a probability, using these overlapping fuzzy regions. Each color description is parameterised by Applicability $$P(k^{true} | x)$$. and Availability which is $$P(k^{said} | k^{true})$$ here $$k^{true}$$ is the predicate that $k$ is a (one of many) correct names for the color $$x$$, and $$k^{said}$$ is the predicate that $$k$$ was the response given by the user.},
  journal   = {Transactions of the Association for Computational Linguistics},
  keywords  = {color},
  owner     = {20361362},
  timestamp = {2017.01.13},
}

@Article{Moroetal:14tacl,
  author    = {Andrea Moro and Alessandro Raganato and Roberto Navigli},
  title     = {{Entity Linking meets Word Sense Disambiguation: a Unified Approach}},
  year      = {2014},
  volume    = {2},
  pages     = {231-244},
  url       = {http://wwwusers.di.uniroma1.it/~navigli/pubs/TACL_2014_Babelfy.pdf},
  journal   = {Transactions of the Association for Computational Linguistics (TACL)},
  owner     = {20361362},
  timestamp = {2017.02.20},
}

@InProceedings{ali2015two,
  author       = {Ali, Chedi Bechikh and Wang, Rui and Haddad, Hatem},
  title        = {A Two-Level Keyphrase Extraction Approach},
  booktitle    = {International Conference on Intelligent Text Processing and Computational Linguistics},
  year         = {2015},
  organization = {Springer},
  pages        = {390--401},
  url          = {https://www.researchgate.net/profile/Hatem_Haddad/publication/275967328_A_Two-Level_Keyphrase_Extraction_Approach/links/55c08bec08aed621de13c1f0.pdf},
  owner        = {20361362},
  timestamp    = {2017.02.20},
}

@InProceedings{White2016SOWE2Sent,
  author    = {White, Lyndon and Togneri,Roberto and Liu,Wei and Bennamoun, Mohammed},
  title     = {Modelling Sentence Generation from Sum of Word Embedding Vectors as a Mixed Integer Programming Problem},
  booktitle = {IEEE International Conference on Data Mining: High Dimensional Data Mining Workshop (ICDM: HDM)},
  year      = {2016},
  doi       = {10.1109/ICDMW.2016.0113},
  url       = {http://white.ucc.asn.au/publications/White2016SOWE2Sent.pdf},
  owner     = {20361362},
  timestamp = {2017.02.28},
}

@Comment{jabref-meta: databaseType:biblatex;}

@Comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:to_read\;0\;;
1 ExplicitGroup:KeyPapers\;0\;;
1 ExplicitGroup:Automatically created groups\;2\;;
2 KeywordGroup:Amr\;0\;keywords\;Amr\;0\;0\;;
2 KeywordGroup:Competition\;0\;keywords\;Competition\;0\;0\;;
2 KeywordGroup:Computational complexity\\\;learning (artificial intelligence)\\\;neural nets\\\;speech recognition\\\;4-gram model\\\;computational complexity\\\;english broadcast news speech recognition task\\\;hash-based implementation\\\;large scale neural network language model training\\\;maximum entropy model\\\;training data\\\;word error rate\\\;artificial neural networks\\\;computational complexity\\\;computational modeling\\\;data models\\\;entropy\\\;training\\\;training data\;0\;keywords\;Computational complexity\\\;learning (artificial intelligence)\\\;neural nets\\\;speech recognition\\\;4-gram model\\\;computational complexity\\\;english broadcast news speech recognition task\\\;hash-based implementation\\\;large scale neural network language model training\\\;maximum entropy model\\\;training data\\\;word error rate\\\;artificial neural networks\\\;computational complexity\\\;computational modeling\\\;data models\\\;entropy\\\;training\\\;training data\;0\;0\;;
2 KeywordGroup:Computer vision\\\;estimation theory\\\;image segmentation\\\;nonparametric statistics\\\;pattern clustering\\\;smoothing methods\\\;nadaraya-watson estimator\\\;algorithm performance\\\;analysis resolution\\\;arbitrarily shaped cluster delineation\\\;color images\\\;complex multimodal feature space\\\;computational module\\\;convergence\\\;density function\\\;density modes detection\\\;discontinuity-preserving image smoothing\\\;discrete data\\\;gray-level images\\\;image segmentation\\\;kernel regression\\\;location estimation\\\;low-level vision algorithms\\\;mean shift\\\;nearest stationary point\\\;nonparametric technique\\\;pattern recognition procedure\\\;recursive mean shift procedure\\\;robust m-estimators\\\;robust feature space analysis\\\;user-set parameter\\\;convergence\\\;density functional theory\\\;image analysis\\\;image color analysis\\\;image resolution\\\;image segmentation\\\;kernel\\\;pattern recognition\\\;robustness\\\;smoothing methods\;0\;keywords\;Computer vision\\\;estimation theory\\\;image segmentation\\\;nonparametric statistics\\\;pattern clustering\\\;smoothing methods\\\;nadaraya-watson estimator\\\;algorithm performance\\\;analysis resolution\\\;arbitrarily shaped cluster delineation\\\;color images\\\;complex multimodal feature space\\\;computational module\\\;convergence\\\;density function\\\;density modes detection\\\;discontinuity-preserving image smoothing\\\;discrete data\\\;gray-level images\\\;image segmentation\\\;kernel regression\\\;location estimation\\\;low-level vision algorithms\\\;mean shift\\\;nearest stationary point\\\;nonparametric technique\\\;pattern recognition procedure\\\;recursive mean shift procedure\\\;robust m-estimators\\\;robust feature space analysis\\\;user-set parameter\\\;convergence\\\;density functional theory\\\;image analysis\\\;image color analysis\\\;image resolution\\\;image segmentation\\\;kernel\\\;pattern recognition\\\;robustness\\\;smoothing methods\;0\;0\;;
2 KeywordGroup:Corpora\;0\;keywords\;Corpora\;0\;0\;;
2 KeywordGroup:Dataset\;0\;keywords\;Dataset\;0\;0\;;
2 KeywordGroup:Delay effects\\\;delay systems\\\;filtering\\\;gaussian noise\\\;nonlinear equations\\\;nonlinear systems\\\;riccati equations\\\;robustness\\\;sufficient conditions\\\;uncertain systems\;0\;keywords\;Delay effects\\\;delay systems\\\;filtering\\\;gaussian noise\\\;nonlinear equations\\\;nonlinear systems\\\;riccati equations\\\;robustness\\\;sufficient conditions\\\;uncertain systems\;0\;0\;;
2 KeywordGroup:Discrete-time systems\;0\;keywords\;Discrete-time systems\;0\;0\;;
2 KeywordGroup:Embeddings\;0\;keywords\;Embeddings\;0\;0\;;
2 KeywordGroup:Entity linking\;0\;keywords\;Entity linking\;0\;0\;;
2 KeywordGroup:Filtering\;0\;keywords\;Filtering\;0\;0\;;
2 KeywordGroup:Gaussian processes\\\;acoustic signal processing\\\;hidden markov models\\\;mixture models\\\;neural nets\\\;speech recognition\\\;asr\\\;dnn\\\;gmm\\\;gaussian mixture models\\\;hmm\\\;acoustic features\\\;acoustic modeling\\\;acoustic models\\\;automatic speech recognition\\\;burgeoning area\\\;deep learning\\\;deep neural networks\\\;hidden markov models\\\;high-level symbolic inputs\\\;human speech production\\\;intermediate acoustic feature sequences\\\;low-level speech waveforms\\\;parametric speech generation\\\;statistical parametric approach\\\;acoustic signal detection\\\;gaussian mixture models\\\;hidden markov models\\\;speech processing\\\;speech recognition\\\;speech synthesis\\\;vocoders\;0\;keywords\;Gaussian processes\\\;acoustic signal processing\\\;hidden markov models\\\;mixture models\\\;neural nets\\\;speech recognition\\\;asr\\\;dnn\\\;gmm\\\;gaussian mixture models\\\;hmm\\\;acoustic features\\\;acoustic modeling\\\;acoustic models\\\;automatic speech recognition\\\;burgeoning area\\\;deep learning\\\;deep neural networks\\\;hidden markov models\\\;high-level symbolic inputs\\\;human speech production\\\;intermediate acoustic feature sequences\\\;low-level speech waveforms\\\;parametric speech generation\\\;statistical parametric approach\\\;acoustic signal detection\\\;gaussian mixture models\\\;hidden markov models\\\;speech processing\\\;speech recognition\\\;speech synthesis\\\;vocoders\;0\;0\;;
2 KeywordGroup:Hidden markov models\\\;speech processing\\\;baseline system\\\;extractive speech summarization\\\;nongenerative probabilistic framework\\\;rhetorical information\\\;rhetorical-state hidden markov models\\\;automatic speech recognition\\\;data mining\\\;decoding\\\;feature extraction\\\;hidden markov models\\\;humans\\\;natural languages\\\;support vector machine classification\\\;support vector machines\\\;text recognition\\\;hidden markov models\\\;rhetorical information\\\;speech features\\\;spoken document summarization\;0\;keywords\;Hidden markov models\\\;speech processing\\\;baseline system\\\;extractive speech summarization\\\;nongenerative probabilistic framework\\\;rhetorical information\\\;rhetorical-state hidden markov models\\\;automatic speech recognition\\\;data mining\\\;decoding\\\;feature extraction\\\;hidden markov models\\\;humans\\\;natural languages\\\;support vector machine classification\\\;support vector machines\\\;text recognition\\\;hidden markov models\\\;rhetorical information\\\;speech features\\\;spoken document summarization\;0\;0\;;
2 KeywordGroup:H8 filtering\;0\;keywords\;H8 filtering\;0\;0\;;
2 KeywordGroup:H8 optimisation\\\;kalman filters\\\;riccati equations\\\;difference equations\\\;discrete time filters\\\;error analysis\\\;filtering theory\\\;game theory\\\;noise\\\;parameter estimation\\\;signal processing\\\;kalman filter\\\;amplification\\\;difference riccati equation\\\;discrete h8 filter design\\\;estimation error signals\\\;exogenous inputs\\\;finite energy signals\\\;finite-horizon discrete h8 filter\\\;game theory\\\;hostile noise signals\\\;linear quadratic game\\\;modified wiener filter\\\;system initial condition\\\;unknown statistics\\\;estimation error\\\;filtering theory\\\;game theory\\\;h infinity control\\\;noise measurement\\\;nonlinear filters\\\;riccati equations\\\;signal design\\\;signal processing\\\;statistics\;0\;keywords\;H8 optimisation\\\;kalman filters\\\;riccati equations\\\;difference equations\\\;discrete time filters\\\;error analysis\\\;filtering theory\\\;game theory\\\;noise\\\;parameter estimation\\\;signal processing\\\;kalman filter\\\;amplification\\\;difference riccati equation\\\;discrete h8 filter design\\\;estimation error signals\\\;exogenous inputs\\\;finite energy signals\\\;finite-horizon discrete h8 filter\\\;game theory\\\;hostile noise signals\\\;linear quadratic game\\\;modified wiener filter\\\;system initial condition\\\;unknown statistics\\\;estimation error\\\;filtering theory\\\;game theory\\\;h infinity control\\\;noise measurement\\\;nonlinear filters\\\;riccati equations\\\;signal design\\\;signal processing\\\;statistics\;0\;0\;;
2 KeywordGroup:Information extraction\;0\;keywords\;Information extraction\;0\;0\;;
2 KeywordGroup:Machine learning\\\; reasoning\\\; recursive networks\;0\;keywords\;Machine learning\\\; reasoning\\\; recursive networks\;0\;0\;;
2 KeywordGroup:Markov processes\\\;speech recognition\\\;balls-in-urns system\\\;coin-tossing\\\;discrete markov chains\\\;ergodic models\\\;hidden markov models\\\;hidden states\\\;left-right models\\\;probabilistic function\\\;speech recognition\\\;distortion\\\;hidden markov models\\\;mathematical model\\\;multiple signal classification\\\;signal processing\\\;speech recognition\\\;statistical analysis\\\;stochastic processes\\\;temperature measurement\\\;tutorial\;0\;keywords\;Markov processes\\\;speech recognition\\\;balls-in-urns system\\\;coin-tossing\\\;discrete markov chains\\\;ergodic models\\\;hidden markov models\\\;hidden states\\\;left-right models\\\;probabilistic function\\\;speech recognition\\\;distortion\\\;hidden markov models\\\;mathematical model\\\;multiple signal classification\\\;signal processing\\\;speech recognition\\\;statistical analysis\\\;stochastic processes\\\;temperature measurement\\\;tutorial\;0\;0\;;
2 KeywordGroup:Max-entropy\;0\;keywords\;Max-entropy\;0\;0\;;
2 KeywordGroup:Mt\;0\;keywords\;Mt\;0\;0\;;
2 KeywordGroup:Named entity recognition\;0\;keywords\;Named entity recognition\;0\;0\;;
2 KeywordGroup:Nlp\;0\;keywords\;Nlp\;0\;0\;;
2 KeywordGroup:Nonlinear systems\;0\;keywords\;Nonlinear systems\;0\;0\;;
2 KeywordGroup:Optimisation\;0\;keywords\;Optimisation\;0\;0\;;
2 KeywordGroup:Paraphrasing\;0\;keywords\;Paraphrasing\;0\;0\;;
2 KeywordGroup:Phrase-embeddings\;0\;keywords\;Phrase-embeddings\;0\;0\;;
2 KeywordGroup:Rae\;0\;keywords\;Rae\;0\;0\;;
2 KeywordGroup:Rcm\;0\;keywords\;Rcm\;0\;0\;;
2 KeywordGroup:Rnn\;0\;keywords\;Rnn\;0\;0\;;
2 KeywordGroup:Robust filtering\;0\;keywords\;Robust filtering\;0\;0\;;
2 KeywordGroup:Rvnn\;0\;keywords\;Rvnn\;0\;0\;;
2 KeywordGroup:Sentiment analysis\\\; product review\\\; neural network language model\\\; semi-supervised learning\;0\;keywords\;Sentiment analysis\\\; product review\\\; neural network language model\\\; semi-supervised learning\;0\;0\;;
2 KeywordGroup:Software\;0\;keywords\;Software\;0\;0\;;
2 KeywordGroup:Summarization\;0\;keywords\;Summarization\;0\;0\;;
2 KeywordGroup:Terminology\;0\;keywords\;Terminology\;0\;0\;;
2 KeywordGroup:Tools\;0\;keywords\;Tools\;0\;0\;;
2 KeywordGroup:Uncertain systems\;0\;keywords\;Uncertain systems\;0\;0\;;
2 KeywordGroup:Urae\;0\;keywords\;Urae\;0\;0\;;
2 KeywordGroup:Word-embeddings\;0\;keywords\;Word-embeddings\;0\;0\;;
2 KeywordGroup:Word-sense-disambiguation\;0\;keywords\;Word-sense-disambiguation\;0\;0\;;
2 KeywordGroup:Wordnet\;0\;keywords\;Wordnet\;0\;0\;;
}

@Comment{jabref-meta: groupsversion:3;}
