% Encoding: UTF-8


@Inproceedings{CRPITV165P53-59,
  Title                    = {A Video-oriented Knowledge Collection and Accumulation System with Associated Multimedia Resource Integration},
  Author                   = {Nakano, R. and Kiyoki, Y.},
  Booktitle                = {11th Asia-Pacific Conference on Conceptual Modelling (APCCM 2015)},
  Year                     = {2015},

  Address                  = {Sydney, Australia},
  Annote                   = {This a example of a papar from the conference I am submitting to},
  Editor                   = {Saeki, M. and Kohler, H.},
  Pages                    = {53-59},
  Publisher                = {ACS},
  Series                   = {CRPIT},
  Volume                   = {165},

  Crossref                 = {ol},
  Owner                    = {20361362},
  Timestamp                = {2015.07.09},
  Url                      = { http://crpit.com/confpapers/CRPITV165Nakano.pdf }
}

@Misc{tensorflow2015-whitepaper,
  Title                    = { {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  Author                   = {
 Mart\'{\i}n~Abadi and
 Ashish~Agarwal and
 Paul~Barham and
 Eugene~Brevdo and
 Zhifeng~Chen and
 Craig~Citro and
 Greg~S.~Corrado and
 Andy~Davis and
 Jeffrey~Dean and
 Matthieu~Devin and
 Sanjay~Ghemawat and
 Ian~Goodfellow and
 Andrew~Harp and
 Geoffrey~Irving and
 Michael~Isard and
 Yangqing Jia and
 Rafal~Jozefowicz and
 Lukasz~Kaiser and
 Manjunath~Kudlur and
 Josh~Levenberg and
 Dan~Man\'{e} and
 Rajat~Monga and
 Sherry~Moore and
 Derek~Murray and
 Chris~Olah and
 Mike~Schuster and
 Jonathon~Shlens and
 Benoit~Steiner and
 Ilya~Sutskever and
 Kunal~Talwar and
 Paul~Tucker and
 Vincent~Vanhoucke and
 Vijay~Vasudevan and
 Fernanda~Vi\'{e}gas and
 Oriol~Vinyals and
 Pete~Warden and
 Martin~Wattenberg and
 Martin~Wicke and
 Yuan~Yu and
 Xiaoqiang~Zheng},
  Note                     = {Software available from tensorflow.org},
  Year                     = {2015},

  Owner                    = {20361362},
  Timestamp                = {2017.06.24},
  Url                      = {http://tensorflow.org/}
}

@Inproceedings{agirre2006,
  Title                    = {Evaluating and optimizing the parameters of an unsupervised graph-based WSD algorithm},
  Author                   = {Agirre, Eneko and Mart{\'\i}nez, David and De Lacalle, Oier L{\'o}pez and Soroa, Aitor},
  Booktitle                = {Proceedings of the first workshop on graph based methods for natural language processing},
  Year                     = {2006},
  Annote                   = {For $H_{w}=\{h_{w,1},h_{w,2},...\}$ the set of induced senses for
the word $w$ \\

And for $S_{w}=\{s_{w,1},s_{w,2},...\}$the set of standard senses
of the word $w$ \\

A mapping matrix $$M_{w}=\{P(s_{w,j}|h_{w,i})\}$$ for $h_{w,i}\in H_{w}$
and $s_{w,j}\in S_{w}$ is constructed by using the Unsupervised
WSD on a a labelled corpus, and by calculating $P(s_{w,j}|h_{w,i})$
though counting how often a word ($w$) annotated with sense $s_{w,j}$
is assigned to the induced sense $h_{w,i}$ (and applying defintion
of conditional probability) -- that is to say Hard (winner takes all)
descsions are made during the unsupervised WSD, and then counted.

With this mapping Mapping matrix defined we can transform

$\bar{h}_{w}=[\bar{h}_{w1},\bar{h}_{w_{2}},...]$ the set of scores
of induced senses for a the word $w$

in to $\bar{s}_{w}=[\bar{s}_{w1},\bar{s}_{w_{2}},...]$

by $\bar{s}_{w}=M\bar{h}_{w}$

As the corpus for evaluation they used the Semeval 2 english lexical sample (S2LS).
Using the training split for Mapping, and the training+test for inducting the senses with hyperlex.



This is the method used by SemEval 2007 \cite{SemEval2007WSIandWSD} for evaluating WSI methods on WSD

Sidenote: Begona Altuna, who I met at CICLing 2016, has authored a paper with several of the authors of this paper, and is in the same lab.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {89--96},

  Keywords                 = {WSD},
  Owner                    = {20361362},
  Timestamp                = {2016.08.10},
  Url                      = {https://www.researchgate.net/profile/David_Martinez14/publication/216591548_Evaluating_and_optimizing_the_parameters_of_an_unsupervised_graph-based_WSD_algorithm/links/0fcfd508f21b273d0e000000.pdf}
}

@Inproceedings{SemEval2007WSIandWSD,
  Title                    = {Semeval-2007 Task 02: Evaluating Word Sense Induction and Discrimination Systems},
  Author                   = {Agirre, Eneko and Soroa, Aitor},
  Booktitle                = {Proceedings of the 4th International Workshop on Semantic Evaluations},
  Year                     = {2007},

  Address                  = {Stroudsburg, PA, USA},
  Annote                   = {Note: uses the same corpus as SemEval-2007 task 17 :English lexical
sample subtask", which has OntoNotes senses.},
  Pages                    = {7--12},
  Publisher                = {Association for Computational Linguistics},
  Series                   = {SemEval '07},

  Acmid                    = {1621476},
  Location                 = {Prague, Czech Republic},
  Numpages                 = {6},
  Owner                    = {20361362},
  Timestamp                = {2016.08.10},
  Url                      = {http://dl.acm.org/citation.cfm?id=1621474.1621476}
}

@Article{aherne1998bhattacharyya,
  Title                    = {The Bhattacharyya metric as an absolute similarity measure for frequency coded data},
  Author                   = {Aherne, Frank J and Thacker, Neil A and Rockett, Peter I},
  Journal                  = {Kybernetika},
  Year                     = {1998},

  Annote                   = {This is about a a distance measure for differentiating distributions},
  Number                   = {4},
  Pages                    = {363--368},
  Volume                   = {34},

  Owner                    = {20361362},
  Publisher                = {Institute of Information Theory and Automation AS CR},
  Timestamp                = {2015.07.29},
  Url                      = {http://www.tina-vision.net/docs/memos/1997-001.pdf}
}

@Inproceedings{Ali2015,
  Title                    = {A Two-Level Keyphrase Extraction Approach},
  Author                   = {Ali, Chedi Bechikh and Wang, Rui and Haddad, Hatem},
  Booktitle                = {International Conference on Intelligent Text Processing and Computational Linguistics},
  Year                     = {2015},
  Organization             = {Springer},
  Pages                    = {390--401},

  Owner                    = {20361362},
  Timestamp                = {2017.02.20},
  Url                      = {https://www.researchgate.net/profile/Hatem_Haddad/publication/275967328_A_Two-Level_Keyphrase_Extraction_Approach/links/55c08bec08aed621de13c1f0.pdf}
}

@Article{1100034,
  Title                    = {Nonlinear Bayesian estimation using Gaussian sum approximations},
  Author                   = {D. Alspach and H. Sorenson},
  Journal                  = {IEEE Transactions on Automatic Control},
  Year                     = {1972},

  Annote                   = {GMMsdra},
  Month                    = {Aug},
  Number                   = {4},
  Pages                    = {439-448},
  Volume                   = {17},

  Abstract                 = {Knowledge of the probability density function of the state conditioned on all available measurement data provides the most complete possible description of the state, and from this density any of the common types of estimates (e.g., minimum variance or maximum a posteriori) can be determined. Except in the linear Gaussian case, it is extremely difficult to determine this density function. In this paper an approximation that permits the explicit calculation of the a posteriori density from the Bayesian recursion relations is discussed and applied to the solution of the nonlinear filtering problem. In particular, it is noted that a weighted sum of Gaussian probability density functions can be used to approximate arbitrarily closely another density function. This representation provides the basis for procedure that is developed and discussed.},
  Doi                      = {10.1109/TAC.1972.1100034},
  ISSN                     = {0018-9286},
  Keywords                 = {Approximation methods;Bayes procedures;Nonlinear estimation;Nonlinear systems, stochastic discrete-time;State estimation;Bayesian methods;Density functional theory;Density measurement;Difference equations;Filtering;Gaussian noise;Nonlinear equations;Nonlinear filters;Probability density function;State estimation},
  Owner                    = {20361362},
  Timestamp                = {2017.09.19}
}

@Article{2017arXivKernalMixtureNetworks,
  Title                    = {{The Kernel Mixture Network: A Nonparametric Method for Conditional Density Estimation of Continuous Random Variables}},
  Author                   = {{Ambrogioni}, L. and {G{\"u}{\c c}l{\"u}}, U. and {van Gerven}, M.~A.~J. and {Maris}, E.},
  Journal                  = {ArXiv e-prints},
  Year                     = {2017},

  Annote                   = {A bit similar to \cite{specht1991general}.
Each training point detines the center of a gaussian (or other distributio
n).
Trick is the weights mixture component is an output of a neural network.},
  Month                    = may,

  Adsnote                  = {Provided by the SAO/NASA Astrophysics Data System},
  Adsurl                   = {http://adsabs.harvard.edu/abs/2017arXiv170507111A},
  Archiveprefix            = {arXiv},
  Eprint                   = {1705.07111},
  Keywords                 = {Statistics - Machine Learning},
  Owner                    = {20361362},
  Primaryclass             = {stat.ML},
  Timestamp                = {2017.08.18},
  Url                      = {https://arxiv.org/pdf/1705.07111.pdf}
}

@Article{androutsopoulos2010survey,
  Title                    = {A survey of paraphrasing and textual entailment methods},
  Author                   = {Androutsopoulos, Ion and Malakasiotis, Prodromos},
  Journal                  = {Journal of Artificial Intelligence Research},
  Year                     = {2010},

  Annote                   = {Long Journal Paper.

Gives bidirectional entailment defintion of semantic equivelence / paraphrase.},
  Pages                    = {135--187},

  Abstract                 = {Paraphrasing methods recognize, generate, or extract phrases, sentences, or longer natural lan-
guage expressions that convey almost the same information. Textual entailment methods, on the
other hand, recognize, generate, or extract pairs of natural language expressions, such that a human
who reads (and trusts) the first element of a pair would most likely infer that the other element is
also true. Paraphrasing can be seen as bidirectional textual entailment and methods from the two
areas are often similar. Both kinds of methods are useful, at least in principle, in a wide range of
natural language processing applications, including question answering, summarization, text gener-
ation, and machine translation. We summarize key ideas from the two areas by considering in turn
recognition, generation, and extraction methods, also pointing to prominent articles and resources.},
  Keywords                 = {paraphrasing},
  Owner                    = {20361362},
  Timestamp                = {2015.08.03},
  Url                      = {https://www.jair.org/media/2985/live-2985-5001-jair.pdf}
}

@Electronic{SCOWL,
  Title                    = {SCOWL (Spell Checker Orientated Word Lists)},
  Annote                   = {This is citing for the word lists in included in linux distributions.},
  Author                   = {Kevub Atkinson},
  Year                     = {2011},

  Owner                    = {20361362},
  Timestamp                = {2015.07.14}
}

@Inproceedings{1988CNN,
  Title                    = {An artificial neural network for spatio-temporal bipolar patterns: Application to phoneme classification},
  Author                   = {Atlas, Les E and Homma, Toshiteru and Marks II, Robert J},
  Booktitle                = {Proc. Neural Information Processing Systems (NIPS)},
  Year                     = {1988},
  Annote                   = {The actual first CNN paper, prior to Lecunn's work},
  Pages                    = {31},

  Owner                    = {20361362},
  Timestamp                = {2015.09.09},
  Url                      = {http://goo.gl/4OApDV}
}

@Article{DBLP:journals/corr/AtzmonBKGC16,
  Title                    = {Learning to generalize to new compositions in image understanding},
  Author                   = {Yuval Atzmon and Jonathan Berant and Vahid Kezami and Amir Globerson and Gal Chechik},
  Journal                  = {CoRR},
  Year                     = {2016},
  Volume                   = {abs/1608.07639},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/AtzmonBKGC16},
  Owner                    = {20361362},
  Timestamp                = {Wed, 07 Jun 2017 14:42:12 +0200},
  Url                      = {http://arxiv.org/abs/1608.07639}
}

@Inproceedings{Bach:2016crossdomainsentiment,
  Title                    = {Cross-domain Sentiment Classification with Word Embeddings and Canonical Correlation Analysis},
  Author                   = {Bach, Ngo Xuan and Hai, Vu Thanh and Phuong, Tu Minh},
  Booktitle                = {Proceedings of the Seventh Symposium on Information and Communication Technology},
  Year                     = {2016},

  Address                  = {New York, NY, USA},
  Annote                   = {Using CCA on word counts and also word2vec for transferring knowledge about sentiment analysis between a source domain with labels and a target domain without labels.

I don't see how this works.
Using CCA on two domain specific word2vec sets, to perform zero-shot transfer learning makes sense.
But I do not think they did this.},
  Pages                    = {159--166},
  Publisher                = {ACM},
  Series                   = {SoICT '16},

  Acmid                    = {3011104},
  Doi                      = {10.1145/3011077.3011104},
  ISBN                     = {978-1-4503-4815-7},
  Keywords                 = {canonical correlation analysis, cross-domain sentiment classification, word embeddings},
  Location                 = {Ho Chi Minh City, Vietnam},
  Numpages                 = {8},
  Owner                    = {20361362},
  Timestamp                = {2017.08.25},
  Url                      = {http://doi.acm.org/10.1145/3011077.3011104}
}

@Article{bacharoglou2010approximation,
  Title                    = {Approximation of probability distributions by convex mixtures of gaussian measures},
  Author                   = {Bacharoglou, Athanassia},
  Journal                  = {Proceedings of the American Mathematical Society},
  Year                     = {2010},
  Number                   = {7},
  Pages                    = {2619--2628},
  Volume                   = {138},

  __markedentry            = {[20361362:1]},
  Owner                    = {20361362},
  Timestamp                = {2017.09.18},
  Url                      = {http://www.mathaware.org/journals/proc/2010-138-07/S0002-9939-10-10340-2/S0002-9939-10-10340-2.pdf}
}

@Article{DBLP:journals/corr/BahdanauCB14,
  Title                    = {Neural Machine Translation by Jointly Learning to Align and Translate},
  Author                   = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  Journal                  = {CoRR},
  Year                     = {2014},

  Annote                   = {attention},
  Volume                   = {abs/1409.0473},

  Archiveprefix            = {arXiv},
  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.org/rec/bib/journals/corr/BahdanauCB14},
  Eprint                   = {1409.0473},
  Owner                    = {20361362},
  Timestamp                = {Wed, 07 Jun 2017 14:40:19 +0200},
  Url                      = {http://arxiv.org/abs/1409.0473}
}

@Article{Balikas,
  Title                    = {Learning language-independent sentence representations for multi-lingual, multi-document summarization},
  Author                   = {Balikas, Georgios and Amini, Massih-Reza},

  Annote                   = {Denoising auto encoder, parallel corpus based sentence representations.
extractive summaristation.

Roughtly take a sentence vector for a sentence, in several languages (repires parallel corpus),
concattenate. 
Use a denoising autoencoeder to get a representation.

Use standard sumarriation of vectorised sentences stuff to make a extactive summary},

  Keywords                 = {summarization, vector-based_summarization},
  Owner                    = {20361362},
  Timestamp                = {2015.10.21},
  Url                      = {ama.liglab.fr/~amini/Publis/MultiLingualTxtSumm_CAp15.pdf}
}

@Manual{AMRspec,
  Title                    = {Abstract meaning representation (AMR) specification},
  Author                   = {Banarescu, Laura and Bonial, Claire and Cai, Shu and Georgescu, Madalina and Griffitt, Kira and Hermjakob, Ulf and Knight, Kevin and Koehn, Philipp and Palmer, Martha and Schneider, Nathan},
  Organization             = {The University of Southern California: Information Sciences Institute},

  Annote                   = {AMR is complicated, yo.d},
  Comment                  = {This is a live resource, it will be updated periodically.},
  Keywords                 = {AMR},
  Owner                    = {20361362},
  Timestamp                = {2015.04.24},
  Url                      = {https://github.com/amrisi/amr-guidelines/blob/master/amr.md}
}

@Article{Banarescu13abstractmeaning,
  Title                    = {Abstract Meaning Representation for Sembanking},
  Author                   = {Laura Banarescu and Claire Bonial and Shu Cai and Madalina Georgescu and Kira Griffitt and Ulf Hermjakob and Kevin Knight and Philipp Koehn and Martha Palmer and Nathan Schneider},
  Year                     = {2013},

  Annote                   = {AMR is a graph based description of a sentences meaning. Very different to a phrase vector. Every sentence with the same maning will have the same AMR (Abstract Meaning Representation). Eg: (d / destroy-01 :arg0 (b / boy) :arg1 (r / room)) the destruction of the room by the boy ... the boyÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬ÃƒÂ¢Ã¢â‚¬Å¾Ã‚Â¢s destruction of the room ... The boy destroyed the room. AMR seeks to create a large annoted corpus of sentences. The annotations describe the Meaning of the sentence. In a Propbank like way.},

  Groups                   = {KeyPapers},
  Owner                    = {20361362},
  Timestamp                = {2015.04.18},
  Url                      = {http://www.isi.edu/natural-language/amr/a.pdf}
}

@Article{barron2013plagiarism,
  Title                    = {Plagiarism meets paraphrasing: Insights for the next generation in automatic plagiarism detection},
  Author                   = {Barr{\'o}n-Cede{\~n}o, Alberto and Vila, Marta and Mart{\'\i}, M Ant{\`o}nia and Rosso, Paolo},
  Journal                  = {Computational Linguistics},
  Year                     = {2013},

  Annote                   = {Discusses the creation of the Paraphrase for plagerism corpus P4P},
  Number                   = {4},
  Pages                    = {917--947},
  Volume                   = {39},

  Keywords                 = {corpora},
  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2015.06.24},
  Url                      = {http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00153}
}

@Article{AdaGrams,
  Title                    = {Breaking Sticks and Ambiguities with Adaptive Skip-gram},
  Author                   = {Sergey Bartunov and
 Dmitry Kondrashkin and
 Anton Osokin and
 Dmitry P. Vetrov},
  Journal                  = {CoRR},
  Year                     = {2015},

  Annote                   = {Honestly, pretty hard to understand.
Its a baysian method.
It models each word as having some number of senses (the number being given by a dirchlet process). 
And then uses stocastic variational Expectation Maximistation
to learn all parameters.
Think fitting a GMM.},
  Volume                   = {abs/1502.07257},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/BartunovKOV15},
  Keywords                 = {wsi, wsd},
  Owner                    = {20361362},
  Timestamp                = {2014.03.03},
  Url                      = {http://arxiv.org/pdf/1502.07257v2.pdf}
}

@Inproceedings{basile-caputo-semeraro:2014:Coling,
  Title                    = {An Enhanced Lesk Word Sense Disambiguation Algorithm through a Distributional Semantic Model},
  Author                   = {Basile, Pierpaolo and Caputo, Annalina and Semeraro, Giovanni},
  Booktitle                = {Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
  Year                     = {2014},

  Address                  = {Dublin, Ireland},
  Annote                   = {Enhances Lesk using LSA (Not word2vec).
Use LSA on a large corpus to get word emeddings.
Expand the gloss, by contattendated the Glosses of related words (similar to extended Lesk)
Sum the embeddings in the expanded Gloss, and the Embeddings in the Context.
Find the cosine distence.between them
Shortest distence, is returned .ord sense},
  Month                    = {August},
  Pages                    = {1591--1600},
  Publisher                = {Dublin City University and Association for Computational Linguistics},

  Abstract                 = {This paper describes a new Word Sense Disambiguation (WSD) algorithm which extends two
well-known variations of the Lesk WSD method. Given a word and its context, Lesk algorithm
exploits the idea of maximum number of shared words (maximum overlaps) between the context
of a word and each definition of its senses (gloss) in order to select the proper meaning. The main
contribution of our approach relies on the use of a word similarity function defined on a distribu-
tional semantic space to compute the gloss-context overlap. As sense inventory we adopt Babel-
Net, a large multilingual semantic network built exploiting both WordNet and Wikipedia. Besides
linguistic knowledge, BabelNet also represents encyclopedic concepts coming from Wikipedia.
The evaluation performed on SemEval-2013 Multilingual Word Sense Disambiguation shows
that our algorithm goes beyond the most frequent sense baseline and the simplified version of the
Lesk algorithm. Moreover, when compared with the other participants in SemEval-2013 task,
our approach is able to outperform the best system for English.},
  Keywords                 = {word-sense-disambiguation},
  Owner                    = {20361362},
  Timestamp                = {2015.07.17},
  Url                      = {http://www.aclweb.org/anthology/C14-1151}
}

@Book{BellmanDynamicProgramming,
  Title                    = {Dynamic Programming},
  Author                   = {R.R. Bellman},
  Publisher                = {Printon University Press},
  Year                     = {1972},
  Edition                  = {6th},
  Note                     = {Orignial Published in 1957},

  Owner                    = {20361362},
  Timestamp                = {2015.09.15},
  Url                      = {http://goo.gl/mHiXxD}
}

@Article{NPLM,
  Title                    = {A Neural Probabilistic Language Model},
  Author                   = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
  Journal                  = {The Journal of Machine Learning Research},
  Year                     = {2003},

  Annote                   = {The Paper that started this all,NNLM NN LM. Language Models},
  Pages                    = {137--186},

  Booktitle                = {Innovations in Machine Learning},
  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2015.09.11},
  Url                      = {http://dl.acm.org/citation.cfm?id=944919.944966}
}

@Inproceedings{bengio2007greedylayerwise,
  Title                    = {Greedy layer-wise training of deep networks},
  Author                   = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {2007},
  Pages                    = {153--160},

  Owner                    = {20361362},
  Timestamp                = {2017.07.25},
  Url                      = {https://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf}
}

@Article{bengio1994learning,
  Title                    = {Learning long-term dependencies with gradient descent is difficult},
  Author                   = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  Journal                  = {IEEE transactions on neural networks},
  Year                     = {1994},
  Number                   = {2},
  Pages                    = {157--166},
  Volume                   = {5},

  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2017.10.16},
  Url                      = {http://www.dlsi.ua.es/~mlf/nnafmc/papers/bengio94learning.pdf}
}

@Article{Berk:1982:HFS:358589.358606,
  Title                    = {A Human Factors Study of Color Notation Systems for Computer Graphics},
  Author                   = {Berk, Toby and Kaufman, Arie and Brownston, Lee},
  Journal                  = {Commun. ACM},
  Year                     = {1982},
  Month                    = aug,
  Number                   = {8},
  Pages                    = {547--550},
  Volume                   = {25},

  Acmid                    = {358606},
  Address                  = {New York, NY, USA},
  Doi                      = {10.1145/358589.358606},
  ISSN                     = {0001-0782},
  Issue_date               = {Aug 1982},
  Keywords                 = {color notation, graphics languages, human engineering, natural language, psychology of computer use},
  Numpages                 = {4},
  Owner                    = {20361362},
  Publisher                = {ACM},
  Timestamp                = {2017.06.30},
  Url                      = {http://doi.acm.org/10.1145/358589.358606}
}

@Book{berlin1969basic,
  Title                    = {Basic color terms: Their university and evolution},
  Author                   = {Berlin, Brent and Kay, Paul},
  Publisher                = {California UP},
  Year                     = {1969},
  Annote                   = {The book about the order cultures develop color names},

  Owner                    = {20361362},
  Timestamp                = {2017.07.03}
}

@Article{Julia,
  Title                    = {{J}ulia: A Fresh Approach to Numerical Computing},
  Author                   = {Jeff Bezanson and Alan Edelman and Stefan Karpinski and Viral B. Shah},
  Year                     = {2014},

  Abstract                 = {The Julia programming language is gaining enormous popularity. Julia was designed to be easy and fast. Most importantly, Julia shatters deeply established notions widely held in the applied community:

1. High-level, dynamic code has to be slow by some sort of law of nature.
2. It is sensible to prototype in one language and then recode in another language.
3. There are parts of a system for the programmer, and other parts best left untouched as they are built by the experts.

Julia began with a deep understanding of the needs of the scientific programmer and the needs of the computer in mind. Bridging cultures that have often been distant, Julia combines expertise from computer science and computational science creating a new approach to scientific computing. This note introduces the programmer to the language and the underlying design theory. It invites the reader to rethink the fundamental foundations of numerical computing systems.

In particular, there is the fascinating dance between specialization and abstraction. Specialization allows for custom treatment. We can pick just the right algorithm for the right circumstance and this can happen at runtime based on argument types (code selection via multiple dispatch). Abstraction recognizes what remains the same after differences are stripped away and ignored as irrelevant. The recognition of abstraction allows for code reuse (generic programming). A simple idea that yields incredible power. The Julia design facilitates this interplay in many explicit and subtle ways for machine performance and, most importantly, human convenience.},
  Eprint                   = {1411.1607},
  Eprintclass              = {cs.MS},
  Eprinttype               = {arXiv},
  Keywords                 = {tools},
  Owner                    = {20361362},
  Timestamp                = {2015.12.18},
  Url                      = {http://arxiv.org/abs/1411.1607}
}

@Article{bimler2005color,
  Title                    = {Are color categories innate or internalized? Hypotheses and implications},
  Author                   = {Bimler, David},
  Journal                  = {Journal of Cognition and Culture},
  Year                     = {2005},
  Number                   = {3},
  Pages                    = {265--292},
  Volume                   = {5},

  Owner                    = {20361362},
  Publisher                = {Brill},
  Timestamp                = {2017.07.04},
  Url                      = {https://www.researchgate.net/profile/David_Bimler/publication/225369667_Are_Color_Categories_Innate_or_Internalized_Hypotheses_and_Implications/links/545ecf360cf27487b44f0e74/Are-Color-Categories-Innate-or-Internalized-Hypotheses-and-Implications.pdf}
}

@Book{bird2009natural,
  Title                    = {Natural language processing with Python},
  Author                   = {Bird, Steven and Klein, Ewan and Loper, Edward},
  Publisher                = {" O'Reilly Media, Inc."},
  Year                     = {2009},
  Annote                   = {This is the prefered citation for NLTK},

  Keywords                 = {software, tools},
  Owner                    = {20361362},
  Timestamp                = {2015.07.12},
  Url                      = {http://www.nltk.org/}
}

@Inproceedings{NLTK,
  Title                    = {NLTK: the natural language toolkit},
  Author                   = {Bird, Steven and Loper, Edward},
  Booktitle                = {Proceedings of the ACL 2004 on Interactive poster and demonstration sessions},
  Year                     = {2004},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {31},

  Owner                    = {20361362},
  Timestamp                = {2018.02.07},
  Url                      = {http://www.aclweb.org/anthology/P04-3031}
}

@Article{bishop1994mixture,
  Title                    = {Mixture density networks},
  Author                   = {Bishop, Christopher M},
  Year                     = {1994},

  Owner                    = {20361362},
  Publisher                = {Aston University},
  Timestamp                = {2017.08.17},
  Url                      = {http://publications.aston.ac.uk/373/1/NCRG_94_004.pdf}
}

@Article{blei2003latent,
  Title                    = {Latent dirichlet allocation},
  Author                   = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  Journal                  = {the Journal of machine Learning research},
  Year                     = {2003},

  Annote                   = {LDA},
  Pages                    = {993--1022},
  Volume                   = {3},

  Owner                    = {20361362},
  Publisher                = {JMLR. org},
  Timestamp                = {2015.07.12},
  Url                      = {https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf}
}

@Article{bojanowski2016enriching,
  Title                    = {Enriching Word Vectors with Subword Information},
  Author                   = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  Journal                  = {Transactions of the Association for Computational Linguistics},
  Year                     = {2017},

  Annote                   = {https://fasttext.cc/docs/en/pretrained-vectors.html},
  Pages                    = {135--146},
  Volume                   = {5},

  Owner                    = {20361362},
  Timestamp                = {2017.11.28},
  Url                      = {https://arxiv.org/abs/1607.04606}
}

@Inproceedings{bolukbasi2016man,
  Title                    = {Man is to computer programmer as woman is to homemaker? Debiasing word embeddings},
  Author                   = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
  Booktitle                = {Advances in Neural Information Processing Systems},
  Year                     = {2016},
  Pages                    = {4349--4357},

  Owner                    = {20361362},
  Timestamp                = {2017.10.09},
  Url                      = {https://arxiv.org/pdf/1607.06520.pdf}
}

@Book{booth2010rhetoric,
  Title                    = {The rhetoric of fiction},
  Author                   = {Booth, Wayne C},
  Publisher                = {University of Chicago Press},
  Year                     = {1961},

  Owner                    = {20361362},
  Timestamp                = {2018.02.08},
  Url                      = {http://krishikosh.egranth.ac.in/bitstream/1/2025424/1/19763.pdf}
}

@Article{borko1963automatic,
  Title                    = {Automatic document classification},
  Author                   = {Borko, Harold and Bernick, Myrna},
  Journal                  = {Journal of the ACM (JACM)},
  Year                     = {1963},

  Annote                   = {Ancient article on document classification},
  Number                   = {2},
  Pages                    = {151--162},
  Volume                   = {10},

  Owner                    = {20361362},
  Publisher                = {ACM},
  Timestamp                = {2015.09.02},
  Url                      = {http://delivery.acm.org/10.1145/330000/321165/p151-borko.pdf?ip=130.95.149.38&id=321165&acc=ACTIVE%20SERVICE&key=65D80644F295BC0D.05ACB6DAA5D75F76.4D4702B0C3E38B35.4D4702B0C3E38B35&CFID=709398561&CFTOKEN=93369711&__acm__=1441184806_42491bd6a71163d771438c4c00eb8888}
}

@Article{Bottou2014,
  Title                    = {From machine learning to machine reasoning},
  Author                   = {Bottou, L},
  Journal                  = {Machine Learning},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {133-149},
  Volume                   = {94},

  Doi                      = {10.1007/s10994-013-5335-x},
  ISSN                     = {0885-6125},
  Keywords                 = {Machine learning; Reasoning; Recursive networks},
  Language                 = {English},
  Owner                    = {20361362},
  Publisher                = {Springer US},
  Timestamp                = {2015.05.27},
  Url                      = {http://dx.doi.org/10.1007/s10994-013-5335-x}
}

@Article{bourlard1988auto,
  Title                    = {Auto-association by multilayer perceptrons and singular value decomposition},
  Author                   = {Bourlard, Herv{\'e} and Kamp, Yves},
  Journal                  = {Biological cybernetics},
  Year                     = {1988},
  Number                   = {4},
  Pages                    = {291--294},
  Volume                   = {59},

  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2017.09.01},
  Url                      = {https://pdfs.semanticscholar.org/f582/1548720901c89b3b7481f7500d7cd64e99bd.pdf}
}

@Article{bowman2016fast,
  Title                    = {A fast unified model for parsing and sentence understanding},
  Author                   = {Bowman, Samuel R and Gauthier, Jon and Rastogi, Abhinav and Gupta, Raghav and Manning, Christopher D and Potts, Christopher},
  Journal                  = {arXiv preprint arXiv:1603.06021},
  Year                     = {2016},

  Owner                    = {20361362},
  Timestamp                = {2017.11.28},
  Url                      = {https://arxiv.org/pdf/1603.06021.pdf}
}

@Article{RvNNLogicalSemantics,
  Title                    = {Recursive Neural Networks for Learning Logical Semantics},
  Author                   = {Bowman, Samuel R and Potts, Christopher and Manning, Christopher D},
  Journal                  = {arXiv preprint arXiv:1406.1827},
  Year                     = {2014},

  Annote                   = {Train RvNN to work on logical structures.
Also trained recursive neural tensor networks (RvNTN) to do the same.
Found RvNTN to be better, and indeedd to work well in all cases.

Taking two sentences, decomposing them,
then at the top using a softmax classifier to deterrmine entailment, reverse entailment, equivalence, alternation, negation, cover or independence (else), between them.

It is noted that the RvNN fails to suitably separate long different expressions in embedded space.
The RvNTN does much better.},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.22},
  Url                      = {http://arxiv.org/pdf/1406.1827v1.pdf}
}

@Article{Bowman2015SmoothGeneration,
  Title                    = {Generating Sentences from a Continuous Space},
  Author                   = {Bowman, Samuel R and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M and Jozefowicz, Rafal and Bengio, Samy},
  Journal                  = {International Conference on Learning Representations (ICLR) Workshop},
  Year                     = {2016},

  Annote                   = {Focus is on generating smooth sentences, using RNN-Language models.

Preprint, under review for ICLR 2016},

  Owner                    = {20361362},
  Timestamp                = {2015.11.26},
  Url                      = {http://arxiv.org/pdf/1511.06349.pdf}
}

@Book{brickell1970differentiable,
  Title                    = {Differentiable Manifolds: An Introduction},
  Author                   = {Brickell, F. and Clark, R.S.},
  Publisher                = {Van Nostrand Reinhold},
  Year                     = {1970},
  Annote                   = {(In Science Library at 516.36 , 1970 DIF)

Good Book.
First few pages are a super consise intro to topology.
THen gets into Differentiable manifolds,
includding Grassmann Manifold.

Concludes with Lie Groups},
  Series                   = {The New University Mathematics Series},

  ISBN                     = {9780442010515},
  Keywords                 = {topology},
  Owner                    = {20361362},
  Timestamp                = {2016.08.18},
  Url                      = {https://books.google.com.au/books?id=25EJNQEACAAJ}
}

@Article{brown1992class,
  Title                    = {Class-based n-gram models of natural language},
  Author                   = {Brown, Peter F and Desouza, Peter V and Mercer, Robert L and Pietra, Vincent J Della and Lai, Jenifer C},
  Journal                  = {Computational linguistics},
  Year                     = {1992},
  Number                   = {4},
  Pages                    = {467--479},
  Volume                   = {18},

  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2017.07.11}
}

@Inproceedings{Bruni:2012:DST:2390524.2390544,
  Title                    = {Distributional Semantics in Technicolor},
  Author                   = {Bruni, Elia and Boleda, Gemma and Baroni, Marco and Tran, Nam-Khanh},
  Booktitle                = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1},
  Year                     = {2012},

  Address                  = {Stroudsburg, PA, USA},
  Annote                   = {not really about colors.},
  Pages                    = {136--145},
  Publisher                = {Association for Computational Linguistics},
  Series                   = {ACL '12},

  Acmid                    = {2390544},
  Location                 = {Jeju Island, Korea},
  Numpages                 = {10},
  Owner                    = {20361362},
  Timestamp                = {2017.04.10},
  Url                      = {http://dl.acm.org/citation.cfm?id=2390524.2390544}
}

@Inproceedings{calcaterra2008linear,
  Title                    = {Linear Combinations of Gaussians with a Single Variance are dense in L2},
  Author                   = {Calcaterra, Craig},
  Booktitle                = {Proceedings of the World Congress on Engineering},
  Year                     = {2008},
  Volume                   = {2},

  Owner                    = {20361362},
  Timestamp                = {2017.09.22},
  Url                      = {https://www.researchgate.net/profile/Craig_Calcaterra/publication/44262098_Linear_Combinations_of_Gaussians_with_a_Single_Variance_are_Dense_in_L2/links/0fcfd50a2a8223bee8000000.pdf}
}

@Article{Caliskan183,
  Title                    = {Semantics derived automatically from language corpora contain human-like biases},
  Author                   = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
  Journal                  = {Science},
  Year                     = {2017},
  Number                   = {6334},
  Pages                    = {183--186},
  Volume                   = {356},

  Abstract                 = {AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs{\textemdash}for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior.Science, this issue p. 183; see also p. 133Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.},
  Doi                      = {10.1126/science.aal4230},
  Eprint                   = {http://science.sciencemag.org/content/356/6334/183.full.pdf},
  ISSN                     = {0036-8075},
  Owner                    = {20361362},
  Publisher                = {American Association for the Advancement of Science},
  Timestamp                = {2017.10.16},
  Url                      = {http://science.sciencemag.org/content/356/6334/183}
}

@Article{caruana1997multitask,
  Title                    = {Multitask learning},
  Author                   = {Caruana, Rich},
  Journal                  = {Machine learning},
  Year                     = {1997},
  Number                   = {1},
  Pages                    = {41--75},
  Volume                   = {28},

  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2017.07.05},
  Url                      = {http://www.dtic.mil/get-tr-doc/pdf?AD=ADA341329}
}

@Inproceedings{phrasaltranslationtool,
  Title                    = {Phrasal: a toolkit for statistical machine translation with facilities for extraction and incorporation of arbitrary model features},
  Author                   = {Cer, Daniel and Galley, Michel and Jurafsky, Daniel and Manning, Christopher D},
  Booktitle                = {Proceedings of the NAACL HLT 2010 Demonstration Session},
  Year                     = {2010},
  Annote                   = {Quiet likely THE engine for phrase-based translation.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {9--12},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://mt-archive.info/NAACL-HLT-2010-Cer-2.pdf}
}

@Inproceedings{cer2010best,
  Title                    = {The best lexical metric for phrase-based statistical MT system optimization},
  Author                   = {Cer, Daniel and Manning, Christopher D and Jurafsky, Daniel},
  Booktitle                = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  Year                     = {2010},
  Annote                   = {A comparason of the alterntives the BLEU},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {555--563},

  Keywords                 = {machine translation},
  Owner                    = {20361362},
  Timestamp                = {2015.10.26},
  Url                      = {https://web.stanford.edu/~jurafsky/N10-1080.pdf}
}

@Article{APS:2745308,
  Title                    = {Contexts of antonymous adjectives},
  Author                   = {Charles,Walter G. and Miller,George A.},
  Journal                  = {Applied Psycholinguistics},
  Year                     = {1989},

  Annote                   = {Suggests that you can not substitute adjectived with their antonyms an get a normal setence, but does not evaluate with a suitablely sized test set to draw strong conclusions.},
  Month                    = {9},
  Pages                    = {357--375},
  Volume                   = {10},

  Abstract                 = { ABSTRACT ABSTRACTThe method of sorting is used to compare sets of adjectival contexts. Contexts of directly antonymous adjectives are found to be highly discriminable, both with sentential and phrasal contexts. These results are used to argue that words with different meanings normally appear in discriminably different contexts, and that the cue for learning to associate direct antonyms is not their substitutability, but rather their relatively frequent co-occurrence in the same sentence. },
  Doi                      = {10.1017/S0142716400008675},
  ISSN                     = {1469-1817},
  Issue                    = {03},
  Numpages                 = {19},
  Owner                    = {20361362},
  Timestamp                = {2015.07.24},
  Url                      = {http://journals.cambridge.org/article_S0142716400008675}
}

@Article{7122294,
  Title                    = {Sentence Compression for Aspect-Based Sentiment Analysis},
  Author                   = {Wanxiang Che and Yanyan Zhao and Honglei Guo and Zhong Su and Ting Liu},
  Journal                  = {Audio, Speech, and Language Processing, IEEE/ACM Transactions on},
  Year                     = {2015},

  Annote                   = {ZA method by which they sterip non-senitiment relevant facts from the document},
  Month                    = {Dec},
  Number                   = {12},
  Pages                    = {2111-2124},
  Volume                   = {23},

  Abstract                 = {Sentiment analysis, which addresses the computational treatment of opinion, sentiment, and subjectivity in text, has received considerable attention in recent years. In contrast to the traditional coarse-grained sentiment analysis tasks, such as document-level sentiment classification, we are interested in the fine-grained aspect-based sentiment analysis that aims to identify aspects that users comment on and these aspects' polarities. Aspect-based sentiment analysis relies heavily on syntactic features. However, the reviews that this task focuses on are natural and spontaneous, thus posing a challenge to syntactic parsers. In this paper, we address this problem by proposing a framework of adding a sentiment sentence compression (Sent_Comp) step before performing the aspect-based sentiment analysis. Different from the previous sentence compression model for common news sentences, Sent_Comp seeks to remove the sentiment-unnecessary information for sentiment analysis, thereby compressing a complicated sentiment sentence into one that is shorter and easier to parse. We apply a discriminative conditional random field model, with certain special features, to automatically compress sentiment sentences. Using the Chinese corpora of four product domains, Sent_Comp significantly improves the performance of the aspect-based sentiment analysis. The features proposed for Sent_Comp, especially the potential semantic features, are useful for sentiment sentence compression.},
  Doi                      = {10.1109/TASLP.2015.2443982},
  ISSN                     = {2329-9290},
  Keywords                 = {data compression;grammars;pattern classification;random processes;text analysis;Chinese corpora;Internet;Sent_Comp;discriminative conditional random field model;document-level sentiment classification;fine-grained aspect-based sentiment analysis;sentiment sentence compression;syntactic parsers;Analytical models;Feature extraction;Semantics;Sentiment analysis;Speech processing;Syntactics;Aspect-based sentiment analysis;potential semantic features;sentence compression;sentiment analysis},
  Owner                    = {20361362},
  Timestamp                = {2015.11.23},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7122294}
}

@Inproceedings{ICML2012Chen_416,
  Title                    = {Marginalized Denoising Autoencoders for Domain Adaptation},
  Author                   = {Minmin Chen and Zhixiang Xu and Kilian Weinberger and Fei Sha},
  Booktitle                = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
  Year                     = {2012},

  Address                  = {New York, NY, USA},
  Editor                   = {John Langford and Joelle Pineau},
  Month                    = {July},
  Pages                    = {767--774},
  Publisher                = {Omnipress},
  Series                   = {ICML '12},

  ISBN                     = {978-1-4503-1285-1},
  Location                 = {Edinburgh, Scotland, GB},
  Owner                    = {20361362},
  Timestamp                = {2017.09.01},
  Url                      = {http://www.icml.cc/2012/papers/416.pdf}
}

@Inproceedings{chen1996empirical,
  Title                    = {An empirical study of smoothing techniques for language modeling},
  Author                   = {Chen, Stanley F and Goodman, Joshua},
  Booktitle                = {Proceedings of the 34th annual meeting on Association for Computational Linguistics},
  Year                     = {1996},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {310--318},

  Doi                      = {10.1006/csla.1999.0128},
  Owner                    = {20361362},
  Timestamp                = {2015.10.20}
}

@Inproceedings{chen2016xgboost,
  Title                    = {Xgboost: A scalable tree boosting system},
  Author                   = {Chen, Tianqi and Guestrin, Carlos},
  Booktitle                = {Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
  Year                     = {2016},
  Organization             = {ACM},
  Pages                    = {785--794},

  Owner                    = {20361362},
  Timestamp                = {2018.01.31},
  Url                      = {https://arxiv.org/pdf/1603.02754.pdf}
}

@Inproceedings{Chen2014,
  Title                    = {A Unified Model for Word Sense Representation and Disambiguation.},
  Author                   = {Chen, Xinxiong and Liu, Zhiyuan and Sun, Maosong},
  Booktitle                = {EMNLP},
  Year                     = {2014},
  Annote                   = {Requires WordNet, plus an unlabelled corpus.
Three major steps.

First learn word vectors using skipgrams.
Then for each word sense in WordNet, consider each word in the gloss with cosine similarity to the word of less than threshold hyperparameter. Average those to get the initital word sense vector.

Then:
For each sensence in the corpus
Calculate a context vectors initially as the average of all word vectors in the sentence.
For each word (Either from left to right, or in order of least word senses to most),
evaluate the cosine similarity of each the word sense to the current contect vector.
If the closest vector wins by greater than a given threshold (over the second closest), 
then replace the word vector within the context vector with the word sense vector.
From this word senses are determined for all words in corpus.

Then
Modify objective of skipgram to predict both context words, and if in the previous step the senses were determined beyond the threshhold (senses are encoded as a integer, based on WordNet index).},
  Organization             = {Citeseer},
  Pages                    = {1025--1035},

  Owner                    = {20361362},
  Timestamp                = {2016.04.19},
  Url                      = {http://emnlp2014.org/papers/pdf/EMNLP2014110.pdf}
}

@Unpublished{chenSpokenDialogueThesisProposal,
  Title                    = {Thesis Proposal: Unsupervised Learning and Modeling of Knowledge and Intent for Spoken Dialogue Systems},
  Author                   = {Yun-Nung (Vivian) Chen},

  Annote                   = {A PhD thesis Proposal. (Or is it the whole thesis? It is huge!) Focus is on Acquiring Knowledge from spoken word.},
  Month                    = {April},
  Year                     = {2015},

  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.cs.cmu.edu/~yvchen/doc/dissertation.pdf}
}

@Inproceedings{cho2014properties,
  Title                    = {On the properties of neural machine translation: Encoder-decoder approaches},
  Author                   = {Cho, Kyunghyun and van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  Booktitle                = {Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)},
  Year                     = {2014},
  Annote                   = {The GRU paper},

  Owner                    = {20361362},
  Timestamp                = {2017.06.22},
  Url                      = {https://arxiv.org/pdf/1409.1259.pdf}
}

@Inproceedings{cho-EtAl:2014:EMNLP2014,
  Title                    = {Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation},
  Author                   = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  Booktitle                = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  Year                     = {2014},

  Address                  = {Doha, Qatar},
  Month                    = {October},
  Pages                    = {1724--1734},
  Publisher                = {Association for Computational Linguistics},

  Owner                    = {20361362},
  Timestamp                = {2017.10.16},
  Url                      = {http://www.aclweb.org/anthology/D14-1179}
}

@Article{2014understandblenetworksnonnegweights,
  Title                    = {Learning Understandable Neural Networks With Nonnegative Weight Constraints},
  Author                   = {J. Chorowski and J. M. Zurada},
  Journal                  = {IEEE Transactions on Neural Networks and Learning Systems},
  Year                     = {2015},
  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {62-69},
  Volume                   = {26},

  Abstract                 = {People can understand complex structures if they relate to more isolated yet understandable concepts. Despite this fact, popular pattern recognition tools, such as decision tree or production rule learners, produce only flat models which do not build intermediate data representations. On the other hand, neural networks typically learn hierarchical but opaque models. We show how constraining neurons' weights to be nonnegative improves the interpretability of a network's operation. We analyze the proposed method on large data sets: the MNIST digit recognition data and the Reuters text categorization data. The patterns learned by traditional and constrained network are contrasted to those learned with principal component analysis and nonnegative matrix factorization.},
  Doi                      = {10.1109/TNNLS.2014.2310059},
  ISSN                     = {2162-237X},
  Keywords                 = {document image processing;learning (artificial intelligence);matrix decomposition;neural nets;pattern classification;principal component analysis;text analysis;MNIST digit recognition data;Reuters text categorization data;neural network learning;nonnegative matrix factorization;nonnegative weight constraints;pattern recognition;principal component analysis;Biological neural networks;Data models;Educational institutions;Neurons;Principal component analysis;Training;Vectors;Multilayer perceptron;pattern analysis;supervised learning;white-box models;white-box models.},
  Owner                    = {20361362},
  Timestamp                = {2017.09.11}
}

@Article{chung2014empirical,
  Title                    = {Empirical evaluation of gated recurrent neural networks on sequence modeling},
  Author                   = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  Journal                  = {arXiv preprint arXiv:1412.3555},
  Year                     = {2014},

  Annote                   = {GRU},

  Owner                    = {20361362},
  Timestamp                = {2017.06.17},
  Url                      = {https://arxiv.org/pdf/1412.3555.pdf}
}

@Inproceedings{collobert2008unified,
  Title                    = {A unified architecture for natural language processing: Deep neural networks with multitask learning},
  Author                   = {Collobert, Ronan and Weston, Jason},
  Booktitle                = {Proceedings of the 25th international conference on Machine learning},
  Year                     = {2008},
  Organization             = {ACM},
  Pages                    = {160--167},

  Keywords                 = {em},
  Owner                    = {20361362},
  Timestamp                = {2015.07.27},
  Url                      = {https://dl.acm.org/citation.cfm?id=1390177}
}

@Article{collobert2011natural,
  Title                    = {Natural language processing (almost) from scratch},
  Author                   = {Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2011},
  Number                   = {Aug},
  Pages                    = {2493--2537},
  Volume                   = {12},

  Owner                    = {20361362},
  Timestamp                = {2017.08.22},
  Url                      = {https://arxiv.org/pdf/1103.0398.pdf}
}

@Inproceedings{colorni1991distributed,
  Title                    = {Distributed optimization by ant colonies},
  Author                   = {Colorni, Alberto and Dorigo, Marco and Maniezzo, Vittorio and others},
  Booktitle                = {Proceedings of the first European conference on artificial life},
  Year                     = {1991},
  Organization             = {Paris, France},
  Pages                    = {134--142},
  Volume                   = {142},

  Owner                    = {20361362},
  Timestamp                = {2015.09.15},
  Url                      = {https://svn-d1.mpi-inf.mpg.de/AG1/MultiCoreLab/papers/DorigoManiezzoColorni91%20-%20Ant%20Colonies.pdf}
}

@Article{meanshift,
  Title                    = {Mean shift: a robust approach toward feature space analysis},
  Author                   = {Comaniciu, D. and Meer, P.},
  Journal                  = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  Year                     = {2002},

  Annote                   = {is in sklearn \url{http://scikit-learn.org/dev/auto_examples/cluster/plot_mean_shift.html}r},
  Month                    = {May},
  Number                   = {5},
  Pages                    = {603-619},
  Volume                   = {24},

  Abstract                 = {A general non-parametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure: the mean shift. For discrete data, we prove the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density. The relation of the mean shift procedure to the Nadaraya-Watson estimator from kernel regression and the robust M-estimators; of location is also established. Algorithms for two low-level vision tasks discontinuity-preserving smoothing and image segmentation - are described as applications. In these algorithms, the only user-set parameter is the resolution of the analysis, and either gray-level or color images are accepted as input. Extensive experimental results illustrate their excellent performance},
  Doi                      = {10.1109/34.1000236},
  ISSN                     = {0162-8828},
  Keywords                 = {computer vision;estimation theory;image segmentation;nonparametric statistics;pattern clustering;smoothing methods;Nadaraya-Watson estimator;algorithm performance;analysis resolution;arbitrarily shaped cluster delineation;color images;complex multimodal feature space;computational module;convergence;density function;density modes detection;discontinuity-preserving image smoothing;discrete data;gray-level images;image segmentation;kernel regression;location estimation;low-level vision algorithms;mean shift;nearest stationary point;nonparametric technique;pattern recognition procedure;recursive mean shift procedure;robust M-estimators;robust feature space analysis;user-set parameter;Convergence;Density functional theory;Image analysis;Image color analysis;Image resolution;Image segmentation;Kernel;Pattern recognition;Robustness;Smoothing methods},
  Owner                    = {20361362},
  Timestamp                = {2015.05.28},
  Url                      = {ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1000236}
}

@Inproceedings{conway1998algorithmicpluralisation,
  Title                    = {An algorithmic approach to english pluralization},
  Author                   = {Conway, DM},
  Booktitle                = {Proceedings of the Second Annual Perl Conference},
  Year                     = {1998},
  Annote                   = {A heuristic based appraoch to pluralising word.},

  Owner                    = {20361362},
  Timestamp                = {2015.06.25}
}

@Inproceedings{conway1992experimental,
  Title                    = {An experimental comparison of three natural language colour naming models},
  Author                   = {Conway, Damian},
  Booktitle                = {Proc. east-west int. conf. on human-computer interaction},
  Year                     = {1992},
  Annote                   = {Distance based},
  Pages                    = {328--339},

  Owner                    = {20361362},
  Timestamp                = {2017.07.03},
  Url                      = {http://users.monash.edu/~damian/papers/PDF/ColourNaming.pdf}
}

@Inproceedings{corston2001machine,
  Title                    = {A machine learning approach to the automatic evaluation of machine translation},
  Author                   = {Corston-Oliver, Simon and Gamon, Michael and Brockett, Chris},
  Booktitle                = {Proceedings of the 39th Annual Meeting on Association for Computational Linguistics},
  Year                     = {2001},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {148--155},

  Owner                    = {20361362},
  Timestamp                = {2018.03.14},
  Url                      = {https://aclanthology.info/pdf/P/P01/P01-1020.pdf}
}

@Article{Cortes1995,
  Title                    = {Support-vector networks},
  Author                   = {Cortes, Corinna and Vapnik, Vladimir},
  Journal                  = {Machine Learning},
  Year                     = {1995},
  Month                    = {Sep},
  Number                   = {3},
  Pages                    = {273--297},
  Volume                   = {20},

  Abstract                 = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
  Day                      = {01},
  Doi                      = {10.1007/BF00994018},
  ISSN                     = {1573-0565},
  Owner                    = {20361362},
  Timestamp                = {2018.03.06},
  Url                      = {https://doi.org/10.1007/BF00994018}
}

@Article{cotterell2017SkipgramisEPCA,
  Title                    = {Explaining and Generalizing Skip-Gram through Exponential Family Principal Component Analysis},
  Author                   = {Cotterell, Ryan and Poliak, Adam and Van Durme, Benjamin and Eisner, Jason},
  Journal                  = {EACL 2017},
  Year                     = {2017},

  Annote                   = {Show s link between Expodential PCA and Skipgrams},
  Volume                   = {175},

  Owner                    = {20361362},
  Timestamp                = {2017.08.25},
  Url                      = {http://www.cs.jhu.edu/~jason/papers/cotterell+al.eacl17.pdf}
}

@Inproceedings{dahl2013reludropout,
  Title                    = {Improving deep neural networks for LVCSR using rectified linear units and dropout},
  Author                   = {Dahl, George E and Sainath, Tara N and Hinton, Geoffrey E},
  Booktitle                = {Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {8609--8613},

  Doi                      = {10.1109/ICASSP.2013.6639346},
  Owner                    = {20361362},
  Timestamp                = {2017.06.22}
}

@Inproceedings{Daiber:2013:IEA:2506182.2506198,
  Title                    = {Improving Efficiency and Accuracy in Multilingual Entity Extraction},
  Author                   = {Daiber, Joachim and Jakob, Max and Hokamp, Chris and Mendes, Pablo N.},
  Booktitle                = {Proceedings of the 9th International Conference on Semantic Systems},
  Year                     = {2013},

  Address                  = {New York, NY, USA},
  Annote                   = {DBpedia},
  Pages                    = {121--124},
  Publisher                = {ACM},
  Series                   = {I-SEMANTICS '13},

  Acmid                    = {2506198},
  Doi                      = {10.1145/2506182.2506198},
  ISBN                     = {978-1-4503-1972-0},
  Keywords                 = {entity linking, information extraction, named entity recognition},
  Location                 = {Graz, Austria},
  Numpages                 = {4},
  Owner                    = {20361362},
  Timestamp                = {2015.06.10},
  Url                      = {http://doi.acm.org/10.1145/2506182.2506198}
}

@Article{5443743,
  Title                    = {Monotone and Partially Monotone Neural Networks},
  Author                   = {H. Daniels and M. Velikova},
  Journal                  = {IEEE Transactions on Neural Networks},
  Year                     = {2010},
  Month                    = {June},
  Number                   = {6},
  Pages                    = {906-917},
  Volume                   = {21},

  Abstract                 = {In many classification and prediction problems it is known that the response variable depends on certain explanatory variables. Monotone neural networks can be used as powerful tools to build monotone models with better accuracy and lower variance compared to ordinary nonmonotone models. Monotonicity is usually obtained by putting constraints on the parameters of the network. In this paper, we will clarify some of the theoretical results on monotone neural networks with positive weights, issues that are sometimes misunderstood in the neural network literature. Furthermore, we will generalize some of the results obtained by Sill for the so-called min-max networks to the case of partially monotone problems. The method is illustrated in practical case studies.},
  Doi                      = {10.1109/TNN.2010.2044803},
  ISSN                     = {1045-9227},
  Keywords                 = {minimax techniques;neural nets;min-max networks;monotone neural networks;partially monotone neural networks;Function approximation;monotone neural networks;monotone prediction problems;partially monotone neural networks;Algorithms;Computer Simulation;Humans;Neural Networks (Computer);Predictive Value of Tests},
  Owner                    = {20361362},
  Timestamp                = {2017.11.07}
}

@Misc{BlogMaxLikeMaxEnt,
  Title                    = {maximum-likelihood-and-entropy},

  Annote                   = {Uncitable Blog, but nicely written into to max entropy and the relationship between -ve log likelyhood, entropy rate (true entropy), and Kullback-Leiber diverence (relitive entropy, between true and estimate) 

Likelyhood is the joint pdf for all the events ($x_i$) occuring, parameterised by $\theta$, we assume that events are IID.

Arg Max the likelyhood of the emperical samples, to get $\theta$
Equivelently can Argmin the -ve log of the likelyhood.

$\theta_0$ is the true parameter
$\hat{\theta}$ is out estimated parameteri

"mean negative log-likelihood converges to the differential entropy under the true distribution plus the Kullback-Leibler divergence between the true distribution and the distribution we guess at"},
  Author                   = {David Darmon},
  HowPublished             = {blog},

  Keywords                 = {max-entropy},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://thirdorderscientist.org/homoclinic-orbit/2013/4/1/maximum-likelihood-and-entropy}
}

@Article{das2014frame,
  Title                    = {Frame-semantic parsing},
  Author                   = {Das, Dipanjan and Chen, Desai and Martins, Andr{\'e} FT and Schneider, Nathan and Smith, Noah A},
  Journal                  = {Computational Linguistics},
  Year                     = {2014},

  Annote                   = {See Eratum (under Note in BibTeX).},
  Note                     = {Has Erratum: \url{http://www.mitpressjournals.org/doi/pdfplus/10.1162/COLI_x_00205}
 
Erratum
The authors of the article -Semantic P
 and a graduate student discovered
that in rows 7 and 8 of Table 8, at inference time for argument identification with
gold frames, the described model included gold spans along with the candidate set
of automatic spans (elaborated in Section 6.1), thus creating an oracle, and artificially
bloating the precision, recall, and F
1 metrics. The revised metrics are:
Naive decoding: Precision=78.65 Recall=72.85 Fscore=75.64 (row 7)
Beam search decoding: Precision=80.40 Recall=72.84 Fscore=76.43 (row 8)
This unintended artifact also changes the
interpretation of Table 9. The reported
results there should be interpreted as an oracle comparison of various inference
methods, that uses both automatically extr
acted candidate spans as well as gold spans
for argument identification.
None of the other results in the article are affected by this error.},
  Number                   = {1},
  Pages                    = {9--56},
  Volume                   = {40},

  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00163}
}

@Article{DBLP:journals/corr/DauphinVCB15,
  Title                    = {RMSProp and equilibrated adaptive learning rates for non-convex optimization},
  Author                   = {Yann N. Dauphin and Harm de Vries and Junyoung Chung and Yoshua Bengio},
  Journal                  = {CoRR},
  Year                     = {2015},

  Annote                   = {jekbradbury e.g. "When diagonal preconditioners are applied in an online optimization setting (i.e. in conjunction with SGD), they are often referred to as adaptive learning rates in the neural network literature." from basically what you really want in HF is not gradient descent preconditioned by the inverse Hessian, but gradient descent preconditioned by the inverse of something Hessian-like that's nonetheless positive-definite "we see that RMSProp fairly well estimates sqrt diag(H)\^2 in the beginning of training, but then quickly diverges" in that paper is summarized as " In 8, it is shown that the sqrt (gt+1) term approximates (in expectation) the diagonal of the absolute value of the Hessian matrix (assuming the update steps are N(0,1) distributed). It is also argued that the absolute value of the Hessian is better to use for non-convex problems which may have many saddle points. " (edited)},
  Volume                   = {abs/1502.04390},

  Archiveprefix            = {arXiv},
  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.org/rec/bib/journals/corr/DauphinVCB15},
  Eprint                   = {1502.04390},
  Owner                    = {20361362},
  Timestamp                = {Wed, 07 Jun 2017 14:41:49 +0200},
  Url                      = {http://arxiv.org/abs/1502.04390}
}

@Article{DeMulder2015,
  Title                    = {A survey on the application of recurrent neural networks to statistical language modeling},
  Author                   = {De Mulder, Wim and Bethard, Steven and Moens, Marie-Francine},
  Journal                  = {Computer Speech \& Language},
  Year                     = {2015},

  Annote                   = {Review article on RNNs
Takes of Language modeling, ie next word predition.

Covers variation on RNNs.},
  Number                   = {1},
  Pages                    = {61--98},
  Volume                   = {30},

  Owner                    = {20361362},
  Publisher                = {Elsevier},
  Timestamp                = {2015.08.26},
  Url                      = {http://ac.els-cdn.com/S088523081400093X/1-s2.0-S088523081400093X-main.pdf?_tid=e8cb0744-4ba0-11e5-8c3c-00000aacb35d&acdnat=1440559197_e56fe4786343f5c77482cbcc845e1bc7}
}

@Article{de2012pattern,
  Title                    = {Pattern for python},
  Author                   = {De Smedt, Tom and Daelemans, Walter},
  Journal                  = {The Journal of Machine Learning Research},
  Year                     = {2012},

  Annote                   = {This is the paper for the patterns project.
barely comments on the features for pluralistation etc

Pluralisation part of the agorithm comes from \cite{conway1998algorithmicpluralisation}},
  Number                   = {1},
  Pages                    = {2063--2067},
  Volume                   = {13},

  Keywords                 = {tools, NLP},
  Owner                    = {20361362},
  Publisher                = {JMLR. org},
  Timestamp                = {2015.06.25},
  Url                      = {http://jmlr.csail.mit.edu/papers/volume13/desmedt12a/desmedt12a.pdf}
}

@Inproceedings{imagenet_cvpr09,
  Title                    = {{ImageNet: A Large-Scale Hierarchical Image Database}},
  Author                   = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
  Booktitle                = {CVPR09},
  Year                     = {2009},

  Bibsource                = {http://www.image-net.org/papers/imagenet_cvpr09.bib},
  Owner                    = {20361362},
  Timestamp                = {2017.12.11},
  Url                      = {http://www.image-net.org/papers/imagenet_cvpr09.pdf}
}

@Article{dernoncourt2012designing,
  Title                    = {Designing an intelligent dialogue system for serious games.},
  Author                   = {Dernoncourt, F.},
  Journal                  = {RJC EIAHÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬ÃƒÂ¢Ã¢â‚¬Å¾Ã‚Â¢2012},
  Year                     = {2012},

  Annote                   = {A classical approch to a NLP/NLU chatterbot.

I vaguely know the author online.},
  Pages                    = {33},

  Owner                    = {20361362},
  Timestamp                = {2015.04.22},
  Url                      = {http://www.francky.me/doc/RJCEIAH2012_articlereviewed-20120412en.pdf}
}

@Inproceedings{dhillon2011multi,
  Title                    = {Multi-view learning of word embeddings via cca},
  Author                   = {Dhillon, Paramveer and Foster, Dean P and Ungar, Lyle H},
  Booktitle                = {Advances in Neural Information Processing Systems},
  Year                     = {2011},
  Annote                   = {One of the only things i've ever seen that suggest that there is a HMM of word-e,mebddings complete with a transition matrix.
That part of the argument is not actually relevant though.},
  Pages                    = {199--207},

  Owner                    = {20361362},
  Timestamp                = {2017.08.24},
  Url                      = {http://repository.upenn.edu/cgi/viewcontent.cgi?article=1716&context=cis_papers}
}

@Inproceedings{Dinu2014CompositionalGeneration,
  Title                    = {How to make words with vectors: Phrase generation in distributional semantics},
  Author                   = {Dinu, Georgiana and Baroni, Marco},
  Booktitle                = {Proceedings of ACL},
  Year                     = {2014},
  Annote                   = {\textcite{Dinu2014CompositionalGeneration} extends the models described by \textcite{zanzotto2010estimating, Guevara2010} for generation. The composition is described as the sum of pair of linear transformations of the input vectors ($\u_1$ and $\u_2$) to get a output vector $\p$. 
The input and target output vectors are found using single value decomposition on the word (for input) and phrase (for output) co-occurrence matrix for the training data. The composition function is given by: $$f_{comp}\::\:\mathbb{R}^d\times\mathbb{R}^d \to \mathbb{R}^d\::\: (\u_1, \u_2)\mapsto W_1\u_2+W_2\u_2$$
Likewise, decomposition can be described by: $$f_{decomp}\::\:\mathbb{R}^d\to \mathbb{R}^d\times\mathbb{R}^d \::\: \p \mapsto (W^\prime_1\p, W^\prime_2\p)$$.
During training linear transformation matrices $W_1$,$W_2$ and $W_1^\prime$,$W_2^\prime$ can be solved for using least squares regression.
While it theoretically generalises to whole sentences, by recursive application of the composition or decomposition functions, Dinu and Baroni's work is quantitatively assessed only for very short phrases.},
  Pages                    = {624--633},

  Owner                    = {20361362},
  Timestamp                = {2015.11.13},
  Url                      = {http://clic.cimec.unitn.it/marco/publications/acl2014/dinu-baroni-generation-acl2014.pdf}
}

@Article{dogan2016multiclassSVM,
  Title                    = {A unified view on multi-class support vector classification},
  Author                   = {Dogan, Ur{\"u}n and Glasmachers, Tobias and Igel, Christian},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2016},
  Number                   = {45},
  Pages                    = {1--32},
  Volume                   = {17},

  Owner                    = {20361362},
  Timestamp                = {2018.02.07},
  Url                      = {http://www.jmlr.org/papers/volume17/11-229/11-229.pdf}
}

@Inproceedings{msrParapharaCorpus,
  Title                    = {Automatically Constructing a Corpus of Sentential Paraphrases},
  Author                   = {William B. Dolan and Chris Brockett},
  Booktitle                = {Third International Workshop on Paraphrasing (IWP2005)},
  Year                     = {2005},
  Annote                   = {This is the paper about how the MSRP corpus was creasted.
MSRPC},
  Publisher                = {Asia Federation of Natural Language Processing},

  Abstract                 = {<p>An obstacle to research in automatic paraphrase identification and generation
 is the lack of large-scale, publiclyavailable labeled corpora of sentential
 paraphrases. This paper describes the creation of the recently-released
 MicrosoftResearch Paraphrase Corpus, which contains 5801 sentence pairs, each
 hand-labeled with a binary judgment as to whether the pair constitutes a
 paraphrase. The corpus was created using heuristic extraction techniques in
 conjunction with an SVM-based classifier to select likely sentence-level
 paraphrases from a large corpus of topicclustered news data. These pairs were
 then submitted to human judges, who confirmed that 67{\%} were in fact semantically
 equivalent. In addition to describing the corpus itself, we explore a number of
 issues that arose in defining guidelines for the human raters.</p>},
  Keywords                 = {paraphrasing, corpora},
  Owner                    = {20361362},
  Timestamp                = {2015.07.15},
  Url                      = {http://research.microsoft.com/pubs/101076/I05-5002[1].pdf}
}

@Article{donahue2014long,
  Title                    = {Long-term recurrent convolutional networks for visual recognition and description},
  Author                   = {Donahue, Jeff and Hendricks, Lisa Anne and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
  Journal                  = {arXiv preprint arXiv:1411.4389},
  Year                     = {2014},

  Keywords                 = {image description},
  Owner                    = {20361362},
  Timestamp                = {2015.12.17},
  Url                      = {http://arxiv.org/pdf/1411.4389.pdf}
}

@Article{drummond2009replicability,
  author    = {Drummond, Chris},
  journal   = {Proceedings of the Evaluation Methods for Machine Learning Workshop at the 26th ICML},
  title     = {{Replicability} is not reproducibility: nor is it good science},
  year      = {2009},
  abstract  = {At various machine learning conferences, atvarious times,
 there have been discussionsarising from the inability to replicate
 theexperimental results published in a paper.There seems to be a
 wide spread view that weneed to do something to address this
 prob-lem, as it is essential to the advancementof ou. The
 most compelling argumentwould seem to be that reproducibility of
 ex-perimental results is the hallmark of science.Therefore, given
 that most of us regard ma-chine learning as 
 discipline, beingable to replicate experiments is paramount.I want
 to challenge this view by separatingthe notion of reproducibility,
 a generally de-sirable property, from replicability, its
 poorcousin. I claim there are important diï¬€er-ences between the
 two. Reproducibility re-quires changes; replicability avoids them.
 Al-though reproducibility is desirable, I contendthat the
 impoverished version, replicability,is one not worth having.},
  owner     = {20361362},
  timestamp = {2018.04.16},
  url       = {http://www.site.uottawa.ca/ICML09WS/papers/w2.pdf},
}

@Article{AdaGrad,
  Title                    = {Adaptive subgradient methods for online learning and stochastic optimization},
  Author                   = {Duchi, John and Hazan, Elad and Singer, Yoram},
  Journal                  = {The Journal of Machine Learning Research},
  Year                     = {2011},

  Annote                   = {People are super keen on this method.

This paper is huge, and full of math i need to look up.

Read these instead: http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf
Adagrad provides a perfeature learning rate, where rare features have a higher learning rate.


what adagrad descr4ibes as $x$ and $x_{t+1}$, would be a row/column of $W$ in a neural network (I think)},
  Pages                    = {2121--2159},
  Volume                   = {12},

  Keywords                 = {optimisation},
  Owner                    = {20361362},
  Publisher                = {JMLR. org},
  Timestamp                = {2015.04.23},
  Url                      = {http://www.magicbroom.info/Papers/DuchiHaSi10.pdf}
}

@Article{Dufort1991,
  Title                    = {Color categorization and color constancy in a neural network model of V4},
  Author                   = {Dufort, P. A. and Lumsden, C. J.},
  Journal                  = {Biological Cybernetics},
  Year                     = {1991},
  Number                   = {4},
  Pages                    = {293--303},
  Volume                   = {65},

  Abstract                 = {We develop a neural network model that instantiates color constancy and color categorization in a single unified framework. Previous models achieve similar effects but ignore important biological constraints. Color constancy in this model is achieved by a new application of the double opponent cells found in the ``blobs'' of the visual cortex. Color categorization emerges naturally, as a consequence of processing chromatic stimuli as vectors in a four-dimensional color space. A computer simulation of this model is subjected to the classic psychophysical tests that first uncovered these phenomena, and its response matches psychophysical results very closely.},
  Doi                      = {10.1007/BF00206226},
  ISSN                     = {1432-0770},
  Owner                    = {20361362},
  Timestamp                = {2017.04.10},
  Url                      = {http://dx.doi.org/10.1007/BF00206226}
}

@Inproceedings{dumais1988using,
  Title                    = {Using latent semantic analysis to improve access to textual information},
  Author                   = {Dumais, Susan T and Furnas, George W and Landauer, Thomas K and Deerwester, Scott and Harshman, Richard},
  Booktitle                = {Proceedings of the SIGCHI conference on Human factors in computing systems},
  Year                     = {1988},
  Annote                   = {LSA},
  Organization             = {Acm},
  Pages                    = {281--285},

  Owner                    = {20361362},
  Timestamp                = {2017.08.25},
  Url                      = {http://delivery.acm.org/10.1145/60000/57214/p281-dumais.pdf?ip=130.95.149.38&id=57214&acc=ACTIVE%20SERVICE&key=65D80644F295BC0D%2E05ACB6DAA5D75F76%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=975588983&CFTOKEN=56546358&__acm__=1503641092_6b391d9a200184eda2841399b576519c}
}

@Article{stacklstm,
  Title                    = {Transition-Based Dependency Parsing with Stack Long Short-Term Memory},
  Author                   = {Chris Dyer and
 Miguel Ballesteros and
 Wang Ling and
 Austin Matthews and
 Noah A. Smith},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1505.08075},

  Archiveprefix            = {arXiv},
  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.org/rec/bib/journals/corr/DyerBLMS15},
  Eprint                   = {1505.08075},
  Owner                    = {20361362},
  Timestamp                = {Wed, 07 Jun 2017 14:41:38 +0200},
  Url                      = {http://arxiv.org/abs/1505.08075}
}

@Article{elman1990finding,
  Title                    = {Finding structure in time},
  Author                   = {Elman, Jeffrey L},
  Journal                  = {Cognitive science},
  Year                     = {1990},
  Number                   = {2},
  Pages                    = {179--211},
  Volume                   = {14},

  Owner                    = {20361362},
  Publisher                = {Wiley Online Library},
  Timestamp                = {2017.08.15},
  Url                      = {http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1/epdf}
}

@Inproceedings{elson2010socialnetworks,
  Title                    = {Extracting Social Networks from Literary Fiction},
  Author                   = {Elson, David K. and Dames, Nicholas and McKeown, Kathleen R.},
  Booktitle                = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  Year                     = {2010},

  Address                  = {Stroudsburg, PA, USA},
  Pages                    = {138--147},
  Publisher                = {Association for Computational Linguistics},
  Series                   = {ACL '10},

  Acmid                    = {1858696},
  Location                 = {Uppsala, Sweden},
  Numpages                 = {10},
  Owner                    = {20361362},
  Timestamp                = {2018.03.19},
  Url                      = {http://dl.acm.org/citation.cfm?id=1858681.1858696}
}

@Article{LIBLIBEAR,
  Title                    = {{LIBLINEAR}: A Library for Large Linear Classification},
  Author                   = {Rong-En Fan and Kai-Wei Chang and Cho-Jui Hsieh and Xiang-Rui Wang and Chih-Jen Lin},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2008},

  Annote                   = {This is the prefered citation for Liblinear (SVM)},
  Pages                    = {1871--1874},
  Volume                   = {9},

  Owner                    = {20361362},
  Timestamp                = {2015.09.08}
}

@Incollection{farhadi2010every,
  Title                    = {Every picture tells a story: Generating sentences from images},
  Author                   = {Farhadi, Ali and Hejrati, Mohsen and Sadeghi, Mohammad Amin and Young, Peter and Rashtchian, Cyrus and Hockenmaier, Julia and Forsyth, David},
  Booktitle                = {Computer Vision--ECCV 2010},
  Publisher                = {Springer},
  Year                     = {2010},
  Pages                    = {15--29},

  Keywords                 = {image description},
  Owner                    = {20361362},
  Timestamp                = {2015.09.09},
  Url                      = {https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf}
}

@Inproceedings{faruqui:2015:Retro,
  Title                    = {Retrofitting Word Vectors to Semantic Lexicons},
  Author                   = {Faruqui, Manaal and Dodge, Jesse and Jauhar, Sujay K. and Dyer, Chris and Hovy, Eduard and Smith, Noah A.},
  Booktitle                = {Proc. of NAACL},
  Year                     = {2015},

  Owner                    = {20361362},
  Timestamp                = {2017.03.29}
}

@Inproceedings{faruqui2014improving,
  Title                    = {Improving vector space word representations using multilingual correlation},
  Author                   = {Faruqui, Manaal and Dyer, Chris},
  Year                     = {2014},
  Organization             = {Association for Computational Linguistics},

  Owner                    = {20361362},
  Timestamp                = {2017.08.24}
}

@Article{fellbaum1990english,
  Title                    = {English verbs as a semantic net},
  Author                   = {Fellbaum, Christiane},
  Journal                  = {International Journal of Lexicography},
  Year                     = {1990},

  Annote                   = {Verbs in Wordnet
Interstingly has differenbt author to other wordnet papers},
  Number                   = {4},
  Pages                    = {278--301},
  Volume                   = {3},

  Keywords                 = {wordnet},
  Owner                    = {20361362},
  Publisher                = {Oxford Univ Press},
  Timestamp                = {2015.07.16},
  Url                      = {http://ijl.oxfordjournals.org/content/3/4/278.full.pdf}
}

@Inproceedings{WordSim353,
  Title                    = {Placing search in context: The concept revisited},
  Author                   = {Finkelstein, Lev and Gabrilovich, Evgeniy and Matias, Yossi and Rivlin, Ehud and Solan, Zach and Wolfman, Gadi and Ruppin, Eytan},
  Booktitle                = {Proceedings of the 10th international conference on World Wide Web},
  Year                     = {2001},
  Annote                   = {Prefered citation for wordsim353},
  Organization             = {ACM},
  Pages                    = {406--414},

  Keywords                 = {corpora},
  Owner                    = {20361362},
  Timestamp                = {2016.06.14},
  Url                      = {http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/}
}

@Inproceedings{JARMparser,
  Title                    = {A Discriminative Graph-Based Parser for the Abstract Meaning Representation},
  Author                   = {Flanigan, Jeffrey and Thomson, Sam and Carbonell, Jaime and Dyer, Chris and Smith, Noah A.},
  Booktitle                = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  Year                     = {2014},

  Address                  = {Baltimore, Maryland},
  Annote                   = {Carbonell, Jaime and Dyer, Chris and Smith, Noah A. are the supervisors of Flanigan, Jeffrey 

Parses sentences into AMR, using graph techniques.
Stuff being done in this is pretty neat.


Evaluation is two steps:
 - Mapping words to concept nodes, done with a linear feature scoring
 - Mapping Relationships to edges (or rather fdereminging when realtiuonship edges exist), done with a maximum spanning subgraph alorigthm which maxised the scrores from a different linear features scoring.


Both feature scorers much be trained.
For the word to concept mapping a "Automatic Aligner" is used to know which concepts (in AMR space), align to which words. That is to say the auytomatic aligner is used to determine the ground truth from the training data (of sentecnec paired wit their AMR rep) to which the concept mapping is trained to replicate.},
  Month                    = {June},
  Pages                    = {1426--1436},
  Publisher                = {Association for Computational Linguistics},

  Abstract                 = {We present a novel per-dimension learning rate method for
gradient descent called ADADELTA. The method dynami-
cally adapts over time using only first order information and
has minimal computational overhead beyond vanilla stochas-
tic gradient descent. The method requires no manual tuning of
a learning rate and appears robust to noisy gradient informa-
tion, different model architecture choices, various data modal-
ities and selection of hyperparameters. We show promising
results compared to other methods on the MNIST digit clas-
sification task using a single machine and on a large scale
voice dataset in a distributed cluster environment},
  Keywords                 = {AMR},
  Owner                    = {20361362},
  Timestamp                = {2015.04.22},
  Url                      = {http://www.cs.cmu.edu/~jmflanig/flanigan+etal.acl2014.pdf}
}

@Article{francis1979brown,
  Title                    = {Brown corpus manual},
  Author                   = {Francis, W Nelson and Kucera, Henry},
  Journal                  = {Brown University},
  Year                     = {1979},

  Abstract                 = {This Manual was first published in 1964, when the Standard Sample of Present-Day American English (the Brown Corpus) was first made available. *) A revised edition was issued in 1971, principally to incorporate information about the text turned up in seven years of use. The present revision is more extensive, since it includes information about recently prepared versions of the Corpus, notably the ÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â«taggedÃƒÆ’Ã¢â‚¬Å¡Ãƒâ€šÃ‚Â» text completed at Brown University in 1979. Two complete proofreadings of the Corpus have resulted in corrections of two kinds: errors in the preparation of the original tape, which have been silently corrected in recently issued copies, and further typographical errors and anomalies in the underlying text, which have been recorded in the descriptions of individual samples on pages 33-176. (Most of these were listed on corrigenda sheets which have been enclosed with recently issued copies of the Manual and incorporated in this web-version.)},
  Keywords                 = {corpora},
  Owner                    = {20361362},
  Timestamp                = {2015.11.26},
  Url                      = {http://clu.uni.no/icame/brown/bcm.html}
}

@Article{frey2007clustering,
  Title                    = {Clustering by passing messages between data points},
  Author                   = {Frey, Brendan J and Dueck, Delbert},
  Journal                  = {Science},
  Year                     = {2007},

  Annote                   = {Quiet possibly the best clustering algorithm out.},
  Number                   = {5814},
  Pages                    = {972--976},
  Volume                   = {315},

  Owner                    = {20361362},
  Publisher                = {American Association for the Advancement of Science},
  Timestamp                = {2015.05.28},
  Url                      = {http://dlib.net.pku.edu.cn/qmeiCourse/files/AffinityPropagation.pdf}
}

@Inproceedings{gcca,
  Title                    = {Efficient and Distributed Algorithms for Large-Scale Generalized Canonical Correlations Analysis},
  Author                   = {X. Fu and K. Huang and E. E. Papalexakis and H. A. Song and P. P. Talukdar and N. D. Sidiropoulos and C. Faloutsos and T. Mitchell},
  Booktitle                = {2016 IEEE 16th International Conference on Data Mining (ICDM)},
  Year                     = {2016},
  Month                    = {Dec},
  Pages                    = {871-876},

  Abstract                 = {Generalized canonical correlation analysis (GCCA) aims at extracting common structure from multiple 'views', i.e., high-dimensional matrices representing the same objects in different feature domains - an extension of classical two-view CCA. Existing (G)CCA algorithms have serious scalability issues, since they involve square root factorization of the correlation matrices of the views. The memory and computational complexity associated with this step grow as a quadratic and cubic function of the problem dimension (the number of samples / features), respectively. To circumvent such difficulties, we propose a GCCA algorithm whose memory and computational costs scale linearly in the problem dimension and the number of nonzero data elements, respectively. Consequently, the proposed algorithm can easily handle very large sparse views whose sample and feature dimensions both exceed 100,000 - while the current approaches can only handle thousands of features / samples. Our second contribution is a distributed algorithm for GCCA, which computes the canonical components of different views in parallel and thus can further reduce the runtime significantly (by >= 30\% in experiments) if multiple cores are available. Judiciously designed synthetic and real-data experiments using a multilingual dataset are employed to showcase the effectiveness of the proposed algorithms.},
  Doi                      = {10.1109/ICDM.2016.0105},
  Keywords                 = {computational complexity;correlation methods;distributed algorithms;GCCA algorithm;computational costs;distributed algorithm;generalized canonical correlations analysis;Algorithm design and analysis;Approximation algorithms;Complexity theory;Correlation;Distributed algorithms;Feature extraction;Sparse matrices;Lagre-scale generalized canonical correlation analysis;distributed GCCA;multilingual word embeddings},
  Owner                    = {20361362},
  Timestamp                = {2017.08.25}
}

@Inproceedings{4518770,
  Title                    = {Rhetorical-State Hidden Markov Models for extractive speech summarization},
  Author                   = {Fung, P. and Chan, R.H.Y. and Zhang, J.J.},
  Booktitle                = {Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on},
  Year                     = {2008},
  Month                    = {March},
  Pages                    = {4957-4960},

  Abstract                 = {We propose an extractive summarization system with a novel non-generative probabilistic framework for speech summarization. One of the most underutilized features in extractive summarization is rhetorical information - semantically cohesive units that are hidden in spoken documents. We propose Rhetorical-State Hidden Markov Models (RSHMMs) to automatically decode this underlying structure in speech. We show that RSHMMs give a ROUGE-L F-measure, a absolute increase in lecture speech summarization performance compared to the baseline system without using RSHMM. It equally outperforms the baseline system with additional discourse features, showing that our RSHMM is a more refined improvement on the conventional discourse feature.},
  Doi                      = {10.1109/ICASSP.2008.4518770},
  ISSN                     = {1520-6149},
  Keywords                 = {hidden Markov models;speech processing;baseline system;extractive speech summarization;nongenerative probabilistic framework;rhetorical information;rhetorical-state hidden Markov models;Automatic speech recognition;Data mining;Decoding;Feature extraction;Hidden Markov models;Humans;Natural languages;Support vector machine classification;Support vector machines;Text recognition;hidden Markov models;rhetorical information;speech features;spoken document summarization},
  Owner                    = {20361362},
  Timestamp                = {2015.05.28}
}

@Inproceedings{Fung:2003:COC:1119312.1119315,
  Title                    = {Combining Optimal Clustering and Hidden Markov Models for Extractive Summarization},
  Author                   = {Fung, Pascale and Ngai, Grace and Cheung, Chi-Shun},
  Booktitle                = {Proceedings of the ACL 2003 Workshop on Multilingual Summarization and Question Answering - Volume 12},
  Year                     = {2003},

  Address                  = {Stroudsburg, PA, USA},
  Pages                    = {21--28},
  Publisher                = {Association for Computational Linguistics},
  Series                   = {MultiSumQA '03},

  Acmid                    = {1119315},
  Doi                      = {10.3115/1119312.1119315},
  Location                 = {Sapporo, Japan},
  Numpages                 = {8},
  Owner                    = {20361362},
  Timestamp                = {2015.05.28},
  Url                      = {http://dx.doi.org/10.3115/1119312.1119315}
}

@Book{Gardenfors2000Cs,
  Title                    = {Conceptual spaces : the geometry of thought},
  Author                   = {G{\"a}rdenfors, Peter},
  Publisher                = {MIT Press},
  Year                     = {2000},

  Address                  = {Cambridge, Mass.},

  ISBN                     = {0262071991},
  Keywords                 = {Artificial intelligence; Cognitive science},
  Language                 = {eng},
  Lccn                     = {99046109},
  Owner                    = {20361362},
  Timestamp                = {2017.07.04}
}

@Inproceedings{ganesan2010opinosis,
  Title                    = {Opinosis: a graph-based approach to abstractive summarization of highly redundant opinions},
  Author                   = {Ganesan, Kavita and Zhai, ChengXiang and Han, Jiawei},
  Booktitle                = {Proceedings of the 23rd International Conference on Computational Linguistics},
  Year                     = {2010},
  Annote                   = {This is the citation for the extractive sumaraistion dataset used by \cite{KaagebExtractiveSummaristation}},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {340--348},

  Keywords                 = {corpora, summarization},
  Owner                    = {20361362},
  Timestamp                = {2015.06.05},
  Url                      = {http://kavita-ganesan.com/opinosis-opinion-dataset}
}

@Article{gao2014learning,
  Title                    = {Learning continuous phrase representations for translation modeling},
  Author                   = {Gao, Jianfeng and He, Xiaodong and Yih, Wen-tau and Deng, Li},
  Journal                  = {Proc. of ACL. Association for Computational Linguistics, June},
  Year                     = {2014},

  Annote                   = {Input is bag of words, where each bag is a phrase.
Translation by projecting to a common embedding space.

The training method is based on L-BFGS.

Notebly they do not use the Cosign Similarity:
Quote: "In our experiments, we compare dot product and the cosine 
similarity functions and find that the former works better for nonlinear multilayer neural networks, and the latter works 
better for linear neural networks"},

  Keywords                 = {MT},
  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://research.microsoft.com/pubs/211749/nn4smt.acl.v9.pdf}
}

@Inproceedings{Germann2001,
  Title                    = {Fast decoding and optimal decoding for machine translation},
  Author                   = {Germann, Ulrich and Jahr, Michael and Knight, Kevin and Marcu, Daniel and Yamada, Kenji},
  Booktitle                = {Proceedings of the 39th Annual Meeting on Association for Computational Linguistics},
  Year                     = {2001},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {228--235},

  Keywords                 = {word-ordering, machine-translation},
  Owner                    = {20361362},
  Timestamp                = {2016.02.04},
  Url                      = {http://www.dtic.mil/dtic/tr/fulltext/u2/a459945.pdf}
}

@Article{gers1999learning,
  Title                    = {Learning to forget: Continual prediction with LSTM},
  Author                   = {Gers, Felix A and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  Year                     = {1999},

  Owner                    = {20361362},
  Publisher                = {IET},
  Timestamp                = {2017.12.06},
  Url                      = {https://www.researchgate.net/profile/Felix_Gers/publication/12292425_Learning_to_Forget_Continual_Prediction_with_LSTM/links/5759414608ae9a9c954e84c5/Learning-to-Forget-Continual-Prediction-with-LSTM.pdf}
}

@Article{gershmanphrase,
  Title                    = {Phrase similarity in humans and machines},
  Author                   = {Gershman, Samuel J and Tenenbaum, Joshua B},
  Journal                  = {Proceedings of the 37th Annual Conference of the Cognitive Science Society},
  Year                     = {2015},

  Annote                   = {It is compairing similarity rank in embedding space to a ranking given by people},

  Owner                    = {20361362},
  Timestamp                = {2015.06.23},
  Url                      = {http://web.mit.edu/sjgershm/www/GershmanTenenbaum15.pdf}
}

@Incollection{gilmour2005understanding,
  Title                    = {Understanding the pheromone system within ant colony optimization},
  Author                   = {Gilmour, Stephen and Dras, Mark},
  Booktitle                = {AI 2005: Advances in Artificial Intelligence},
  Publisher                = {Springer},
  Year                     = {2005},
  Pages                    = {786--789},

  Owner                    = {20361362},
  Timestamp                = {2015.10.19},
  Url                      = {http://web.science.mq.edu.au/~madras/papers/ai05.pdf}
}

@Inproceedings{gladkova2016analogy,
  Title                    = {Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn't.},
  Author                   = {Gladkova, Anna and Drozd, Aleksandr and Matsuoka, Satoshi},
  Booktitle                = {SRW@ HLT-NAACL},
  Year                     = {2016},
  Annote                   = {Bigger and with more categories},
  Pages                    = {8--15},

  Owner                    = {20361362},
  Timestamp                = {2017.08.22},
  Url                      = {http://www.aclweb.org/website/old_anthology/N/N16/N16-2002.pdf}
}

@Inproceedings{glorot2011deepRELUsparse,
  Title                    = {Deep sparse rectifier neural networks},
  Author                   = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  Booktitle                = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  Year                     = {2011},
  Pages                    = {315--323},

  Owner                    = {20361362},
  Timestamp                = {2017.07.25},
  Url                      = {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf}
}

@Inproceedings{goller1996BPstructure,
  Title                    = {Learning task-dependent distributed representations by backpropagation through structure},
  Author                   = {Goller, Christoph and Kuchler, Andreas},
  Booktitle                = {Neural Networks, 1996., IEEE International Conference on},
  Year                     = {1996},
  Annote                   = {This is not a fun paper to read or understand.
It is however cruicial for the understanding of Socher2011

It deals with how to apply Back-propergation Through Structure (
BPTS) to Directed Acyclic Graphs (DAG) (notables including trees).
Which is like Back-propergation Though Time.},
  Organization             = {IEEE},
  Pages                    = {347--352},
  Volume                   = {1},

  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.1968&rep=rep1&type=pdf}
}

@Inproceedings{gong2011dynamic,
  Title                    = {Dynamic manifold warping for view invariant action recognition},
  Author                   = {Gong, Dian and Medioni, Gerard},
  Booktitle                = {2011 International Conference on Computer Vision},
  Year                     = {2011},
  Organization             = {IEEE},
  Pages                    = {571--578},

  Keywords                 = {computer-vision},
  Owner                    = {20361362},
  Timestamp                = {2016.08.22},
  Url                      = {http://www-scf.usc.edu/~diangong/index_files/ICCV2011.pdf}
}

@Incollection{gonzalez2012graph,
  Title                    = {A graph-based method to improve wordnet domains},
  Author                   = {Gonz{\'a}lez, Aitor and Rigau, German and Castillo, Mauro},
  Booktitle                = {Computational Linguistics and Intelligent Text Processing},
  Publisher                = {Springer},
  Year                     = {2012},
  Annote                   = {This is the paper for exented wordnet domains.},
  Pages                    = {17--28},

  Keywords                 = {wordnet},
  Owner                    = {20361362},
  Timestamp                = {2015.07.16},
  Url                      = {http://adimen.si.ehu.es/~rigau/publications/cicling12-gcr.pdf}
}

@Book{TheDeepLearningBook,
  Title                    = {Deep Learning},
  Author                   = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  Publisher                = {MIT Press},
  Year                     = {2016},
  Note                     = {\url{http://www.deeplearningbook.org}},

  Owner                    = {20361362},
  Timestamp                = {2017.12.08}
}

@Article{lookafterdata,
  Title                    = {Ten Simple Rules for the Care and Feeding of Scientific Data},
  Author                   = {Goodman, Alyssa AND Pepe, Alberto AND Blocker, Alexander W. AND Borgman, Christine L. AND Cranmer, Kyle AND Crosas, Merce AND Di Stefano, Rosanne AND Gil, Yolanda AND Groth, Paul AND Hedstrom, Margaret AND Hogg, David W. AND Kashyap, Vinay AND Mahabal, Ashish AND Siemiginowska, Aneta AND Slavkovic, Aleksandra},
  Journal                  = {PLOS Computational Biology},
  Year                     = {2014},
  Month                    = {04},
  Number                   = {4},
  Pages                    = {1-5},
  Volume                   = {10},

  Doi                      = {10.1371/journal.pcbi.1003542},
  Owner                    = {20361362},
  Publisher                = {Public Library of Science},
  Timestamp                = {2018.01.29}
}

@Article{DBLP:journals/corr/cs-CL-0108005,
  Title                    = {A Bit of Progress in Language Modeling},
  Author                   = {Joshua Goodman},
  Journal                  = {CoRR},
  Year                     = {2001},
  Volume                   = {cs.CL/0108005},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/cs-CL-0108005},
  Owner                    = {20361362},
  Timestamp                = {Wed, 07 Jun 2017 14:40:38 +0200},
  Url                      = {http://arxiv.org/abs/cs.CL/0108005}
}

@Article{DBLP:journals/corr/GravesWD14,
  Title                    = {Neural Turing Machines},
  Author                   = {Alex Graves and
 Greg Wayne and
 Ivo Danihelka},
  Journal                  = {CoRR},
  Year                     = {2014},

  Annote                   = {Not well typeset. Figures miss-placed -- check they are lining up with text before over thinking it.

Several experiments are using a Neural network which control a memory bank.
Notably every part of the system is differntable allowing graident desent to be used.

Head "Head nearons" that have read and a write weight matrix to contol how they interact with the memory store.},
  Volume                   = {abs/1410.5401},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/GravesWD14},
  Owner                    = {20361362},
  Timestamp                = {2015.04.11},
  Url                      = {http://arxiv.org/pdf/1410.5401v2.pdf}
}

@Article{Grebla,
  Title                    = {Multi-Dimensional OFDMA Scheduling in a Wireless Network with Relay Nodes},
  Author                   = {Grebla, Reuven Cohen Guy},

  Annote                   = {Has a heuristic solution for sparse multidimentional multichoice knapsack problems.},

  Owner                    = {20361362},
  Timestamp                = {2015.11.06},
  Url                      = {http://www.cs.technion.ac.il/~rcohen/PAPERS/mult-OFDMA.pdf}
}

@Article{Grefenstette2014,
  Title                    = {A Deep Architecture for Semantic Parsing},
  Author                   = {Grefenstette, Edward and Blunsom, Phil and de Freitas, Nando and Hermann, Karl Moritz},
  Journal                  = {arXiv preprint arXiv:1404.7296},
  Year                     = {2014},

  Annote                   = {A neural network approch that takes in raw prhases, and converst it to ontologu specific queires.(SO more knowledge base querying).
Apparently without partsing.

It does use word embeddigns

I have not read this paper in deatil},

  Abstract                 = {Many successful approaches to semantic
parsing build on top of the syntactic anal-
ysis of text, and make use of distribu-
tional representations or statistical mod-
els to match parses to ontology-specific
queries. This paper presents a novel deep
learning architecture which provides a se-
mantic parsing system through the union
of two neural models of language se-
mantics. It allows for the generation of
ontology-specific queries from natural lan-
guage statements and questions without
the need for parsing, which makes it es-
pecially suitable to grammatically mal-
formed or syntactically atypical text, such
as tweets, as well as permitting the devel-
opment of semantic parsers for resource-
poor languages.},
  Keywords                 = {phrase-embeddings},
  Owner                    = {20361362},
  Timestamp                = {2015.04.18},
  Url                      = {http://yoavartzi.com/sp14/pub/gbfh-sp14-2014.pdf}
}

@Article{Gudmundsson1999,
  Title                    = {A fast approximation algorithm for TSP with neighborhoods},
  Author                   = {Gudmundsson, Joachim and Levcopoulos, Christos},
  Journal                  = {Nord. J. Comput.},
  Year                     = {1999},
  Number                   = {4},
  Pages                    = {469},
  Volume                   = {6},

  Owner                    = {20361362},
  Timestamp                = {2016.02.25},
  Url                      = {http://rp-www.cs.usyd.edu.au/~joachim/Papers/J2.pdf}
}

@Inproceedings{Guevara2010,
  Title                    = {A regression model of adjective-noun compositionality in distributional semantics},
  Author                   = {Guevara, Emiliano},
  Booktitle                = {Proceedings of the 2010 Workshop on Geometrical Models of Natural Language Semantics},
  Year                     = {2010},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {33--37},

  Owner                    = {20361362},
  Timestamp                = {2015.11.19},
  Url                      = {http://www.aclweb.org/anthology/W10-28#page=43}
}

@Inproceedings{translating-unknown-words-2016,
  Title                    = {Translation of Unknown Words in Low Resource Languages},
  Author                   = {Biman Gujral and Huda Khayrallah and Philipp Koehn},
  Booktitle                = {Proceedings of the Conference of the Association for Machine Translation in the Americas (AMTA)},
  Year                     = {2016},
  Annote                   = {Uses CCA on word embeddings as part of a hybrid system translate out of vocabulary words.},

  Owner                    = {20361362},
  Timestamp                = {2017.08.25},
  Url                      = {http://www.cs.jhu.edu/~huda/papers/gujral2016AMTA.pdf}
}

@Article{Gupta2010,
  Title                    = {A survey of text summarization extractive techniques},
  Author                   = {Gupta, Vishal and Lehal, Gurpreet Singh},
  Journal                  = {Journal of Emerging Technologies in Web Intelligence},
  Year                     = {2010},

  Annote                   = {(Its not not published in a preditory journal, I checked. JETWI is a real thing and Academy Publisher too. It doesn't look nice)
Seems like quiet a nice review of text sumarisation.
focus on extractive, but does define abstractive..},
  Number                   = {3},
  Pages                    = {258--268},
  Volume                   = {2},

  Owner                    = {20361362},
  Timestamp                = {2015.04.17},
  Url                      = {http://learnpunjabi.org/pdf/survey-paper.pdf}
}

@Article{gutmann2012noise,
  Title                    = {Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics},
  Author                   = {Gutmann, Michael U and Hyv{\"a}rinen, Aapo},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2012},
  Number                   = {Feb},
  Pages                    = {307--361},
  Volume                   = {13},

  Owner                    = {20361362},
  Timestamp                = {2017.10.11},
  Url                      = {http://www.jmlr.org/papers/volume13/gutmann12a/gutmann12a.pdf}
}

@Article{ha2009extending,
  Title                    = {Extending Zipf's law to n-grams for large corpora},
  Author                   = {Ha, Le Quan and Hanna, Philip and Ming, Ji and Smith, F Jack},
  Journal                  = {Artificial Intelligence Review},
  Year                     = {2009},
  Number                   = {1},
  Pages                    = {101--113},
  Volume                   = {32},

  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2017.07.11},
  Url                      = {https://link.springer.com/article/10.1007/s10462-009-9135-4}
}

@Article{hahnloser1998piecewise,
  Title                    = {On the piecewise analysis of networks of linear threshold neurons},
  Author                   = {Hahnloser, Richard LT},
  Journal                  = {Neural Networks},
  Year                     = {1998},
  Number                   = {4},
  Pages                    = {691--697},
  Volume                   = {11},

  Owner                    = {20361362},
  Publisher                = {Elsevier},
  Timestamp                = {2017.07.31},
  Url                      = {https://pdfs.semanticscholar.org/a09e/f3a49654fdbc7c446884369f8c9ca542b012.pdf}
}

@Inproceedings{Harabagiu:2006:NCC:1597538.1597659,
  Title                    = {Negation, Contrast and Contradiction in Text Processing},
  Author                   = {Harabagiu, Sanda and Hickl, Andrew and Lacatusu, Finley},
  Booktitle                = {Proceedings of the 21st National Conference on Artificial Intelligence - Volume 1},
  Year                     = {2006},
  Annote                   = {Cool.About Negation/Contradiction and Contrast (as two sperate tasks)
Negation/Contradoiction is the opposite of textual entilment,contrast is "X, but Y" etc.


They use a pile of features and some graph stuff (to find antonyms) to do the dections.
They had human annotators manually},
  Pages                    = {755--762},
  Publisher                = {AAAI Press},
  Series                   = {AAAI'06},

  Acmid                    = {1597659},
  ISBN                     = {978-1-57735-281-5},
  Location                 = {Boston, Massachusetts},
  Numpages                 = {8},
  Owner                    = {20361362},
  Timestamp                = {2015.08.04},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.7652&rep=rep1&type=pdf}
}

@Book{hebb1949organization,
  Title                    = {The Organization of Behavior: A Neuropsychological Theory},
  Author                   = {Hebb, D.O.},
  Publisher                = {Wiley},
  Year                     = {1949},
  Annote                   = {The book that contains the theory of how the brain works that insired nerual networks},
  Series                   = {A Wiley book in clinical psychology},

  Lccn                     = {49050182},
  Owner                    = {20361362},
  Timestamp                = {2017.10.13},
  Url                      = {https://books.google.com.au/books?id=yReB9eGP2pwC}
}

@Article{heider1972universals,
  Title                    = {Universals in color naming and memory.},
  Author                   = {Heider, Eleanor R},
  Journal                  = {Journal of experimental psychology},
  Year                     = {1972},
  Number                   = {1},
  Pages                    = {10},
  Volume                   = {93},

  Owner                    = {20361362},
  Publisher                = {American Psychological Association},
  Timestamp                = {2017.07.03}
}

@Article{HEIDER1972337,
  Title                    = {The structure of the color space in naming and memory for two languages},
  Author                   = {Eleanor Rosch Heider and Donald C. Olivier},
  Journal                  = {Cognitive Psychology},
  Year                     = {1972},
  Number                   = {2},
  Pages                    = {337 - 354},
  Volume                   = {3},

  Abstract                 = {Ss from two cultures with markedly different color terminologies were tested on two color-judgment tasks. One was a nonverbal task of color matching from memory, while the other was a verbal task of color-naming. Both tasks were performed by 41 American Ss and 40 New Guinea Dani (who have a basically two-term color language). Multidimensional scaling based upon the four resulting sets of data yielded structures that were more similar under the memory condition than under the naming condition. For neither culture were equally distant colors confused in memory more within than across name boundaries. Retention of color images in short-term memory appears to be unaffected by wide cultural differences in the semantic reference of color words.},
  Doi                      = {http://dx.doi.org/10.1016/0010-0285(72)90011-4},
  ISSN                     = {0010-0285},
  Owner                    = {20361362},
  Timestamp                = {2017.07.03},
  Url                      = {http://www.sciencedirect.com/science/article/pii/0010028572900114}
}

@Article{hennig2010methods,
  Title                    = {Methods for merging Gaussian mixture components},
  Author                   = {Hennig, Christian},
  Journal                  = {Advances in data analysis and classification},
  Year                     = {2010},

  Annote                   = {DEMP},
  Number                   = {1},
  Pages                    = {3--34},
  Volume                   = {4},

  Abstract                 = {The problem of merging Gaussian mixture components is discussed in
situations where a Gaussian mixture is fitted but the mixture components are not
separated enough from each other to interpret them as ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã¢â‚¬Å“clustersÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬ÃƒÂ¯Ã‚Â¿Ã‚Â½. The problem of
merging Gaussian mixtures is not statistically identifiable, therefore merging algo-
rithms have to be based on subjective cluster concepts. Cluster concepts based on
unimodality and misclassification probabilities (ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã¢â‚¬Å“patternsÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬ÃƒÂ¯Ã‚Â¿Ã‚Â½) are distinguished. Several
different hierarchical merging methods are proposed for different cluster concepts,
based on the ridgeline analysis of modality of Gaussian mixtures, the dip test, the
Bhattacharyya dissimilarity, a direct estimator of misclassification and the strength of
predicting pairwise cluster memberships. The methods are compared by a simulation
study and application to two real datasets. A new visualisation method of the separation
of Gaussian mixture components, the ordered posterior plot, is also introduced.},
  Keywords                 = {model merging, clustering},
  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2016.06.27},
  Url                      = {http://download.springer.com/static/pdf/26/art%253A10.1007%252Fs11634-010-0058-3.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs11634-010-0058-3&token2=exp=1467010407~acl=%2Fstatic%2Fpdf%2F26%2Fart%25253A10.1007%25252Fs11634-010-0058-3.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Farticle%252F10.1007%252Fs11634-010-0058-3*~hmac=fe04e8ccd01691d56a798ab15d25fec16e57323cedaeb5a7a2ec9d33ed4b2032}
}

@Article{DBLP:journals/corr/HermannB13,
  Title                    = {A Simple Model for Learning Multilingual Compositional Semantics},
  Author                   = {Karl Moritz Hermann and
 Phil Blunsom},
  Journal                  = {CoRR},
  Year                     = {2013},

  Annote                   = {Good Stuff Sentece embeddings via parallel corpora},
  Volume                   = {abs/1312.6173},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/HermannB13},
  Owner                    = {20361362},
  Timestamp                = {2015.08.04},
  Url                      = {http://arxiv.org/abs/1312.6173}
}

@Inproceedings{hermann-blunsom:2013:ACL2013,
  Title                    = {{The Role of Syntax in Vector Space Models of Compositional Semantics}},
  Author                   = {Hermann, Karl Moritz and Blunsom, Phil},
  Booktitle                = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  Year                     = {2013},

  Address                  = {Sofia, Bulgaria},
  Month                    = {August},
  Pages                    = {894--904},
  Publisher                = {Association for Computational Linguistics},

  Owner                    = {20361362},
  Timestamp                = {2015.08.31},
  Url                      = {http://www.karlmoritz.com/_media/hermannblunsom_acl2013.pdf}
}

@Article{hinton2002RBM,
  Title                    = {Training products of experts by minimizing contrastive divergence},
  Author                   = {Hinton, Geoffrey E},
  Journal                  = {Neural computation},
  Year                     = {2002},

  Annote                   = {this is the RBM paper},
  Number                   = {8},
  Pages                    = {1771--1800},
  Volume                   = {14},

  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2017.09.01},
  Url                      = {http://www.mitpressjournals.org/doi/pdfplus/10.1162/089976602760128018}
}

@Article{hinton2006fastDBN,
  Title                    = {A fast learning algorithm for deep belief nets},
  Author                   = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  Journal                  = {Neural computation},
  Year                     = {2006},
  Number                   = {7},
  Pages                    = {1527--1554},
  Volume                   = {18},

  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2017.07.25}
}

@Article{hinton2006reducing,
  Title                    = {Reducing the dimensionality of data with neural networks},
  Author                   = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  Journal                  = {science},
  Year                     = {2006},
  Number                   = {5786},
  Pages                    = {504--507},
  Volume                   = {313},

  Owner                    = {20361362},
  Publisher                = {American Association for the Advancement of Science},
  Timestamp                = {2017.09.01},
  Url                      = {http://www.cs.toronto.edu/~hinton/science.pdf}
}

@Article{hochreiter1997long,
  Title                    = {Long short-term memory},
  Author                   = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  Journal                  = {Neural computation},
  Year                     = {1997},
  Number                   = {8},
  Pages                    = {1735--1780},
  Volume                   = {9},

  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2016.02.09},
  Url                      = {http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf}
}

@Inproceedings{hofmann2000learning,
  Title                    = {Learning the similarity of documents: An information-geometric approach to document retrieval and categorization},
  Author                   = {Hofmann, Thomas},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {2000},
  Pages                    = {914--920},

  Owner                    = {20361362},
  Timestamp                = {2017.11.22},
  Url                      = {http://papers.nips.cc/paper/1654-learning-the-similarity-of-documents-an-information-geometric-approach-to-document-retrieval-and-categorization.pdf}
}

@Inproceedings{spacy,
  Title                    = {An Improved Non-monotonic Transition System for Dependency Parsing},
  Author                   = {Honnibal, Matthew and Johnson, Mark},
  Booktitle                = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  Year                     = {2015},

  Address                  = {Lisbon, Portugal},
  Month                    = {September},
  Pages                    = {1373--1378},
  Publisher                = {Association for Computational Linguistics},

  Owner                    = {20361362},
  Timestamp                = {2018.03.02},
  Url                      = {http://aclweb.org/anthology/D15-1162}
}

@Inproceedings{Horvat2014,
  Title                    = {A Graph-Based Approach to String Regeneration.},
  Author                   = {Horvat, Matic and Byrne, William},
  Booktitle                = {EACL},
  Year                     = {2014},
  Annote                   = {Generate ordering word order bow


I was sure someone would have thought to do this before.
Rewriting the bag of words ordering problem as some form of TSP problem.
There were a few notions around it going back to 1999ish, but they tend to be mixed in with other machine translation concerns.

but there is a 2014 paper that is almost exactly what I have done.
(It came out of the first authors master's thesis).

The difference between there method and mine are 2:
 1 - They go to a true closed TSP, where as I go to a Open TSP with known start and beginning vexticies.
 2 - They feed that to a TSP solver, where as I process it further and give it to a MIPs solver

They translate the problem to a Generalised Asymetric TSP (GATSP), then feed that to a Traveling Salesman solver.
I translate the problem to an variation on the GATSP, where the tour does not begin where it starts,
then translate that problem in to a Mixed Integer Programming (MIPS) problem, which I feed to a MIPS solver.

My variation of the GATSP, can be turned back into a normal closed GATSP, by adding a zero cost edge between the END vertex and the START vertex. This does potentially change the performance characteristics, (though not in a big O kinda way).

Under the hood of the Traveling Salesman Solver is a optimized MIPs solver (possibly some other things to, but I know there is a MIPs solver from looking at their website).},
  Pages                    = {85--95},

  Owner                    = {20361362},
  Timestamp                = {2016.02.05},
  Url                      = {https://www.cl.cam.ac.uk/~mh693/files/eacl2014_paper_prepublish.pdf}
}

@Article{hotelling1933analysis,
  Title                    = {Analysis of a complex of statistical variables into principal components.},
  Author                   = {Hotelling, Harold},
  Journal                  = {Journal of educational psychology},
  Year                     = {1933},

  Annote                   = {The original PCA paper.},
  Number                   = {6},
  Pages                    = {417},
  Volume                   = {24},

  Owner                    = {20361362},
  Publisher                = {Warwick \& York},
  Timestamp                = {2015.07.10},
  Url                      = {http://ovidsp.tx.ovid.com.ezproxy.library.uwa.edu.au/sp-3.16.0a/ovidweb.cgi?WebLinkFrameset=1&S=KLGPFPKHCMDDAGDLNCKKHFDCNILNAA00&returnUrl=ovidweb.cgi%3fMain%2bSearch%2bPage%3d1%26S%3dKLGPFPKHCMDDAGDLNCKKHFDCNILNAA00&directlink=http%3a%2f%2fgraphics.tx.ovid.com%2fovftpdfs%2fFPDDNCDCHFDLCM00%2ffs046%2fovft%2flive%2fgv023%2f00004760%2f00004760-193309000-00003.pdf&filename=Analysis+of+a+complex+of+statistical+variables+into+principal+components.&navigation_links=NavLinks.S.sh.22.1&link_from=S.sh.22|1&pdf_key=FPDDNCDCHFDLCM00&pdf_index=/fs046/ovft/live/gv023/00004760/00004760-193309000-00003&D=yrovft&link_set=S.sh.22|1|sl_10|resultSet|S.sh.22.23|0}
}

@Inproceedings{Huang2012,
  Title                    = {Improving word representations via global context and multiple word prototypes},
  Author                   = {Huang, Eric H and Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
  Booktitle                = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1},
  Year                     = {2012},
  Annote                   = {First learn (single prototype) word embeddings, using a neural network trained to predict next word. This network architecture is itself novel. It is split to have a local subnetwork and a global subnetwork. The local subnetwork uses as its input a concatenated window of words embeddings prior to the word to be predicted. The global subnetwork uses as its input a weighted average of all word vectors in the document (or alternativelty a large window), concatenated with the wordvector for the word immediately prior to the word to be predicted (i.e. the last word embedding concatenated in to the local subnetwork's input vector). These single prototype word vectors are used to create multiprototype word vectors by clustering the contexts of each work instance. Then for each word, sum up word vectors of its context words, then cluster the context sums using spherical K-means -- Huang et al set K to 10 i.e there to be 10 senses for all words. Then relabel each word in the corpus with the cluster id that it's context belongs to -- to get a sense label. Finally retrain on the relabelled words, to get word sense embeddings.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {873--882},

  __markedentry            = {[20361362:]},
  Groups                   = {to_read},
  Keywords                 = {WSD, word-embeddings},
  Owner                    = {20361362},
  Timestamp                = {2016.06.01},
  Url                      = {http://www.aclweb.org/anthology/P12-1092}
}

@Book{huddleston2002cambridge,
  Title                    = {The Cambridge Grammar of the English Language},
  Author                   = {Huddleston, R.D. and Pullum, G.K.},
  Publisher                = {Cambridge University Press},
  Year                     = {2002},
  Series                   = {Cambridge textbooks in linguistics},

  ISBN                     = {9780521431460},
  Lccn                     = {20025630},
  Owner                    = {20361362},
  Timestamp                = {2015.07.23},
  Url                      = {https://books.google.com.au/books?id=2yoQhHikxE8C}
}

@Article{huffman1952method,
  Title                    = {A method for the construction of minimum-redundancy codes},
  Author                   = {Huffman, David A},
  Journal                  = {Proceedings of the IRE},
  Year                     = {1952},
  Number                   = {9},
  Pages                    = {1098--1101},
  Volume                   = {40},

  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2017.10.11},
  Url                      = {https://www.ic.tu-berlin.de/fileadmin/fg121/Source-Coding_WS12/selected-readings/10_04051119.pdf}
}

@Article{hunter1958photoelectric,
  Title                    = {Photoelectric color difference meter},
  Author                   = {Hunter, Richard S},
  Journal                  = {Josa},
  Year                     = {1958},
  Number                   = {12},
  Pages                    = {985--995},
  Volume                   = {48},

  Owner                    = {20361362},
  Publisher                = {Optical Society of America},
  Timestamp                = {2017.06.08},
  Url                      = {https://www.osapublishing.org/josa/fulltext.cfm?uri=josa-48-12-985&id=76613}
}

@Article{hutchinson2013tensor,
  Title                    = {Tensor deep stacking networks},
  Author                   = {Hutchinson, Brian and Deng, Li and Yu, Dong},
  Journal                  = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  Year                     = {2013},
  Number                   = {8},
  Pages                    = {1944--1957},
  Volume                   = {35},

  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2015.09.15}
}

@Inproceedings{iacobacci2015sensembed,
  Title                    = {SensEmbed: learning sense embeddings for word and relational similarity},
  Author                   = {Iacobacci, Ignacio and Pilehvar, Mohammad Taher and Navigli, Roberto},
  Booktitle                = {Proceedings of ACL},
  Year                     = {2015},
  Annote                   = {Direct supervised apprach.
First Babelfly is used to label the Corpus, this is then used to train CBOW (The nonskip-gram word2vec method).
Rather than the labels being words, the labels are word senses.
Babelfly is independent preexisting, nonneural network WSD software.

We could actually use the sense embeddings and language model output by SenseEmbed to perform WSD;
it is just bypassing out alignment step. Iacobacci et al do not attempt this},
  Pages                    = {95--105},

  Owner                    = {20361362},
  Timestamp                = {2016.04.19},
  Url                      = {http://anthology.aclweb.org/P/P15/P15-1010.pdf}
}

@Inproceedings{2017focus,
  Title                    = {Focus location extraction from political news reports with bias correction},
  Author                   = {M. B. Imani and S. Chandra and S. Ma and L. Khan and B. Thuraisingham},
  Booktitle                = {2017 IEEE International Conference on Big Data (Big Data)},
  Year                     = {2017},
  Month                    = {Dec},
  Pages                    = {1956-1964},

  Abstract                 = {Automatic identification of geolocation mentioned in online news articles provide vital information for understanding associated events. While numerous open-source and commercial tools exist for geolocation extraction, they lack in reliable identification of fine-grained location, i.e., they identify location at country-level rather than a fine-grained city or locality level. The problem of location identification has been widely studied. Yet, most techniques depend on external knowledge-base or view the problem only in terms of Named Entity Recognition (NER), only to identify country-level location information. In this paper, we focus on news articles describing an event. A set of locations directly associated with the event are called focus locations. However, an event can occur only at a single location. Therefore, we aim to extract this location among focus locations, and call this as primary focus location. We propose a mechanism that utilizes the named entities to identify potential sentences containing focus locations, and then employ a supervised classification mechanism over sentence embedding to predict the primary focused geolocation. However, the main issue with such an approach is the unavailability of ground truth (i.e., whether words in a sentence is focus or non-focus) for training a classifier. In practice, labels from only a small number of news articles may be available for training due to high cost of manual labeling. If these articles are not a good representation of news articles in the wild, the classifier may not perform well. Therefore, we utilize an adaptation mechanism to overcome sampling bias in training data. Particularly, we train a classifier by using bias-corrected training data obtained from news articles published by an agency, while testing it on news articles published by a different agency. Our empirical results show superior performance compared to baseline approaches on real-world datasets consisting of news articles.},
  Doi                      = {10.1109/BigData.2017.8258141},
  Keywords                 = {Internet;information retrieval;natural language processing;pattern classification;politics;text analysis;adaptation mechanism;bias-corrected training data;classifier training;fine-grained location;focus location extraction;geolocation extraction;location identification;online news articles;political news reports;sentence embedding;supervised classification mechanism;Feature extraction;Geology;Kernel;Tools;Training;Training data;Urban areas;Bias Correction;Focus Location Extraction;Sentence Embedding},
  Owner                    = {20361362},
  Timestamp                = {2018.03.12}
}

@Inproceedings{iyyer2014neural,
  Title                    = {A neural network for factoid question answering over paragraphs},
  Author                   = {Iyyer, Mohit and Boyd-Graber, Jordan and Claudino, Leonardo and Socher, Richard and {Daum{\'e} III}, Hal},
  Booktitle                = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  Year                     = {2014},
  Pages                    = {633--644},

  Owner                    = {20361362},
  Timestamp                = {2015.08.31},
  Url                      = {https://cs.umd.edu/~miyyer/pubs/2014_qb_rnn.pdf}
}

@Inproceedings{iyyer2014generating,
  Title                    = {Generating Sentences from Semantic Vector Space Representations},
  Author                   = {Iyyer, Mohit and Boyd-Graber, Jordan and {Daum{\'e} III}, Hal},
  Booktitle                = {NIPS Workshop on Learning Semantics},
  Year                     = {2014},

  Keywords                 = {phrase-embeddings},
  Owner                    = {20361362},
  Timestamp                = {2015.06.30},
  Url                      = {http://cs.umd.edu/~miyyer/pubs/2014_nips_generation.pdf}
}

@Incollection{jager2010natural,
  Title                    = {Natural color categories are convex sets},
  Author                   = {J{\"a}ger, Gerhard},
  Booktitle                = {Logic, language and meaning},
  Publisher                = {Springer},
  Year                     = {2010},
  Pages                    = {11--20},

  Owner                    = {20361362},
  Timestamp                = {2017.07.04},
  Url                      = {https://pdfs.semanticscholar.org/235b/f4f85297f9d0c72f580bfbe95b76db9bce0a.pdf}
}

@Article{japkowicz2000nonlinear,
  Title                    = {Nonlinear autoassociation is not equivalent to PCA},
  Author                   = {Japkowicz, Nathalie and Hanson, Stephen Jose and Gluck, Mark A},
  Journal                  = {Neural computation},
  Year                     = {2000},
  Number                   = {3},
  Pages                    = {531--545},
  Volume                   = {12},

  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2017.09.01}
}

@Article{2016arXiv160404661J,
  Title                    = {{Parallelizing Word2Vec in Shared and Distributed Memory}},
  Author                   = {{Ji}, S. and {Satish}, N. and {Li}, S. and {Dubey}, P.},
  Journal                  = {ArXiv e-prints},
  Year                     = {2016},

  Annote                   = {Interestding discussion of paralising word2vec wrt negitive sampling. convert vector dot products to matrix products. and some other tequnieues.h},
  Month                    = apr,

  Adsnote                  = {Provided by the SAO/NASA Astrophysics Data System},
  Adsurl                   = {http://adsabs.harvard.edu/abs/2016arXiv160404661J},
  Archiveprefix            = {arXiv},
  Eprint                   = {1604.04661},
  Groups                   = {to_read},
  Keywords                 = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Computation and Language, Statistics - Machine Learning},
  Owner                    = {20361362},
  Primaryclass             = {cs.DC},
  Timestamp                = {2016.06.01},
  Url                      = {http://arxiv.org/pdf/1604.04661v1.pdf}
}

@Article{jones1984remark,
  Title                    = {Remark AS R50: a remark on algorithm AS 176. Kernal density estimation using the fast Fourier transform},
  Author                   = {Jones, MC and Lotwick, HW},
  Journal                  = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  Year                     = {1984},
  Number                   = {1},
  Pages                    = {120--122},
  Volume                   = {33},

  Owner                    = {20361362},
  Publisher                = {JSTOR},
  Timestamp                = {2018.06.05}
}

@Article{jordan1986rnnTR,
  Title                    = {Serial Order: A Parallel Distributed Processing Approach (Tech. Rep. No. 8604)},
  Author                   = {Jordan, MI},
  Journal                  = {San Diego, La Jolla, CA: Institute for Cognitive Science, University of California. Cited on},
  Year                     = {1986},

  Annote                   = {Best citation for Jordan RNNs},
  Pages                    = {48},

  Owner                    = {20361362},
  Timestamp                = {2017.10.18},
  Url                      = {http://cseweb.ucsd.edu/~gary/PAPER-SUGGESTIONS/Jordan-TR-8604.pdf}
}

@Inproceedings{jozefowicz2015empirical,
  Title                    = {An empirical exploration of recurrent network architectures},
  Author                   = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  Booktitle                = {Proceedings of the 32nd International Conference on Machine Learning (ICML-15)},
  Year                     = {2015},
  Annote                   = {Comares GRU and LSTM for language modelling (and a bunch of other tasks)},
  Pages                    = {2342--2350},

  Owner                    = {20361362},
  Timestamp                = {2017.08.15},
  Url                      = {http://proceedings.mlr.press/v37/jozefowicz15.pdf?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=revue}
}

@Inproceedings{judge2006questionbank,
  Title                    = {Questionbank: Creating a corpus of parse-annotated questions},
  Author                   = {Judge, John and Cahill, Aoife and Van Genabith, Josef},
  Booktitle                = {Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics},
  Year                     = {2006},
  Annote                   = {About the QuestionBank corpus -- 4000 tree parsed sentences -- all of which are questions.

Apparently this works really well as training data, and that models trained with it are still good at parsing normal sentences.
In someway related to ATIS

download from \url{http://www.computing.dcu.ie/~jjudge/qtreebank/}},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {497--504},

  Keywords                 = {corpora},
  Owner                    = {20361362},
  Timestamp                = {2015.06.10},
  Url                      = {http://doras.dcu.ie/15271/1/judge_et_al_06.pdf}
}

@Inproceedings{jurgens2012semeval,
  Title                    = {Semeval-2012 task 2: Measuring degrees of relational similarity},
  Author                   = {Jurgens, David A and Turney, Peter D and Mohammad, Saif M and Holyoak, Keith J},
  Booktitle                = {Proceedings of the Sixth International Workshop on Semantic Evaluation},
  Year                     = {2012},
  Annote                   = {Described the semantic relations analogy task},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {356--364},

  Owner                    = {20361362},
  Timestamp                = {2015.09.04},
  Url                      = {http://ixa2.si.ehu.eus/starsem/proc/pdf/STARSEM-SEMEVAL047.pdf}
}

@Inproceedings{kaageback2015neural,
  Title                    = {Neural context embeddings for automatic discovery of word senses},
  Author                   = {K{\aa}geb{\"a}ck, Mikael and Johansson, Fredrik and Johansson, Richard and Dubhashi, Devdatt},
  Booktitle                = {Proceedings of NAACL-HLT},
  Year                     = {2015},
  Annote                   = {Does word sense induction (WSI).
combine the word-vectors of the context of the word, and a temportal and semantic factors, to produce a Instance Context Embedding (ICE).
Use kmeans clustering on the ICE, to induce a cluster for each sense.

Perform Word Sense Disabiguagation on a given word instance, by calculating it's ICE, then selecting the sense cluster it is most similar too.},
  Pages                    = {25--32},

  Keywords                 = {word-embeddings},
  Owner                    = {20361362},
  Timestamp                = {2015.06.30},
  Url                      = {http://www.aclweb.org/anthology/W/W15/W15-1504.pdf}
}

@Inproceedings{KaagebExtractiveSummaristation,
  Title                    = {Extractive summarization using continuous vector space models},
  Author                   = {K{\aa}geb{\"a}ck, Mikael and Mogren, Olof and Tahmasebi, Nina and Dubhashi, Devdatt},
  Booktitle                = {Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)@ EACL},
  Year                     = {2014},
  Annote                   = {Focus in on using word and phrase embeedings to do multidocument extractive summary. Quote: "To the best of our knowledge, continuous vector space models have not previously been used in summarization tasks" Has a nice summery of the medthods used to produce word embeddings.\ Discusses two methos for phrase embeedings: Unfolder Recussive Autoencers (Socher) And simple addition (apparently done by Mikolov) It seems to really get the purpose of \cite{mikolovSkip} wrong though, saying the opposite of what that paper said. Usign the Opinosis dataset, which might be good to look into, it contaiend human gnerated summerys. Assessess accuracy by compairing various word overlap between output summaries and the gold standard summaries. Which seems dodgy for me -- but is apparently a normal method called ROUGE.},
  Pages                    = {31--39},

  Groups                   = {KeyPapers},
  Keywords                 = {RvNN, URAE, summarization, vector-based_summarization},
  Owner                    = {20361362},
  Timestamp                = {2015.04.17},
  Url                      = {http://www.aclweb.org/anthology/W14-1504}
}

@Article{Kalchbrenner2014,
  Title                    = {A convolutional neural network for modelling sentences},
  Author                   = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
  Journal                  = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
  Year                     = {2014},
  Volume                   = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},

  Keywords                 = {phrase-embeddings},
  Owner                    = {20361362},
  Timestamp                = {2015.08.31},
  Url                      = {https://arxiv.org/pdf/1404.2188.pdf)}
}

@Book{karp1972reducibility,
  Title                    = {Reducibility among combinatorial problems},
  Author                   = {Karp, Richard M},
  Publisher                = {Springer},
  Year                     = {1972},
  Annote                   = {This si the big theing with 21 NP-HArd problems.
It is a very short "book:"

when it says Knapsack (\#18) it means subset sum.},

  Keywords                 = {knapsack},
  Owner                    = {20361362},
  Timestamp                = {2015.11.05},
  Url                      = {http://www.cs.berkeley.edu/~luca/cs172/karp.pdf}
}

@Article{karrasch2014introduction,
  Title                    = {An Introduction to Grassmann Manifolds and their Matrix Representation},
  Author                   = {Karrasch, Daniel},
  Year                     = {2014},

  Owner                    = {20361362},
  Timestamp                = {2016.08.05},
  Url                      = {http://www.zfm.ethz.ch/~karrasch/Intro_Grassmann.pdf}
}

@Article{katz1987estimation,
  Title                    = {Estimation of probabilities from sparse data for the language model component of a speech recognizer},
  Author                   = {Katz, Slava M},
  Journal                  = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
  Year                     = {1987},
  Number                   = {3},
  Pages                    = {400--401},
  Volume                   = {35},

  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2015.10.20},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.129.7219&rep=rep1&type=pdf}
}

@Article{DBLP:journals/corr/KawakamiDRS16,
  Title                    = {Character Sequence Models for ColorfulWords},
  Author                   = {Kazuya Kawakami and
 Chris Dyer and
 Bryan R. Routledge and
 Noah A. Smith},
  Journal                  = {CoRR},
  Year                     = {2016},
  Volume                   = {abs/1609.08777},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/KawakamiDRS16},
  Owner                    = {20361362},
  Timestamp                = {Mon, 03 Oct 2016 17:51:10 +0200},
  Url                      = {http://arxiv.org/abs/1609.08777}
}

@Article{kelly1955iscc,
  Title                    = {ISCC-NBS method of designating colors and a dictionary of color names},
  Author                   = {Kelly, Kenneth Low and others},
  Year                     = {1955},

  Owner                    = {20361362},
  Publisher                = {US Dept. of Commerce, National Bureau of Standards},
  Timestamp                = {2017.07.03}
}

@Inbook{Kilgarriff2004,
  Title                    = {How Dominant Is the Commonest Sense of a Word?},
  Author                   = {Kilgarriff, Adam},
  Editor                   = {Sojka, Petr and Kopecek, Ivan and Pala, Karel and Sojka, Petr and Kopecek, Ivan and Pala, Karel},
  Pages                    = {103--111},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2004},

  Address                  = {Berlin, Heidelberg},

  Booktitle                = {Text, Speech and Dialogue: 7th International Conference, TSD 2004, Brno, Czech Republic, September 8-11, 2004. Proceedings},
  Doi                      = {10.1007/978-3-540-30120-2_14},
  ISBN                     = {978-3-540-30120-2},
  Owner                    = {20361362},
  Timestamp                = {2016.10.14},
  Url                      = {https://pdfs.semanticscholar.org/a271/edc22c4eb649a92d5e66a5ff2f0359968bb7.pdf}
}

@Article{DBLP:journals/corr/Kim14f,
  Title                    = {Convolutional Neural Networks for Sentence Classification},
  Author                   = {Yoon Kim},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1408.5882},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/Kim14f},
  Owner                    = {20361362},
  Timestamp                = {2015.08.04},
  Url                      = {http://arxiv.org/abs/1408.5882}
}

@Article{kingma2014adam,
  Title                    = {Adam: A method for stochastic optimization},
  Author                   = {Kingma, Diederik and Ba, Jimmy},
  Journal                  = {arXiv preprint arXiv:1412.6980},
  Year                     = {2014},

  Owner                    = {20361362},
  Timestamp                = {2017.08.01},
  Url                      = {https://arxiv.org/pdf/1412.6980.pdf}
}

@Article{2014VAE,
  Title                    = {{Auto-Encoding Variational Bayes}},
  Author                   = {{Kingma}, D.~P and {Welling}, M.},
  Journal                  = {The International Conference on Learning Representations (ICLR)},
  Year                     = {2014},

  Adsnote                  = {Provided by the SAO/NASA Astrophysics Data System},
  Adsurl                   = {http://adsabs.harvard.edu/abs/2013arXiv1312.6114K},
  Archiveprefix            = {arXiv},
  Eprint                   = {1312.6114},
  Keywords                 = {Statistics - Machine Learning, Computer Science - Learning},
  Owner                    = {20361362},
  Primaryclass             = {stat.ML},
  Timestamp                = {2017.09.01},
  Url                      = {https://arxiv.org/pdf/1312.6114.pdf}
}

@Inproceedings{kingsbury2002treebank,
  Title                    = {From TreeBank to PropBank.},
  Author                   = {Kingsbury, Paul and Palmer, Martha},
  Booktitle                = {LREC},
  Year                     = {2002},
  Annote                   = {About lableing the penn treebank with propositional struction.So that each sentence has a number o predicated with positional arguments.


This paper has interesting footnotes.},
  Organization             = {Citeseer},

  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.180.5642&rep=rep1&type=pdf}
}

@Article{DBLP:journals/corr/KirosZSZTUF15,
  Title                    = {Skip-Thought Vectors},
  Author                   = {Ryan Kiros and
 Yukun Zhu and
 Ruslan Salakhutdinov and
 Richard S. Zemel and
 Antonio Torralba and
 Raquel Urtasun and
 Sanja Fidler},
  Journal                  = {CoRR},
  Year                     = {2015},
  Volume                   = {abs/1506.06726},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/KirosZSZTUF15},
  Owner                    = {20361362},
  Timestamp                = {2016.06.01},
  Url                      = {http://arxiv.org/pdf/1506.06726v1.pdf}
}

@Article{kiss2006unsupervised,
  Title                    = {Unsupervised multilingual sentence boundary detection},
  Author                   = {Kiss, Tibor and Strunk, Jan},
  Journal                  = {Computational Linguistics},
  Year                     = {2006},
  Number                   = {4},
  Pages                    = {485--525},
  Volume                   = {32},

  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2018.02.05},
  Url                      = {https://www.mitpressjournals.org/doi/pdfplus/10.1162/coli.2006.32.4.485}
}

@Inproceedings{klein2015associating,
  Title                    = {Associating neural word embeddings with deep image representations using fisher vectors},
  Author                   = {Klein, Benjamin and Lev, Guy and Sadeh, Gil and Wolf, Lior},
  Booktitle                = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  Year                     = {2015},
  Pages                    = {4437--4446},

  Owner                    = {20361362},
  Timestamp                = {2017.11.22},
  Url                      = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Klein_Associating_Neural_Word_2015_CVPR_paper.pdf}
}

@Inproceedings{kleinberg2003impossibility,
  Title                    = {An impossibility theorem for clustering},
  Author                   = {Kleinberg, Jon M},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {2003},
  Pages                    = {463--470},

  Owner                    = {20361362},
  Timestamp                = {2017.12.12},
  Url                      = {http://alexhwilliams.info/itsneuronalblog/papers/clustering/Kleinberg_2002.pdf}
}

@Inproceedings{kneser1995improved,
  Title                    = {Improved backing-off for m-gram language modeling},
  Author                   = {Kneser, Reinhard and Ney, Hermann},
  Booktitle                = {Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on},
  Year                     = {1995},
  Annote                   = {This is the Kneser Ney Smoothing paper.
It was the state of the art that Bengio beat with the PPNLM in 2010},
  Organization             = {IEEE},
  Pages                    = {181--184},
  Volume                   = {1},

  Owner                    = {20361362},
  Timestamp                = {2015.11.23},
  Url                      = {http://www-i6.informatik.rwth-aachen.de/publications/download/951/Kneser-ICASSP-1995.pdf}
}

@Article{Knight1999,
  Title                    = {Decoding complexity in word-replacement translation models},
  Author                   = {Knight, Kevin},
  Journal                  = {Computational Linguistics},
  Year                     = {1999},
  Number                   = {4},
  Pages                    = {607--615},
  Volume                   = {25},

  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2016.02.05},
  Url                      = {http://www.aclweb.org/anthology/J99-4005}
}

@Article{Kolmogorov2009,
  Title                    = {Blossom V: a new implementation of a minimum cost perfect matching algorithm},
  Author                   = {Kolmogorov, Vladimir},
  Journal                  = {Mathematical Programming Computation},
  Year                     = {2009},
  Number                   = {1},
  Pages                    = {43--67},
  Volume                   = {1},

  __markedentry            = {[20361362:]},
  Abstract                 = {We describe a new implementation of the EdmondsÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬ÃƒÂ¢Ã¢â‚¬Å¾Ã‚Â¢s algorithm for
computing a perfect matching of minimum cost, to which we refer as
Blossom V.},
  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2016.07.13},
  Url                      = {http://mpc.zib.de/index.php/MPC/article/viewFile/11/4}
}

@Article{kongbayesian,
  Title                    = {Bayesian Optimization of Text Representations},
  Author                   = {Kong, Dani Yogatama Lingpeng and Smith, Noah A},
  Journal                  = {Conference on Empirical Methods in Natural Language Processing},
  Year                     = {2015},

  Annote                   = {Uses n-grams as input to logistic regression,
plus baysian methods for tuning,

to perform many NLP tasks,
including sentiment analysis.

With performance almost as good as state of the art neural network methods.
and without hyper parameters.},

  Owner                    = {20361362},
  Timestamp                = {2015.09.02},
  Url                      = {www.cs.cmu.edu/~lingpenk/paper/emnlp15.pdf}
}

@Article{Kong20082672,
  Title                    = {A new ant colony optimization algorithm for the multidimensional Knapsack problem },
  Author                   = {Min Kong and Peng Tian and Yucheng Kao},
  Journal                  = {Computers \& Operations Research },
  Year                     = {2008},
  Note                     = {Queues in Practice },
  Number                   = {8},
  Pages                    = {2672 - 2683},
  Volume                   = {35},

  Abstract                 = {The paper proposes a new ant colony optimization (ACO) approach, called binary ant system (BAS), to multidimensional Knapsack problem (MKP). Different from other ACO-based algorithms applied to MKP, \{BAS\} uses a pheromone laying method specially designed for the binary solution structure, and allows the generation of infeasible solutions in the solution construction procedure. A problem specific repair operator is incorporated to repair the infeasible solutions generated in every iteration. Pheromone update rule is designed in such a way that pheromone on the paths can be directly regarded as selecting probability. To avoid premature convergence, the pheromone re-initialization and different pheromone intensification strategy depending on the convergence status of the algorithm are incorporated. Experimental results show the advantages of \{BAS\} over other ACO-based approaches for the benchmark problems selected from \{OR\} library. },
  Doi                      = {http://dx.doi.org/10.1016/j.cor.2006.12.029},
  ISSN                     = {0305-0548},
  Keywords                 = {Ant colony optimization},
  Owner                    = {20361362},
  Timestamp                = {2015.10.16},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0305054806003236}
}

@Inproceedings{kremer2014substitutes,
  Title                    = {What substitutes tell us -- analysis of an ``all-words'' lexical substitution corpus},
  Author                   = {Kremer, Gerhard and Erk, Katrin and Pad{\'o}, Sebastian and Thater, Stefan},
  Booktitle                = {Proceedings of EACL},
  Year                     = {2014},
  Annote                   = {This is about the CoInCo (Concepts in Context) corpus.
It was created by crowd sourcing word substitutions for a portion of the MASC},

  Keywords                 = {corpora},
  Owner                    = {20361362},
  Timestamp                = {2015.07.20},
  Url                      = {http://www.aclweb.org/anthology/E14-1057}
}

@Article{krizhevsky2010convolutional,
  Title                    = {Convolutional deep belief networks on cifar-10},
  Author                   = {Krizhevsky, Alex and Hinton, G},
  Journal                  = {Unpublished manuscript},
  Year                     = {2010},

  Annote                   = {RELU6 paper},
  Volume                   = {40},

  Owner                    = {20361362},
  Timestamp                = {2017.07.31},
  Url                      = {https://www.cs.toronto.edu/~kriz/conv-cifar10-aug2010.pdf}
}

@Article{kulik2010there,
  Title                    = {There is no EPTAS for two-dimensional knapsack},
  Author                   = {Kulik, Ariel and Shachnai, Hadas},
  Journal                  = {Information Processing Letters},
  Year                     = {2010},
  Number                   = {16},
  Pages                    = {707--710},
  Volume                   = {110},

  Keywords                 = {knapsack},
  Owner                    = {20361362},
  Publisher                = {Elsevier},
  Timestamp                = {2015.11.09},
  Url                      = {http://www.researchgate.net/profile/Ariel_Kulik/publication/220115358_There_is_no_EPTAS_for_two-dimensional_knapsack/links/0f31753961d659735f000000.pdf}
}

@Article{lagaris1998artificial,
  Title                    = {Artificial neural networks for solving ordinary and partial differential equations},
  Author                   = {Lagaris, Isaac E and Likas, Aristidis and Fotiadis, Dimitrios I},
  Journal                  = {IEEE Transactions on Neural Networks},
  Year                     = {1998},
  Number                   = {5},
  Pages                    = {987--1000},
  Volume                   = {9},

  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2017.09.11},
  Url                      = {https://arxiv.org/pdf/physics/9705023.pdf}
}

@Phdthesis{ele1994computational,
  author    = {Lammens, Johan Maurice Gisele},
  school    = {State University of New York},
  title     = {A Computational Model of Color Perception and Color Naming},
  year      = {1994},
  annote    = {Using Gaussian Classifiers to name colors},
  owner     = {20361362},
  timestamp = {2017.07.03},
  url       = {https://goo.gl/2JvdyS},
}

@Article{wordvecispca,
  Title                    = {word2vec Skip-Gram with Negative Sampling is a Weighted Logistic {PCA}},
  Author                   = {Andrew J. Landgraf and
 Jeremy Bellay},
  Journal                  = {CoRR},
  Year                     = {2017},
  Volume                   = {abs/1705.09755},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/LandgrafB17},
  Owner                    = {20361362},
  Timestamp                = {Wed, 07 Jun 2017 14:41:39 +0200},
  Url                      = {http://arxiv.org/abs/1705.09755}
}

@Book{larson1995knowledge,
  Title                    = {Knowledge of Meaning: An Introduction to Semantic Theory},
  Author                   = {Larson, R.K. and Segal, G.},
  Publisher                = {MIT Press},
  Year                     = {1995},
  Annote                   = {Presents Semantics.
In terms of T Theory, in the section i read (chapter 2)

From this the defintions of synonym aimplication and converse can be derived.
I could cite it for those definions.

The presentation that T Theory explains how people learn langiuage is not relevant to my work.
Only that T Theory cna describe language.},
  Series                   = {A Bradford book},

  ISBN                     = {9780262121934},
  Lccn                     = {95005324},
  Owner                    = {20361362},
  Timestamp                = {2015.08.03},
  Url                      = {https://books.google.com.au/books?id=DcKyQgAACAAJ}
}

@Article{lau2016doc2vecissues,
  Title                    = {An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation},
  Author                   = {Lau, Jey Han and Baldwin, Timothy},
  Journal                  = {ACL 2016},
  Year                     = {2016},

  Annote                   = {Checks doc2vec for issues.},
  Pages                    = {78},

  Owner                    = {20361362},
  Timestamp                = {2017.11.23},
  Url                      = {https://arxiv.org/pdf/1607.05368.pdf}
}

@Inproceedings{le2014distributed,
  Title                    = {Distributed Representations of Sentences and Documents},
  Author                   = {Le, Quoc and Mikolov, Tomas},
  Booktitle                = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
  Year                     = {2014},
  Annote                   = {An actual Paragraph vector approach. commonly called doc2vec PV-DM: As per learning Word Representation, but along side word repressentaion provide a paragraph vector as an extra word. This provides more context to the learning. And that extra context required identifies the paragraph. PV-DBOW: I don't fully understand how this works. It seem they train parageraph vectors by asking htme ot predict what words the paragraph contains.},
  Pages                    = {1188--1196},

  Groups                   = {KeyPapers},
  Keywords                 = {phrase-embeddings},
  Owner                    = {20361362},
  Timestamp                = {2015.06.10},
  Url                      = {http://jmlr.org/proceedings/papers/v32/le14.pdf}
}

@Incollection{lecun2012efficient,
  Title                    = {Efficient backprop},
  Author                   = {LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  Booktitle                = {Neural networks: Tricks of the trade},
  Publisher                = {Springer},
  Year                     = {2012},
  Annote                   = {Practical advice for training nearula netowrks},
  Pages                    = {9--48},

  Owner                    = {20361362},
  Timestamp                = {2016.10.08},
  Url                      = {http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf}
}

@Article{LeCunCNN,
  Title                    = {Gradient-based learning applied to document recognition},
  Author                   = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  Journal                  = {Proceedings of the IEEE},
  Year                     = {1998},
  Number                   = {11},
  Pages                    = {2278--2324},
  Volume                   = {86},

  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2015.09.09},
  Url                      = {http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf}
}

@Inproceedings{leguizamon1999new,
  Title                    = {A new version of ant system for subset problems},
  Author                   = {Leguizamon, Guillermo and Michalewicz, Zbigniew},
  Booktitle                = {Proceedings of the 1999 Congress on Evolutionary Computation},
  Year                     = {1999},
  Annote                   = {Fairly obvious what I am doing basically. Not 100% sure on the N_max stuff.

I think I need to look at appximated subset sum},
  Organization             = {Piscataway, New Jersey, USA},
  Pages                    = {1458--1464},
  Volume                   = {2},

  Keywords                 = {ant colony, subset sum, graph search},
  Owner                    = {20361362},
  Timestamp                = {2015.10.16},
  Url                      = {https://cs.adelaide.edu.au/users/zbyszek/Papers/as4.pdf}
}

@Inproceedings{lemme2010efficient,
  Title                    = {Efficient online learning of a non-negative sparse autoencoder.},
  Author                   = {Lemme, Andre and Reinhart, Ren{\'e} Felix and Steil, Jochen Jakob},
  Booktitle                = {ESANN},
  Year                     = {2010},

  Owner                    = {20361362},
  Timestamp                = {2017.09.18},
  Url                      = {https://pdfs.semanticscholar.org/818d/59561c1dbcef7c4941b64991d61c91147fbe.pdf}
}

@Article{leshno1993uat,
  Title                    = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
  Author                   = {Leshno, Moshe and Lin, Vladimir Ya and Pinkus, Allan and Schocken, Shimon},
  Journal                  = {Neural networks},
  Year                     = {1993},
  Number                   = {6},
  Pages                    = {861--867},
  Volume                   = {6},

  Owner                    = {20361362},
  Publisher                = {Elsevier},
  Timestamp                = {2017.12.18},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.6041&rep=rep1&type=pdf}
}

@Inproceedings{Lesk:1986:ASD:318723.318728,
  Title                    = {Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone},
  Author                   = {Lesk, Michael},
  Booktitle                = {Proceedings of the 5th Annual International Conference on Systems Documentation},
  Year                     = {1986},

  Address                  = {New York, NY, USA},
  Pages                    = {24--26},
  Publisher                = {ACM},
  Series                   = {SIGDOC '86},

  Acmid                    = {318728},
  Doi                      = {10.1145/318723.318728},
  ISBN                     = {0-89791-224-1},
  Location                 = {Toronto, Ontario, Canada},
  Numpages                 = {3},
  Owner                    = {20361362},
  Timestamp                = {2017.11.13},
  Url                      = {http://doi.acm.org/10.1145/318723.318728}
}

@Inproceedings{levy2014neural,
  Title                    = {Neural word embedding as implicit matrix factorization},
  Author                   = {Levy, Omer and Goldberg, Yoav},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {2014},
  Pages                    = {2177--2185},

  Owner                    = {20361362},
  Timestamp                = {2017.08.16},
  Url                      = {http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf}
}

@Article{levy2015lsaisbasicallyskipgramswithexperimentstoprove,
  Title                    = {Improving Distributional Similarity with Lessons Learned from Word Embeddings},
  Author                   = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
  Journal                  = {Transactions of the Association for Computational Linguistics},
  Year                     = {2015},

  Annote                   = {This shows via experiments that LSA like matrix factorisations of coo-occcurances is basically as good as skipgrams and glove.},
  Pages                    = {211--225},
  Volume                   = {3},

  Abstract                 = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
  ISSN                     = {2307-387X},
  Owner                    = {20361362},
  Timestamp                = {2017.08.16},
  Url                      = {https://transacl.org/ojs/index.php/tacl/article/view/570}
}

@Inproceedings{li2017neural,
  Title                    = {Neural Bag-of-Ngrams.},
  Author                   = {Li, Bofang and Liu, Tao and Zhao, Zhe and Wang, Puwei and Du, Xiaoyong},
  Booktitle                = {AAAI},
  Year                     = {2017},
  Pages                    = {3067--3074},

  Owner                    = {20361362},
  Timestamp                = {2017.12.12},
  Url                      = {https://zhezhaoa.github.io/pub/neural_bag-of-ngrams.pdf}
}

@Inproceedings{li2015wordemedingasEMF,
  Title                    = {Word Embedding Revisited: A New Representation Learning and Explicit Matrix Factorization Perspective.},
  Author                   = {Li, Yitan and Xu, Linli and Tian, Fei and Jiang, Liang and Zhong, Xiaowei and Chen, Enhong},
  Booktitle                = {IJCAI},
  Year                     = {2015},
  Pages                    = {3650--3656},

  Owner                    = {20361362},
  Timestamp                = {2017.08.25},
  Url                      = {http://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/download/10863/11249}
}

@Inproceedings{liang2006alignment,
  Title                    = {Alignment by agreement},
  Author                   = {Liang, Percy and Taskar, Ben and Klein, Dan},
  Booktitle                = {Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics},
  Year                     = {2006},
  Annote                   = {Word Alignment seems to be the problem of how to align sentence in two languages with each word maps to another word in the other sentence.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {104--111},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://repository.upenn.edu/cgi/viewcontent.cgi?article=1569&context=cis_papers}
}

@Article{likas2001probability,
  Title                    = {Probability density estimation using artificial neural networks},
  Author                   = {Likas, Aristidis},
  Journal                  = {Computer physics communications},
  Year                     = {2001},

  Annote                   = {This seems bad. Use network to estimate a function. That function is turned into a PDF by dividing its value by its integral over the domain of interest. The integration has to be done numerically. vs estimating the CDF like \cite{magdon1998neural} and then differnetiating -- which can be done analytically. However, it does have a very nice set of example functions for evaluation on.},
  Number                   = {2},
  Pages                    = {167--175},
  Volume                   = {135},

  Doi                      = {10.1016/S0010-4655(00)00235-6},
  Owner                    = {20361362},
  Publisher                = {Elsevier},
  Timestamp                = {2017.08.28}
}

@Inproceedings{lin2011class,
  Title                    = {A class of submodular functions for document summarization},
  Author                   = {Lin, Hui and Bilmes, Jeff},
  Booktitle                = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1},
  Year                     = {2011},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {510--520},

  Keywords                 = {summarization, vector-based_summarization},
  Owner                    = {20361362},
  Timestamp                = {2015.06.05},
  Url                      = {http://melodi.ee.washington.edu/~hlin/papers/lin-acl11-summ.pdf}
}

@Inproceedings{lin2012syntactic,
  Title                    = {Syntactic annotations for the google books ngram corpus},
  Author                   = {Lin, Yuri and Michel, Jean-Baptiste and Aiden, Erez Lieberman and Orwant, Jon and Brockman, Will and Petrov, Slav},
  Booktitle                = {Proceedings of the ACL 2012 system demonstrations},
  Year                     = {2012},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {169--174},

  Owner                    = {20361362},
  Timestamp                = {2017.07.11},
  Url                      = {http://www.petrovi.de/data/acl12b.pdf}
}

@Article{7078992,
  Title                    = {Deep Learning for Acoustic Modeling in Parametric Speech Generation: A systematic review of existing techniques and future trends},
  Author                   = {Zhen-Hua Ling and Shi-Yin Kang and Heiga Zen and Senior, A. and Schuster, M. and Xiao-Jun Qian and Meng, H.M. and Li Deng},
  Journal                  = {Signal Processing Magazine, IEEE},
  Year                     = {2015},

  Annote                   = {Review article.How we got from text to speech (Via HMMs).
Also talks about voice conversion (changing peoples voices to other peoples voices, sayign the same words)

Covers the istory of using HMM and GMMs.


Also covers a hughe background.Interestign stuff like considfering RBMs are PoE and as GMMs.Also covers Contional RBMS},
  Month                    = {May},
  Number                   = {3},
  Pages                    = {35-52},
  Volume                   = {32},

  Abstract                 = {Hidden Markov models (HMMs) and Gaussian mixture models (GMMs) are the two most common types of acoustic models used in statistical parametric approaches for generating low-level speech waveforms from high-level symbolic inputs via intermediate acoustic feature sequences. However, these models have their limitations in representing complex, nonlinear relationships between the speech generation inputs and the acoustic features. Inspired by the intrinsically hierarchical process of human speech production and by the successful application of deep neural networks (DNNs) to automatic speech recognition (ASR), deep learning techniques have also been applied successfully to speech generation, as reported in recent literature. This article systematically reviews these emerging speech generation approaches, with the dual goal of helping readers gain a better understanding of the existing techniques as well as stimulating new work in the burgeoning area of deep learning for parametric speech generation.},
  Doi                      = {10.1109/MSP.2014.2359987},
  ISSN                     = {1053-5888},
  Keywords                 = {Gaussian processes;acoustic signal processing;hidden Markov models;mixture models;neural nets;speech recognition;ASR;DNN;GMM;Gaussian mixture models;HMM;acoustic features;acoustic modeling;acoustic models;automatic speech recognition;burgeoning area;deep learning;deep neural networks;hidden Markov models;high-level symbolic inputs;human speech production;intermediate acoustic feature sequences;low-level speech waveforms;parametric speech generation;statistical parametric approach;Acoustic signal detection;Gaussian mixture models;Hidden Markov models;Speech processing;Speech recognition;Speech synthesis;Vocoders},
  Owner                    = {20361362},
  Timestamp                = {2015.05.22}
}

@Article{liu2015combining,
  Title                    = {Combining Relevance Language Modeling and Clarity Measure for Extractive Speech Summarization},
  Author                   = {Liu, Shih-Hung and Chen, Kuan-Yu and Chen, Berlin and Wang, Hsin-Min and Yen, Hsu-Chun and Hsu, Wen-Lian},
  Journal                  = {Audio, Speech, and Language Processing, IEEE/ACM Transactions on},
  Year                     = {2015},

  Annote                   = {Using Language models, and the cross entropy between the sentence LM and the document LM (each of which are a prob distribution on the next work, give nthe previous),
To find the most relevant sentences.

It is run on Speech data. (rather than simply text.)},
  Number                   = {6},
  Pages                    = {957--969},
  Volume                   = {23},

  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2015.05.04},
  Url                      = {http://www.iis.sinica.edu.tw/papers/whm/18321-F.pdf}
}

@Article{jump,
  Title                    = {Computing in operations research using Julia},
  Author                   = {Lubin, Miles and Dunning, Iain},
  Journal                  = {INFORMS Journal on Computing},
  Year                     = {2015},
  Number                   = {2},
  Pages                    = {238--248},
  Volume                   = {27},

  Owner                    = {20361362},
  Publisher                = {INFORMS},
  Timestamp                = {2016.01.26},
  Url                      = {http://arxiv.org/pdf/1312.1431.pdf}
}

@Article{PLN847,
  Title                    = {Paraphrase Concept and Typology. A Linguistically Based and Computationally Oriented Approach},
  Author                   = {Marta Vila y M. Antnia Mart Horacio Rodriguez},
  Journal                  = {Procesamiento del Lenguaje Natural},
  Year                     = {2010},

  Annote                   = {This one defines the typeology, not the corpus sing the typology},
  Number                   = {0},
  Pages                    = {83--90},
  Volume                   = {46},

  Abstract                 = {In this paper, we present a critical analysis of the state of the art in the definition and typologies of paraphrasing. This analysis shows that there exists no characterization of paraphrasing that is comprehensive, linguistically based and computationally tractable at the same time. The following sets out to define and delimit the concept on the basis of the propositional content. We present a general, inclusive and computationally oriented typology of the linguistic mechanisms that give rise to form variations between paraphrase pairs.},
  ISSN                     = {1989-7553},
  Owner                    = {20361362},
  Timestamp                = {2015.06.24},
  Url                      = {http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/847}
}

@Article{maaten2008tsne,
  Title                    = {Visualizing data using t-SNE},
  Author                   = {Maaten, Laurens van der and Hinton, Geoffrey},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2008},
  Number                   = {Nov},
  Pages                    = {2579--2605},
  Volume                   = {9},

  Owner                    = {20361362},
  Timestamp                = {2018.01.18},
  Url                      = {http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf}
}

@Article{magazine1984note,
  Title                    = {A note on approximation schemes for multidimensional knapsack problems},
  Author                   = {Magazine, Michael J and Chern, Maw-Sheng},
  Journal                  = {Mathematics of Operations Research},
  Year                     = {1984},
  Number                   = {2},
  Pages                    = {244--247},
  Volume                   = {9},

  Keywords                 = {knapsack},
  Owner                    = {20361362},
  Publisher                = {INFORMS},
  Timestamp                = {2015.11.09},
  Url                      = {http://www.jstor.org/stable/pdf/3689249.pdf?acceptTC=true}
}

@Inproceedings{1998NNpdfDiffCdf,
  Title                    = {Neural networks for density estimation},
  Author                   = {Magdon-Ismail, Malik and Atiya, Amir},
  Booktitle                = {NIPS},
  Year                     = {1998},
  Annote                   = {with my probability distribution idea (not directly related to DIffEq).
it is very similar to "Smooth Interpolation of the Cumulative" method from https://papers.nips.cc/paper/1624-neural-networks-for-density-estimation.pdf (Magdon-Ismail and Atiya, 1998)
Which is probably a good thing, because then I don't have to establish the method as valid, but can go straight to showing how I extend it for conditional distributions, and apply it in my particular domain (As far as I can see that paper has never been applied to any real data).

Anyway, the interesting part for Neural network DiffEq, is that they present a proof of convergence properties.},
  Pages                    = {522--528},

  Owner                    = {20361362},
  Timestamp                = {2017.05.25},
  Url                      = {https://papers.nips.cc/paper/1624-neural-networks-for-density-estimation.pdf}
}

@Inproceedings{Malouf2002,
  Title                    = {A comparison of algorithms for maximum entropy parameter estimation},
  Author                   = {Malouf, Robert},
  Booktitle                = {proceedings of the 6th conference on Natural language learning-Volume 20},
  Year                     = {2002},
  Annote                   = {This is not useful.
It is a review of different algorithms for maximum entropy estimation,but does not explain how the more compelx ones work.
Does present some data on the comparative speed etc.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {1--7},

  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://www.coli.uni-saarland.de/groups/HU/HUwm4/Slides/malouf.pdf}
}

@Incollection{Manne2012,
  Title                    = {Extraction Based Automatic Text Summarization System with HMM Tagger},
  Author                   = {Manne, Suneetha and Shaik Mohd., ZaheerParvez and Sameen Fatima, S.},
  Booktitle                = {Proceedings of the International Conference on Information Systems Design and Intelligent Applications 2012 (INDIA 2012) held in Visakhapatnam, India, January 2012},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2012},
  Editor                   = {Satapathy, SureshChandra and Avadhani, P.S. and Abraham, Ajith},
  Pages                    = {421-428},
  Series                   = {Advances in Intelligent and Soft Computing},
  Volume                   = {132},

  Doi                      = {10.1007/978-3-642-27443-5_48},
  ISBN                     = {978-3-642-27442-8},
  Language                 = {English},
  Owner                    = {20361362},
  Timestamp                = {2015.06.08},
  Url                      = {http://dx.doi.org/10.1007/978-3-642-27443-5_48}
}

@Book{manning1999foundations,
  Title                    = {Foundations of Statistical Natural Language Processing},
  Author                   = {Manning, C.D. and Sch{\"u}tze, H.},
  Publisher                = {MIT Press},
  Year                     = {1999},
  Annote                   = {Big book, good reference},

  ISBN                     = {9780262133609},
  Lccn                     = {99021137},
  Owner                    = {20361362},
  Timestamp                = {2015.07.17},
  Url                      = {https://books.google.com.au/books?id=YiFDxbEX3SUC}
}

@Inproceedings{manning-EtAl:2014:P14-5,
  Title                    = {The {Stanford} {CoreNLP} Natural Language Processing Toolkit},
  Author                   = {Manning, Christopher D. and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven J. and McClosky, David},
  Booktitle                = {Association for Computational Linguistics (ACL) System Demonstrations},
  Year                     = {2014},
  Pages                    = {55--60},

  Owner                    = {20361362},
  Timestamp                = {2018.03.02},
  Url                      = {http://www.aclweb.org/anthology/P/P14/P14-5010}
}

@Article{2015arXiv151102793M,
  Title                    = {{Generating Images from Captions with Attention}},
  Author                   = {{Mansimov}, E. and {Parisotto}, E. and {Lei Ba}, J. and {Salakhutdinov}, R.},
  Journal                  = {ArXiv e-prints},
  Year                     = {2015},
  Month                    = nov,

  Adsnote                  = {Provided by the SAO/NASA Astrophysics Data System},
  Adsurl                   = {http://adsabs.harvard.edu/abs/2015arXiv151102793M},
  Archiveprefix            = {arXiv},
  Eprint                   = {1511.02793},
  Keywords                 = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
  Owner                    = {20361362},
  Primaryclass             = {cs.LG},
  Timestamp                = {2017.06.08}
}

@Article{Mao2014GenerateImageDesc,
  Title                    = {Explain Images with Multimodal Recurrent Neural Networks},
  Author                   = {Junhua Mao and
 Wei Xu and
 Yi Yang and
 Jiang Wang and
 Alan L. Yuille},
  Journal                  = {CoRR},
  Year                     = {2014},

  Annote                   = {Uses multimodel RNN (m-RNN), 
to combine language modelling with image description.},
  Volume                   = {abs/1410.1090},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/MaoXYWY14},
  Keywords                 = {image description},
  Owner                    = {20361362},
  Timestamp                = {2016.01.01},
  Url                      = {http://arxiv.org/abs/1410.1090}
}

@Electronic{treebank3,
  Title                    = {Treebank-3 LDC99T42},

  Address                  = {Philadelphia},
  Annote                   = {This is the dataset everyone uses. The Penn Treebank.

There is a subset of this in nltk.corpus.treebank

There is a mistakenly unprotected copy of the full set in http://trac.csail.mit.edu/workbench-projects/browser/trunk/edu.upenn.treebank-3?rev=748},
  Author                   = {Marcus, Mitchell},
  HowPublished             = {corpus},
  Organization             = {Linguistic Data Consortium},
  Year                     = {1999},

  Owner                    = {20361362},
  Timestamp                = {2015.05.21}
}

@Article{BuildingPennTreebank,
  Title                    = {Building a large annotated corpus of English: The Penn Treebank},
  Author                   = {Marcus, Mitchell P and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
  Journal                  = {Computational linguistics},
  Year                     = {1993},

  Annote                   = {This is aboiut how the Penn Treebank was built.

Of intest to me right now is what the Penn Treebank POS tagset is.},
  Number                   = {2},
  Pages                    = {313--330},
  Volume                   = {19},

  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2015.07.14},
  Url                      = {http://repository.upenn.edu/cgi/viewcontent.cgi?article=1246&context=cis_reports}
}

@Article{de2008finding,
  Title                    = {Finding Contradictions in Text},
  Author                   = {de Marneffe, Marie-Catherine and Rafferty, Anna N and Manning, Christopher D},
  Journal                  = {ACL-08: HLT},
  Year                     = {2008},

  Annote                   = {Link to their datasets was broken but can now be found at: \url{http://nlp.stanford.edu/projects/contradiction/}

Classes contracticions into several catogoies:

Easy:
 - Antonym
 - Negation
 - Numeric

Hard:
 - Factive
 - Structure
 - Lexical
 - World Knowledge},
  Pages                    = {1039},

  Owner                    = {20361362},
  Publisher                = {Citeseer},
  Timestamp                = {2015.08.04},
  Url                      = {http://www-nlp.stanford.edu/pubs/contradiction-acl08.pdf}
}

@Article{maron1961automatic,
  author    = {Maron, Melvin Earl},
  journal   = {Journal of the ACM (JACM)},
  title     = {Automatic indexing: an experimental inquiry},
  year      = {1961},
  volume    = {8},
  number    = {3},
  pages     = {404--417},
  annote    = {Ancient paper on topic classification},
  owner     = {20361362},
  publisher = {ACM},
  timestamp = {2015.09.02},
  url       = {http://sci2s.ugr.es/keel/pdf/algorithm/classification-algorithm/Maron1961.pdf},
}

@Article{10.2307/1263890,
  Author                   = {George V. Maverick},
  Journal                  = {International Journal of American Linguistics},
  Year                     = {1969},
  Number                   = {1},
  Pages                    = {71-75},
  Volume                   = {35},

  ISSN                     = {00207071, 15457001},
  Owner                    = {20361362},
  Publisher                = {The University of Chicago Press},
  Reviewed-author          = {Henry Kucera, W. Nelson Francis},
  Timestamp                = {2015.11.26},
  Url                      = {http://www.jstor.org/stable/1263890}
}

@Article{McKevitt1992,
  Title                    = {Approaches to natural language discourse processing},
  Author                   = {Mc Kevitt, Paul and Partridge, Derek and Wilks, Yorick},
  Journal                  = {Artificial Intelligence Review},
  Year                     = {1992},
  Number                   = {4},
  Pages                    = {333--364},
  Volume                   = {6},

  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2015.04.18},
  Url                      = {http://www.paulmckevitt.com/pubs/airenlp.pdf}
}

@Article{mccarthy2009english,
  Title                    = {The English lexical substitution task},
  Author                   = {McCarthy, Diana and Navigli, Roberto},
  Journal                  = {Language resources and evaluation},
  Year                     = {2009},

  Annote                   = {This is the discussion of the SemEval task of substituting the correct word.
It is a word sense disabmiguation problem.

The SemEval 2007 task defintion exists, online. Apparently there is a 2002 paper discussing the idea.},
  Number                   = {2},
  Pages                    = {139--159},
  Volume                   = {43},

  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2015.07.15},
  Url                      = {http://wwwusers.di.uniroma1.it/~navigli/pubs/LRE_2009_McCarthy_Navigli.pdf}
}

@Article{mcmahan2015bayesian,
  Title                    = {A Bayesian model of grounded color semantics},
  Author                   = {McMahan, Brian and Stone, Matthew},
  Journal                  = {Transactions of the Association for Computational Linguistics},
  Year                     = {2015},

  Annote                   = {Basically an approximation to the histogram of color decritions response over the HSV space, using fuzzy squares, fitted using MCMC. There are 829 color descriptions, and for each point, each one is assigned a probability, using these overlapping fuzzy regions. Each color description is parameterised by Applicability $$P(k^{true} | x)$$. and Availability which is $$P(k^{said} | k^{true})$$ here $$k^{true}$$ is the predicate that $k$ is a (one of many) correct names for the color $$x$$, and $$k^{said}$$ is the predicate that $$k$$ was the response given by the user.},
  Pages                    = {103--115},
  Volume                   = {3},

  Keywords                 = {color},
  Owner                    = {20361362},
  Timestamp                = {2017.01.13},
  Url                      = {http://www.aclweb.org/anthology/Q15-1008}
}

@Article{meindl2012analysis,
  Title                    = {Analysis of commercial and free and open source solvers for linear optimization problems},
  Author                   = {Meindl, Bernhard and Templ, Matthias},
  Journal                  = {Eurostat and Statistics Netherlands},
  Year                     = {2012},

  Annote                   = {Compairs GLPK and Gurobi and a number of other solves to find which is fastest},

  Owner                    = {20361362},
  Timestamp                = {2016.01.26},
  Url                      = {http://neon.vb.cbs.nl/cascprivate/..%5Ccasc%5CESSNet2%5Cdeliverable_solverstudy.pdf}
}

@Article{menegaz2007discrete,
  Title                    = {A discrete model for color naming},
  Author                   = {Menegaz, Gloria and Le Troter, Arnaud and Sequeira, Jean and Boi, Jean-Marc},
  Journal                  = {EURASIP Journal on Applied Signal Processing},
  Year                     = {2007},
  Number                   = {1},
  Pages                    = {113--113},
  Volume                   = {2007},

  Doi                      = {10.1155/2007/29125},
  Owner                    = {20361362},
  Publisher                = {Hindawi Publishing Corp.},
  Timestamp                = {2017.07.03}
}

@Article{meomcmahanstone:color,
  Title                    = {Generating and Resolving Vague Color Reference},
  Author                   = {T. Meo and B. McMahan and M. Stone},
  Journal                  = {Proc. 18th Workshop Semantics and Pragmatics of Dialogue (SemDial)},
  Year                     = {2014},

  Owner                    = {20361362},
  Timestamp                = {2017.06.22},
  Url                      = {https://www.cs.rutgers.edu/~mdstone/pubs/semdial14.pdf}
}

@Article{mesnil2014ensemble,
  Title                    = {Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews},
  Author                   = {Mesnil, Gr{\'e}goire and Mikolov, Tomas and Ranzato, Marc'Aurelio and Bengio, Yoshua},
  Journal                  = {arXiv preprint arXiv:1412.5335},
  Year                     = {2014},

  Owner                    = {20361362},
  Timestamp                = {2017.11.28},
  Url                      = {https://arxiv.org/pdf/1412.5335.pdf}
}

@Article{mhaskar1992uat,
  Title                    = {Approximation by superposition of sigmoidal and radial basis functions},
  Author                   = {Mhaskar, Hrushikesh N and Micchelli, Charles A},
  Journal                  = {Advances in Applied mathematics},
  Year                     = {1992},
  Number                   = {3},
  Pages                    = {350--373},
  Volume                   = {13},

  Owner                    = {20361362},
  Publisher                = {Elsevier},
  Timestamp                = {2017.10.18},
  Url                      = {https://ac.els-cdn.com/019688589290016P/1-s2.0-019688589290016P-main.pdf?_tid=cd6caede-b3e6-11e7-b6cf-00000aab0f6c&acdnat=1508319133_6901c3297f871136a2fdd72ae66cb268}
}

@Inproceedings{mihalcea2004senseval,
  Title                    = {The Senseval-3 English lexical sample task},
  Author                   = {Mihalcea, Rada and Chklovski, Timothy Anatolievich and Kilgarriff, Adam},
  Year                     = {2004},
  Organization             = {Association for Computational Linguistics},

  Owner                    = {20361362},
  Timestamp                = {2016.10.17},
  Url                      = {http://www.aclweb.org/anthology/W04-0807}
}

@Inproceedings{mihalcea2006corpus,
  Title                    = {Corpus-based and knowledge-based measures of text semantic similarity},
  Author                   = {Mihalcea, Rada and Corley, Courtney and Strapparava, Carlo},
  Booktitle                = {AAAI},
  Year                     = {2006},
  Annote                   = {Combining various measures of word semantics to try to get sentence semantics.},
  Pages                    = {775--780},
  Volume                   = {6},

  Owner                    = {20361362},
  Timestamp                = {2015.07.31},
  Url                      = {http://www.aaai.org/Papers/AAAI/2006/AAAI06-123.pdf}
}

@Article{mikolov2013efficient,
  Title                    = {Efficient estimation of word representations in vector space},
  Author                   = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  Journal                  = {arXiv:1301.3781},
  Year                     = {2013},

  Annote                   = {word2vec skipgram

Compares several methods for finding single word vector repressentations.

A "You shall know a word by the company it keeps" type paper.

the paper for CBOW and skip nGram},

  Owner                    = {20361362},
  Timestamp                = {2015.04.15},
  Url                      = {http://arxiv.org/pdf/1301.3781v3}
}

@Inproceedings{mikolov2011trainingstrategies,
  Title                    = {Strategies for training large scale neural network language models},
  Author                   = {Mikolov, T. and Deoras, A. and Povey, D. and Burget, L. and Cernocky, J.},
  Booktitle                = {Automatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on},
  Year                     = {2011},
  Annote                   = {This is a discussion of the Class based maximum entropy model.
With RNNs
I have not read it in much detail.},
  Month                    = {Dec},
  Pages                    = {196-201},

  Abstract                 = {We describe how to effectively train neural network based language models on large data sets. Fast convergence during training and better overall performance is observed when the training data are sorted by their relevance. We introduce hash-based implementation of a maximum entropy model, that can be trained as a part of the neural network model. This leads to significant reduction of computational complexity. We achieved around 10% relative reduction of word error rate on English Broadcast News speech recognition task, against large 4-gram model trained on 400M tokens.},
  Doi                      = {10.1109/ASRU.2011.6163930},
  Keywords                 = {computational complexity;learning (artificial intelligence);neural nets;speech recognition;4-gram model;computational complexity;english broadcast news speech recognition task;hash-based implementation;large scale neural network language model training;maximum entropy model;training data;word error rate;Artificial neural networks;Computational complexity;Computational modeling;Data models;Entropy;Training;Training data},
  Owner                    = {20361362},
  Timestamp                = {2015.06.23},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6163930}
}

@Article{mikolovLongerSTM,
  Title                    = {Learning Longer Memory in Recurrent Neural Networks},
  Author                   = {Tomas Mikolov and
 Armand Joulin and
 Sumit Chopra and
 Micha{\"{e}}l Mathieu and
 Marc'Aurelio Ranzato},
  Journal                  = {CoRR},
  Year                     = {2014},
  Volume                   = {abs/1412.7753},

  Archiveprefix            = {arXiv},
  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.org/rec/bib/journals/corr/MikolovJCMR14},
  Eprint                   = {1412.7753},
  Owner                    = {20361362},
  Timestamp                = {Wed, 07 Jun 2017 14:42:04 +0200},
  Url                      = {http://arxiv.org/abs/1412.7753}
}

@Inproceedings{mikolov2010recurrent,
  Title                    = {Recurrent neural network based language model.},
  Author                   = {Mikolov, Tomas and Karafi{\'a}t, Martin and Burget, Lukas and Cernock{\`y}, Jan and Khudanpur, Sanjeev},
  Booktitle                = {Interspeech},
  Year                     = {2010},
  Annote                   = {Note that equation 1 is misleading. text says concatenate, equtions says add.},
  Pages                    = {3},
  Volume                   = {2},

  Owner                    = {20361362},
  Timestamp                = {2017.06.29},
  Url                      = {http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf}
}

@Inproceedings{mikolov2011RnnLM,
  Title                    = {Extensions of recurrent neural network language model},
  Author                   = {Mikolov, Tomas and Kombrink, Stefan and Burget, Lukas and Cernocky, Jan H and Khudanpur, Sanjeev},
  Booktitle                = {Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on},
  Year                     = {2011},
  Annote                   = {Probably first RNN language model (Recurrent Neural net). Input is One Hot, Output is a proability distribution over classes of the next word, (from which a uni Output is Probabilituy distribution over some number of word classes, where words are assigned to classes based on frequencuy (so each class containes the same number of probaility, but the ones containing low frewency words, contain more different words). ()Eg The might be in a class of its own). Then uses a Probility distribution within the class to choose the word. This hgive speedup in training (ast a small loss of accurasy)},
  Organization             = {IEEE},
  Pages                    = {5528--5531},

  Doi                      = {10.1109/ICASSP.2011.5947611},
  Owner                    = {20361362},
  Timestamp                = {2015.04.20}
}

@Inproceedings{mikolovSkip,
  Title                    = {Distributed representations of words and phrases and their compositionality},
  Author                   = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  Booktitle                = {Advances in Neural Information Processing Systems},
  Year                     = {2013},
  Annote                   = {Most of the paper focuses on local structure for relationships, ands is similar ot \cite{mikolov2013linguisticsubstructures} This phrasal section of the work is not for per sentence but rather small phrases that are reused, so called "idiomatic phrases" Eg "Austrian Airlines", "New York Times" As I understand the discussion of phrases mostly focuses on having this short phrases preidentified (Eg by a tokenizer, NER, or via staticical cooccurance (that last one what how they did it), and treating them as one phrase.},
  Pages                    = {3111--3119},

  Groups                   = {KeyPapers},
  Owner                    = {20361362},
  Timestamp                = {2015.04.24},
  Url                      = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@Inproceedings{mikolov2013linguisticsubstructures,
  Title                    = {Linguistic Regularities in Continuous Space Word Representations.},
  Author                   = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  Booktitle                = {HLT-NAACL},
  Year                     = {2013},
  Annote                   = {Not skipgrams or CBOW.
Just on the testing method
notes the existence of linear substructures in single-word embeddings for example: v("king")-v("man")+v("woman") has nearest neighbor v("queen") Where nearest neighber was defined as the cosign similarity. Experiements were carried out on the use of vector offsets to solve the: A is to B as C is to ? sytle question. Best 40\% correct. (Not good enough for use for resythisis) *May* have been overly difficult since a closely related word might end up nearer. Eg: Australia is to Canberra as USA is to ? May have answered New York, instead of Washington -- Washinton and New York both being quiet close (for example) in Vector space. So having overly dense vocabulary *may* have made the task harder. (Need to check up on Cosign Simliarity)},
  Pages                    = {746--751},

  Groups                   = {KeyPapers},
  Owner                    = {20361362},
  Timestamp                = {2015.04.15},
  Url                      = {http://www.aclweb.org/anthology/N13-1090}
}

@Inproceedings{mikolov2012contextRNNLM,
  Title                    = {Context dependent recurrent neural network language model.},
  Author                   = {Mikolov, Tomas and Zweig, Geoffrey},
  Booktitle                = {SLT},
  Year                     = {2012},
  Annote                   = {Exention to RNN Language model which track the Topic of the document using LDA This is the paper that does this. It seems that it was naturally extened for Le and Mikolov's paper on PV-DM and PV-DBOW},
  Pages                    = {234--239},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424228}
}

@Article{miller1995wordnet,
  author    = {Miller, George A},
  journal   = {Communications of the ACM},
  title     = {WordNet: a lexical database for English},
  year      = {1995},
  volume    = {38},
  number    = {11},
  pages     = {39--41},
  annote    = {This is the preferred citation for WordNet},
  keywords  = {wordnet},
  owner     = {20361362},
  publisher = {ACM},
  timestamp = {2015.07.14},
}

@Article{WORDNET5miller1990nouns,
  Title                    = {Nouns in WordNet: a lexical inheritance system},
  Author                   = {Miller, George A},
  Journal                  = {International journal of Lexicography},
  Year                     = {1990},

  Annote                   = {POne of the 5 key papers on wordnet.I might like ot cite this for the defnions of noun synonyms.},
  Number                   = {4},
  Pages                    = {245--264},
  Volume                   = {3},

  Keywords                 = {wordnet},
  Owner                    = {20361362},
  Publisher                = {Oxford Univ Press},
  Timestamp                = {2015.07.14},
  Url                      = {http://ijl.oxfordjournals.org/content/3/4/245.full.pdf}
}

@Article{miller1991contextualsemantics,
  Title                    = {Contextual correlates of semantic similarity},
  Author                   = {Miller, George A and Charles, Walter G},
  Journal                  = {Language and cognitive processes},
  Year                     = {1991},

  Annote                   = {I had to find this in *print*
For reference it ins at code P401.9 in the 3rd floor of reid.},
  Number                   = {1},
  Pages                    = {1--28},
  Volume                   = {6},

  File                     = {:..\\annotated_documents\\miller1991contextualsemantics.pdf:PDF},
  Owner                    = {20361362},
  Publisher                = {Taylor \& Francis},
  Timestamp                = {2015.07.23}
}

@Book{minsky2017perceptrons,
  Title                    = {Perceptrons: An introduction to computational geometry},
  Author                   = {Minsky, Marvin and Papert, Seymour A and Bottou, L{\'e}on},
  Publisher                = {MIT press},
  Year                     = {2017},

  Owner                    = {20361362},
  Timestamp                = {2018.02.12}
}

@Inproceedings{Mitchell2008,
  Title                    = {Vector-based Models of Semantic Composition.},
  Author                   = {Mitchell, Jeff and Lapata, Mirella},
  Booktitle                = {ACL},
  Year                     = {2008},
  Pages                    = {236--244},

  Abstract                 = {This paper proposes a framework for repre-
senting the meaning of phrases and sentences
in vector space. Central to our approach is
vector composition which we operationalize
in terms of additive and multiplicative func-
tions. Under this framework, we introduce a
wide range of composition models which we
evaluate empirically on a sentence similarity
task. Experimental results demonstrate that
the multiplicative models are superior to the
additive alternatives when compared against
human judgments},
  Owner                    = {20361362},
  Timestamp                = {2015.11.19},
  Url                      = {https://www.aclweb.org/anthology/P/P08/P08-1028.pdf}
}

@Article{1994estimatedensity,
  Title                    = {A learning law for density estimation},
  Author                   = {D. S. Modha and Y. Fainman},
  Journal                  = {IEEE Transactions on Neural Networks},
  Year                     = {1994},

  Annote                   = {Basis of \cite{likas2001probability}},
  Month                    = {May},
  Number                   = {3},
  Pages                    = {519-523},
  Volume                   = {5},

  Abstract                 = {Probability density functions are estimated by an exponential family of densities based on multilayer feedforward networks. The role of the multilayer feedforward networks, in the proposed estimator, is to approximate the logarithm of the probability density functions. The method of maximum likelihood is used, as the main contribution, to derive an unsupervised backpropagation learning law to estimate the probability density functions. Computer simulation results demonstrating the use of the derived learning law are presented},
  Doi                      = {10.1109/72.286931},
  ISSN                     = {1045-9227},
  Keywords                 = {backpropagation;feedforward neural nets;probability;exponential family;maximum likelihood;multilayer feedforward networks;probability density function estimation;unsupervised backpropagation learning law;Feedforward neural networks;Function approximation;Linear approximation;Multi-layer neural network;Multilayer perceptrons;Neural networks;Neurons;Optimized production technology;Upper bound},
  Owner                    = {20361362},
  Timestamp                = {2017.08.29}
}

@Article{mojsilovic2005computational,
  Title                    = {A computational model for color naming and describing color composition of images},
  Author                   = {Mojsilovic, Aleksandra},
  Journal                  = {IEEE Transactions on Image Processing},
  Year                     = {2005},
  Number                   = {5},
  Pages                    = {690--699},
  Volume                   = {14},

  Doi                      = {10.1109/TIP.2004.841201},
  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2017.07.31}
}

@Article{2016arXiv160603821M,
  Title                    = {{Learning to Generate Compositional Color Descriptions}},
  Author                   = {{Monroe}, W. and {Goodman}, N.~D. and {Potts}, C.},
  Journal                  = {ArXiv e-prints},
  Year                     = {2016},

  Annote                   = {Uses LSTM to to learn to output a language model for describing a color, when input a color. Has a fancy fourier-basis color represention, but I don't think much of that as compaired to any other option from \url{http://stats.stackexchange.com/questions/218407/}. It is kinda similar to the shifted gaussian sum reprention there. The introductory paragraph is something I've been wanting to say. It is really well written: "Color descriptions represent a microcosm of grounded language semantics. Basic color terms like ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã¢â‚¬Å“redÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬ÃƒÂ¯Ã‚Â¿Ã‚Â½ and ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã¢â‚¬Å“blueÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬ÃƒÂ¯Ã‚Â¿Ã‚Â½ provide a rich set of semantic building blocks in a continuous meaning space; in addition, people employ compositional color descriptions to express meanings not covered by basic terms, such as ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã¢â‚¬Å“greenish blueÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬ÃƒÂ¯Ã‚Â¿Ã‚Â½ or ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã¢â‚¬Å“the color of the rust on my auntÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬ÃƒÂ¢Ã¢â‚¬Å¾Ã‚Â¢s old ChevroletÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬ÃƒÂ¯Ã‚Â¿Ã‚Â½ (Berlin and Kay, 1991). The production of color language is essential for referring expression generation (Krahmer and Van Deemter, 2012) and image captioning (Kulkarni et al., 2011; Mitchell et al., 2012), among other grounded language generation problems"},
  Month                    = jun,

  Adsnote                  = {Provided by the SAO/NASA Astrophysics Data System},
  Adsurl                   = {http://adsabs.harvard.edu/abs/2016arXiv160603821M},
  Archiveprefix            = {arXiv},
  Eprint                   = {1606.03821},
  Keywords                 = {color},
  Owner                    = {20361362},
  Primaryclass             = {cs.CL},
  Timestamp                = {2016.11.16},
  Url                      = {https://arxiv.org/pdf/1606.03821v2.pdf}
}

@Article{DBLP:journals/corr/MonroeHGP17,
  Title                    = {Colors in Context: {A} Pragmatic Neural Model for Grounded Language
 Understanding},
  Author                   = {Will Monroe and
 Robert X. D. Hawkins and
 Noah D. Goodman and
 Christopher Potts},
  Journal                  = {CoRR},
  Year                     = {2017},
  Volume                   = {abs/1703.10186},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/MonroeHGP17},
  Owner                    = {20361362},
  Timestamp                = {Wed, 07 Jun 2017 01:00:00 +0200},
  Url                      = {http://arxiv.org/abs/1703.10186}
}

@Inproceedings{morin2005hierarchical,
  Title                    = {Hierarchical probabilistic neural network language model},
  Author                   = {Morin, Frederic and Bengio, Yoshua},
  Booktitle                = {Proceedings of the international workshop on artificial intelligence and statistics},
  Year                     = {2005},
  Annote                   = {Hierarchical Softmax.},
  Organization             = {Citeseer},
  Pages                    = {246--252},

  Owner                    = {20361362},
  Timestamp                = {2015.06.18},
  Url                      = {http://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf}
}

@Article{moro2015semeval,
  Title                    = {SemEval-2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking},
  Author                   = {Moro, Andrea and Navigli, Roberto},
  Journal                  = {Proceedings of SemEval-2015},
  Year                     = {2015},

  Annote                   = {BabelNet word sense disambiguation.
Nothing beat Most Frequent Sense.},

  Keywords                 = {competition, word-sense-disambiguation},
  Owner                    = {20361362},
  Timestamp                = {2015.07.17},
  Url                      = {http://www.aclweb.org/anthology/S15-2049}
}

@Article{Moro2014,
  Title                    = {{Entity Linking meets Word Sense Disambiguation: a Unified Approach}},
  Author                   = {Andrea Moro and Alessandro Raganato and Roberto Navigli},
  Journal                  = {Transactions of the Association for Computational Linguistics (TACL)},
  Year                     = {2014},

  Annote                   = {BabelFly},
  Pages                    = {231-244},
  Volume                   = {2},

  Owner                    = {20361362},
  Timestamp                = {2017.02.20},
  Url                      = {http://wwwusers.di.uniroma1.it/~navigli/pubs/TACL_2014_Babelfy.pdf}
}

@Article{Munroe2010XKCDdataset,
  Title                    = {XKCD: Color Survey Results},
  Author                   = {Munroe, Randall},
  Year                     = {2010},
  Month                    = {April},

  HowPublished             = {Website},
  Owner                    = {20361362},
  Timestamp                = {2017.06.06},
  Url                      = {https://blog.xkcd.com/2010/05/03/color-survey-results/}
}

@Article{mylonas2012colour,
  Title                    = {Colour naming for colour communication},
  Author                   = {Mylonas, D and MacDonald, L},
  Journal                  = {Colour Design: Theories and Applications},
  Year                     = {2012},

  Annote                   = {2500 subjects 50,000 total observations 7 languagages conducted during 2009-2011},
  Pages                    = {254--270},

  Owner                    = {20361362},
  Publisher                = {Elsevier},
  Timestamp                = {2017.07.04},
  Url                      = {http://www.academia.edu/download/40794615/Colour_Naming_for_Colour_Communication_author_copy.pdf}
}

@Inproceedings{mylonas2010online,
  Title                    = {Online colour naming experiment using Munsell samples},
  Author                   = {Mylonas, Dimitris and MacDonald, Lindsay},
  Booktitle                = {Conference on Colour in Graphics, Imaging, and Vision},
  Year                     = {2010},
  Number                   = {1},
  Organization             = {Society for Imaging Science and Technology},
  Pages                    = {27--32},
  Volume                   = {2010},

  Owner                    = {20361362},
  Timestamp                = {2017.04.11},
  Url                      = {https://www.researchgate.net/publication/258831962_Online_Colour_Naming_Experiment_Using_Munsell_Samples}
}

@Inproceedings{mylonas2015use,
  Title                    = {The Use of English Colour Terms in Big Data},
  Author                   = {Mylonas, Dimitris and Purver, Matthew and Sadrzadeh, Mehrnoosh and MacDonald, Lindsay and Griffin, Lewis},
  Year                     = {2015},
  Organization             = {The Color Science Association of Japan},

  Owner                    = {20361362},
  Timestamp                = {2017.04.11},
  Url                      = {http://eecs.qmul.ac.uk/~mpurver/papers/mylonas-et-al15aic.pdf}
}

@Article{nation2006large,
  Title                    = {How large a vocabulary is needed for reading and listening?},
  Author                   = {Nation, I},
  Journal                  = {Canadian Modern Language Review},
  Year                     = {2006},

  Annote                   = {Never more than about 10,000},
  Number                   = {1},
  Pages                    = {59--82},
  Volume                   = {63},

  Owner                    = {20361362},
  Publisher                = {University of Toronto Press},
  Timestamp                = {2016.02.09},
  Url                      = {http://muse.jhu.edu/journals/canadian_modern_language_review/v063/63.1nation.html}
}

@Inproceedings{navigli2013semeval,
  Title                    = {Semeval-2013 task 12: Multilingual word sense disambiguation},
  Author                   = {Navigli, Roberto and Jurgens, David and Vannella, Daniele},
  Booktitle                = {Second Joint Conference on Lexical and Computational Semantics (* SEM)},
  Year                     = {2013},
  Pages                    = {222--231},
  Volume                   = {2},

  Abstract                 = {Word sense diambiguation competion.
We see only very small gains over the base case of using most common word (MFS = Most Frequent Sense).

Best on ENglish wordnet was F1 score of 0.685 vs MFS of 0.665},
  Keywords                 = {word-sense-disambiguation, competition},
  Owner                    = {20361362},
  Timestamp                = {2015.07.17},
  Url                      = {http://wwwusers.di.uniroma1.it/~navigli/pubs/Semeval_2013_Navigli_etal.pdf}
}

@Inproceedings{Navigli:2007:STC:1621474.1621480,
  Title                    = {SemEval-2007 Task 07: Coarse-grained English All-words Task},
  Author                   = {Navigli, Roberto and Litkowski, Kenneth C. and Hargraves, Orin},
  Booktitle                = {Proceedings of the 4th International Workshop on Semantic Evaluations},
  Year                     = {2007},

  Address                  = {Stroudsburg, PA, USA},
  Pages                    = {30--35},
  Publisher                = {Association for Computational Linguistics},
  Series                   = {SemEval '07},

  Acmid                    = {1621480},
  Location                 = {Prague, Czech Republic},
  Numpages                 = {6},
  Owner                    = {20361362},
  Timestamp                = {2016.10.13},
  Url                      = {http://dl.acm.org/citation.cfm?id=1621474.1621480}
}

@Inproceedings{navigli2010babelnet,
  Title                    = {BabelNet: Building a very large multilingual semantic network},
  Author                   = {Navigli, Roberto and Ponzetto, Simone Paolo},
  Booktitle                = {Proceedings of the 48th annual meeting of the association for computational linguistics},
  Year                     = {2010},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {216--225},

  Owner                    = {20361362},
  Timestamp                = {2015.07.17},
  Url                      = {http://www.anthology.aclweb.org/P/P10/P10-1023.pdf}
}

@Article{neelakantan2015efficient,
  Title                    = {Efficient non-parametric estimation of multiple embeddings per word in vector space},
  Author                   = {Neelakantan, Arvind and Shankar, Jeevan and Passos, Alexandre and McCallum, Andrew},
  Journal                  = {arXiv preprint arXiv:1504.06654},
  Year                     = {2015},

  Annote                   = {Two vector tables, Vs(word, sense_id), Vg(word)
v_context=sum(Vg,context_words)
sense_t = arg max(Similarity(v_context, Vs(word, sense), over all senses)
update Vg(word, sense_t)
also update Vg(word).

most similar to mine idea.},

  Abstract                 = {There is rising interest in vector-space word embeddings and their use in NLP, especially given recent methods for their fast estimation at very large scale. Nearly all this work, however, assumes a single vector per word typeÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬ÃƒÂ¢Ã¢â€šÂ¬Ã¯Â¿Â½ignoring poly semy and thus jeopardizing their usefulness for downstream tasks. 

 We presentan extension to the Skip-gram model that efficiently learns multiple embeddings per word type. It differs from recent related work by jointly performing word sense discrimination and embedding learning, by non-parametrically estimating the number of senses per word type, and by its efficiency and scalability. We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on acorpus of nearly 1 billion tokens in less than 6 hours.},
  Keywords                 = {WSI, WSD},
  Owner                    = {20361362},
  Timestamp                = {2016.05.30},
  Url                      = {http://arxiv.org/pdf/1504.06654.pdf}
}

@Article{NESTORIDIS20111783,
  Title                    = {Universal series induced by approximate identities and some relevant applications},
  Author                   = {Vassili Nestoridis and Sebastian Schmutzhard and Vangelis Stefanopoulos},
  Journal                  = {Journal of Approximation Theory},
  Year                     = {2011},
  Number                   = {12},
  Pages                    = {1783 - 1797},
  Volume                   = {163},

  __markedentry            = {[20361362:1]},
  Abstract                 = {Abstract We prove the existence of series , whose coefficients (an) are in and whose terms are translates by rational vectors in Rd of a family of approximations to the identity, having the property that the partial sums are dense in various spaces of functions such as WienerÃ¯Â¿Â½s algebra , for every , and the space of measurable functions. Applying this theory to particular situations, we establish approximations by such series to solutions of the heat and Laplace equations as well as to probability density functions.},
  Doi                      = {https://doi.org/10.1016/j.jat.2011.06.001},
  ISSN                     = {0021-9045},
  Keywords                 = {Universal series},
  Owner                    = {20361362},
  Timestamp                = {2017.10.03},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S002190451100102X}
}

@Inproceedings{Ngiam2011,
  Title                    = {On optimization methods for deep learning},
  Author                   = {Ngiam, Jiquan and Coates, Adam and Lahiri, Ahbik and Prochnow, Bobby and Le, Quoc V and Ng, Andrew Y},
  Booktitle                = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  Year                     = {2011},
  Annote                   = {Discusses why we shouldn't use SGD for deep leanrning adsn should use somes uaw BFGS or CG},
  Pages                    = {265--272},

  Owner                    = {20361362},
  Timestamp                = {2015.05.19},
  Url                      = {http://www.icml-2011.org/papers/210_icmlpaper.pdf}
}

@Inproceedings{Nguyen2015a,
  Title                    = {Tensor-Variate Restricted Boltzmann Machines.},
  Author                   = {Nguyen, Tu Dinh and Tran, Truyen and Phung, Dinh Q and Venkatesh, Svetha},
  Booktitle                = {AAAI},
  Year                     = {2015},
  Pages                    = {2887--2893},

  Owner                    = {20361362},
  Timestamp                = {2016.03.18}
}

@Book{WebBookBackprop,
  Title                    = {Neural Networks and Deep Learning},
  Author                   = {Nielsen, Michael A},
  Publisher                = {Determination Press},
  Year                     = {2015},
  Annote                   = {By far best description of backpropergation I have seen.
It is a incomplete work, available online under creative commons.},

  Owner                    = {20361362},
  Timestamp                = {2015.05.17},
  Url                      = {http://neuralnetworksanddeeplearning.com/chap2.html}
}

@Article{nocedal1980updating,
  Title                    = {Updating quasi-Newton matrices with limited storage},
  Author                   = {Nocedal, Jorge},
  Journal                  = {Mathematics of computation},
  Year                     = {1980},

  Annote                   = {L-BFGS paper. LBFGS},
  Number                   = {151},
  Pages                    = {773--782},
  Volume                   = {35},

  Keywords                 = {optimisation},
  Owner                    = {20361362},
  Timestamp                = {2016.10.24},
  Url                      = {http://www.ii.uib.no/~lennart/drgrad/Nocedal1980.pdf}
}

@Article{oord2016pixel,
  Title                    = {Pixel recurrent neural networks},
  Author                   = {Oord, Aaron van den and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  Journal                  = {arXiv preprint arXiv:1601.06759},
  Year                     = {2016},

  Owner                    = {20361362},
  Timestamp                = {2018.01.15},
  Url                      = {https://arxiv.org/pdf/1601.06759.pdf}
}

@Article{DBLP:journals/corr/OordDZSVGKSK16,
  Title                    = {WaveNet: {A} Generative Model for Raw Audio},
  Author                   = {A{\"{a}}ron van den Oord and
 Sander Dieleman and
 Heiga Zen and
 Karen Simonyan and
 Oriol Vinyals and
 Alex Graves and
 Nal Kalchbrenner and
 Andrew W. Senior and
 Koray Kavukcuoglu},
  Journal                  = {CoRR},
  Year                     = {2016},
  Volume                   = {abs/1609.03499},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/OordDZSVGKSK16},
  Owner                    = {20361362},
  Timestamp                = {Mon, 03 Oct 2016 17:51:10 +0200},
  Url                      = {http://arxiv.org/abs/1609.03499}
}

@Article{DBLP:journals/corr/PaineKCZRHH16,
  Title                    = {Fast Wavenet Generation Algorithm},
  Author                   = {Tom Le Paine and
 Pooya Khorrami and
 Shiyu Chang and
 Yang Zhang and
 Prajit Ramachandran and
 Mark A. Hasegawa{-}Johnson and
 Thomas S. Huang},
  Journal                  = {CoRR},
  Year                     = {2016},
  Volume                   = {abs/1611.09482},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/PaineKCZRHH16},
  Owner                    = {20361362},
  Timestamp                = {Thu, 01 Dec 2016 19:32:08 +0100},
  Url                      = {http://arxiv.org/abs/1611.09482}
}

@Inproceedings{pang2005seeing,
  Title                    = {Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales},
  Author                   = {Pang, Bo and Lee, Lillian},
  Booktitle                = {Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics},
  Year                     = {2005},
  Annote                   = {This eventually becomes the Sentiment Treebank. it is the movie reviews from \cite{SocherEtAl2011:RAE}},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {115--124},

  Keywords                 = {corpora},
  Owner                    = {20361362},
  Timestamp                = {2015.09.01},
  Url                      = {http://ssli.ee.washington.edu/conferences/ACL2005/ACL/pdf/ACL15.pdf}
}

@Inproceedings{pantel2002WSI,
  Title                    = {Discovering word senses from text},
  Author                   = {Pantel, Patrick and Lin, Dekang},
  Booktitle                = {Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining},
  Year                     = {2002},
  Annote                   = {A nonneural network clustering based approach. Every word instance is transformed into a feature vector. Patel and Lin call the features contexts, but we will avoid that term for clarity. the word instance wine in the sentence "Fine diners only sip wine, never gulp" exhibits the the verb-object feature "sip \_\_\_\_" These are then clustered using a novel variation on heirachical clustering. They compare there word sense clusters to Wordnet's senses, by clustering wordnet synsets using Hypernym distence. This seems rather dubious.},
  Organization             = {ACM},
  Pages                    = {613--619},

  Abstract                 = {Inventories of manually compiled dictionaries usually serve as a source for word senses. However, they often include many rare senses while missing corpus/domain-specific senses. We present a clustering algorithm called CBC (Clustering By Committee) that automatically discovers word senses from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the cluster. We proceed by assigning words to their most similar clusters. After assigning an element to a cluster, we remove their overlapping features from the element. This allows CBC to discover the less frequent senses of a word and to avoid discovering duplicate senses. Each cluster that a word belongs to represents one of its senses. We also present an evaluation methodology for automatically measuring the precision and recall of discovered senses.},
  Owner                    = {20361362},
  Timestamp                = {2016.08.11},
  Url                      = {https://pdfs.semanticscholar.org/dea5/438eb02d5bf68a12e7a73a18b7071bd3ec09.pdf}
}

@Inproceedings{Papineni2002,
  Title                    = {BLEU: a method for automatic evaluation of machine translation},
  Author                   = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  Booktitle                = {Proceedings of the 40th annual meeting on association for computational linguistics},
  Year                     = {2002},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {311--318},

  Owner                    = {20361362},
  Timestamp                = {2015.11.20},
  Url                      = {http://aclweb.org/anthology/P/P02/P02-1040.pdf}
}

@Article{Park,
  Title                    = {Expressing an Image Stream with a Sequence of Natural Sentences},
  Author                   = {Park, Cesc Chunseong and Kim, Gunhee},

  Annote                   = {Matches text segments from blogs against the images from same post.},

  Owner                    = {20361362},
  Timestamp                = {2015.11.17},
  Url                      = {http://www.cs.cmu.edu/~gunhee/publish/nips15_stream2text.pdf}
}

@Article{10.2307/2237880,
  Title                    = {On Estimation of a Probability Density Function and Mode},
  Author                   = {Emanuel Parzen},
  Journal                  = {The Annals of Mathematical Statistics},
  Year                     = {1962},

  Annote                   = {Parzen Windows Purely mathematical. Nice math but no real details on much in a specific implementable way. eg no exapmples of use here.},
  Number                   = {3},
  Pages                    = {1065-1076},
  Volume                   = {33},

  ISSN                     = {00034851},
  Owner                    = {20361362},
  Publisher                = {Institute of Mathematical Statistics},
  Timestamp                = {2017.08.28},
  Url                      = {http://www.jstor.org/stable/2237880}
}

@Inproceedings{passonneau2012masc,
  Title                    = {The MASC word sense sentence corpus},
  Author                   = {Passonneau, Rebecca J and Baker, Collin and Fellbaum, Christiane and Ide, Nancy},
  Booktitle                = {Proceedings of LREC},
  Year                     = {2012},
  Annote                   = {This is the preferred citation for MASC (Manually Annotated Subcorpus of the Open American National Corpus},

  Owner                    = {20361362},
  Timestamp                = {2015.07.20}
}

@Article{scikit-learn,
  Title                    = {Scikit-learn: Machine Learning in {P}ython},
  Author                   = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
 and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
 and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
 Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2011},

  Annote                   = {This is the prefered citation for Sklearn},
  Pages                    = {2825--2830},
  Volume                   = {12},

  Keywords                 = {tools},
  Owner                    = {20361362},
  Timestamp                = {2015.09.08}
}

@Inproceedings{pennington2014glove,
  Title                    = {GloVe: Global Vectors for Word Representation},
  Author                   = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  Booktitle                = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014)},
  Year                     = {2014},
  Pages                    = {1532--1543},

  Owner                    = {20361362},
  Timestamp                = {2015.12.15},
  Url                      = {http://www.aclweb.org/anthology/D14-1162}
}

@Inproceedings{pinto2007upv,
  Title                    = {UPV-SI: Word sense induction using self term expansion},
  Author                   = {Pinto, David and Rosso, Paolo and Jim{\'e}nez-Salazar, H{\'e}ctor},
  Booktitle                = {Proceedings of the 4th International Workshop on Semantic Evaluations},
  Year                     = {2007},
  Annote                   = {An extension to Simplified Lesk, and to Simplified Extended Lesk.
This varient expands the Context by adding the 100 most similar words to content word in the context.
The expanded context is then compared with the words in the definitions (Simplified Lesk), or in definitions plus hypernym defintions (Simplified Extended Lesk),


The similarity is determined by using a distributional thesaurus, which they construct by extracting relations using a depency parse of a corpus.
Which they use as features, for each word, and apply feature reduction.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {430--433},

  Abstract                 = {We explore the contribution of distributional information for purely knowledge-based word
sense disambiguation. Specifically, we use a distributional thesaurus, computed from a large
parsed corpus, for lexical expansion of context and sense information. This bridges the lexical
gap that is seen as the major obstacle for word overlapÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬ÃƒÂ¢Ã¢â€šÂ¬Ã…â€œbased approaches. We apply this
mechanism to two traditional knowledge-based methods and show that distributional informa-
tion significantly improves disambiguation results across several data sets. This improvement
exceeds the state of the art for disambiguation without sense frequency informationÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬ÃƒÂ¢Ã¢â€šÂ¬Ã¯Â¿Â½a situation
which is especially encountered with new domains or languages for which no sense-annotated
corpus is available.},
  Keywords                 = {WSD},
  Owner                    = {20361362},
  Timestamp                = {2016.08.11},
  Url                      = {https://www.ukp.tu-darmstadt.de/fileadmin/user_upload/Group_UKP/publikationen/2012/MillerBiemannZeschGurevych_COLING2012_WSD.pdf}
}

@Article{Pollack199077,
  Title                    = {Recursive distributed representations},
  Author                   = {Jordan B. Pollack},
  Journal                  = {Artificial Intelligence},
  Year                     = {1990},
  Number                   = {1},
  Pages                    = {77 - 105},
  Volume                   = {46},

  Abstract                 = {A longstanding difficulty for connectionist modeling has been how to represent variable-sized recursive data structures, such as trees and lists, in fixed-width patterns. This paper presents a connectionist architecture which automatically develops compact distributed representations for such compositional structures, as well as efficient accessing mechanisms for them. Patterns which stand for the internal nodes of fixed-valence trees are devised through the recursive use of backpropagation on three-layer auto-associative encoder networks. The resulting representations are novel, in that they combine apparently immiscible aspects of features, pointers, and symbol structures. They form a bridge between the data structures necessary for high-level cognitive tasks and the associative, pattern recognition machinery provided by neural networks.},
  Doi                      = {http://dx.doi.org/10.1016/0004-3702(90)90005-K},
  ISSN                     = {0004-3702},
  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.sciencedirect.com/science/article/pii/000437029090005K}
}

@Article{pourdamghanialigning,
  Title                    = {Aligning English Strings with Abstract Meaning Representation Graphs},
  Author                   = {Pourdamghani, Nima and Gao, Yang and Hermjakob, Ulf and Knight, Kevin},

  Annote                   = {Seaks to align English strings with words in the AMR.

(Also speaks in introduction about how it would be good just to use a generative model between AMR and Strings, but we don't got one for Strings<-> Graphs).

Has a preprocessing precedure to linearise the AMR, depths first.},

  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.isi.edu/natural-language/mt/amr_eng_align.pdf}
}

@Book{press2007numerical,
  Title                    = {Numerical Recipes 3rd Edition: The Art of Scientific Computing},
  Author                   = {Press, William H. and Teukolsky, Saul A. and Vetterling, William T. and Flannery, Brian P.},
  Publisher                = {Cambridge University Press},
  Year                     = {2007},

  Address                  = {New York, NY, USA},
  Edition                  = {3},

  ISBN                     = {0521880688, 9780521880688},
  Owner                    = {20361362},
  Timestamp                = {2016.10.21}
}

@Inproceedings{Qiu2014,
  Title                    = {Learning Word Representation Considering Proximity and Ambiguity.},
  Author                   = {Qiu, Lin and Cao, Yong and Nie, Zaiqing and Yu, Yong and Rui, Yong},
  Booktitle                = {AAAI},
  Year                     = {2014},
  Annote                   = {Adds an additional "proximity weighting" factors to Skip-Grams/CBOWS.
Also relabelled/works with words as Word+Part Of Speech.
So handles POS ambigurity.
Does not do sense ambigurity (beyond POS).},
  Pages                    = {1572--1578},

  Keywords                 = {POS, word-embeddings},
  Owner                    = {20361362},
  Timestamp                = {2016.05.31},
  Url                      = {http://msr-waypoint.com/en-us/um/people/znie/embedding_aaai2014.pdf}
}

@Inproceedings{rehurek_lrec,
  Title                    = {{Software Framework for Topic Modelling with Large Corpora}},
  Author                   = {Radim {R}eh{\r u}{r}ek and Petr Sojka},
  Booktitle                = {{Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks}},
  Year                     = {2010},

  Address                  = {Valletta, Malta},
  Annote                   = {This is the preferred citation for Gensim.an2009},
  Month                    = May,
  Note                     = {\url{http://is.muni.cz/publication/884893/en}},
  Pages                    = {45--50},
  Publisher                = {ELRA},

  Day                      = {22},
  Language                 = {English},
  Owner                    = {20361362},
  Timestamp                = {2015.07.12},
  Url                      = {https://github.com/piskvorky/gensim}
}

@Article{hmmsSpeechTuit,
  Title                    = {A tutorial on hidden Markov models and selected applications in speech recognition},
  Author                   = {Rabiner, L.},
  Journal                  = {Proceedings of the IEEE},
  Year                     = {1989},

  Annote                   = {A very good introduction to HMMs for speech},
  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {257-286},
  Volume                   = {77},

  Abstract                 = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described},
  Doi                      = {10.1109/5.18626},
  ISSN                     = {0018-9219},
  Keywords                 = {Markov processes;speech recognition;balls-in-urns system;coin-tossing;discrete Markov chains;ergodic models;hidden Markov models;hidden states;left-right models;probabilistic function;speech recognition;Distortion;Hidden Markov models;Mathematical model;Multiple signal classification;Signal processing;Speech recognition;Statistical analysis;Stochastic processes;Temperature measurement;Tutorial},
  Owner                    = {20361362},
  Timestamp                = {2015.06.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=18626}
}

@Book{radford2004english,
  Title                    = {English syntax: An introduction},
  Author                   = {Radford, Andrew},
  Publisher                = {Cambridge University Press},
  Year                     = {2004},
  Annote                   = {This is a good book for knowing about Auxiliary verbs.},

  Owner                    = {20361362},
  Timestamp                = {2015.07.14},
  Url                      = {http://faculty.ksu.edu.sa/mmazen/Documents/English_Syntax__Radford-%20up%20%20to%20p%2070.pdf}
}

@Book{radford2004minimalist,
  Title                    = {Minimalist Syntax: Exploring the Structure of English},
  Author                   = {Radford, A.},
  Publisher                = {Cambridge University Press},
  Year                     = {2004},
  Series                   = {Cambridge Textbooks in Linguistics},

  ISBN                     = {9780521542746},
  Lccn                     = {2003055385},
  Owner                    = {20361362},
  Timestamp                = {2015.07.24},
  Url                      = {https://books.google.com.au/books?id=y5VJLP-NW1gC}
}

@Inproceedings{reed2016generative,
  Title                    = {Generative adversarial text to image synthesis},
  Author                   = {Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
  Booktitle                = {Proceedings of The 33rd International Conference on Machine Learning},
  Year                     = {2016},
  Annote                   = {Super impressive.
Seems to understand complex color descriptions.},
  Volume                   = {3},

  Owner                    = {20361362},
  Timestamp                = {2017.06.08},
  Url                      = {http://proceedings.mlr.press/v48/reed16.pdf}
}

@Article{rei2014looking,
  Title                    = {Looking for Hyponyms in Vector Space},
  Author                   = {Rei, Marek and Briscoe, Ted},
  Journal                  = {CoNLL-2014},
  Year                     = {2014},

  Annote                   = {considers using word embeddings to generate hyponyms.
It uses a similarity metrics to find similar words, which are thus likely hyponyms.},
  Pages                    = {68},

  Owner                    = {20361362},
  Timestamp                = {2015.06.05},
  Url                      = {http://www.aclweb.org/anthology/W/W14/W14-16.pdf#page=78}
}

@Inproceedings{Reisinger2010,
  Title                    = {Multi-prototype vector-space models of word meaning},
  Author                   = {Reisinger, Joseph and Mooney, Raymond J},
  Booktitle                = {Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  Year                     = {2010},
  Annote                   = {The original word emebedding WSD paper. Older than word2vec. Words instance features tdf features or $\chi^2$ features detected in the context around the word instance. These are clustered, using a mixture model (von Mises-Fisher distributions). Thus giving word senses clusters. Nice quote: "Word meaning violates the triangle inequality when viewed at the level of word types, posing a problem for vector-space models (Tversky and Gati, 1982)"},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {109--117},

  __markedentry            = {[20361362:]},
  Groups                   = {to_read},
  Keywords                 = {WSD, word-embeddings},
  Owner                    = {20361362},
  Timestamp                = {2016.06.01},
  Url                      = {http://www.anthology.aclweb.org/N/N10/N10-1013.pdf}
}

@Article{RitterPosition,
  Title                    = {Leveraging Preposition Ambiguity to Assess Compositional Distributional Models of Semantics},
  Author                   = {Ritter, Samuel and Long, Cotie and Paperno, Denis and Baroni, Marco and Botvinick, Matthew and Goldberg, Adele},
  Journal                  = {The Fourth Joint Conference on Lexical and Computational Semantics},
  Year                     = {2015},

  Annote                   = {This is the paper where they assess phrase embeddins by seeing if they can be classified as to wheich kind ogf spacial stituation they describe.},

  Owner                    = {20361362},
  Timestamp                = {2015.09.04},
  Url                      = {http://clic.cimec.unitn.it/marco/publications/ritter-etal-prepositions-starsem-2015.pdf}
}

@Article{rong2014word2vec,
  Title                    = {word2vec Parameter Learning Explained},
  Author                   = {Rong, Xin},
  Journal                  = {arXiv preprint arXiv:1411.2738},
  Year                     = {2014},

  Owner                    = {20361362},
  Timestamp                = {2015.06.19},
  Url                      = {http://arxiv.org/pdf/1411.2738.pdf}
}

@Techreport{1961Perceptron,
  Title                    = {Principles of neurodynamics. perceptrons and the theory of brain mechanisms},
  Author                   = {Rosenblatt, Frank},
  Institution              = {DTIC Document},
  Year                     = {1961},

  Owner                    = {20361362},
  Timestamp                = {2015.09.09}
}

@Article{rosenfeld2000two,
  Title                    = {Two decades of statistical language modeling: Where do we go from here?},
  Author                   = {Rosenfeld, Ronald},
  Journal                  = {Proceedings of the IEEE},
  Year                     = {2000},

  Annote                   = {Reivw of Statistical Lnaguage modelling,},
  Number                   = {8},
  Pages                    = {1270--1278},
  Volume                   = {88},

  Doi                      = {10.1109/5.880083},
  Owner                    = {20361362},
  Publisher                = {Institute of Electrical and Electronics Engineers},
  Timestamp                = {2016.10.24}
}

@Article{Ruder17crosslingreview,
  Title                    = {A survey of cross-lingual embedding models},
  Author                   = {Sebastian Ruder},
  Journal                  = {CoRR},
  Year                     = {2017},

  Annote                   = {This guy does great work has a very good blog.},
  Volume                   = {abs/1706.04902},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/Ruder17},
  Owner                    = {20361362},
  Timestamp                = {Mon, 03 Jul 2017 13:29:02 +0200},
  Url                      = {http://arxiv.org/abs/1706.04902}
}

@Article{rudin1976principles,
  Title                    = {Principles of Mathematical Analysis (International Series in Pure \& Applied Mathematics)},
  Author                   = {Rudin, Walter},
  Year                     = {1976},

  Annote                   = {Book on real analysis},

  __markedentry            = {[20361362:1]},
  Owner                    = {20361362},
  Publisher                = {McGraw-Hill Publishing Co.},
  Timestamp                = {2017.10.03}
}

@Article{backprop,
  Title                    = {Learning representations by back-propagating errors},
  Author                   = {Rumelhart, David E and Hintont, Geoffrey E and Williams, Ronald J},
  Journal                  = {Nature},
  Year                     = {1986},
  Pages                    = {9},
  Volume                   = {323},

  Owner                    = {20361362},
  Timestamp                = {2015.09.09},
  Url                      = {http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf}
}

@Article{DBLP:journals/corr/RusuRDSKKPH16,
  Title                    = {Progressive Neural Networks},
  Author                   = {Andrei A. Rusu and
 Neil C. Rabinowitz and
 Guillaume Desjardins and
 Hubert Soyer and
 James Kirkpatrick and
 Koray Kavukcuoglu and
 Razvan Pascanu and
 Raia Hadsell},
  Journal                  = {CoRR},
  Year                     = {2016},
  Volume                   = {abs/1606.04671},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/RusuRDSKKPH16},
  Owner                    = {20361362},
  Timestamp                = {Fri, 01 Jul 2016 17:39:49 +0200},
  Url                      = {http://arxiv.org/abs/1606.04671}
}

@Inproceedings{saarikoski2006building,
  Title                    = {Building an optimal WSD ensemble using per-word selection of best system},
  Author                   = {Saarikoski, Harri MT and Legrand, Steve},
  Booktitle                = {Iberoamerican Congress on Pattern Recognition},
  Year                     = {2006},
  Organization             = {Springer},
  Pages                    = {864--872},

  Owner                    = {20361362},
  Timestamp                = {2016.10.14},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.221.2996&rep=rep1&type=pdf#page=140}
}

@Inproceedings{saarikoski2006defining,
  Title                    = {Defining classifier regions for WSD ensembles using word space features},
  Author                   = {Saarikoski, Harri MT and Legrand, Steve and Gelbukh, Alexander},
  Booktitle                = {Mexican International Conference on Artificial Intelligence},
  Year                     = {2006},
  Organization             = {Springer},
  Pages                    = {855--867},

  Owner                    = {20361362},
  Timestamp                = {2016.10.14},
  Url                      = {http://www.gelbukh.com/CV/Publications/2006/MICAI-2006-WSD.pdf}
}

@Inproceedings{2009DBM,
  Title                    = {Deep boltzmann machines},
  Author                   = {Salakhutdinov, Ruslan and Hinton, Geoffrey E},
  Booktitle                = {International Conference on Artificial Intelligence and Statistics},
  Year                     = {2009},
  Annote                   = {Has a Boltzman Machine Learning procedure nicely defined.


In terms of Fantasy Particles and Sample
Boltsman machiens have 3 weight matrixes: visible-visible, visible-hidden, and vhidden-hidden. (Unlioke RBM which has the first and last set to zero).

Focus is however on deep boltman machines (not just plan regular ones).

"... unlike deep belief networks, the approxi-
mate inference procedure, in addition to an initial bottom-
up pass, can incorporate top-down feedback, allowing deep
Boltzmann machines to better propagate uncertainty about,
and hence deal more robustly with, ambiguous inputs."


Shows ghow to use a modifed RBM for pretraining},
  Pages                    = {448--455},

  Owner                    = {20361362},
  Timestamp                = {2015.04.17},
  Url                      = {http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS09_SalakhutdinovH.pdf}
}

@Inproceedings{CRPITV165P43-52,
  Title                    = { A Semi-supervised Topic-based User Model for Web Information Visualization },
  Author                   = { Saleheen, S. and Lai, W. },
  Booktitle                = { 11th Asia-Pacific Conference on Conceptual Modelling (APCCM 2015) },
  Year                     = { 2015 },

  Address                  = { Sydney, Australia },
  Annote                   = {A paper from conference I am submitting to},
  Editor                   = { Saeki, M. and Kohler, H. },
  Pages                    = { 43-52 },
  Publisher                = {ACS},
  Series                   = {CRPIT},
  Volume                   = { 165 },

  Owner                    = {20361362},
  Timestamp                = {2015.07.09},
  Url                      = { http://crpit.com/confpapers/CRPITV165Saleheen.pdf }
}

@Article{Penn_POS_Taggening,
  Title                    = {Part-of-speech tagging guidelines for the Penn Treebank Project (3rd revision)},
  Author                   = {Santorini, Beatrice},
  Year                     = {1990},

  Owner                    = {20361362},
  Timestamp                = {2015.07.14},
  Url                      = {http://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&context=cis_reports}
}

@Article{Schutze:1998wordsenseclustering,
  Title                    = {Automatic Word Sense Discrimination},
  Author                   = {Sch\"{u}tze, Hinrich},
  Journal                  = {Comput. Linguist.},
  Year                     = {1998},
  Month                    = mar,
  Number                   = {1},
  Pages                    = {97--123},
  Volume                   = {24},

  Acmid                    = {972724},
  Address                  = {Cambridge, MA, USA},
  ISSN                     = {0891-2017},
  Issue_date               = {March 1998},
  Numpages                 = {27},
  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2017.11.14},
  Url                      = {http://dl.acm.org/citation.cfm?id=972719.972724}
}

@Article{schioler1997neural,
  Title                    = {Neural network for estimating conditional distributions},
  Author                   = {Schioler, Henrik and Kulczycki, Piotr},
  Journal                  = {IEEE Transactions on Neural Networks},
  Year                     = {1997},
  Number                   = {5},
  Pages                    = {1015--1025},
  Volume                   = {8},

  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2017.07.21},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.943.3173&rep=rep1&type=pdf}
}

@Article{2016arXiv160408633S,
  Title                    = {{Word Ordering Without Syntax}},
  Author                   = {{Schmaltz}, A. and {Rush}, A.~M. and {Shieber}, S.~M.},
  Journal                  = {ArXiv e-prints},
  Year                     = {2016},

  Annote                   = {Use a LSTM as a language model, then use beam search to search it, to do word ordering.},
  Month                    = apr,

  Adsnote                  = {Provided by the SAO/NASA Astrophysics Data System},
  Adsurl                   = {http://adsabs.harvard.edu/abs/2016arXiv160408633S},
  Archiveprefix            = {arXiv},
  Eprint                   = {1604.08633},
  Keywords                 = {word ordering},
  Owner                    = {20361362},
  Primaryclass             = {cs.CL},
  Timestamp                = {2016.06.07},
  Url                      = {https://arxiv.org/pdf/1604.08633v1.pdf}
}

@Article{schuster1997bidirectional,
  Title                    = {Bidirectional recurrent neural networks},
  Author                   = {Schuster, Mike and Paliwal, Kuldip K},
  Journal                  = {IEEE Transactions on Signal Processing},
  Year                     = {1997},
  Number                   = {11},
  Pages                    = {2673--2681},
  Volume                   = {45},

  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2017.12.05},
  Url                      = {https://www.researchgate.net/profile/Mike_Schuster/publication/3316656_Bidirectional_recurrent_neural_networks/links/56861d4008ae19758395f85c.pdf}
}

@Inproceedings{schwenk2012translating,
  Title                    = {Continuous Space Translation Models for Phrase-Based Statistical Machine Translation.},
  Author                   = {Schwenk, Holger},
  Booktitle                = {COLING (Posters)},
  Year                     = {2012},
  Pages                    = {1071--1080},

  Owner                    = {20361362},
  Timestamp                = {2017.12.08},
  Url                      = {https://aclweb.org/anthology/C/C12/C12-2104.pdf}
}

@Inproceedings{schwenk2004efficient,
  Title                    = {Efficient training of large neural networks for language modeling},
  Author                   = {Schwenk, Holger},
  Booktitle                = {Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on},
  Year                     = {2004},
  Organization             = {IEEE},
  Pages                    = {3059--3064},
  Volume                   = {4},

  Owner                    = {20361362},
  Timestamp                = {2017.07.24},
  Url                      = {http://www.tcstar.org/pubblicazioni/scientific_publications/LIMSI/schwenk.ijcnn04.pdf}
}

@Article{564201,
  Title                    = {Game theory approach to discrete H infin; filter design},
  Author                   = {Xuemin Shen and Li Deng},
  Journal                  = {Signal Processing, IEEE Transactions on},
  Year                     = {1997},
  Month                    = {Apr},
  Number                   = {4},
  Pages                    = {1092-1095},
  Volume                   = {45},

  Abstract                 = {This paper presents a systematic analysis of twenty four performance measures used in the complete spectrum of Machine Learning classification tasks, i.e., binary, multi-class, multi-labelled, and hierarchical. For each classification task, the study relates a set of changes in a confusion matrix to specific characteristics of data. Then the analysis concentrates on the type of changes to a confusion matrix that do not change a measure, therefore, preserve a classifierÃ¯Â¿Â½s evaluation (measure invariance). The result is the measure invariance taxonomy with respect to all relevant label distribution changes in a classification problem. This formal analysis is supported by examples of applications where invariance properties of measures lead to a more reliable evaluation of classifiers. Text classification supplements the discussion with several case studies.},
  Doi                      = {10.1109/78.564201},
  ISSN                     = {1053-587X},
  Keywords                 = {H8 optimisation;Kalman filters;Riccati equations;difference equations;discrete time filters;error analysis;filtering theory;game theory;noise;parameter estimation;signal processing;Kalman filter;amplification;difference Riccati equation;discrete H8 filter design;estimation error signals;exogenous inputs;finite energy signals;finite-horizon discrete H8 filter;game theory;hostile noise signals;linear quadratic game;modified Wiener filter;system initial condition;unknown statistics;Estimation error;Filtering theory;Game theory;H infinity control;Noise measurement;Nonlinear filters;Riccati equations;Signal design;Signal processing;Statistics},
  Owner                    = {20361362},
  Timestamp                = {2015.05.05}
}

@Article{shenHinftySpeachEnhancement,
  Title                    = {H8 Filtering for Speech Ehancement},
  Author                   = {Shen, Xuemin and Deng, Li and Yasmin, Anisa},

  Annote                   = {I feel there must be a similar paper to this that has actually been published.

It is woth noting the state matrix, is [x_{n}, x_{n-1}, x_{n-2}]
and the state transition matrix is just the shift of these plus scaling each.
The Observation matrix, simply picks out the current.

This is a pretty great idea, and you see it in many places.},

  Abstract                 = {In this paper, a new approach based on the H8
filtering is presented
for speech enhancement. This approach differs from the trad
itional
modified Wiener/Kalman filtering approach in the following two as-
pects: 1) no a priori knowledge of the noise statistics is required;
instead the noise signals are only assumed to have finite energy; 2)
the estimation criterion for the filter design is to minimize the worst
possible amplification of the estimation error signal in terms of the
modeling errors and additive noises. Since most additive noises in
speech are not Gaussian, this approach is highly robust and is more
appropriate in practical speech enhancement. The global signal-to-
noise ratio (SNR), time domain speech representation and listening
evaluations are used to verify the performance of the H8 filtering
algorithm. Experimented results show that the filtering performance
is better than other speech enhancement approaches in the
literature under similar experimental conditions.},
  Owner                    = {20361362},
  Timestamp                = {2015.05.07},
  Url                      = {http://research.microsoft.com/pubs/194586/ShenDengYasmin1996.pdf}
}

@Inproceedings{shi2015learningbiligualcofactorisation,
  Title                    = {Learning Cross-lingual Word Embeddings via Matrix Co-factorization.},
  Author                   = {Shi, Tianze and Liu, Zhiyuan and Liu, Yang and Sun, Maosong},
  Booktitle                = {ACL (2)},
  Year                     = {2015},
  Pages                    = {567--572},

  Owner                    = {20361362},
  Timestamp                = {2017.08.25},
  Url                      = {http://www.thunlp.org/~lzy/publications/acl2015_bilingual_long.pdf}
}

@Article{siddique2017zara,
  Title                    = {Zara Returns: Improved Personality Induction and Adaptation by an Empathetic Virtual Agent},
  Author                   = {Siddique, Farhad Bin and Kampman, Onno and Yang, Yang and Dey, Anik and Fung, Pascale},
  Journal                  = {Proceedings of ACL 2017, System Demonstrations},
  Year                     = {2017},
  Pages                    = {121--126},

  Owner                    = {20361362},
  Timestamp                = {2018.02.27},
  Url                      = {http://www.aclweb.org/anthology/P17-4021}
}

@Inproceedings{sill1997monotonicity,
  Title                    = {Monotonicity hints},
  Author                   = {Sill, Joseph and Abu-Mostafa, Yaser S},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {1997},
  Annote                   = {I find this unsatisfying, because it basically is generating a second input point for every training point that is advanced forward a small random amount in the monontic dimention,
and adding a extra cost term of the difference between the outputs.},
  Pages                    = {634--640},

  Owner                    = {20361362},
  Timestamp                = {2017.08.09},
  Url                      = {http://papers.nips.cc/paper/1270-monotonicity-hints.pdf}
}

@Book{silverman1986density,
  Title                    = {Density Estimation for Statistics and Data Analysis},
  Author                   = {Silverman, B.W.},
  Publisher                = {Taylor \& Francis},
  Year                     = {1986},
  Series                   = {Chapman \& Hall/CRC Monographs on Statistics \& Applied Probability},

  ISBN                     = {9780412246203},
  Lccn                     = {86021347},
  Owner                    = {20361362},
  Timestamp                = {2018.06.29},
  Url                      = {https://books.google.com.au/books?id=e-xsrjsL7WkC}
}

@Article{silverman1982algorithm,
  Title                    = {Algorithm AS 176: Kernel density estimation using the fast Fourier transform},
  Author                   = {Silverman, BW},
  Journal                  = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  Year                     = {1982},
  Number                   = {1},
  Pages                    = {93--99},
  Volume                   = {31},

  Owner                    = {20361362},
  Publisher                = {JSTOR},
  Timestamp                = {2018.06.05}
}

@Book{BookOptimalStateEstimation,
  Title                    = {Optimal state estimation: Kalman, H infinity, and nonlinear approaches},
  Author                   = {Simon, Dan},
  Publisher                = {John Wiley \& Sons},
  Year                     = {2006},
  Annote                   = {Nice book.
In the science library.

Covers most things on Kalman, EKF, UKF, H infinity, and particle filters.},

  Owner                    = {20361362},
  Timestamp                = {2015.05.05}
}

@Article{SimonsFeatureArticleHInfinity,
  Title                    = {From Here to Infinity},
  Author                   = {Dan Simon},
  Journal                  = {Embedded Systems Programming},
  Year                     = {2001},

  Annote                   = {Nice.
Written in Feature Article Style.

Some of the math formatting is messed up.

Similar to the description of the filter from his book: \cite{BookOptimalStateEstimation}.

The last section in the article talks about how to handle poor knowledge of the system function.},
  Note                     = {Is a Feature article},

  Keywords                 = {filtering},
  Owner                    = {20361362},
  Timestamp                = {2015.05.05},
  Url                      = {http://academic.csuohio.edu/simond/courses/eec641/hinfinity.pdf}
}

@Article{slama2015accurate,
  Title                    = {Accurate 3D action recognition using learning on the Grassmann manifold},
  Author                   = {Slama, Rim and Wannous, Hazem and Daoudi, Mohamed and Srivastava, Anuj},
  Journal                  = {Pattern Recognition},
  Year                     = {2015},
  Number                   = {2},
  Pages                    = {556--567},
  Volume                   = {48},

  Owner                    = {20361362},
  Publisher                = {Elsevier},
  Timestamp                = {2016.08.22}
}

@Article{smith1978color,
  Title                    = {Color gamut transform pairs},
  Author                   = {Smith, Alvy Ray},
  Journal                  = {ACM Siggraph Computer Graphics},
  Year                     = {1978},

  Annote                   = {THE original HSV paper:},
  Number                   = {3},
  Pages                    = {12--19},
  Volume                   = {12},

  Owner                    = {20361362},
  Publisher                = {ACM},
  Timestamp                = {2017.06.30},
  Url                      = {http://alvyray.com/Papers/CG/color78.pdf}
}

@Phdthesis{socher2014recursive,
  Title                    = {Recursive Deep Learning for Natural Language Processing and Computer Vision},
  Author                   = {Socher, Richard},
  School                   = {Stanford University},
  Year                     = {2014},
  Annote                   = {This thesis sumerises most of socher's papers. see also: http://lxmls.it.pt/2014/socher-lxmls.pdf Currently up to chapter 3.},

  File                     = {:..\\annotated_documents\\socher_thesis.pdf:PDF},
  Groups                   = {KeyPapers},
  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://nlp.stanford.edu/~socherr/thesis.pdf}
}

@Incollection{SocherEtAl2013:CVG,
  Title                    = {Parsing With Compositional Vector Grammars},
  Author                   = {Richard Socher and John Bauer and Christopher D. Manning and Andrew Y. Ng},
  Booktitle                = {ACL},
  Year                     = {2013},
  Annote                   = {This is the improved RvNN parser that combines it with Grammar,
This is what is in the Stanford Parser.},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.socher.org/uploads/Main/SocherBauerManningNg_ACL2013.pdf}
}

@Incollection{Socher2013TensorReasoning,
  Title                    = {Reasoning With Neural Tensor Networks For Knowledge Base Completion},
  Author                   = {Richard Socher and Danqi Chen and Christopher D. Manning and Andrew Y. Ng},
  Booktitle                = {Advances in Neural Information Processing Systems 26},
  Year                     = {2013},
  Annote                   = {Simarlar to mikolov2013linguisticsubstructures
Instead of , A is to B as C is to D (as in Mikolov et al)

This reasons in triples of a knowledge base,
trying to create more triples.

Going form a large number of (A,R,B) where R is a relationship between A and B.
To having even more of these.},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.15},
  Url                      = {http://nlp.stanford.edu/~socherr/SocherChenManningNg_NIPS2013.pdf}
}

@Inproceedings{Socher2010,
  Title                    = {Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora},
  Author                   = {Socher, Richard and Fei-Fei, Li},
  Booktitle                = {Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on},
  Year                     = {2010},
  Annote                   = {This is heavily inspired by machine translation.
Image is mapped to Visual Words, which are then moved to a common vector space that both visual words and textural words are mapped to .

Does Not Use Word Embedding, I think??},
  Organization             = {IEEE},
  Pages                    = {966--973},

  Owner                    = {20361362},
  Timestamp                = {2015.04.23},
  Url                      = {http://www.socher.org/uploads/Main/SocherFeiFei_CVPR2010.pdf}
}

@Incollection{SocherEtAl2011:PoolRAE,
  Title                    = {Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection},
  Author                   = {Richard Socher and Eric H. Huang and Jeffrey Pennington and Andrew Y. Ng and Christopher D. Manning},
  Booktitle                = {{Advances in Neural Information Processing Systems 24}},
  Year                     = {2011},

  Owner                    = {20361362},
  Timestamp                = {2015.04.24},
  Url                      = {http://www.socher.org/uploads/Main/SocherHuangPenningtonNgManning_NIPS2011.pdf}
}

@Inproceedings{SocherMVRNN,
  Title                    = {Semantic compositionality through recursive matrix-vector spaces},
  Author                   = {Socher, Richard and Huval, Brody and Manning, Christopher D and Ng, Andrew Y},
  Booktitle                = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
  Year                     = {2012},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {1201--1211},

  Owner                    = {20361362},
  Timestamp                = {2015.09.09},
  Url                      = {http://www.aclweb.org/anthology/D12-1110}
}

@Article{socherDTRNN,
  Title                    = {Grounded compositional semantics for finding and describing images with sentences},
  Author                   = {Socher, Richard and Karpathy, Andrej and Le, Quoc V and Manning, Christopher D and Ng, Andrew Y},
  Journal                  = {Transactions of the Association for Computational Linguistics},
  Year                     = {2014},
  Pages                    = {207--218},
  Volume                   = {2},

  Keywords                 = {image description},
  Owner                    = {20361362},
  Timestamp                = {2015.09.09},
  Url                      = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/download/325/45}
}

@Inproceedings{Socher2011ParsingPhrases,
  Title                    = {Parsing natural scenes and natural language with recursive neural networks},
  Author                   = {Socher, Richard and Lin, Cliff C and Manning, Chris and Ng, Andrew Y},
  Booktitle                = {Proceedings of the 28th international conference on machine learning (ICML-11)},
  Year                     = {2011},
  Annote                   = {Defines a tree of recurrsively applied neural networks for parsing.

Each network has:
2 split inputs, $c_i$, $c_j$ each 1 hot word vector
each goto $L$: the word embeeding matrix, with the output then concatenated which goes
to $W$: the hidden layer 

output of $W$ is $p_{i,j}$ -- the merged word vector
which also goes to two places:
To $W_{score}$ -- the score of how good the merge was
and To $W_{label}$ -- the softmaxed Part of Speech (POS) tagging layer


The paper describes how the parse trees are selected.
It also (poorly) decribes how the learning is carried out (to understand will need to chase up references).

-----
This set of slides: http://lxmls.it.pt/2014/socher-lxmls.pdf
Describes all things quiet well},
  Pages                    = {129--136},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://nlp.stanford.edu/pubs/SocherLinNgManning_ICML2011.pdf}
}

@Inproceedings{socher2010PhraseEmbedding,
  Title                    = {Learning continuous phrase representations and syntactic parsing with recursive neural networks},
  Author                   = {Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
  Booktitle                = {Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop},
  Year                     = {2010},
  Annote                   = {Sorcher and Mannings earlier work. Predecessor to \cite{socher2010PhraseEmbedding} Likely a easier paper to understand that due to its earlier nature. Embeeds Phrases via Parse of speach Does Assess Nerest Neighbour Phrases, which is interesting. This is done by Embeeding a chunk of Wall Street Journal, then finding nearest neighbours on using a undsiclosed metric (Possibly equclidean) Claims: "parser can accurately recover tree structures using only the distributed phrase representations. Furthermore, it provides semantic information even" But I can't see how.},
  Pages                    = {1--9},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.186.5946&rep=rep1&type=pdf}
}

@Inproceedings{SocherEtAl2011:RAE,
  Title                    = {Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions},
  Author                   = {Richard Socher and Jeffrey Pennington and Eric H. Huang and Andrew Y. Ng and Christopher D. Manning},
  Booktitle                = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  Year                     = {2011},
  Annote                   = {Similar to the RvNN, the Recursive Auto Encoder applies an Auto Encoder while generating the tree, in the place of the neural network shown in[fig:The-Neural-Net]. Roughly replacing the Max-Margin derived scoring matrix with a reconstruction error.

The tree selected is selecting much the say way as for the RvNN -- by merge resulting in best reconstruction.

It is noted that if the left child (for example) is a node made by merging 5 children and the right a node by merging only 2, then it is more important to correctly reconstruct the left child than the right. The weighting function in the RAE in that paper is adjusted as such. 

It can be extended in a similar way to the RvNN by adding additional trained outputs at each node.

It is not purely greedy, on simply training the to minimize the error of the merge at each node. Like the RvNN, it uses BPTS, and attempt to minimize the sum of all errors across the tree. Unlike the RvNN, there is no target structure -- it is free to use what ever structure minimizes total error.},

  Keywords                 = {RAE},
  Owner                    = {20361362},
  Timestamp                = {2015.04.24},
  Url                      = {http://www.socher.org/uploads/Main/SocherPenningtonHuangNgManning_EMNLP2011.pdf}
}

@Inproceedings{RvNTN,
  Title                    = {Recursive deep models for semantic compositionality over a sentiment treebank},
  Author                   = {Socher, Richard and Perelygin, Alex and Wu, Jean Y and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  Booktitle                = {Proceedings of the conference on empirical methods in natural language processing (EMNLP)},
  Year                     = {2013},
  Annote                   = {This is the preferece cisation for the sentiment treeba nk},
  Organization             = {Citeseer},
  Pages                    = {1642},
  Volume                   = {1631},

  Owner                    = {20361362},
  Timestamp                = {2015.09.01},
  Url                      = {nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf}
}

@Article{sokolova2009systematica,
  Title                    = {A systematic analysis of performance measures for classification tasks},
  Author                   = {Sokolova, Marina and Lapalme, Guy},
  Journal                  = {Information Processing \& Management},
  Year                     = {2009},
  Number                   = {4},
  Pages                    = {427--437},
  Volume                   = {45},

  Owner                    = {20361362},
  Publisher                = {Elsevier},
  Timestamp                = {2018.02.07},
  Url                      = {http://rali.iro.umontreal.ca/rali/sites/default/files/publis/SokolovaLapalme-JIPM09.pdf}
}

@Article{Song2016,
  Title                    = {Word Embeddings, Sense Embeddings and their Application to Word Sense Induction},
  Author                   = {Song, Linfeng},
  Year                     = {2016},

  Annote                   = {Very long (41 pages) review article on WSI using word emebddings.
Solid summary. 
Also presents own new method, which I haven't read into closely yet.},

  Keywords                 = {WSI; WSD;},
  Owner                    = {20361362},
  Timestamp                = {2016.05.30},
  Url                      = {https://www.cs.rochester.edu/~lsong10/papers/area.pdf}
}

@Techreport{export:226586,
  Title                    = {Unsupervised Learning of Word Semantic Embedding using the Deep Structured
 Semantic Model},
  Author                   = {Xinying Song and Xiaodong He and Jianfeng Gao and Li Deng},
  Year                     = {2014},
  Annote                   = {A method for learning Word Embeddings, using DNNs.

Bag of words based.
Embed the whole bag of words, in a document.
Use cosine similarity to embedding of bag of wards in query.},
  Month                    = {August},
  Number                   = {MSR-TR-2014-109},

  Owner                    = {20361362},
  Publisher                = {Microsoft Research},
  Timestamp                = {2015.05.26},
  Url                      = {http://research.microsoft.com/apps/pubs/default.aspx?id=226586}
}

@Article{SONODA2017uat,
  Title                    = {Neural network with unbounded activation functions is universal approximator},
  Author                   = {Sho Sonoda and Noboru Murata},
  Journal                  = {Applied and Computational Harmonic Analysis},
  Year                     = {2017},
  Number                   = {2},
  Pages                    = {233 - 268},
  Volume                   = {43},

  __markedentry            = {[20361362:1]},
  Abstract                 = {Abstract This paper presents an investigation of the approximation property of neural networks with unbounded activation functions, such as the rectified linear unit (ReLU), which is the new de-facto standard of deep learning. The ReLU network can be analyzed by the ridgelet transform with respect to Lizorkin distributions. By showing three reconstruction formulas by using the Fourier slice theorem, the Radon transform, and Parseval's relation, it is shown that a neural network with unbounded activation functions still satisfies the universal approximation property. As an additional consequence, the ridgelet transform, or the backprojection filter in the Radon domain, is what the network learns after backpropagation. Subject to a constructive admissibility condition, the trained network can be obtained by simply discretizing the ridgelet transform, without backpropagation. Numerical examples not only support the consistency of the admissibility condition but also imply that some non-admissible cases result in low-pass filtering.},
  Doi                      = {https://doi.org/10.1016/j.acha.2015.12.005},
  ISSN                     = {1063-5203},
  Keywords                 = {Neural network},
  Owner                    = {20361362},
  Timestamp                = {2017.10.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1063520315001748}
}

@Article{specht1991general,
  Title                    = {A general regression neural network},
  Author                   = {Specht, Donald F},
  Journal                  = {IEEE transactions on neural networks},
  Year                     = {1991},

  Annote                   = {connectioned to probabiulity density estimation Not a neural network as we know them today. Regession version of a probabilistic neural network. Basically does regession based on the distance of the query point to the training points.},
  Number                   = {6},
  Pages                    = {568--576},
  Volume                   = {2},

  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2017.08.28},
  Url                      = {http://research.vuse.vanderbilt.edu/vuwal/paul/paper/references/grnn.pdf}
}

@Article{srivastava2014dropout,
  Title                    = {Dropout: A simple way to prevent neural networks from overfitting},
  Author                   = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  Journal                  = {The Journal of Machine Learning Research},
  Year                     = {2014},
  Number                   = {1},
  Pages                    = {1929--1958},
  Volume                   = {15},

  Owner                    = {20361362},
  Publisher                = {JMLR. org},
  Timestamp                = {2017.06.14},
  Url                      = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@Inproceedings{stenetorp2013transition,
  Title                    = {Transition-based Dependency Parsing Using Recursive Neural Networks},
  Author                   = {Stenetorp, Pontus},
  Booktitle                = {Deep Learning Workshop at the 2013 Conference on Neural Information Processing Systems (NIPS)},
  Year                     = {2013},

  Address                  = {Lake Tahoe, Nevada, USA},
  Month                    = {December},

  Day                      = {9},
  Pdf_url                  = {https://pontus.stenetorp.se/res/pdf/stenetorp2013transition.pdf},
  Poster_url               = {https://pontus.stenetorp.se/res/pdf/stenetorp2013transition_poster.pdf},
  Url                      = {https://pontus.stenetorp.se/res/pdf/stenetorp2013transition.pdf}
}

@Article{STOCKMAN1999perception,
  Title                    = {The spectral sensitivity of the human short-wavelength sensitive cones derived from thresholds and color matches},
  Author                   = {Andrew Stockman and Lindsay T. Sharpe and Clemens Fach},
  Journal                  = {Vision Research},
  Year                     = {1999},
  Number                   = {17},
  Pages                    = {2901 - 2927},
  Volume                   = {39},

  Abstract                 = {We used two methods to estimate short-wave (S) cone spectral sensitivity. Firstly, we measured S-cone thresholds centrally and peripherally in five trichromats, and in three blue-cone monochromats, who lack functioning middle-wave (M) and long-wave (L) cones. Secondly, we analyzed standard color-matching data. Both methods yielded equivalent results, on the basis of which we propose new S-cone spectral sensitivity functions. At short and middle-wavelengths, our measurements are consistent with the color matching data of Stiles and Burch (1955, Optica Acta, 2, 168â€“181; 1959, Optica Acta, 6, 1â€“26), and other psychophysically measured functions, such as 3 (Stiles, 1953, Coloquio sobre problemas opticos de la vision, 1, 65â€“103). At longer wavelengths, S-cone sensitivity has previously been over-estimated.},
  Doi                      = {https://doi.org/10.1016/S0042-6989(98)00225-9},
  ISSN                     = {0042-6989},
  Keywords                 = {S cones, Spectral sensitivity, Blue-cone monochromacy, Cone fundamental, Photopigment, Macular pigment, Lens pigment, Color matching},
  Owner                    = {20361362},
  Timestamp                = {2018.02.16},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0042698998002259}
}

@Inproceedings{sundermeyer2012lstm,
  Title                    = {LSTM neural networks for language modeling},
  Author                   = {Sundermeyer, Martin and Schl{\"u}ter, Ralf and Ney, Hermann},
  Booktitle                = {Thirteenth Annual Conference of the International Speech Communication Association},
  Year                     = {2012},

  Owner                    = {20361362},
  Timestamp                = {2017.08.15},
  Url                      = {https://pdfs.semanticscholar.org/f9a1/b3850dfd837793743565a8af95973d395a4e.pdf}
}

@Inproceedings{Szegedy2015Inception,
  Title                    = {Going Deeper With Convolutions},
  Author                   = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  Booktitle                = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  Year                     = {2015},
  Month                    = {June},

  Owner                    = {20361362},
  Timestamp                = {2018.02.13},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf}
}

@Inproceedings{Taghipour2015,
  Title                    = {Semi-Supervised Word Sense Disambiguation Using Word Embeddings in General and Specific Domains},
  Author                   = {Taghipour, Kaveh and Ng, Hwee Tou},
  Booktitle                = {The 2015 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  Year                     = {2015},
  Annote                   = {Uses word emebeddings as a feature for WSD},
  Pages                    = {314--323},

  Keywords                 = {WSD, word embeddings},
  Owner                    = {20361362},
  Timestamp                = {2016.04.19},
  Url                      = {http://www.aclweb.org/anthology/N15-1035}
}

@Article{CulturomicOpenProblems,
  Title                    = {Visions and open challenges for a knowledge-based culturomics},
  Author                   = {Tahmasebi, Nina and Borin, Lars and Capannini, Gabriele and Dubhashi, Devdatt and Exner, Peter and Forsberg, Markus and Gossen, Gerhard and Johansson, Fredrik D and Johansson, Richard and K{\aa}geb{\"a}ck, Mikael and others},
  Journal                  = {International Journal on Digital Libraries},
  Year                     = {2015},

  Annote                   = {A collection of opentasks and motivations for the field of culturomics,
which involves doign large text processing, taking into acount development of culture over time.},
  Number                   = {2-4},
  Pages                    = {169--187},
  Volume                   = {15},

  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2015.04.17},
  Url                      = {http://link.springer.com/article/10.1007/s00799-015-0139-1/fulltext.html}
}

@Article{TarasovGenerateAbstractSummary,
  Title                    = {Natural Language Generation Paraphrasing and Summarization
 of User Reviews with Recurrent
 Neural Networks},
  Author                   = {D. S. Tarasov},
  Year                     = {n.d.},

  Annote                   = {Makes a lot more senese if read inconcert with Mao et al's paper.

I don't know that this is it actually published anywhere with peer review.

Combining language modeling with the Summarisation learning task.
It doesn't use sentence vectors, though the authors indicate how it could.},

  Keywords                 = {summarization, vector-based_summarization},
  Owner                    = {20361362},
  Timestamp                = {2015.09.13},
  Url                      = {http://www.dialog-21.ru/digests/dialog2015/materials/pdf/TarasovDS2.pdf}
}

@Inproceedings{Taskar2004,
  Title                    = {Max-Margin Parsing.},
  Author                   = {Taskar, Ben and Klein, Dan and Collins, Michael and Koller, Daphne and Manning, Christopher D},
  Booktitle                = {EMNLP},
  Year                     = {2004},
  Annote                   = {This is NOT a good paper to read to help undersstand Socher2011.
It is a predecessor to it, but not direct enough.
It does not example the max-margin method well (but reference a baper that does)},
  Number                   = {1.1},
  Organization             = {Citeseer},
  Pages                    = {3},
  Volume                   = {1},

  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://ai.stanford.edu/~koller/Papers/Taskar+al:EMNLP04.pdf}
}

@Inbook{tengi1998design,
  Title                    = {WordNet: an electronic lexical database, The MIT Press, Cambridge, Massachusetts},
  Author                   = {Tengi, Randee I},
  Editor                   = {Fellbaum, Christiane (r{\'e}d.)},
  Pages                    = {105},
  Year                     = {1998},
  Annote                   = {Book chapter about how wordnet works.
This is the source for things like wordnet only contains lemmas.
Kind of. Uses the phrase "base word form"},

  Chapter                  = {Design and implementation of the WordNet lexical database and searching software},
  Journal                  = {Fellbaum, Christiane (r{\'e}d.), WordNet: an electronic lexical database, The MIT Press, Cambridge, Massachusetts},
  Owner                    = {20361362},
  Timestamp                = {2015.07.27}
}

@Inproceedings{tian2014probabilistic,
  Title                    = {A Probabilistic Model for Learning Multi-Prototype Word Embeddings.},
  Author                   = {Tian, Fei and Dai, Hanjun and Bian, Jiang and Gao, Bin and Zhang, Rui and Chen, Enhong and Liu, Tie-Yan},
  Booktitle                = {COLING},
  Year                     = {2014},
  Pages                    = {151--160},

  Owner                    = {20361362},
  Timestamp                = {2016.10.17},
  Url                      = {http://www.aclweb.org/anthology/C14-1016}
}

@Article{tibshirani2005cluster,
  Title                    = {Cluster validation by prediction strength},
  Author                   = {Tibshirani, Robert and Walther, Guenther},
  Journal                  = {Journal of Computational and Graphical Statistics},
  Year                     = {2005},
  Number                   = {3},
  Pages                    = {511--528},
  Volume                   = {14},

  Keywords                 = {clustering},
  Owner                    = {20361362},
  Publisher                = {Taylor \& Francis},
  Timestamp                = {2016.06.28},
  Url                      = {http://www.tandfonline.com/doi/pdf/10.1198/106186005X59243}
}

@Inproceedings{Tillmann2000,
  Title                    = {Word re-ordering and DP-based search in statistical machine translation},
  Author                   = {Tillmann, Christoph and Ney, Hermann},
  Booktitle                = {Proceedings of the 18th conference on Computational linguistics-Volume 2},
  Year                     = {2000},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {850--856},

  Keywords                 = {word-ordering, machine-translation},
  Owner                    = {20361362},
  Timestamp                = {2016.02.04},
  Url                      = {http://www.aclweb.org/anthology/C00-2123}
}

@Article{Tomalin,
  Title                    = {Word Ordering with Phrase-Based Grammars},
  Author                   = {Tomalin, Marcus and Byrne, William},

  Owner                    = {20361362},
  Timestamp                = {2016.02.16},
  Url                      = {http://mi.eng.cam.ac.uk/~wjb31/ppubs/EACL2014.pdf}
}

@Phdthesis{mikolov2012thesis,
  Title                    = {Statistical language models based on neural networks},
  Author                   = {Tomas, Mikolov},
  School                   = {PhD thesis, Brno University of Technology},
  Year                     = {2012},

  Owner                    = {20361362},
  Timestamp                = {2017.12.22},
  Url                      = {http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf}
}

@Inproceedings{StandfordPOSTagger,
  Title                    = {Feature-rich part-of-speech tagging with a cyclic dependency network},
  Author                   = {Toutanova, Kristina and Klein, Dan and Manning, Christopher D and Singer, Yoram},
  Booktitle                = {Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1},
  Year                     = {2003},
  Annote                   = {This is the preferred citation for the Standford POS tagger},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {173--180},

  Owner                    = {20361362},
  Timestamp                = {2015.07.14},
  Url                      = {ilpubs.stanford.edu:8090/603/1/2003-43.pdf}
}

@Inproceedings{turaga2008statistical,
  Title                    = {Statistical analysis on Stiefel and Grassmann manifolds with applications in computer vision},
  Author                   = {Turaga, Pavan and Veeraraghavan, Ashok and Chellappa, Rama},
  Booktitle                = {Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on},
  Year                     = {2008},
  Organization             = {IEEE},
  Pages                    = {1--8},

  Abstract                 = {Many applications in computer vision and pattern recog-
nition involve drawing inferences on certain manifold-
valued parameters. In order to develop accurate inference
algorithms on these manifolds we need to a) understand the
geometric structure of these manifolds b) derive appropri-
ate distance measures and c) develop probability distribu-
tion functions (pdf) and estimation techniques that are con-
sistent with the geometric structure of these manifolds. In
this paper, we consider two related manifolds - the Stiefel
manifold and the Grassmann manifold, which arise natu-
rally in several vision applications such as spatio-temporal
modeling, affine invariant shape analysis, image matching
and learning theory. We show how accurate statistical char-
acterization that reflects the geometry of these manifolds
allows us to design efficient algorithms that compare fa-
vorably to the state of the art in these very different ap-
plications. In particular, we describe appropriate distance
measures and parametric and non-parametric density esti-
mators on these manifolds. These methods are then used
to learn class conditional densities for applications such as
activity recognition, video based face recognition and shape
classification},
  Keywords                 = {computer vision},
  Owner                    = {20361362},
  Timestamp                = {2016.08.18},
  Url                      = {http://www.public.asu.edu/~pturaga/papers/Manifolds.pdf}
}

@Inproceedings{turian2010word,
  Title                    = {Word representations: a simple and general method for semi-supervised learning},
  Author                   = {Turian, Joseph and Ratinov, Lev and Bengio, Yoshua},
  Booktitle                = {Proceedings of the 48th annual meeting of the association for computational linguistics},
  Year                     = {2010},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {384--394},

  Owner                    = {20361362},
  Timestamp                = {2017.07.11},
  Url                      = {http://www.anthology.aclweb.org/P/P10/P10-1040.pdf}
}

@Inproceedings{turpin2012attempt,
  Title                    = {An attempt to measure the quality of questions in question time of the Australian Federal Parliament},
  Author                   = {Turpin, Andrew},
  Booktitle                = {Proceedings of the Seventeenth Australasian Document Computing Symposium},
  Year                     = {2012},
  Annote                   = {This is mostly interesting because it is associated with a Hansard subset.
The Hansard can be downloaded yourself -- it is a very nicely formatted xml dataset labeled with all the desirable meta data like speaker name and party.
Apprently though the Hansard changed format in 2011.

this also features a question time subcorpus.},
  Organization             = {ACM},
  Pages                    = {96--103},

  Keywords                 = {corpora},
  Owner                    = {20361362},
  Timestamp                = {2015.06.15},
  Url                      = {http://people.eng.unimelb.edu.au/aturpin/qtCorpus/adcs2012.pdf}
}

@Book{tymoczko1995sweet,
  Title                    = {Sweet Reason: A Field Guide to Modern Logic},
  Author                   = {Tymoczko, T. and Henle, J. and Henle, J.M.},
  Publisher                = {Key College},
  Year                     = {1995},
  Annote                   = {Includes the discussion of Buffello buffello},
  Series                   = {Textbooks in Mathematical Sciences},

  ISBN                     = {9780387989303},
  Lccn                     = {99043387},
  Owner                    = {20361362},
  Timestamp                = {2016.02.11},
  Url                      = {https://books.google.com.au/books?id=LQnsSuvP9dAC}
}

@Article{Usui:92,
  Title                    = {Reconstruction of Munsell color space by a five-layer neural network},
  Author                   = {Shiro Usui and Shigeki Nakauchi and Masae Nakano},
  Journal                  = {J. Opt. Soc. Am. A},
  Year                     = {1992},

  Annote                   = {Bottle-necking autoencoder. over the Spectral repressentation of a color (using 81, wavelength bins) Does minibatch learning in a rather non-traditional way. First train on 10, until good results, than add another 10, and train on the full 20, etc until whole training set used.},
  Month                    = {Apr},
  Number                   = {4},
  Pages                    = {516--520},
  Volume                   = {9},

  Abstract                 = {We have constructed a wine-glass-type five-layer neural network and generated an identity mapping of the surface spectral-reflectance data of 1280 Munsell color chips, using a backpropagation learning algorithm. To achieve an identity mapping, the same data set is used for the input and for the teacher. After the learning was completed, we analyzed the responses to individual chips of the three hidden units in the middle layer in order to obtain the internal representation of the color information. We found that each of the three hidden units corresponds to a psychological color attribute, that is, the Munsell value (luminance), red--green, and yellow--blue. We also examined the relationship between the internal representation and the number of hidden units and found that the network with three hidden units acquires optimum color representation. The five-layer neural network is shown to be an efficient method for reproducing the transformation of color information (or color coding) in the visual system.},
  Doi                      = {10.1364/JOSAA.9.000516},
  Owner                    = {20361362},
  Publisher                = {OSA},
  Timestamp                = {2017.04.10},
  Url                      = {http://josaa.osa.org/abstract.cfm?URI=josaa-9-4-516}
}

@Article{veronis2004hyperlex,
  Title                    = {Hyperlex: lexical cartography for information retrieval},
  Author                   = {V{\'e}ronis, Jean},
  Journal                  = {Computer Speech \& Language},
  Year                     = {2004},
  Number                   = {3},
  Pages                    = {223--252},
  Volume                   = {18},

  Keywords                 = {WSD, graph},
  Owner                    = {20361362},
  Publisher                = {Elsevier},
  Timestamp                = {2016.08.11},
  Url                      = {http://sites.univ-provence.fr/~veronis/pdf/2004-hyperlex-CSL.pdf}
}

@Inproceedings{veronis1998study,
  Title                    = {A study of polysemy judgements and inter-annotator agreement},
  Author                   = {V{\'e}ronis, Jean},
  Booktitle                = {Programme and advanced papers of the Senseval workshop},
  Year                     = {1998},
  Pages                    = {2--4},

  Owner                    = {20361362},
  Timestamp                = {2016.10.19},
  Url                      = {http://sites.univ-provence.fr/veronis/pdf/1998senseval.pdf}
}

@Article{van2009learning,
  Title                    = {Learning color names for real-world applications},
  Author                   = {Van De Weijer, Joost and Schmid, Cordelia and Verbeek, Jakob and Larlus, Diane},
  Journal                  = {IEEE Transactions on Image Processing},
  Year                     = {2009},

  Annote                   = {Topic based model, where the color},
  Number                   = {7},
  Pages                    = {1512--1523},
  Volume                   = {18},

  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2017.07.03},
  Url                      = {https://hal.inria.fr/inria-00439284/document}
}

@Article{VabdewakkeReproduceableResearch,
  Title                    = {Reproducible research in signal processing},
  Author                   = {P. Vandewalle and J. Kovacevic and M. Vetterli},
  Journal                  = {IEEE Signal Processing Magazine},
  Year                     = {2009},
  Month                    = {May},
  Number                   = {3},
  Pages                    = {37-47},
  Volume                   = {26},

  Abstract                 = {What should we do to raise the quality of signal processing publications to an even higher level? We believe it to be crucial to maintain the precision in describing our work in publications, ensured through a high-quality reviewing process. We also believe that if the experiments are performed on a large data set, the algorithm is compared to the state-of-the-art methods, the code and/or data are well documented and available online, we will all benefit and make it easier to build upon each other's work. It is a clear win-win situation for our community: we will have access to more and more algorithms and can spend time inventing new things rather than recreating existing ones.},
  Doi                      = {10.1109/MSP.2009.932122},
  ISSN                     = {1053-5888},
  Keywords                 = {research and development;signal processing;high-quality reviewing process;large data set;reproducible research;signal processing;win-win situation;Advertising;Digital signal processing;Education;Programming;Reproducibility of results;Scholarships;Signal processing;Signal processing algorithms;Testing;Wikipedia},
  Owner                    = {20361362},
  Timestamp                = {2018.01.15}
}

@Article{vincent2010stacked,
  Title                    = {Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion},
  Author                   = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2010},
  Number                   = {Dec},
  Pages                    = {3371--3408},
  Volume                   = {11},

  Owner                    = {20361362},
  Timestamp                = {2017.09.01},
  Url                      = {http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf}
}

@Article{Waibel1989,
  Title                    = {Phoneme recognition using time-delay neural networks},
  Author                   = {Waibel, Alexander and Hanazawa, Toshiyuki and Hinton, Geoffrey and Shikano, Kiyohiro and Lang, Kevin J},
  Journal                  = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
  Year                     = {1989},
  Number                   = {3},
  Pages                    = {328--339},
  Volume                   = {37},

  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2015.09.11},
  Url                      = {http://www.cs.toronto.edu/~fritz/absps/waibelTDNN.pdf}
}

@Article{Wang2015,
  Title                    = {Semantic Expansion using Word Embedding Clustering and Convolutional Neural Network for Improving Short Text Classification },
  Author                   = {Peng Wang and Bo Xu and Jiaming Xu and Guanhua Tian and Cheng-Lin Liu and Hongwei Hao},
  Journal                  = {Neurocomputing },
  Year                     = {2015},
  Pages                    = { - },

  Abstract                 = {Abstract Text classification can help users to effectively handle and exploit useful information hidden in large-scale documents. However, the sparsity of data and the semantic sensitivity to context often hinder the classification performance of short texts. In order to overcome the weakness, we propose a unified framework to expand short texts based on word embedding clustering and convolutional neural network (CNN). Empirically, the semantically related words are usually close to each other in embedding spaces. Thus, we first discover semantic cliques via fast clustering. Then, by using additive composition over word embeddings from context with variable window width, the representations of multi-scale semantic units11 Semantic units are defined as n-grams which have dominant meaning of text. With n varying, multi-scale contextual information can be exploited. in short texts are computed. In embedding spaces, the restricted nearest word embeddings (NWEs)22 In order to prevent outliers, a Euclidean distance threshold is preset between semantic cliques and semantic units, which is used as restricted condition. of the semantic units are chosen to constitute expanded matrices, where the semantic cliques are used as supervision information. Finally, for a short text, the projected matrix33 The projected matrix is obtained by table looking up, which encodes Unigram level features. and expanded matrices are combined and fed into \{CNN\} in parallel. Experimental results on two open benchmarks validate the effectiveness of the proposed method.},
  Doi                      = {http://dx.doi.org/10.1016/j.neucom.2015.09.096},
  ISSN                     = {0925-2312},
  Keywords                 = {Short Text},
  Owner                    = {20361362},
  Timestamp                = {2015.10.21},
  Url                      = {http://ac.els-cdn.com/S0925231215014502/1-s2.0-S0925231215014502-main.pdf?_tid=30e8e386-77be-11e5-bafa-00000aacb35f&acdnat=1445409625_60fc1cf534425eb6d5d0282d1e94a6f7}
}

@Inproceedings{rui2017mvrusemantic,
  Title                    = {A Matrix-Vector Recurrent Unit Model for Capturing Compositional Semantics in Phrase Embeddings},
  Author                   = {Wang, Rui and Liu, Wei and McDonald, Chris},
  Booktitle                = {International Conference on Information and Knowledge Management},
  Year                     = {2017},

  Owner                    = {20361362},
  Timestamp                = {2017.11.29}
}

@Inproceedings{wang2012baselines,
  Title                    = {Baselines and bigrams: Simple, good sentiment and topic classification},
  Author                   = {Wang, Sida and Manning, Christopher D},
  Booktitle                = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2},
  Year                     = {2012},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {90--94},

  Owner                    = {20361362},
  Timestamp                = {2017.12.01},
  Url                      = {http://wing.comp.nus.edu.sg/~antho/P/P12/P12-2018.pdf}
}

@Incollection{Wang2014,
  Title                    = {Word Vector Modeling for Sentiment Analysis of Product Reviews},
  Author                   = {Wang, Yuan and Li, Zhaohui and Liu, Jie and He, Zhicheng and Huang, Yalou and Li, Dong},
  Booktitle                = {Natural Language Processing and Chinese Computing},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2014},
  Editor                   = {Zong, Chengqing and Nie, Jian-Yun and Zhao, Dongyan and Feng, Yansong},
  Pages                    = {168-180},
  Series                   = {Communications in Computer and Information Science},
  Volume                   = {496},

  Doi                      = {10.1007/978-3-662-45924-9_16},
  ISBN                     = {978-3-662-45923-2},
  Keywords                 = {Sentiment Analysis; Product Review; Neural Network Language Model; Semi-supervised Learning},
  Language                 = {English},
  Owner                    = {20361362},
  Timestamp                = {2015.06.19},
  Url                      = {http://dx.doi.org/10.1007/978-3-662-45924-9_16}
}

@Article{webber2006short,
  Title                    = {A short introduction to the Penn Discourse Tree Bank},
  Author                   = {Webber, Bonnie and Joshi, Aravind and Miltsakaki, Eleni and Prasad, Rashmi and Dinesh, Nikhil and Lee, Alan and Forbes, Katherine},
  Journal                  = {COPENHAGEN STUDIES IN LANGUAGE},
  Year                     = {2006},

  Annote                   = {Explains why we have binary trees.


base-NPs 
CCG combinational Categorical Grammar},
  Pages                    = {9},
  Volume                   = {32},

  Keywords                 = {corpora},
  Owner                    = {20361362},
  Publisher                = {Samfundslitteratur},
  Timestamp                = {2015.05.08},
  Url                      = {http://sydney.edu.au/engineering/it/~dvadas1/papers/vadas07_np_data_penn_treebank.pdf}
}

@Article{MemoryNN,
  Title                    = {Memory Networks},
  Author                   = {Jason Weston and
 Sumit Chopra and
 Antoine Bordes},
  Journal                  = {CoRR},
  Year                     = {2014},

  Annote                   = {A generalised model for a memory network.
Application in question answering.
Controls its own knowlege base with neurons.

Interestingly has a leaned tokeniser (segementor as it is called in text) as part of its write operation.

Example of expiment:
"Joe went to the kitchen. Fred went to the kitchen. Joe picked u
p the milk.
Joe travelled to the office. Joe left the milk. Joe went to the b
athroom.
Where is the milk now?
A: office
Where is Joe?
A: bathroom
Where was Joe before the office?
A: kitchen"},
  Volume                   = {abs/1410.3916},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/WestonCB14},
  Owner                    = {20361362},
  Timestamp                = {2015.04.12},
  Url                      = {http://arxiv.org/abs/1410.3916}
}

@Misc{WhiteRepresentingAnglesSE,
  Title                    = {Encoding Angle Data for Neural Networks},
  Author                   = {Lyndon White},
  HowPublished             = {Cross Validated Stack Exchange},
  Year                     = {2016},

  Eprint                   = {https://math.stackexchange.com/q/2369786},
  Owner                    = {20361362},
  Timestamp                = {2017.07.24},
  Url                      = {https://stats.stackexchange.com/a/218547/36769}
}

@Article{DataDeps,
  Title                    = {DataDeps.jl: Repeatable Data Setup for Reproducible Data Science},
  Author                   = {White, Lyndon and Togneri,Roberto and Liu,Wei and Bennamoun, Mohammed},
  Journal                  = {Pending submission},
  Year                     = {2018},

  Abstract                 = {We present a framework DataDeps.jl for the reproducible handling of static datasets to enhance the repeatability of software scripts used in the data and computational sciences. DataDeps.jl is a library for the Julia programming language. It is used to automate the data setup part of running software accompanies a paper to replicate a result. This step is commonly done manually, which expends time and allows for confusion. This functionality is also useful for other packages which require data to function. It simplifies extending research software via traditional means of a software dependency, as the extension does not have to worry about ensuring the data is setup for its dependency. DataDeps.jl makes it easier to rerun another authors code, thus enhancing the reproducibility of data science research.},
  Owner                    = {20361362},
  Timestamp                = {2018.03.02}
}

@Article{novelperspective,
  Title                    = {NovelPerspective},
  Author                   = {White, Lyndon and Togneri,Roberto and Liu,Wei and Bennamoun, Mohammed},
  Journal                  = {Pending submission},
  Year                     = {2018},

  Abstract                 = {We present a proof of concept tool to allow consumers to subset ebooks, based on the main character of the section.
Many novels have multiple main characters each with their own storyline running in parallel.
A well known example is George R. R. Martin's ``Game of Thrones'' novel, and others from that series.
The NovelPerspective tool detects which character the section is about,
and allows the user to generate a new ebook with only those sections.
This gives consumers new options in how they consume their media, allowing them to pursue the storylines sequentially, or skip chapters about characters they find boring.
We present two simple baselines, and several machine learning based methods for the detection of the main character.},
  Owner                    = {20361362},
  Timestamp                = {2018.03.02}
}

@Book{NRoNL,
  Title                    = {Neural Representations of Natural Language},
  Author                   = {White, Lyndon and Togneri,Roberto and Liu,Wei and Bennamoun, Mohammed},
  Publisher                = {SpringerBriefs},
  Year                     = {2018},

  Abstract                 = {Popular deep neural networks for languages processing comprehensively reviewed and explained from a practical perspective.
Language modelling, vector representations, and challenging tasks such as WSD, and sentence embeddings investigated to illustrate the used of these networks.
This book is packed with valuable advice and experiences
obtained from practical development and implementations 
in the up-and-coming Julia programming language for technical computing.
It is a solid introduction to one of the most exciting new areas of natural language processing and computational linguistics.},
  Owner                    = {20361362},
  Timestamp                = {2018.03.02}
}

@Article{WhiteRefittingSenses,
  Title                    = {Finding Word Sense Embeddings Of Known Meaning},
  Author                   = {White, Lyndon and Togneri,Roberto and Liu,Wei and Bennamoun, Mohammed},
  Journal                  = {19th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing)},
  Year                     = {2018},

  Abstract                 = {Word sense embeddings are vector representations of polysemous words -- words with multiple meanings.
These induced sense embeddings, however, do not necessarily correspond to any dictionary senses of the word.
This limits their applicability in traditional semantic-orientated tasks such as lexical word sense disambiguation.
To overcome this, we propose a method to find new sense embeddings of known meaning.
We term this method refitting, as the new embedding is fitted to model the meaning of a target word in the example sentence.
This is accomplished using the probabilities of the existing induced sense embeddings, as well as their vector values.
Our contributions are threefold:
(1) The refitting method to find the new sense embeddings;
 (2) a novel smoothing technique, for use with the refitting method;
and (3) a new similarity measure for words in context, defined by using the refitted sense embeddings.
We show how our techniques improve the performance of the Adaptive Skip-Gram sense embeddings for word similarly evaluation; and how they allow the embeddings to be used for lexical word sense disambiguation -- which was not possible using the induced sense embeddings.},
  Owner                    = {20361362},
  Timestamp                = {2014.03.04}
}

@Article{2017arXiv170909360W,
  Title                    = {{Learning Distributions of Meant Color}},
  Author                   = {{White}, L. and {Togneri}, R. and {Liu}, W. and {Bennamoun}, M.},
  Journal                  = {ArXiv e-prints},
  Year                     = {2017},
  Month                    = sep,

  Abstract                 = {When a speaker says the name of a color, the color that they picture is not necessarily the same as the listener imagines.
Color is a grounded semantic task, but that grounding is not a mapping of a single word (or phrase) to a single point in color-space.
Proper understanding of color language requires the capacity to map a sequence of words to a probability distribution in color-space.
A distribution is required as there is no clear agreement between people as to what a particular color describes -- different people have a different idea of what it means to be ``very dark orange''.
We propose a novel GRU-based model to handle this case.
Learning how each word in a color name contributes to the color described,
allows for knowledge sharing between uses of the words in different color names.
This knowledge sharing significantly improves predicative capacity for color names with sparse training data.
The extreme case of this challenge in data sparsity is for color names without any direct training data.
Our model is able to predict reasonable distributions for these cases, as evaluated on a held-out dataset consisting only of such terms.},
  Adsnote                  = {Provided by the SAO/NASA Astrophysics Data System},
  Adsurl                   = {http://adsabs.harvard.edu/abs/2017arXiv170909360W},
  Archiveprefix            = {arXiv},
  Eprint                   = {1709.09360},
  Keywords                 = {Computer Science - Computation and Language},
  Owner                    = {20361362},
  Primaryclass             = {cs.CL},
  Timestamp                = {2017.10.02},
  Url                      = {https://arxiv.org/abs/1709.09360}
}

@Article{SOWEgen,
  Title                    = {A Two Step Process for Generating Sentences from the Sums of their Embeddings},
  Author                   = {White, Lyndon and Togneri,Roberto and Liu,Wei and Bennamoun, Mohammed},
  Year                     = {2016},
  Volume                   = {3 (March)},

  Abstract                 = {Converting a sentence to a meaningful vector representation has uses in many NLP tasks, however very few methods allow that representation to be restored to a human readable sentence. Being able to generate sentences from the vector representations is expected to open up many new applications. We introduce such a method for moving from sum of word embedding representations back to the original sentences. This is done using a greedy algorithm to convert the vector to a bag of words. We then show how the bag of words can be ordered using simple probabilistic language models to get back the sentence. To our knowledge this is the first work to demonstrate qualitatively the ability to reproduce text from a large corpus based on its sentence embeddings. 
As well as practical applications for sentence generation, the success of this method has theoretical implications on the degree of information maintained by the sum of embeddings representation. },
  Timestamp                = {2014.03.04}
}

@Inproceedings{White2015BOWgen,
  Title                    = {Generating Bags of Words from the Sums of their Word Embeddings},
  Author                   = {White, Lyndon and Togneri,Roberto and Liu,Wei and Bennamoun, Mohammed},
  Booktitle                = {17th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing)},
  Year                     = {2016},

  Abstract                 = {Many methods have been proposed to generate sentence vector representations, such as recursive neural networks, latent distributed memory models, and the simple sum of word embeddings (SOWE). However, very few methods demonstrate the ability to reverse the process -- recovering sentences from sentence embeddings. Amongst the many sentence embeddings, SOWE has been shown to maintain semantic meaning, so in this paper we introduce a method for moving from the SOWE representations back to the bag of words (BOW) for the original sentences. This is a part way step towards recovering the whole sentence and has useful theoretical and practical applications of its own. This is done using a greedy algorithm to convert the vector to a bag of words. To our knowledge this is the first such work. It demonstrates qualitatively the ability to recreate the words from a large corpus based on its sentence embeddings.

As well as practical applications for allowing classical information retrieval methods to be combined with more recent methods using the sums of word embeddings, the success of this method has theoretical implications on the degree of information maintained by the sum of embeddings representation. This lends some credence to the consideration of the SOWE as a dimensionality reduced, and meaning enhanced, data manifold for the bag of words. },
  Timestamp                = {2013.08.15}
}

@Inproceedings{White2016a,
  Title                    = {Modelling Sentence Generation from Sum of Word Embedding Vectors as a Mixed Integer Programming Problem},
  Author                   = {White, Lyndon and Togneri,Roberto and Liu,Wei and Bennamoun, Mohammed},
  Booktitle                = {IEEE International Conference on Data Mining: High Dimensional Data Mining Workshop (ICDM: HDM)},
  Year                     = {2016},

  Abstract                 = {Converting a sentence to a meaningful vector representation
has uses in many NLP tasks, however very few methods allow that
representation to be restored to a human readable sentence. Being able
to generate sentences from the vector representations demonstrates the
level of information maintained by the embedding representation ÃƒÂ¢Ã¢â€šÂ¬Ã¢â‚¬Å“ in this
case a simple sum of word embeddings. We introduce such a method
for moving from this vector representation back to the original sentences.
This is done using a two stage process; first a greedy algorithm is utilised
to convert the vector to a bag of words, and second a simple probabilistic
language model is used to order the words to get back the sentence.
To the best of our knowledge this is the first work to demonstrate
quantitatively the ability to reproduce text from a large corpus based
directly on its sentence embeddings},
  Doi                      = {10.1109/ICDMW.2016.0113},
  Owner                    = {20361362},
  Timestamp                = {2017.02.28},
  Url                      = {http://white.ucc.asn.au/publications/White2016SOWE2Sent.pdf}
}

@Inproceedings{White2015SentVecMeaning,
  Title                    = {How Well Sentence Embeddings Capture Meaning},
  Author                   = {White, Lyndon and Togneri, Roberto and Liu, Wei and Bennamoun, Mohammed},
  Booktitle                = {Proceedings of the 20th Australasian Document Computing Symposium},
  Year                     = {2015},
  Pages                    = {9:1--9:8},
  Publisher                = {ACM},
  Series                   = {ADCS '15},

  Abstract                 = {Several approaches for embedding a sentence into a vector space have been developed. However, it is unclear to what extent the sentence's position in the vector space reflects its semantic meaning, rather than other factors such as syntactic structure. Depending on the model used for the embeddings this will vary -- different models are suited for different down-stream applications. For applications such as machine translation and automated summarization, it is highly desirable to have semantic meaning encoded in the embedding. We consider this to be the quality of semantic localization for the model -- how well the sentences' meanings coincides with their embedding's position in vector space. Currently the semantic localization is assessed indirectly through practical benchmarks for specific applications. 

In this paper, we ground the semantic localization problem through a semantic classification task. The task is to classify sentences according to their meaning. A SVM with a linear kernel is used to perform the classification using the sentence vectors as its input. The sentences from subsets of two corpora, the Microsoft Research Paraphrase corpus and the Opinosis corpus, were partitioned according to their semantic equivalence. These partitions give the target classes for the classification task. Several existing models, including URAE, PV--DM and PV--DBOW, were assessed against a bag of words benchmark},
  Acmid                    = {2838932},
  Articleno                = {9},
  Doi                      = {10.1145/2838931.2838932},
  ISBN                     = {978-1-4503-4040-3},
  Keywords                 = {Semantic vector space representations, semantic consistency evaluation, sentence embeddings, word embeddings},
  Location                 = {Parramatta, NSW, Australia},
  Numpages                 = {8},
  Owner                    = {20361362},
  Timestamp                = {2015.11.19},
  Url                      = {http://doi.acm.org/10.1145/2838931.2838932}
}

@Inproceedings{7376619,
  Title                    = {Transistor Sizing Using Particle Swarm Optimisation},
  Author                   = {L. White and L. While and B. Deeks and F. Boussaid},
  Booktitle                = {2015 IEEE Symposium Series on Computational Intelligence},
  Year                     = {2015},
  Month                    = {Dec},
  Pages                    = {259-266},

  Abstract                 = {We describe an application of particle swarm optimisation to the problem of determining the optimal sizing of transistors in an integrated circuit. The algorithm minimises the total area of silicon utilised by a given circuit, whilst maintaining the propagation delay of the circuit within a hard limit. It assesses designs using the well-known circuit simulation engine SPICE, making allowance for the inability of SPICE to assess poorly-designed circuits within a reasonable timeframe. Experiments on three different types of circuits demonstrate that the algorithm is able to derive excellent designs for a range of problem instances, including several problems where the Monte Carlo method is unable to find any feasible solutions at all.},
  Doi                      = {10.1109/SSCI.2015.46},
  Keywords                 = {MOSFET;Monte Carlo methods;elemental semiconductors;particle swarm optimisation;silicon;Monte Carlo method;Si;circuit simulation engine SPICE;integrated circuit;optimal transistor sizing;particle swarm optimisation;propagation delay;silicon;Algorithm design and analysis;Birds;Integrated circuit modeling;Optimization;Propagation delay;SPICE;Transistors},
  Owner                    = {20361362},
  Timestamp                = {2017.05.03}
}

@Article{raey,
  Title                    = {Annotating Expressions of Opinions and Emotions in Language},
  Author                   = {Wiebe, Janyce and Wilson, Theresa and Cardie, Claire},
  Journal                  = {Language Resources and Evaluation},
  Year                     = {2005},

  Annote                   = {Preferred citation for use of MPQA},
  Number                   = {2-3},
  Pages                    = {165-210},
  Volume                   = {39},

  Doi                      = {10.1007/s10579-005-7880-9},
  ISSN                     = {1574-020X},
  Keywords                 = {corpora},
  Language                 = {English},
  Owner                    = {20361362},
  Publisher                = {Kluwer Academic Publishers},
  Timestamp                = {2015.09.15},
  Url                      = {http://dx.doi.org/10.1007/s10579-005-7880-9}
}

@Article{wieting2015towards,
  Title                    = {Towards Universal Paraphrastic Sentence Embeddings},
  Author                   = {Wieting, John and Bansal, Mohit and Gimpel, Kevin and Livescu, Karen},
  Journal                  = {International Conference on Learning Representations (ICLR)},
  Year                     = {2016},

  Keywords                 = {sentence embeddings},
  Owner                    = {20361362},
  Timestamp                = {2016.06.09},
  Url                      = {http://ttic.uchicago.edu/~wieting/wieting2016ICLR.pdf}
}

@Article{10.1371/journal.pbio.1001745,
  Title                    = {Best Practices for Scientific Computing},
  Author                   = {Wilson, Greg AND Aruliah, D. A. AND Brown, C. Titus AND Chue Hong, Neil P. AND Davis, Matt AND Guy, Richard T. AND Haddock, Steven H. D. AND Huff, Kathryn D. AND Mitchell, Ian M. AND Plumbley, Mark D. AND Waugh, Ben AND White, Ethan P. AND Wilson, Paul},
  Journal                  = {PLOS Biology},
  Year                     = {2014},
  Month                    = {01},
  Number                   = {1},
  Pages                    = {1-7},
  Volume                   = {12},

  Abstract                 = {We describe a set of best practices for scientific software development, based on research and experience, that will improve scientists' productivity and the reliability of their software.},
  Doi                      = {10.1371/journal.pbio.1001745},
  Owner                    = {20361362},
  Publisher                = {Public Library of Science},
  Timestamp                = {2018.01.16},
  Url                      = {https://doi.org/10.1371/journal.pbio.1001745}
}

@Inproceedings{wohlgenannt2016extracting,
  Title                    = {Extracting social networks from literary text with word embedding tools},
  Author                   = {Wohlgenannt, Gerhard and Chernyak, Ekaterina and Ilvovsky, Dmitry},
  Booktitle                = {Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities (LT4DH)},
  Year                     = {2016},
  Pages                    = {18--25},

  Owner                    = {20361362},
  Timestamp                = {2018.03.19},
  Url                      = {https://www.clarin-d.net/images/lt4dh/pdf/LT4DH04.pdf}
}

@Article{wren2008url,
  Title                    = {URL decay in MEDLINE: a 4-year follow-up study},
  Author                   = {Wren, Jonathan D},
  Journal                  = {Bioinformatics},
  Year                     = {2008},
  Number                   = {11},
  Pages                    = {1381--1385},
  Volume                   = {24},

  Owner                    = {20361362},
  Publisher                = {Oxford University Press},
  Timestamp                = {2018.01.30},
  Url                      = {https://academic.oup.com/bioinformatics/article/24/11/1381/191025}
}

@Other{xiao2017/online,
  Title                    = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  Author                   = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  Date                     = {2017-08-28},
  Eprint                   = {cs.LG/1708.07747},
  Eprintclass              = {cs.LG},
  Eprinttype               = {arXiv},
  Owner                    = {20361362},
  Timestamp                = {2017.12.04},
  Url                      = {https://github.com/zalandoresearch/fashion-mnist},
  Year                     = {2017}
}

@Article{RNC:RNC234,
  Title                    = {Robust filtering for a class of discrete-time uncertain nonlinear systems: An H8 approach},
  Author                   = {Xie, Lihua and De Souza, Carlos E. and Wang, Youyi},
  Journal                  = {International Journal of Robust and Nonlinear Control},
  Year                     = {1996},
  Number                   = {4},
  Pages                    = {297--312},
  Volume                   = {6},

  Abstract                 = {This paper deals with the H8 filtering problem for a class of discrete-time nonlinear systems with or without real time-varying parameter uncertainty and unknown initial state. For the case when there is no parametric uncertainty in the system, we are concerned with designing a nonlinear H8 filter such that the induced l2 norm of the mapping from the noise signal to the estimation error is within a specified bound. It is shown that this problem can be solved via one Riccati equation. We also consider the design of nonlinear filters which guarantee a prescribed H8 performance in the presence of parametric uncertainties. In this situation, a solution is obtained in terms of two Riccati equations.},
  Doi                      = {10.1002/(SICI)1099-1239(199605)6:4<297::AID-RNC234>3.0.CO;2-V},
  ISSN                     = {1099-1239},
  Keywords                 = {robust filtering, H8 filtering, uncertain systems, discrete-time systems, nonlinear systems},
  Owner                    = {20361362},
  Publisher                = {Wiley Subscription Services, Inc., A Wiley Company},
  Timestamp                = {2015.05.07},
  Url                      = {http://dx.doi.org/10.1002/(SICI)1099-1239(199605)6:4<297::AID-RNC234>3.0.CO;2-V}
}

@Article{1159121,
  Title                    = {Robust H infin; filtering for a class of discrete-time uncertain nonlinear systems with state delay},
  Author                   = {Shengyuan Xu},
  Journal                  = {Circuits and Systems I: Fundamental Theory and Applications, IEEE Transactions on},
  Year                     = {2002},

  Annote                   = {Robust $H/infty$ filters are of interst to me.
This however covers the more general case of Robut H \inftyu filters with time lag in state.},
  Month                    = {Dec},
  Number                   = {12},
  Pages                    = {1853-1859},
  Volume                   = {49},

  Doi                      = {10.1109/TCSI.2002.805736},
  ISSN                     = {1057-7122},
  Keywords                 = {Delay effects;Delay systems;Filtering;Gaussian noise;Nonlinear equations;Nonlinear systems;Riccati equations;Robustness;Sufficient conditions;Uncertain systems},
  Owner                    = {20361362},
  Timestamp                = {2015.05.07}
}

@Book{yadav2015introduction,
  Title                    = {An introduction to neural network methods for differential equations},
  Author                   = {Yadav, Neha and Yadav, Anupam and Kumar, Manoj},
  Publisher                = {Springer},
  Year                     = {2015},

  Owner                    = {20361362},
  Timestamp                = {2017.03.20},
  Url                      = {https://onesearch.library.uwa.edu.au/discovery/fulldisplay?docid=alma99548634602101&context=L&vid=61UWA_INST:UWA&search_scope=MyInst_and_CI&tab=Everything&lang=en}
}

@Inproceedings{SemHuff,
  Title                    = {Optimize Hierarchical Softmax with Word Similarity Knowledge},
  Author                   = {Yang, Zhixuan and Ruan, Chong and Li, Caihua and Hu, Junfeng},
  Booktitle                = {17th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing)},
  Year                     = {2016},
  Annote                   = {username: cicling2016
password: hatem@konya

Reoroganise huffman tree, only within level.
So the unigram probability still determines what level a thing is on, but its semantic similarity determines where it is.
Putting similar things next to each other

This restructuring can bring heirachical softmax up to similar performance to negative sampling.

Possibly issues is that is seems like an NP hard problem to reorganise the tree.

Also only shown on chinese, which could mean the data was cherry picked.},

  Owner                    = {20361362},
  Timestamp                = {2016.07.08},
  Url                      = {http://www.cicling.org/2016/intranet/papers/paper_21.pdf}
}

@Article{yin2014exploration,
  Title                    = {An Exploration of Embeddings for Generalized Phrases},
  Author                   = {Yin, Wenpeng and Sch{\"u}tze, Hinrich},
  Journal                  = {ACL 2014},
  Year                     = {2014},

  Annote                   = {Conserides various kinsd sof short phrase embeddings. This is the paper I am going to Cite for saying that terminology is inconsistent.},
  Pages                    = {41},

  Groups                   = {KeyPapers},
  Keywords                 = {embeddings, terminology},
  Owner                    = {20361362},
  Timestamp                = {2015.06.24},
  Url                      = {https://www.aclweb.org/anthology/P/P14/P14-3006.pdf}
}

@Article{Yin2015,
  Title                    = {Learning Word Meta-Embeddings by Using Ensembles of Embedding Sets},
  Author                   = {Wenpeng Yin and Hinrich SchÃƒÆ’Ã†â€™Ãƒâ€šÃ‚Â¼tze},
  Year                     = {2015},

  Annote                   = {considers using multiple word embeddings, via ensembles.

They uses SVD (~= PCA) to cut down dimentions.

Includes Giant table of evaluation of various word embeddings. and of ensembles of them
Including on Milokovs A is to B as C is to _ test.
In both syntactic and semeantic quality,

Really though ensembles only yeild magiunal improvement over word2vec.

Apparently they have reased the embeddings publically.},
  Month                    = aug,

  Abstract                 = {Word embeddings -- distributed representations for words -- in deep learning are beneficial for many tasks in Natural Language Processing (NLP). However, different embedding sets vary greatly in quality and characteristics of the captured semantics. Instead of relying on a more advanced algorithm for embedding learning, this paper proposes an ensemble approach of combining different public embedding sets with the aim of learning meta-embeddings. Experiments on word similarity and analogy tasks and on part-of-speech (POS) tagging show better performance of meta-embeddings compared to individual embedding sets. One advantage of meta-embeddings is that they have increased coverage of the vocabulary. We will release our meta-embeddings publicly.},
  Comments                 = {10 pages, 6 figures},
  Eprint                   = {1508.04257},
  Oai2identifier           = {1508.04257},
  Owner                    = {20361362},
  Timestamp                = {2015.09.01}
}

@Article{yogatamaextractive,
  Title                    = {Extractive Summarization by Maximizing Semantic Volume},
  Author                   = {Yogatama, Dani and Liu, Fei and Smith, Noah A},
  Journal                  = {Conference on Empirical Methods in Natural Language Processing},
  Year                     = {2015},

  Keywords                 = {summarization, vector-based_summarization},
  Owner                    = {20361362},
  Timestamp                = {2015.09.02},
  Url                      = {http://www.cs.cmu.edu/~dyogatam/papers/yogatama+liu+smith.emnlp2015.pdf}
}

@Article{younes1989parametric,
  Title                    = {Parametric inference for imperfectly observed Gibbsian fields},
  Author                   = {Younes, Laurent},
  Journal                  = {Probability theory and related fields},
  Year                     = {1989},

  Annote                   = {Part of the reason why \cite{2009DBM} became possible.
Not certasin why though.

A very heavy mathematical paper.
I have not got through it all yet.},
  Number                   = {4},
  Pages                    = {625--645},
  Volume                   = {82},

  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2015.04.17},
  Url                      = {http://www.researchgate.net/profile/Laurent_Younes/publication/227121666_Parametric_Inference_for_imperfectly_observed_Gibbsian_fields/links/00b7d5268355a8fdb7000000.pdf}
}

@Article{yu2013deep,
  Title                    = {The deep tensor neural network with applications to large vocabulary speech recognition},
  Author                   = {Yu, Dong and Deng, Li and Seide, Frank},
  Journal                  = {Audio, Speech, and Language Processing, IEEE Transactions on},
  Year                     = {2013},
  Number                   = {2},
  Pages                    = {388--396},
  Volume                   = {21},

  Keywords                 = {audio},
  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2015.09.15},
  Url                      = {http://www.msr-waypoint.net/pubs/177443/DTNN-TASLP2012-Proof.pdf}
}

@Article{TACL15CompVector,
  Title                    = {Learning Composition Models for Phrase Embeddings},
  Author                   = {Yu, Mo and Dredze, Mark},
  Journal                  = {Transactions of the Association for Computational Linguistics},
  Year                     = {2015},

  Annote                   = {New compositional vector sentence embeeding

Can be seen as simplifying Tensor RNNs to be diagonal matrixes,
or broadeding weighted sum to to elementwise weighted some.},
  Pages                    = {227--242},
  Volume                   = {3},

  Abstract                 = {Lexical embeddings can serve as useful representations for words for a variety of NLP tasks, but learning embeddings for phrases can be challenging. While separate embeddings are learned for each word, this is infeasible for every phrase. We construct phrase embeddings by learning how to compose word embeddings using features that capture phrase structure and context. We propose efficient unsupervised and task-specific learning objectives that scale our model to large datasets. We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths. We make the implementation of our model and the datasets available for general use.},
  ISSN                     = {2307-387X},
  Owner                    = {20361362},
  Timestamp                = {2016.01.18},
  Url                      = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/586}
}

@Inproceedings{yu2014improving,
  Title                    = {Improving lexical embeddings with semantic knowledge},
  Author                   = {Yu, Mo and Dredze, Mark},
  Booktitle                = {Association for Computational Linguistics (ACL)},
  Year                     = {2014},
  Annote                   = {Purpoise: to show that using suplimentary semantic iformation can improve word embeddings

Approach taken: Multimodal training of some so form, using second objective of making ssynomymn embeddings localised. Extends word2vec

Evaluation 3 task: Sematnic Nearness to synomnyms, Predicting human judgemnet, and language modeling

How to apply to may work: Combining this extension of word2vec onto doc2vec could imporve phrase semantics

Agortihms: is calleds Relation Constrained Model (RCM)

Further reading: the Paraphases Database},
  Pages                    = {545--550},

  Keywords                 = {RCM, word-embeddings},
  Owner                    = {20361362},
  Timestamp                = {2015.06.30},
  Url                      = {www.aclweb.org/anthology/P/P14/P14-2089.pdf}
}

@Inproceedings{zanzotto2010estimating,
  Title                    = {Estimating linear models for compositional distributional semantics},
  Author                   = {Zanzotto, Fabio Massimo and Korkontzelos, Ioannis and Fallucchi, Francesca and Manandhar, Suresh},
  Booktitle                = {Proceedings of the 23rd International Conference on Computational Linguistics},
  Year                     = {2010},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {1263--1271},

  Owner                    = {20361362},
  Timestamp                = {2015.11.19},
  Url                      = {https://art.torvergata.it/retrieve/handle/2108/32520/38263/2010_COLING_ZanzottoKorkontzelosFallucchiManandhar.pdf}
}

@Article{DBLP:journals/corr/abs-1212-5701,
  Title                    = {{ADADELTA:} An Adaptive Learning Rate Method},
  Author                   = {Matthew D. Zeiler},
  Journal                  = {CoRR},
  Year                     = {2012},

  Annote                   = {Improved over AdaGrad by not depending on a set global learning rate hyper parameter. As well as not having the gradient always decrease overtime.},
  Volume                   = {abs/1212.5701},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1212-5701},
  Keywords                 = {optimisation},
  Owner                    = {20361362},
  Timestamp                = {2015.05.14},
  Url                      = {http://arxiv.org/abs/1212.5701}
}

@Article{zhang2002shape,
  Title                    = {Shape-based image retrieval using generic Fourier descriptor},
  Author                   = {Zhang, Dengsheng and Lu, Guojun},
  Journal                  = {Signal Processing: Image Communication},
  Year                     = {2002},

  Annote                   = {Very details about ways of representing shapes. Supposed to be what inspired \cite{2016arXiv160603821M}.Possibly a good souce to read about representations including angle.},
  Number                   = {10},
  Pages                    = {825--848},
  Volume                   = {17},

  Owner                    = {20361362},
  Publisher                = {Elsevier},
  Timestamp                = {2017.01.13},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.6312&rep=rep1&type=pdf}
}

@Inproceedings{zhang2014BRAE,
  Title                    = {Bilingually-constrained Phrase Embeddings for Machine Translation},
  Author                   = {Jiajun Zhang and Shujie Liu and Mu Li and Ming Zhou and Chengqing Zong},
  Year                     = {2014},
  Publisher                = {ACL},

  Abstract                 = {<p>We propose Bilingually-constrained Recursive Auto-encoders (BRAE) to learn
 phrase embeddings (compact vector representations for phrases), which can
 distinguish the phrases in different semantic meanings. The BRAE is trained with
 the objective to minimize the semantic distance of translation equivalents and
 maximize the semantic distance of nontranslation pairs. The learned model can
 embed any phrase semantically in two languages and can transform semantic space
 in one language to the other. We evaluate the BRAE on two end-to-end SMT tasks
 (phrase table pruning and translation hypotheses reranking) which need to measure
 semantic similarity between a source phrase and its translation candidates.
 Extensive experiments show that the BRAE is spectacularly successful in these two
 tasks.</p>},
  Owner                    = {20361362},
  Timestamp                = {2015.09.09},
  Url                      = {http://anthology.aclweb.org/P/P14/P14-1011.pdf}
}

@Inproceedings{zhang2014chinese,
  Title                    = {Chinese poetry generation with recurrent neural networks},
  Author                   = {Zhang, Xingxing and Lapata, Mirella},
  Booktitle                = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  Year                     = {2014},
  Pages                    = {670--680},

  Abstract                 = {We propose a model for Chinese poem
generation based on recurrent neural net-
works which we argue is ideally suited to
capturing poetic content and form. Our
generator
jointly
performs content selec-
tion (ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã¢â‚¬Å“what to sayÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬ÃƒÂ¯Ã‚Â¿Ã‚Â½) and surface realization
(ÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬Ãƒâ€¦Ã¢â‚¬Å“how to sayÃƒÆ’Ã‚Â¢ÃƒÂ¢Ã¢â‚¬Å¡Ã‚Â¬ÃƒÂ¯Ã‚Â¿Ã‚Â½) by learning representations
of individual characters, and their com-
binations into one or more lines as well
as how these mutually reinforce and con-
strain each other. Poem lines are gener-
ated incrementally by taking into account
the entire history of what has been gen-
erated so far rather than the limited hori-
zon imposed by the previous line or lexical
n
-grams. Experimental results show that
our model outperforms competitive Chi-
nese poetry generation systems using both
automatic and manual evaluation methods.},
  Keywords                 = {RNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://emnlp2014.org/papers/pdf/EMNLP2014074.pdf}
}

@Article{DBLP:journals/corr/ZhangL15,
  Title                    = {Text Understanding from Scratch},
  Author                   = {Xiang Zhang and
 Yann LeCun},
  Journal                  = {CoRR},
  Year                     = {2015},

  Annote                   = {This is the paper where embeddings are learn from the character level.
Via convolutoional neural network with many layers.},
  Volume                   = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/ZhangL15},
  Keywords                 = {phrase-embeddings},
  Owner                    = {20361362},
  Timestamp                = {2015.08.31},
  Url                      = {http://arxiv.org/abs/1502.01710}
}

@Article{ZhangWordOrderSyntax,
  Title                    = {Discriminative Syntax-based Word Ordering for Text Generation},
  Author                   = {Zhang, Yue and Clark, Stephen},
  Journal                  = {Comput. Linguist.},
  Year                     = {2015},
  Month                    = sep,
  Number                   = {3},
  Pages                    = {503--538},
  Volume                   = {41},

  Acmid                    = {2847427},
  Address                  = {Cambridge, MA, USA},
  Doi                      = {10.1162/COLI_a_00229},
  ISSN                     = {0891-2017},
  Issue_date               = {September 2015},
  Keywords                 = {word ordering},
  Numpages                 = {36},
  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2016.06.07},
  Url                      = {http://delivery.acm.org/10.1145/2850000/2847427/coli_a_00229.pdf?ip=130.95.149.38&id=2847427&acc=PUBLIC&key=65D80644F295BC0D.05ACB6DAA5D75F76.4D4702B0C3E38B35.4D4702B0C3E38B35&CFID=626603773&CFTOKEN=69370806&__acm__=1465292119_1dd1318ddcf7f6ed98f5d82aa72b92ea}
}

@Inproceedings{moviebook,
  Title                    = {Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  Author                   = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  Booktitle                = {Proceedings of the IEEE international conference on computer vision},
  Year                     = {2015},
  Annote                   = {This is the preferred citatin for},
  Pages                    = {19--27},

  Owner                    = {20361362},
  Timestamp                = {2016.02.09},
  Url                      = {http://arxiv.org/abs/1506.06724}
}

@Book{17003720060101,
  Title                    = {Semi-Supervised Learning.},
  Author                   = {Zien, Alexander and SchoÃƒÆ’Ã…â€™Ãƒâ€¹Ã¢â‚¬Â lkopf, Bernhard and Chapelle, Olivier},
  Publisher                = {The MIT Press},
  Year                     = {2006},
  Series                   = {Adaptive Computation and Machine Learning},

  Abstract                 = {In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research.Semi-Supervised Learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assum},
  ISBN                     = {9780262033589},
  Owner                    = {20361362},
  Timestamp                = {2016.10.19},
  Url                      = {http://search.ebscohost.com.ezproxy.library.uwa.edu.au/login.aspx?direct=true&db=nlebk&AN=170037&site=ehost-live}
}

@Book{zipf1949human,
  Title                    = {Human behavior and the principle of least effort: an introduction to human ecology},
  Author                   = {Zipf, G.K.},
  Publisher                = {Addison-Wesley Press},
  Year                     = {1949},

  Lccn                     = {49007787},
  Owner                    = {20361362},
  Timestamp                = {2016.10.13},
  Url                      = {https://books.google.com.au/books?id=1tx9AAAAIAAJ}
}

@Article{zipf1945meaning,
  Title                    = {The meaning-frequency relationship of words},
  Author                   = {Zipf, George Kingsley},
  Journal                  = {The Journal of general psychology},
  Year                     = {1945},

  Annote                   = {about how many senses frequent words had},
  Number                   = {2},
  Pages                    = {251--256},
  Volume                   = {33},

  Owner                    = {20361362},
  Publisher                = {Taylor \& Francis},
  Timestamp                = {2016.10.04},
  Url                      = {http://search.proquest.com/docview/1290515471?accountid=14681}
}

@Inproceedings{zou2013bilingual,
  Title                    = {Bilingual Word Embeddings for Phrase-Based Machine Translation.},
  Author                   = {Zou, Will Y and Socher, Richard and Cer, Daniel M and Manning, Christopher D},
  Booktitle                = {EMNLP},
  Year                     = {2013},
  Annote                   = {Basic idea is to create bilingual word embeddings, (from unsupervised data??)
then use them with a standard phrasal translator.


Makes use of /cite{liang2006alignment} to align the Embeddings.
Normal Embedding is done 
which are then},
  Pages                    = {1393--1398},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://nlp.stanford.edu/pubs/emnlp2013_ZouSocherCerManning.pdf}
}

@Inproceedings{acl2018WinnLighter,
  author    = {Winn, Olivia and Muresan, Smaranda},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  title     = {'Lighter' Can Still Be Dark: Modeling Comparative Color Descriptions},
  year      = {2018},
  pages     = {790--795},
  publisher = {Association for Computational Linguistics},
  location  = {Melbourne, Australia},
  owner     = {20361362},
  timestamp = {2018.07.27},
  url       = {http://aclweb.org/anthology/P18-2125},
}

@Inproceedings{acl2018bleuopposedmeaning,
  author    = {C{\'i}fka, Ond{\v{r}}ej and Bojar, Ond{\v{r}}ej},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {Are BLEU and Meaning Representation in Opposition?},
  year      = {2018},
  pages     = {1362--1371},
  publisher = {Association for Computational Linguistics},
  location  = {Melbourne, Australia},
  owner     = {20361362},
  timestamp = {2018.07.25},
  url       = {http://aclweb.org/anthology/P18-1126},
}

@Inproceedings{ac2018probingsentencevectors,
  author    = {Conneau, Alexis and Kruszewski, Germ{\'a}n and Lample, Guillaume and Barrault, Lo{\"i}c and Baroni, Marco},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {What you can cram into a single {\backslash}{\$}{\&}!{\#}* vector: Probing sentence embeddings for linguistic properties},
  year      = {2018},
  pages     = {2126--2136},
  publisher = {Association for Computational Linguistics},
  annote    = {Our first striking result is the good overall per- formance of Bag-of-Vectors, confirming early in- sights that aggregated word embeddings capture surprising amounts of sentence information},
  location  = {Melbourne, Australia},
  owner     = {20361362},
  timestamp = {2018.07.25},
  url       = {http://aclweb.org/anthology/P18-1198},
}

@Article{Aditya2017,
  author    = {Somak Aditya and Yezhou Yang and Chitta Baral and Yiannis Aloimonos and Cornelia Fermüller},
  journal   = {Computer Vision and Image Understanding},
  title     = {Image Understanding using vision and reasoning through Scene Description Graph},
  year      = {2017},
  note      = {In Press, Accepted Manuscript},
  annote    = {This is the preferred citation for the COMPOSITE dataset for image captioning evaluation. Thought it does not use that as the name of the dataset.},
  doi       = {https://doi.org/10.1016/j.cviu.2017.12.004},
  issn      = {1077-3142},
  owner     = {20361362},
  timestamp = {2018.07.30},
  url       = {https://www.sciencedirect.com/science/article/pii/S1077314217302291},
}

@Inproceedings{spice2016,
  author    = {Peter Anderson and Basura Fernando and Mark Johnson and Stephen Gould},
  booktitle = {ECCV},
  title     = {SPICE: Semantic Propositional Image Caption Evaluation},
  year      = {2016},
  owner     = {20361362},
  timestamp = {2018.07.31},
}

@Inproceedings{bender2015ERS,
  author    = {Bender, Emily M and Flickinger, Dan and Oepen, Stephan and Packard, Woodley and Copestake, Ann},
  booktitle = {Proceedings of the 11th international conference on Computational Semantics},
  title     = {Layers of interpretation: On grammar and compositionality},
  year      = {2015},
  pages     = {239--249},
  owner     = {20361362},
  timestamp = {2018.08.03},
  url       = {http://www.aclweb.org/anthology/W15-0128},
}

@Article{2018arXiv180801091W,
  author        = {{White}, L. and {Togneri}, R. and {Liu}, W. and {Bennamoun}, M.},
  title         = {{DataDeps.jl: Repeatable Data Setup for Replicable Data Science}},
  journal       = {ArXiv e-prints},
  year          = {2018},
  month         = aug,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2018arXiv180801091W},
  archiveprefix = {arXiv},
  eprint        = {1808.01091},
  keywords      = {Computer Science - Software Engineering},
  owner         = {20361362},
  primaryclass  = {cs.SE},
  timestamp     = {2018.08.09},
@Article{grice1975logic,
  author    = {Grice, H Paul},
  journal   = {Speech Acts},
  title     = {Logic and conversation},
  year      = {1975},
  volume    = {3},
  pages     = {41--58},
  owner     = {20361362},
  timestamp = {2018.08.20},
  url       = {http://www.communicationcache.com/uploads/1/0/8/8/10887248/logic_and_conversation.pdf},
}

@Comment{jabref-meta: databaseType:bibtex;}
