% This file was created with JabRef 2.10.
% Encoding: Cp1252


@Misc{,
  Title                    = {maximum-likelihood-and-entropy},

  Annote                   = {Uncitable Blog, but nicely written into to max entropy and the relationship between -ve log likelyhood, entropy rate (true entropy), and Kullback-Leiber diverence (relitive entropy, between true and estimate) 

Likelyhood is the joint pdf for all the events ($x_i$) occuring, parameterised by $\theta$, we assume that events are IID.

Arg Max the likelyhood of the emperical samples, to get $\theta$
Equivelently can Argmin the -ve log of the likelyhood.

$\theta_0$ is the true parameter
$\hat{\theta}$ is out estimated parameteri

"mean negative log-likelihood converges to the differential entropy under the true distribution plus the Kullback-Leibler divergence between the true distribution and the distribution we guess at"},
  Author                   = {David Darmon},
  HowPublished             = {blog},

  Keywords                 = {max-entropy},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://thirdorderscientist.org/homoclinic-orbit/2013/4/1/maximum-likelihood-and-entropy}
}

@Inproceedings{Malouf2002,
  Title                    = {A comparison of algorithms for maximum entropy parameter estimation},
  Author                   = {Malouf, Robert},
  Booktitle                = {proceedings of the 6th conference on Natural language learning-Volume 20},
  Year                     = {2002},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {1--7},

  __markedentry            = {[20361362:6]},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://www.coli.uni-saarland.de/groups/HU/HUwm4/Slides/malouf.pdf}
}

@Inproceedings{Socher2011,
  Title                    = {Parsing natural scenes and natural language with recursive neural networks},
  Author                   = {Socher, Richard and Lin, Cliff C and Manning, Chris and Ng, Andrew Y},
  Booktitle                = {Proceedings of the 28th international conference on machine learning (ICML-11)},
  Year                     = {2011},
  Annote                   = {Defines a tree of recurrsively applied neural networks for parsing.

Each network has:
2 split inputs, $c_i$, $c_j$ each  1 hot word vector
each goto $L$: the word embeeding matrix, with the output then concatenated which goes
to $W$: the hidden layer 

output of $W$ is $p_{i,j}$ -- the merged word vector
which also goes to two places:
To $W_{score}$ -- the score of how good the merge was
and To $W_{label}$ -- the softmaxed Part of Speech (POS) tagging layer


The paper describes how the parse trees are selected.
It also (poorly) decribes how the learning is carried out (to understand will need to chase up references).},
  Pages                    = {129--136},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://nlp.stanford.edu/pubs/SocherLinNgManning_ICML2011.pdf}
}

@Inproceedings{Taskar2004,
  Title                    = {Max-Margin Parsing.},
  Author                   = {Taskar, Ben and Klein, Dan and Collins, Michael and Koller, Daphne and Manning, Christopher D},
  Booktitle                = {EMNLP},
  Year                     = {2004},
  Annote                   = {This is NOT a good paper to read to help undersstand Socher2011.
It is a predecessor to it, but not direct enough.},
  Number                   = {1.1},
  Organization             = {Citeseer},
  Pages                    = {3},
  Volume                   = {1},

  __markedentry            = {[20361362:]},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14}
}

