% This file was created with JabRef 2.10.
% Encoding: Cp1252


@Manual{AMRspec,
  Title                    = {Abstract meaning representation (AMR) specification},
  Author                   = {Banarescu, Laura and Bonial, Claire and Cai, Shu and Georgescu, Madalina and Griffitt, Kira and Hermjakob, Ulf and Knight, Kevin and Koehn, Philipp and Palmer, Martha and Schneider, Nathan},
  Organization             = {The University of Southern California: Information Sciences Institute},

  Annote                   = {AMR is complicated, yo.},
  Comment                  = {This is a live resource, it will be updated periodically.},
  Keywords                 = {AMR},
  Owner                    = {20361362},
  Timestamp                = {2015.04.24},
  Url                      = {https://github.com/amrisi/amr-guidelines/blob/master/amr.md}
}

@Article{Banarescu13abstractmeaning,
  Title                    = {Abstract Meaning Representation for Sembanking},
  Author                   = {Laura Banarescu and Claire Bonial and Shu Cai and Madalina Georgescu and Kira Griffitt and Ulf Hermjakob and Kevin Knight and Philipp Koehn and Martha Palmer and Nathan Schneider},
  Year                     = {2013},

  Annote                   = {AMR seeks to create a large annoted corpus of sentences.
The annotations describe the Meaning of the sentence.
In a Propbank like way.

Every sentence with the same maning will have the same AMR (Abstract Meaning Representation).

Eg:

(d / destroy-01
:arg0 (b / boy)
:arg1 (r / room))
the destruction of the room by the boy ...
the boy’s destruction of the room ...
The boy destroyed the room.},

  Owner                    = {20361362},
  Timestamp                = {2015.04.18},
  Url                      = {http://www.isi.edu/natural-language/amr/a.pdf}
}

@Article{RvNNLogicalSemantics,
  Title                    = {Recursive Neural Networks for Learning Logical Semantics},
  Author                   = {Bowman, Samuel R and Potts, Christopher and Manning, Christopher D},
  Journal                  = {arXiv preprint arXiv:1406.1827},
  Year                     = {2014},

  Annote                   = {Train RvNN to work on logical structures.
Also trained recursive neural tensor networks (RvNTN) to do the same.
Found RvNTN to be better, and indeedd to work well in all cases.

Taking two sentences, decomposing them,
then at the top using a softmax classifier to deterrmine entailment, reverse entailment, equivalence, alternation, negation, cover or independence (else), between them.

It is noted that the RvNN fails to suitably separate long different expressions in embedded space.
The RvNTN does much better.},

  __markedentry            = {[20361362:]},
  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.22},
  Url                      = {http://arxiv.org/pdf/1406.1827v1.pdf}
}

@Inproceedings{phrasaltranslationtool,
  Title                    = {Phrasal: a toolkit for statistical machine translation with facilities for extraction and incorporation of arbitrary model features},
  Author                   = {Cer, Daniel and Galley, Michel and Jurafsky, Daniel and Manning, Christopher D},
  Booktitle                = {Proceedings of the NAACL HLT 2010 Demonstration Session},
  Year                     = {2010},
  Annote                   = {Quiet likely THE engine for phrase-based translation.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {9--12},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://mt-archive.info/NAACL-HLT-2010-Cer-2.pdf}
}

@Unpublished{chenSpokenDialogueThesisProposal,
  Title                    = {Thesis Proposal: Unsupervised Learning and Modeling of Knowledge and Intent for Spoken Dialogue Systems},
  Author                   = {Yun-Nung (Vivian) Chen},

  Annote                   = {A PhD thesis Proposal.Focus is on Acquiring Knowledge from spoken word.},
  Month                    = {April},
  Year                     = {2015},

  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.cs.cmu.edu/~yvchen/doc/dissertation.pdf}
}

@Misc{BlogMaxLikeMaxEnt,
  Title                    = {maximum-likelihood-and-entropy},

  Annote                   = {Uncitable Blog, but nicely written into to max entropy and the relationship between -ve log likelyhood, entropy rate (true entropy), and Kullback-Leiber diverence (relitive entropy, between true and estimate) 

Likelyhood is the joint pdf for all the events ($x_i$) occuring, parameterised by $\theta$, we assume that events are IID.

Arg Max the likelyhood of the emperical samples, to get $\theta$
Equivelently can Argmin the -ve log of the likelyhood.

$\theta_0$ is the true parameter
$\hat{\theta}$ is out estimated parameteri

"mean negative log-likelihood converges to the differential entropy under the true distribution plus the Kullback-Leibler divergence between the true distribution and the distribution we guess at"},
  Author                   = {David Darmon},
  HowPublished             = {blog},

  Keywords                 = {max-entropy},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://thirdorderscientist.org/homoclinic-orbit/2013/4/1/maximum-likelihood-and-entropy}
}

@Article{das2014frame,
  Title                    = {Frame-semantic parsing},
  Author                   = {Das, Dipanjan and Chen, Desai and Martins, Andr{\'e} FT and Schneider, Nathan and Smith, Noah A},
  Journal                  = {Computational Linguistics},
  Year                     = {2014},

  Annote                   = {See Eratum (under Note in BibTeX).},
  Note                     = {Has Erratum: \url{http://www.mitpressjournals.org/doi/pdfplus/10.1162/COLI_x_00205}
 
Erratum
The authors of the article ”Frame-Semantic P
arsing” and a graduate student discovered
that in rows 7 and 8 of Table 8, at inference time for argument identification with
gold frames, the described model included gold spans along with the candidate set
of automatic spans (elaborated in Section 6.1), thus creating an oracle, and artificially
bloating the precision, recall, and F
1 metrics. The revised metrics are:
Naive decoding: Precision=78.65 Recall=72.85 Fscore=75.64 (row 7)
Beam search decoding: Precision=80.40 Recall=72.84 Fscore=76.43 (row 8)
This unintended artifact also changes the
interpretation of Table 9. The reported
results there should be interpreted as an oracle comparison of various inference
methods, that uses both automatically extr
acted candidate spans as well as gold spans
for argument identification.
None of the other results in the article are affected by this error.},
  Number                   = {1},
  Pages                    = {9--56},
  Volume                   = {40},

  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00163}
}

@Inproceedings{2008findingContradictions,
  Title                    = {Finding Contradictions in Text.},
  Author                   = {De Marneffe, Marie-Catherine and Rafferty, Anna N and Manning, Christopher D},
  Booktitle                = {ACL},
  Year                     = {2008},
  Pages                    = {1039--1047},
  Volume                   = {8},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://nlp.stanford.edu/pubs/contradiction-acl08.pdf}
}

@Article{dernoncourt2012designing,
  Title                    = {Designing an intelligent dialogue system for serious games.},
  Author                   = {Dernoncourt, F.},
  Journal                  = {RJC EIAH’2012},
  Year                     = {2012},

  Annote                   = {A classical approch to a NLP/NLU chatterbot.

I vaguely know the author online.},
  Pages                    = {33},

  Owner                    = {20361362},
  Timestamp                = {2015.04.22},
  Url                      = {http://www.francky.me/doc/RJCEIAH2012_articlereviewed-20120412en.pdf}
}

@Article{AdaGrad,
  Title                    = {Adaptive subgradient methods for online learning and stochastic optimization},
  Author                   = {Duchi, John and Hazan, Elad and Singer, Yoram},
  Journal                  = {The Journal of Machine Learning Research},
  Year                     = {2011},

  Annote                   = {People are super keen on this method.

This paper is huge, and full of math i need to look up.

Read these instead: http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf
Adagrad provides a perfeature learning rate, where rare features have a higher learning rate.


what adagrad descr4ibes as $x$ and $x_{t+1}$, would be a row/column of $W$ in a neural network (I think)},
  Pages                    = {2121--2159},
  Volume                   = {12},

  Owner                    = {20361362},
  Publisher                = {JMLR. org},
  Timestamp                = {2015.04.23},
  Url                      = {http://www.magicbroom.info/Papers/DuchiHaSi10.pdf}
}

@Inproceedings{JARMparser,
  Title                    = {A Discriminative Graph-Based Parser for the Abstract Meaning Representation},
  Author                   = {Flanigan, Jeffrey and Thomson, Sam and Carbonell, Jaime and Dyer, Chris and Smith, Noah A.},
  Booktitle                = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  Year                     = {2014},

  Address                  = {Baltimore, Maryland},
  Annote                   = {Carbonell, Jaime and Dyer, Chris and Smith, Noah A. are the supervisors of Flanigan, Jeffrey 

Parses sentences into AMR, using graph techniques.
Stuff being done in this is pretty neat.


Evaluation is two steps:
 - Mapping words to concept nodes, done with a linear feature scoring
 - Mapping Relationships to edges (or rather fdereminging when realtiuonship edges exist), done with a maximum spanning subgraph alorigthm which maxised the scrores from a different linear features scoring.


Both feature scorers much be trained.
For the word to concept mapping a "Automatic Aligner" is used to know which concepts (in AMR space), align to which words. That is to say the auytomatic aligner is used to determine the ground truth from the training data (of sentecnec paired wit their AMR rep) to which the concept mapping is trained to replicate.},
  Month                    = {June},
  Pages                    = {1426--1436},
  Publisher                = {Association for Computational Linguistics},

  Keywords                 = {AMR},
  Owner                    = {20361362},
  Timestamp                = {2015.04.22},
  Url                      = {http://www.cs.cmu.edu/~jmflanig/flanigan+etal.acl2014.pdf}
}

@Article{gao2014learning,
  Title                    = {Learning continuous phrase representations for translation modeling},
  Author                   = {Gao, Jianfeng and He, Xiaodong and Yih, Wen-tau and Deng, Li},
  Journal                  = {Proc. of ACL. Association for Computational Linguistics, June},
  Year                     = {2014},

  Annote                   = {Input is bag of words, where each bag is a phrase.
Translation by projecting to a common embedding space.


The training method is based on L-BFGS.

Notebly they do not use the Cosign Similarity:
Quote: "In our experiments, we compare dot product and the cosine 
similarity functions an
d find that the former works better for 
nonlinear multi
-
layer neural networks, and the latter works 
better for linear neural networks"},

  Keywords                 = {MT},
  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://research.microsoft.com/pubs/211749/nn4smt.acl.v9.pdf}
}

@Inproceedings{goller1996BPstructure,
  Title                    = {Learning task-dependent distributed representations by backpropagation through structure},
  Author                   = {Goller, Christoph and Kuchler, Andreas},
  Booktitle                = {Neural Networks, 1996., IEEE International Conference on},
  Year                     = {1996},
  Annote                   = {This is not a fun paper to read or understand.
It is however cruicial for the understanding of Socher2011

It deals with how to apply Back-propergation Through Structure (
BPTS)  to Directed Acyclic Graphs (DAG) (notables including trees).
Which is like Back-propergation Though Time.},
  Organization             = {IEEE},
  Pages                    = {347--352},
  Volume                   = {1},

  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.1968&rep=rep1&type=pdf}
}

@Article{DBLP:journals/corr/GravesWD14,
  Title                    = {Neural Turing Machines},
  Author                   = {Alex Graves and
 Greg Wayne and
 Ivo Danihelka},
  Journal                  = {CoRR},
  Year                     = {2014},

  Annote                   = {Not well typeset. Figures miss-placed -- check they are lining up with text before over thinking it.

Several experiments are using a Neural network which control a memory bank.
Notably every part of the system is differntable allowing graident desent to be used.

Head "Head nearons" that have read and a write weight matrix to contol how they interact with the memory store.},
  Volume                   = {abs/1410.5401},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/GravesWD14},
  Owner                    = {20361362},
  Timestamp                = {2015.04.11},
  Url                      = {http://arxiv.org/pdf/1410.5401v2.pdf}
}

@Article{Grefenstette2014,
  Title                    = {A Deep Architecture for Semantic Parsing},
  Author                   = {Grefenstette, Edward and Blunsom, Phil and de Freitas, Nando and Hermann, Karl Moritz},
  Journal                  = {arXiv preprint arXiv:1404.7296},
  Year                     = {2014},

  Annote                   = {A neural network approch that takes in raw prhases, and converst it to ontologu specific queires.(SO more knowledge base querying).
Apparently without partsing.

It does use word embeddigns

I have not read this paper in deatil},

  Abstract                 = {Many successful approaches to semantic
parsing build on top of the syntactic anal-
ysis of text, and make use of distribu-
tional representations or statistical mod-
els to match parses to ontology-specific
queries. This paper presents a novel deep
learning architecture which provides a se-
mantic parsing system through the union
of two neural models of language se-
mantics. It allows for the generation of
ontology-specific queries from natural lan-
guage statements and questions without
the need for parsing, which makes it es-
pecially suitable to grammatically mal-
formed or syntactically atypical text, such
as tweets, as well as permitting the devel-
opment of semantic parsers for resource-
poor languages.},
  Owner                    = {20361362},
  Timestamp                = {2015.04.18},
  Url                      = {http://yoavartzi.com/sp14/pub/gbfh-sp14-2014.pdf}
}

@Article{Gupta2010,
  Title                    = {A survey of text summarization extractive techniques},
  Author                   = {Gupta, Vishal and Lehal, Gurpreet Singh},
  Journal                  = {Journal of Emerging Technologies in Web Intelligence},
  Year                     = {2010},

  Annote                   = {(Its not not published in a preditory journal, I checked. JETWI is a real thing and Academy Publisher too. It doesn't look nice)
Seems like quiet a nice review of text sumarisation.
focus on extractive, but does define abstractive..},
  Number                   = {3},
  Pages                    = {258--268},
  Volume                   = {2},

  Owner                    = {20361362},
  Timestamp                = {2015.04.17},
  Url                      = {http://learnpunjabi.org/pdf/survey-paper.pdf}
}

@Inproceedings{ExtractiveSummaristation,
  Title                    = {Extractive summarization using continuous vector space models},
  Author                   = {K{\aa}geb{\"a}ck, Mikael and Mogren, Olof and Tahmasebi, Nina and Dubhashi, Devdatt},
  Booktitle                = {Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)@ EACL},
  Year                     = {2014},
  Annote                   = {Focus in on using word and phrase embeedings to do multidocument extractive summary.

Quote: "To the best of our knowledge, continuous vector
space models have not previously been used in
summarization tasks"

Has a nice summery of the medthods used to produce word embeddings.\

Discusses two methos for phrase embeedings: Unfolder Recussive Autoencers (Socher)
And simple addition (apparently done by Mikolov)
It seems to really get the purpose of \cite{mikolovSkip} wrong though, saying the opposite of what that paper said.


Usign the Opinosis dataset, which might be good to look into, it contaiend human gnerated summerys.},
  Pages                    = {31--39},

  Keywords                 = {summarization, RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.17},
  Url                      = {http://www.aclweb.org/anthology/W14-1504}
}

@Inproceedings{kingsbury2002treebank,
  Title                    = {From TreeBank to PropBank.},
  Author                   = {Kingsbury, Paul and Palmer, Martha},
  Booktitle                = {LREC},
  Year                     = {2002},
  Annote                   = {About lableing the penn treebank with propositional struction.So that each sentence has a number o predicated with positional arguments.


This paper has interesting footnotes.},
  Organization             = {Citeseer},

  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.180.5642&rep=rep1&type=pdf}
}

@Inproceedings{liang2006alignment,
  Title                    = {Alignment by agreement},
  Author                   = {Liang, Percy and Taskar, Ben and Klein, Dan},
  Booktitle                = {Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics},
  Year                     = {2006},
  Annote                   = {Word Alignment seems to be the problem of how to align sentence in two languages with each word maps to another word in the other sentence.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {104--111},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://repository.upenn.edu/cgi/viewcontent.cgi?article=1569&context=cis_papers}
}

@Inproceedings{Malouf2002,
  Title                    = {A comparison of algorithms for maximum entropy parameter estimation},
  Author                   = {Malouf, Robert},
  Booktitle                = {proceedings of the 6th conference on Natural language learning-Volume 20},
  Year                     = {2002},
  Annote                   = {This is not useful.
It is a review of different algorithms for maximum entropy estimation,but does not explain how the more compelx ones work.
Does present some data on the comparative speed etc.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {1--7},

  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://www.coli.uni-saarland.de/groups/HU/HUwm4/Slides/malouf.pdf}
}

@Article{McKevitt1992,
  Title                    = {Approaches to natural language discourse processing},
  Author                   = {Mc Kevitt, Paul and Partridge, Derek and Wilks, Yorick},
  Journal                  = {Artificial Intelligence Review},
  Year                     = {1992},
  Number                   = {4},
  Pages                    = {333--364},
  Volume                   = {6},

  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2015.04.18},
  Url                      = {http://www.paulmckevitt.com/pubs/airenlp.pdf}
}

@Article{mikolov2013efficient,
  Title                    = {Efficient estimation of word representations in vector space},
  Author                   = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  Journal                  = {arXiv preprint arXiv:1301.3781},
  Year                     = {2013},

  Annote                   = {Compares several methods for finding single word vector repressentations.
A "You shall know a word by the company it keeps" type paper.},

  Owner                    = {20361362},
  Timestamp                = {2015.04.15},
  Url                      = {http://arxiv.org/pdf/1301.3781v3}
}

@Inproceedings{mikolov2011RnnLM,
  Title                    = {Extensions of recurrent neural network language model},
  Author                   = {Mikolov, Tomas and Kombrink, Stefan and Burget, Lukas and Cernocky, Jan H and Khudanpur, Sanjeev},
  Booktitle                = {Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on},
  Year                     = {2011},
  Annote                   = {Probably first RNN language model (Recurrent Neural net).
Input is One Hot,
Output is a proability distribution over classes of the next word,
(from which a uni

Output is Probabilituy distribution over some number of word classes,
where words are assigned to classes based on frequencuy (so each class containes the same number of probaility, but the ones containing low frewency words, contain more different words). ()Eg The might be in a class of its own).

Then uses a Probility distribution within the class to choose the word.

This hgive speedup in training (ast a small loss of accurasy)},
  Organization             = {IEEE},
  Pages                    = {5528--5531},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5947611}
}

@Inproceedings{mikolovSkip,
  Title                    = {Distributed representations of words and phrases and their compositionality},
  Author                   = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  Booktitle                = {Advances in Neural Information Processing Systems},
  Year                     = {2013},
  Annote                   = {Most of the paper focuses on local structure for relationships, ands is similar ot \cite{mikolov2013linguisticsubstructures}
This phrasal section of the work is not for per sentence but rather small phrases that are reused, so called "idiomatic phrases"

Eg "Austrian Airlines", "New York Times"

As I understand the discussion of phrases mostly focuses on having this short phrases preidentified (Eg by a tokenizer, NER, or via staticical cooccurance (that last one what how they did it),
and treating them as one phrase.},
  Pages                    = {3111--3119},

  Owner                    = {20361362},
  Timestamp                = {2015.04.24},
  Url                      = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@Inproceedings{mikolov2013linguisticsubstructures,
  Title                    = {Linguistic Regularities in Continuous Space Word Representations.},
  Author                   = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  Booktitle                = {HLT-NAACL},
  Year                     = {2013},
  Annote                   = {notes the existence of linear substructures in single-word embeddings for example: v("king")-v("man")+v("woman") has nearest neighbor v("queen")
Where nearest neighber was defined as the cosign similarity.


Experiements were carried out on the use of vector offsets to solve the:
A is to B as C is to ? sytle question.
Best 40% correct.
(Not good enough for use for resythisis)

*May* have been overly difficult since a closely related word might end up nearer.
Eg: Australia is to Canberra as USA is to ?
May have answered New York, instead of Washington -- Washinton and New York both being quiet close (for example) in Vector space.
So having overly dense vocabulary *may* have made the task harder. (Need to check up on Cosign Simliarity)},
  Pages                    = {746--751},

  Owner                    = {20361362},
  Timestamp                = {2015.04.15},
  Url                      = {http://www.msr-waypoint.com/en-us/um/people/gzweig/Pubs/NAACL2013Regularities.pdf}
}

@Inproceedings{mikolov2012contextRNNLM,
  Title                    = {Context dependent recurrent neural network language model.},
  Author                   = {Mikolov, Tomas and Zweig, Geoffrey},
  Booktitle                = {SLT},
  Year                     = {2012},
  Annote                   = {Exention to RNN Language model which track the 
Topic of the document using LDA},
  Pages                    = {234--239},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://www.msr-waypoint.com/en-us/um/people/gzweig/Pubs/SLT2012.pdf}
}

@Article{Pollack199077,
  Title                    = {Recursive distributed representations },
  Author                   = {Jordan B. Pollack},
  Journal                  = {Artificial Intelligence },
  Year                     = {1990},
  Number                   = {1â€“2},
  Pages                    = {77 - 105},
  Volume                   = {46},

  Abstract                 = {A longstanding difficulty for connectionist modeling has been how to represent variable-sized recursive data structures, such as trees and lists, in fixed-width patterns. This paper presents a connectionist architecture which automatically develops compact distributed representations for such compositional structures, as well as efficient accessing mechanisms for them. Patterns which stand for the internal nodes of fixed-valence trees are devised through the recursive use of backpropagation on three-layer auto-associative encoder networks. The resulting representations are novel, in that they combine apparently immiscible aspects of features, pointers, and symbol structures. They form a bridge between the data structures necessary for high-level cognitive tasks and the associative, pattern recognition machinery provided by neural networks.},
  Doi                      = {http://dx.doi.org/10.1016/0004-3702(90)90005-K},
  ISSN                     = {0004-3702},
  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.sciencedirect.com/science/article/pii/000437029090005K}
}

@Article{pourdamghanialigning,
  Title                    = {Aligning English Strings with Abstract Meaning Representation Graphs},
  Author                   = {Pourdamghani, Nima and Gao, Yang and Hermjakob, Ulf and Knight, Kevin},

  Annote                   = {Seaks to align English strings with words in the AMR.

(Also speaks in introduction about how it would be good just to use a generative model between AMR and Strings, but we don't got one for Strings<-> Graphs).

Has a preprocessing precedure to linearise the AMR, depths first.},

  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.isi.edu/natural-language/mt/amr_eng_align.pdf}
}

@Inproceedings{2009DBM,
  Title                    = {Deep boltzmann machines},
  Author                   = {Salakhutdinov, Ruslan and Hinton, Geoffrey E},
  Booktitle                = {International Conference on Artificial Intelligence and Statistics},
  Year                     = {2009},
  Annote                   = {Has a Boltzman Machine Learning procedure nicely defined.


In terms of Fantasy Particles and Sample
Boltsman machiens have 3 weight matrixes: visible-visible, visible-hidden, and vhidden-hidden. (Unlioke RBM which has the first and last set to zero).

Fiocus is however on deep boltman machines (not just plan regular ones).

"... unlike deep belief networks, the approxi-
mate inference procedure, in addition to an initial bottom-
up pass, can incorporate top-down feedback, allowing deep
Boltzmann machines to better propagate uncertainty about,
and hence deal more robustly with, ambiguous inputs."


Shows ghow to use a modifed RBM for pretraining},
  Pages                    = {448--455},

  Owner                    = {20361362},
  Timestamp                = {2015.04.17},
  Url                      = {http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS09_SalakhutdinovH.pdf}
}

@Phdthesis{socher2014recursive,
  Title                    = {Recursive Deep Learning for Natural Language Processing and Computer Vision},
  Author                   = {Socher, Richard},
  School                   = {Stanford University},
  Year                     = {2014},
  Annote                   = {see also: http://lxmls.it.pt/2014/socher-lxmls.pdf


Currently up to chapter 3.},

  File                     = {:..\\annotated_documents\\socher_thesis.pdf:PDF},
  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://nlp.stanford.edu/~socherr/thesis.pdf}
}

@Incollection{SocherEtAl2013:CVG,
  Title                    = {Parsing With Compositional Vector Grammars},
  Author                   = {Richard Socher and John Bauer and Christopher D. Manning and Andrew Y. Ng},
  Booktitle                = {ACL},
  Year                     = {2013},
  Annote                   = {This is the improved RvNN parser that combines it with Grammar,
This is what is in the Stanford Parser.},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.socher.org/uploads/Main/SocherBauerManningNg_ACL2013.pdf}
}

@Incollection{Socher2013TensorReasoning,
  Title                    = {Reasoning With Neural Tensor Networks For Knowledge Base Completion},
  Author                   = {Richard Socher and Danqi Chen and Christopher D. Manning and Andrew Y. Ng},
  Booktitle                = {Advances in Neural Information Processing Systems 26},
  Year                     = {2013},
  Annote                   = {Simarlar to mikolov2013linguisticsubstructures
Instead of , A is to B as C is to D (as in Mikolov et al)

This reasons in triples of a knowledge base,
trying to create more triples.

Going form a large number of (A,R,B) where R is a relationship between A and B.
To having even more of these.},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.15},
  Url                      = {http://nlp.stanford.edu/~socherr/SocherChenManningNg_NIPS2013.pdf}
}

@Inproceedings{Socher2010,
  Title                    = {Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora},
  Author                   = {Socher, Richard and Fei-Fei, Li},
  Booktitle                = {Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on},
  Year                     = {2010},
  Annote                   = {This is heavily inspired by machine translation.
Image is mapped to Visual Words, which are then moved to a common vector space that both visual words and textural words are mapped to .

Does Not Use Word Embedding, I think??},
  Organization             = {IEEE},
  Pages                    = {966--973},

  __markedentry            = {[20361362:6]},
  Owner                    = {20361362},
  Timestamp                = {2015.04.23},
  Url                      = {http://www.socher.org/uploads/Main/SocherFeiFei_CVPR2010.pdf}
}

@Incollection{SocherEtAl2011:PoolRAE,
  Title                    = {Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection},
  Author                   = {Richard Socher and Eric H. Huang and Jeffrey Pennington and Andrew Y. Ng and Christopher D. Manning},
  Booktitle                = {{Advances in Neural Information Processing Systems 24}},
  Year                     = {2011},

  Owner                    = {20361362},
  Timestamp                = {2015.04.24},
  Url                      = {http://www.socher.org/uploads/Main/SocherHuangPenningtonNgManning_NIPS2011.pdf}
}

@Inproceedings{Socher2011ParsingPhrases,
  Title                    = {Parsing natural scenes and natural language with recursive neural networks},
  Author                   = {Socher, Richard and Lin, Cliff C and Manning, Chris and Ng, Andrew Y},
  Booktitle                = {Proceedings of the 28th international conference on machine learning (ICML-11)},
  Year                     = {2011},
  Annote                   = {Defines a tree of recurrsively applied neural networks for parsing.

Each network has:
2 split inputs, $c_i$, $c_j$ each 1 hot word vector
each goto $L$: the word embeeding matrix, with the output then concatenated which goes
to $W$: the hidden layer 

output of $W$ is $p_{i,j}$ -- the merged word vector
which also goes to two places:
To $W_{score}$ -- the score of how good the merge was
and To $W_{label}$ -- the softmaxed Part of Speech (POS) tagging layer


The paper describes how the parse trees are selected.
It also (poorly) decribes how the learning is carried out (to understand will need to chase up references).

-----
This set of slides: http://lxmls.it.pt/2014/socher-lxmls.pdf
Describes all things quiet well},
  Pages                    = {129--136},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://nlp.stanford.edu/pubs/SocherLinNgManning_ICML2011.pdf}
}

@Inproceedings{socher2010PhraseEmbedding,
  Title                    = {Learning continuous phrase representations and syntactic parsing with recursive neural networks},
  Author                   = {Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
  Booktitle                = {Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop},
  Year                     = {2010},
  Annote                   = {Sorcher and Mannings earlier work.
Predecessor to \cite{socher2010PhraseEmbedding} Likely a better paper to understand that due to its earlier nature.


Embeeds Phrases via Parse of speach 


Does Assess Nerest Neighbour Phrases, which is interesting.
This is done by Embeeding a chunk of Wall Street Journal, then finding nearest neighbours on using a undsiclosed metric (Possibly equclidean)

Claims: "parser can accurately recover tree structures using only the distributed phrase representations. Furthermore, it provides semantic information even" But I can't see how.},
  Pages                    = {1--9},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://wuawua.googlecode.com/files/Learning%20Continuous%20Phrase%20Representations%20and%20Syntactic%20Parsing%20with%20Recursive%20Neural%20Networks.pdf}
}

@Inproceedings{SocherEtAl2011:RAE,
  Title                    = {Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions},
  Author                   = {Richard Socher and Jeffrey Pennington and Eric H. Huang and Andrew Y. Ng and Christopher D. Manning},
  Booktitle                = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  Year                     = {2011},
  Annote                   = {Similar to the RvNN, the Recursive Auto Encoder applies an Auto Encoder while generating the tree, in the place of the neural network shown in[fig:The-Neural-Net]. Roughly replacing the Max-Margin derived scoring matrix with a reconstruction error.

The tree selected is selecting much the say way as for the RvNN -- by merge resulting in best reconstruction.

It is noted that if the left child (for example) is a node made by merging 5 children and the right a node by merging only 2, then it is more important to correctly reconstruct the left child than the right. The weighting function in the RAE in that paper is adjusted as such. 

It can be extended in a similar way to the RvNN by adding additional trained outputs at each node.

It is not purely greedy, on simply training the to minimize the error of the merge at each node. Like the RvNN, it uses BPTS, and attempt to minimize the sum of all errors across the tree. Unlike the RvNN, there is no target structure -- it is free to use what ever structure minimizes total error.},

  Keywords                 = {RAE},
  Owner                    = {20361362},
  Timestamp                = {2015.04.24},
  Url                      = {http://www.socher.org/uploads/Main/SocherPenningtonHuangNgManning_EMNLP2011.pdf}
}

@Article{CulturomicOpenProblems,
  Title                    = {Visions and open challenges for a knowledge-based culturomics},
  Author                   = {Tahmasebi, Nina and Borin, Lars and Capannini, Gabriele and Dubhashi, Devdatt and Exner, Peter and Forsberg, Markus and Gossen, Gerhard and Johansson, Fredrik D and Johansson, Richard and K{\aa}geb{\"a}ck, Mikael and others},
  Journal                  = {International Journal on Digital Libraries},
  Year                     = {2015},

  Annote                   = {A collection of opentasks and motivations for the field of culturomics,
which involves doign large text processing, taking into acount development of culture over time.},
  Number                   = {2-4},
  Pages                    = {169--187},
  Volume                   = {15},

  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2015.04.17},
  Url                      = {http://link.springer.com/article/10.1007/s00799-015-0139-1/fulltext.html}
}

@Inproceedings{Taskar2004,
  Title                    = {Max-Margin Parsing.},
  Author                   = {Taskar, Ben and Klein, Dan and Collins, Michael and Koller, Daphne and Manning, Christopher D},
  Booktitle                = {EMNLP},
  Year                     = {2004},
  Annote                   = {This is NOT a good paper to read to help undersstand Socher2011.
It is a predecessor to it, but not direct enough.
It does not example the max-margin method well (but reference a baper that does)},
  Number                   = {1.1},
  Organization             = {Citeseer},
  Pages                    = {3},
  Volume                   = {1},

  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://ai.stanford.edu/~koller/Papers/Taskar+al:EMNLP04.pdf}
}

@Article{MemoryNN,
  Title                    = {Memory Networks},
  Author                   = {Jason Weston and
 Sumit Chopra and
 Antoine Bordes},
  Journal                  = {CoRR},
  Year                     = {2014},

  Annote                   = {A generalised model for a memory network.
Application in question answering.
Controls its own knowlege base with neurons.

Interestingly has a leaned tokeniser (segementor as it is called in text) as part of its write operation.

Example of expiment:
"Joe went to the kitchen. Fred went to the kitchen. Joe picked u
p the milk.
Joe travelled to the office. Joe left the milk. Joe went to the b
athroom.
Where is the milk now?
A: office
Where is Joe?
A: bathroom
Where was Joe before the office?
A: kitchen"},
  Volume                   = {abs/1410.3916},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/WestonCB14},
  Owner                    = {20361362},
  Timestamp                = {2015.04.12},
  Url                      = {http://arxiv.org/abs/1410.3916}
}

@Article{younes1989parametric,
  Title                    = {Parametric inference for imperfectly observed Gibbsian fields},
  Author                   = {Younes, Laurent},
  Journal                  = {Probability theory and related fields},
  Year                     = {1989},

  Annote                   = {Part of the reason why \cite{2009DBM} became possible.
Not certasin why though.

A very heavy mathematical paper.
I have not got through it all yet.},
  Number                   = {4},
  Pages                    = {625--645},
  Volume                   = {82},

  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2015.04.17},
  Url                      = {http://www.researchgate.net/profile/Laurent_Younes/publication/227121666_Parametric_Inference_for_imperfectly_observed_Gibbsian_fields/links/00b7d5268355a8fdb7000000.pdf}
}

@Inproceedings{zhang2014chinese,
  Title                    = {Chinese poetry generation with recurrent neural networks},
  Author                   = {Zhang, Xingxing and Lapata, Mirella},
  Booktitle                = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  Year                     = {2014},
  Pages                    = {670--680},

  Abstract                 = {We propose a model for Chinese poem
generation based on recurrent neural net-
works which we argue is ideally suited to
capturing poetic content and form. Our
generator
jointly
performs content selec-
tion (“what to say”) and surface realization
(“how to say”) by learning representations
of individual characters, and their com-
binations into one or more lines as well
as how these mutually reinforce and con-
strain each other. Poem lines are gener-
ated incrementally by taking into account
the entire history of what has been gen-
erated so far rather than the limited hori-
zon imposed by the previous line or lexical
n
-grams. Experimental results show that
our model outperforms competitive Chi-
nese poetry generation systems using both
automatic and manual evaluation methods.},
  Keywords                 = {RNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://emnlp2014.org/papers/pdf/EMNLP2014074.pdf}
}

@Inproceedings{zou2013bilingual,
  Title                    = {Bilingual Word Embeddings for Phrase-Based Machine Translation.},
  Author                   = {Zou, Will Y and Socher, Richard and Cer, Daniel M and Manning, Christopher D},
  Booktitle                = {EMNLP},
  Year                     = {2013},
  Annote                   = {Basic idea is to create bilingual word embeddings, (from unsupervised data??)
then use them with a standard phrasal translator.


Makes use of /cite{liang2006alignment} to align the Embeddings.
Normal Embedding is done 
which are then},
  Pages                    = {1393--1398},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://jan.stanford.edu/pubs/emnlp2013_ZouSocherCerManning.pdf}
}

