% This file was created with JabRef 2.10.
% Encoding: UTF8


@Manual{AMRspec,
  Title                    = {Abstract meaning representation (AMR) specification},
  Author                   = {Banarescu, Laura and Bonial, Claire and Cai, Shu and Georgescu, Madalina and Griffitt, Kira and Hermjakob, Ulf and Knight, Kevin and Koehn, Philipp and Palmer, Martha and Schneider, Nathan},
  Organization             = {The University of Southern California: Information Sciences Institute},

  Annote                   = {AMR is complicated, yo.},
  Comment                  = {This is a live resource, it will be updated periodically.},
  Keywords                 = {AMR},
  Owner                    = {20361362},
  Timestamp                = {2015.04.24},
  Url                      = {https://github.com/amrisi/amr-guidelines/blob/master/amr.md}
}

@Article{Banarescu13abstractmeaning,
  Title                    = {Abstract Meaning Representation for Sembanking},
  Author                   = {Laura Banarescu and Claire Bonial and Shu Cai and Madalina Georgescu and Kira Griffitt and Ulf Hermjakob and Kevin Knight and Philipp Koehn and Martha Palmer and Nathan Schneider},
  Year                     = {2013},

  Annote                   = {AMR seeks to create a large annoted corpus of sentences.
The annotations describe the Meaning of the sentence.
In a Propbank like way.

Every sentence with the same maning will have the same AMR (Abstract Meaning Representation).

Eg:

(d / destroy-01
:arg0 (b / boy)
:arg1 (r / room))
the destruction of the room by the boy ...
the boy’s destruction of the room ...
The boy destroyed the room.},

  Owner                    = {20361362},
  Timestamp                = {2015.04.18},
  Url                      = {http://www.isi.edu/natural-language/amr/a.pdf}
}

@Article{raey,
  Title                    = {From machine learning to machine reasoning},
  Author                   = {Bottou, Léon},
  Journal                  = {Machine Learning},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {133-149},
  Volume                   = {94},

  Doi                      = {10.1007/s10994-013-5335-x},
  ISSN                     = {0885-6125},
  Keywords                 = {Machine learning; Reasoning; Recursive networks},
  Language                 = {English},
  Owner                    = {20361362},
  Publisher                = {Springer US},
  Timestamp                = {2015.05.27},
  Url                      = {http://dx.doi.org/10.1007/s10994-013-5335-x}
}

@Article{RvNNLogicalSemantics,
  Title                    = {Recursive Neural Networks for Learning Logical Semantics},
  Author                   = {Bowman, Samuel R and Potts, Christopher and Manning, Christopher D},
  Journal                  = {arXiv preprint arXiv:1406.1827},
  Year                     = {2014},

  Annote                   = {Train RvNN to work on logical structures.
Also trained recursive neural tensor networks (RvNTN) to do the same.
Found RvNTN to be better, and indeedd to work well in all cases.

Taking two sentences, decomposing them,
then at the top using a softmax classifier to deterrmine entailment, reverse entailment, equivalence, alternation, negation, cover or independence (else), between them.

It is noted that the RvNN fails to suitably separate long different expressions in embedded space.
The RvNTN does much better.},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.22},
  Url                      = {http://arxiv.org/pdf/1406.1827v1.pdf}
}

@Inproceedings{phrasaltranslationtool,
  Title                    = {Phrasal: a toolkit for statistical machine translation with facilities for extraction and incorporation of arbitrary model features},
  Author                   = {Cer, Daniel and Galley, Michel and Jurafsky, Daniel and Manning, Christopher D},
  Booktitle                = {Proceedings of the NAACL HLT 2010 Demonstration Session},
  Year                     = {2010},
  Annote                   = {Quiet likely THE engine for phrase-based translation.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {9--12},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://mt-archive.info/NAACL-HLT-2010-Cer-2.pdf}
}

@Unpublished{chenSpokenDialogueThesisProposal,
  Title                    = {Thesis Proposal: Unsupervised Learning and Modeling of Knowledge and Intent for Spoken Dialogue Systems},
  Author                   = {Yun-Nung (Vivian) Chen},

  Annote                   = {A PhD thesis Proposal.Focus is on Acquiring Knowledge from spoken word.},
  Month                    = {April},
  Year                     = {2015},

  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.cs.cmu.edu/~yvchen/doc/dissertation.pdf}
}

@Misc{BlogMaxLikeMaxEnt,
  Title                    = {maximum-likelihood-and-entropy},

  Annote                   = {Uncitable Blog, but nicely written into to max entropy and the relationship between -ve log likelyhood, entropy rate (true entropy), and Kullback-Leiber diverence (relitive entropy, between true and estimate) 

Likelyhood is the joint pdf for all the events ($x_i$) occuring, parameterised by $\theta$, we assume that events are IID.

Arg Max the likelyhood of the emperical samples, to get $\theta$
Equivelently can Argmin the -ve log of the likelyhood.

$\theta_0$ is the true parameter
$\hat{\theta}$ is out estimated parameteri

"mean negative log-likelihood converges to the differential entropy under the true distribution plus the Kullback-Leibler divergence between the true distribution and the distribution we guess at"},
  Author                   = {David Darmon},
  HowPublished             = {blog},

  Keywords                 = {max-entropy},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://thirdorderscientist.org/homoclinic-orbit/2013/4/1/maximum-likelihood-and-entropy}
}

@Article{das2014frame,
  Title                    = {Frame-semantic parsing},
  Author                   = {Das, Dipanjan and Chen, Desai and Martins, Andr{\'e} FT and Schneider, Nathan and Smith, Noah A},
  Journal                  = {Computational Linguistics},
  Year                     = {2014},

  Annote                   = {See Eratum (under Note in BibTeX).},
  Note                     = {Has Erratum: \url{http://www.mitpressjournals.org/doi/pdfplus/10.1162/COLI_x_00205}
 
Erratum
The authors of the article ”Frame-Semantic P
arsing” and a graduate student discovered
that in rows 7 and 8 of Table 8, at inference time for argument identification with
gold frames, the described model included gold spans along with the candidate set
of automatic spans (elaborated in Section 6.1), thus creating an oracle, and artificially
bloating the precision, recall, and F
1 metrics. The revised metrics are:
Naive decoding: Precision=78.65 Recall=72.85 Fscore=75.64 (row 7)
Beam search decoding: Precision=80.40 Recall=72.84 Fscore=76.43 (row 8)
This unintended artifact also changes the
interpretation of Table 9. The reported
results there should be interpreted as an oracle comparison of various inference
methods, that uses both automatically extr
acted candidate spans as well as gold spans
for argument identification.
None of the other results in the article are affected by this error.},
  Number                   = {1},
  Pages                    = {9--56},
  Volume                   = {40},

  Owner                    = {20361362},
  Publisher                = {MIT Press},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00163}
}

@Inproceedings{2008findingContradictions,
  Title                    = {Finding Contradictions in Text.},
  Author                   = {De Marneffe, Marie-Catherine and Rafferty, Anna N and Manning, Christopher D},
  Booktitle                = {ACL},
  Year                     = {2008},
  Pages                    = {1039--1047},
  Volume                   = {8},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://nlp.stanford.edu/pubs/contradiction-acl08.pdf}
}

@Article{dernoncourt2012designing,
  Title                    = {Designing an intelligent dialogue system for serious games.},
  Author                   = {Dernoncourt, F.},
  Journal                  = {RJC EIAH’2012},
  Year                     = {2012},

  Annote                   = {A classical approch to a NLP/NLU chatterbot.

I vaguely know the author online.},
  Pages                    = {33},

  Owner                    = {20361362},
  Timestamp                = {2015.04.22},
  Url                      = {http://www.francky.me/doc/RJCEIAH2012_articlereviewed-20120412en.pdf}
}

@Article{AdaGrad,
  Title                    = {Adaptive subgradient methods for online learning and stochastic optimization},
  Author                   = {Duchi, John and Hazan, Elad and Singer, Yoram},
  Journal                  = {The Journal of Machine Learning Research},
  Year                     = {2011},

  Annote                   = {People are super keen on this method.

This paper is huge, and full of math i need to look up.

Read these instead: http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf
Adagrad provides a perfeature learning rate, where rare features have a higher learning rate.


what adagrad descr4ibes as $x$ and $x_{t+1}$, would be a row/column of $W$ in a neural network (I think)},
  Pages                    = {2121--2159},
  Volume                   = {12},

  Keywords                 = {optimisation},
  Owner                    = {20361362},
  Publisher                = {JMLR. org},
  Timestamp                = {2015.04.23},
  Url                      = {http://www.magicbroom.info/Papers/DuchiHaSi10.pdf}
}

@Inproceedings{JARMparser,
  Title                    = {A Discriminative Graph-Based Parser for the Abstract Meaning Representation},
  Author                   = {Flanigan, Jeffrey and Thomson, Sam and Carbonell, Jaime and Dyer, Chris and Smith, Noah A.},
  Booktitle                = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  Year                     = {2014},

  Address                  = {Baltimore, Maryland},
  Annote                   = {Carbonell, Jaime and Dyer, Chris and Smith, Noah A. are the supervisors of Flanigan, Jeffrey 

Parses sentences into AMR, using graph techniques.
Stuff being done in this is pretty neat.


Evaluation is two steps:
 - Mapping words to concept nodes, done with a linear feature scoring
 - Mapping Relationships to edges (or rather fdereminging when realtiuonship edges exist), done with a maximum spanning subgraph alorigthm which maxised the scrores from a different linear features scoring.


Both feature scorers much be trained.
For the word to concept mapping a "Automatic Aligner" is used to know which concepts (in AMR space), align to which words. That is to say the auytomatic aligner is used to determine the ground truth from the training data (of sentecnec paired wit their AMR rep) to which the concept mapping is trained to replicate.},
  Month                    = {June},
  Pages                    = {1426--1436},
  Publisher                = {Association for Computational Linguistics},

  Abstract                 = {We present a novel per-dimension learning rate method for
gradient descent called ADADELTA. The method dynami-
cally adapts over time using only first order information and
has minimal computational overhead beyond vanilla stochas-
tic gradient descent. The method requires no manual tuning of
a learning rate and appears robust to noisy gradient informa-
tion, different model architecture choices, various data modal-
ities and selection of hyperparameters. We show promising
results compared to other methods on the MNIST digit clas-
sification task using a single machine and on a large scale
voice dataset in a distributed cluster environment},
  Keywords                 = {AMR},
  Owner                    = {20361362},
  Timestamp                = {2015.04.22},
  Url                      = {http://www.cs.cmu.edu/~jmflanig/flanigan+etal.acl2014.pdf}
}

@Article{gao2014learning,
  Title                    = {Learning continuous phrase representations for translation modeling},
  Author                   = {Gao, Jianfeng and He, Xiaodong and Yih, Wen-tau and Deng, Li},
  Journal                  = {Proc. of ACL. Association for Computational Linguistics, June},
  Year                     = {2014},

  Annote                   = {Input is bag of words, where each bag is a phrase.
Translation by projecting to a common embedding space.

The training method is based on L-BFGS.

Notebly they do not use the Cosign Similarity:
Quote: "In our experiments, we compare dot product and the cosine 
similarity functions and find that the former works better for nonlinear multilayer neural networks, and the latter works 
better for linear neural networks"},

  Keywords                 = {MT},
  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://research.microsoft.com/pubs/211749/nn4smt.acl.v9.pdf}
}

@Inproceedings{goller1996BPstructure,
  Title                    = {Learning task-dependent distributed representations by backpropagation through structure},
  Author                   = {Goller, Christoph and Kuchler, Andreas},
  Booktitle                = {Neural Networks, 1996., IEEE International Conference on},
  Year                     = {1996},
  Annote                   = {This is not a fun paper to read or understand.
It is however cruicial for the understanding of Socher2011

It deals with how to apply Back-propergation Through Structure (
BPTS) to Directed Acyclic Graphs (DAG) (notables including trees).
Which is like Back-propergation Though Time.},
  Organization             = {IEEE},
  Pages                    = {347--352},
  Volume                   = {1},

  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.1968&rep=rep1&type=pdf}
}

@Article{DBLP:journals/corr/GravesWD14,
  Title                    = {Neural Turing Machines},
  Author                   = {Alex Graves and
 Greg Wayne and
 Ivo Danihelka},
  Journal                  = {CoRR},
  Year                     = {2014},

  Annote                   = {Not well typeset. Figures miss-placed -- check they are lining up with text before over thinking it.

Several experiments are using a Neural network which control a memory bank.
Notably every part of the system is differntable allowing graident desent to be used.

Head "Head nearons" that have read and a write weight matrix to contol how they interact with the memory store.},
  Volume                   = {abs/1410.5401},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/GravesWD14},
  Owner                    = {20361362},
  Timestamp                = {2015.04.11},
  Url                      = {http://arxiv.org/pdf/1410.5401v2.pdf}
}

@Article{Grefenstette2014,
  Title                    = {A Deep Architecture for Semantic Parsing},
  Author                   = {Grefenstette, Edward and Blunsom, Phil and de Freitas, Nando and Hermann, Karl Moritz},
  Journal                  = {arXiv preprint arXiv:1404.7296},
  Year                     = {2014},

  Annote                   = {A neural network approch that takes in raw prhases, and converst it to ontologu specific queires.(SO more knowledge base querying).
Apparently without partsing.

It does use word embeddigns

I have not read this paper in deatil},

  Abstract                 = {Many successful approaches to semantic
parsing build on top of the syntactic anal-
ysis of text, and make use of distribu-
tional representations or statistical mod-
els to match parses to ontology-specific
queries. This paper presents a novel deep
learning architecture which provides a se-
mantic parsing system through the union
of two neural models of language se-
mantics. It allows for the generation of
ontology-specific queries from natural lan-
guage statements and questions without
the need for parsing, which makes it es-
pecially suitable to grammatically mal-
formed or syntactically atypical text, such
as tweets, as well as permitting the devel-
opment of semantic parsers for resource-
poor languages.},
  Owner                    = {20361362},
  Timestamp                = {2015.04.18},
  Url                      = {http://yoavartzi.com/sp14/pub/gbfh-sp14-2014.pdf}
}

@Article{Gupta2010,
  Title                    = {A survey of text summarization extractive techniques},
  Author                   = {Gupta, Vishal and Lehal, Gurpreet Singh},
  Journal                  = {Journal of Emerging Technologies in Web Intelligence},
  Year                     = {2010},

  Annote                   = {(Its not not published in a preditory journal, I checked. JETWI is a real thing and Academy Publisher too. It doesn't look nice)
Seems like quiet a nice review of text sumarisation.
focus on extractive, but does define abstractive..},
  Number                   = {3},
  Pages                    = {258--268},
  Volume                   = {2},

  Owner                    = {20361362},
  Timestamp                = {2015.04.17},
  Url                      = {http://learnpunjabi.org/pdf/survey-paper.pdf}
}

@Inproceedings{ExtractiveSummaristation,
  Title                    = {Extractive summarization using continuous vector space models},
  Author                   = {K{\aa}geb{\"a}ck, Mikael and Mogren, Olof and Tahmasebi, Nina and Dubhashi, Devdatt},
  Booktitle                = {Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)@ EACL},
  Year                     = {2014},
  Annote                   = {Focus in on using word and phrase embeedings to do multidocument extractive summary.

Quote: "To the best of our knowledge, continuous vector
space models have not previously been used in
summarization tasks"

Has a nice summery of the medthods used to produce word embeddings.\

Discusses two methos for phrase embeedings: Unfolder Recussive Autoencers (Socher)
And simple addition (apparently done by Mikolov)
It seems to really get the purpose of \cite{mikolovSkip} wrong though, saying the opposite of what that paper said.


Usign the Opinosis dataset, which might be good to look into, it contaiend human gnerated summerys.},
  Pages                    = {31--39},

  Keywords                 = {summarization, RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.17},
  Url                      = {http://www.aclweb.org/anthology/W14-1504}
}

@Inproceedings{kingsbury2002treebank,
  Title                    = {From TreeBank to PropBank.},
  Author                   = {Kingsbury, Paul and Palmer, Martha},
  Booktitle                = {LREC},
  Year                     = {2002},
  Annote                   = {About lableing the penn treebank with propositional struction.So that each sentence has a number o predicated with positional arguments.


This paper has interesting footnotes.},
  Organization             = {Citeseer},

  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.180.5642&rep=rep1&type=pdf}
}

@Inproceedings{liang2006alignment,
  Title                    = {Alignment by agreement},
  Author                   = {Liang, Percy and Taskar, Ben and Klein, Dan},
  Booktitle                = {Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics},
  Year                     = {2006},
  Annote                   = {Word Alignment seems to be the problem of how to align sentence in two languages with each word maps to another word in the other sentence.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {104--111},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://repository.upenn.edu/cgi/viewcontent.cgi?article=1569&context=cis_papers}
}

@Article{7078992,
  Title                    = {Deep Learning for Acoustic Modeling in Parametric Speech Generation: A systematic review of existing techniques and future trends},
  Author                   = {Zhen-Hua Ling and Shi-Yin Kang and Heiga Zen and Senior, A. and Schuster, M. and Xiao-Jun Qian and Meng, H.M. and Li Deng},
  Journal                  = {Signal Processing Magazine, IEEE},
  Year                     = {2015},

  Annote                   = {Review article.How we got from text to speech (Via HMMs).
Also talks about voice conversion (changing peoples voices to other peoples voices, sayign the same words)

Covers the istory of using HMM and GMMs.


Also covers a hughe background.Interestign stuff like considfering RBMs are PoE and as GMMs.Also covers Contional RBMS},
  Month                    = {May},
  Number                   = {3},
  Pages                    = {35-52},
  Volume                   = {32},

  Abstract                 = {Hidden Markov models (HMMs) and Gaussian mixture models (GMMs) are the two most common types of acoustic models used in statistical parametric approaches for generating low-level speech waveforms from high-level symbolic inputs via intermediate acoustic feature sequences. However, these models have their limitations in representing complex, nonlinear relationships between the speech generation inputs and the acoustic features. Inspired by the intrinsically hierarchical process of human speech production and by the successful application of deep neural networks (DNNs) to automatic speech recognition (ASR), deep learning techniques have also been applied successfully to speech generation, as reported in recent literature. This article systematically reviews these emerging speech generation approaches, with the dual goal of helping readers gain a better understanding of the existing techniques as well as stimulating new work in the burgeoning area of deep learning for parametric speech generation.},
  Doi                      = {10.1109/MSP.2014.2359987},
  ISSN                     = {1053-5888},
  Keywords                 = {Gaussian processes;acoustic signal processing;hidden Markov models;mixture models;neural nets;speech recognition;ASR;DNN;GMM;Gaussian mixture models;HMM;acoustic features;acoustic modeling;acoustic models;automatic speech recognition;burgeoning area;deep learning;deep neural networks;hidden Markov models;high-level symbolic inputs;human speech production;intermediate acoustic feature sequences;low-level speech waveforms;parametric speech generation;statistical parametric approach;Acoustic signal detection;Gaussian mixture models;Hidden Markov models;Speech processing;Speech recognition;Speech synthesis;Vocoders},
  Owner                    = {20361362},
  Timestamp                = {2015.05.22}
}

@Article{liu2015combining,
  Title                    = {Combining Relevance Language Modeling and Clarity Measure for Extractive Speech Summarization},
  Author                   = {Liu, Shih-Hung and Chen, Kuan-Yu and Chen, Berlin and Wang, Hsin-Min and Yen, Hsu-Chun and Hsu, Wen-Lian},
  Journal                  = {Audio, Speech, and Language Processing, IEEE/ACM Transactions on},
  Year                     = {2015},

  Annote                   = {Using Language models, and the cross entropy between the sentence LM and the document LM (each of which are a prob distribution on the next work, give nthe previous),
To find the most relevant sentences.

It is run on Speech data. (rather than simply text.)},
  Number                   = {6},
  Pages                    = {957--969},
  Volume                   = {23},

  Owner                    = {20361362},
  Publisher                = {IEEE},
  Timestamp                = {2015.05.04},
  Url                      = {http://www.iis.sinica.edu.tw/papers/whm/18321-F.pdf}
}

@Inproceedings{Malouf2002,
  Title                    = {A comparison of algorithms for maximum entropy parameter estimation},
  Author                   = {Malouf, Robert},
  Booktitle                = {proceedings of the 6th conference on Natural language learning-Volume 20},
  Year                     = {2002},
  Annote                   = {This is not useful.
It is a review of different algorithms for maximum entropy estimation,but does not explain how the more compelx ones work.
Does present some data on the comparative speed etc.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {1--7},

  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://www.coli.uni-saarland.de/groups/HU/HUwm4/Slides/malouf.pdf}
}

@Electronic{treebank3,
  Title                    = {Treebank-3 LDC99T42},

  Address                  = {Philadelphia},
  Annote                   = {This is the dataset everyone uses. The Penn Treebank.

There is a subset of this in nltk.corpus.treebank

There is a mistakenly unprotected copy of the full set in http://trac.csail.mit.edu/workbench-projects/browser/trunk/edu.upenn.treebank-3?rev=748},
  Author                   = {Marcus, Mitchell},
  HowPublished             = {corpus},
  Organization             = {Linguistic Data Consortium},
  Year                     = {1999},

  Owner                    = {20361362},
  Timestamp                = {2015.05.21}
}

@Article{McKevitt1992,
  Title                    = {Approaches to natural language discourse processing},
  Author                   = {Mc Kevitt, Paul and Partridge, Derek and Wilks, Yorick},
  Journal                  = {Artificial Intelligence Review},
  Year                     = {1992},
  Number                   = {4},
  Pages                    = {333--364},
  Volume                   = {6},

  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2015.04.18},
  Url                      = {http://www.paulmckevitt.com/pubs/airenlp.pdf}
}

@Article{mikolov2013efficient,
  Title                    = {Efficient estimation of word representations in vector space},
  Author                   = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  Journal                  = {arXiv preprint arXiv:1301.3781},
  Year                     = {2013},

  Annote                   = {Compares several methods for finding single word vector repressentations.
A "You shall know a word by the company it keeps" type paper.},

  Owner                    = {20361362},
  Timestamp                = {2015.04.15},
  Url                      = {http://arxiv.org/pdf/1301.3781v3}
}

@Inproceedings{mikolov2011RnnLM,
  Title                    = {Extensions of recurrent neural network language model},
  Author                   = {Mikolov, Tomas and Kombrink, Stefan and Burget, Lukas and Cernocky, Jan H and Khudanpur, Sanjeev},
  Booktitle                = {Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on},
  Year                     = {2011},
  Annote                   = {Probably first RNN language model (Recurrent Neural net).
Input is One Hot,
Output is a proability distribution over classes of the next word,
(from which a uni

Output is Probabilituy distribution over some number of word classes,
where words are assigned to classes based on frequencuy (so each class containes the same number of probaility, but the ones containing low frewency words, contain more different words). ()Eg The might be in a class of its own).

Then uses a Probility distribution within the class to choose the word.

This hgive speedup in training (ast a small loss of accurasy)},
  Organization             = {IEEE},
  Pages                    = {5528--5531},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5947611}
}

@Inproceedings{mikolovSkip,
  Title                    = {Distributed representations of words and phrases and their compositionality},
  Author                   = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  Booktitle                = {Advances in Neural Information Processing Systems},
  Year                     = {2013},
  Annote                   = {Most of the paper focuses on local structure for relationships, ands is similar ot \cite{mikolov2013linguisticsubstructures}
This phrasal section of the work is not for per sentence but rather small phrases that are reused, so called "idiomatic phrases"

Eg "Austrian Airlines", "New York Times"

As I understand the discussion of phrases mostly focuses on having this short phrases preidentified (Eg by a tokenizer, NER, or via staticical cooccurance (that last one what how they did it),
and treating them as one phrase.},
  Pages                    = {3111--3119},

  Owner                    = {20361362},
  Timestamp                = {2015.04.24},
  Url                      = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@Inproceedings{mikolov2013linguisticsubstructures,
  Title                    = {Linguistic Regularities in Continuous Space Word Representations.},
  Author                   = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  Booktitle                = {HLT-NAACL},
  Year                     = {2013},
  Annote                   = {notes the existence of linear substructures in single-word embeddings for example: v("king")-v("man")+v("woman") has nearest neighbor v("queen")
Where nearest neighber was defined as the cosign similarity.


Experiements were carried out on the use of vector offsets to solve the:
A is to B as C is to ? sytle question.
Best 40% correct.
(Not good enough for use for resythisis)

*May* have been overly difficult since a closely related word might end up nearer.
Eg: Australia is to Canberra as USA is to ?
May have answered New York, instead of Washington -- Washinton and New York both being quiet close (for example) in Vector space.
So having overly dense vocabulary *may* have made the task harder. (Need to check up on Cosign Simliarity)},
  Pages                    = {746--751},

  Owner                    = {20361362},
  Timestamp                = {2015.04.15},
  Url                      = {http://www.msr-waypoint.com/en-us/um/people/gzweig/Pubs/NAACL2013Regularities.pdf}
}

@Inproceedings{mikolov2012contextRNNLM,
  Title                    = {Context dependent recurrent neural network language model.},
  Author                   = {Mikolov, Tomas and Zweig, Geoffrey},
  Booktitle                = {SLT},
  Year                     = {2012},
  Annote                   = {Exention to RNN Language model which track the 
Topic of the document using LDA},
  Pages                    = {234--239},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://www.msr-waypoint.com/en-us/um/people/gzweig/Pubs/SLT2012.pdf}
}

@Inproceedings{Ngiam2011,
  Title                    = {On optimization methods for deep learning},
  Author                   = {Ngiam, Jiquan and Coates, Adam and Lahiri, Ahbik and Prochnow, Bobby and Le, Quoc V and Ng, Andrew Y},
  Booktitle                = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  Year                     = {2011},
  Annote                   = {Discusses why we shouldn't use SGD for deep leanrning adsn should use somes uaw BFGS or CG},
  Pages                    = {265--272},

  __markedentry            = {[20361362:6]},
  Abstract                 = {The predominant methodology in training
deep learning advocates the use of stochastic
gradient descent methods (SGDs). Despite
its ease of implementation, SGDs are diffi-
cult to tune and parallelize. These problems
make it challenging to develop, debug and
scale up deep learning algorithms with SGDs.
In this paper, we show that more sophisti-
cated off-the-shelf optimization methods such
as Limited memory BFGS (L-BFGS) and
Conjugate gradient (CG) with line search
can significantly simplify and speed up the
process of pretraining deep algorithms. In
our experiments, the difference between L-
BFGS/CG and SGDs are more pronounced
if we consider algorithmic extensions (e.g.,
sparsity regularization) and hardware ex-
tensions (e.g., GPUs or computer clusters).
Our experiments with distributed optimiza-
tion support the use of L-BFGS with locally
connected networks and convolutional neural
networks. Using L-BFGS, our convolutional
network model achieves 0.69% on the stan-
dard MNIST dataset. This is a state-of-the-
art result on MNIST among algorithms that
do not use distortions or pretraining.},
  Owner                    = {20361362},
  Timestamp                = {2015.05.19},
  Url                      = {http://www.icml-2011.org/papers/210_icmlpaper.pdf}
}

@Book{WebBookBackprop,
  Title                    = {Neural Networks and Deep Learning},
  Author                   = {Nielsen, Michael A},
  Publisher                = {Determination Press},
  Year                     = {2014},
  Annote                   = {By far best description of backpropergation I have seen.
It is a incomplete work, available online under creative commons.},

  Owner                    = {20361362},
  Timestamp                = {2015.05.17},
  Url                      = {http://neuralnetworksanddeeplearning.com/chap2.html}
}

@Article{Pollack199077,
  Title                    = {Recursive distributed representations },
  Author                   = {Jordan B. Pollack},
  Journal                  = {Artificial Intelligence },
  Year                     = {1990},
  Number                   = {1â€“2},
  Pages                    = {77 - 105},
  Volume                   = {46},

  Abstract                 = {A longstanding difficulty for connectionist modeling has been how to represent variable-sized recursive data structures, such as trees and lists, in fixed-width patterns. This paper presents a connectionist architecture which automatically develops compact distributed representations for such compositional structures, as well as efficient accessing mechanisms for them. Patterns which stand for the internal nodes of fixed-valence trees are devised through the recursive use of backpropagation on three-layer auto-associative encoder networks. The resulting representations are novel, in that they combine apparently immiscible aspects of features, pointers, and symbol structures. They form a bridge between the data structures necessary for high-level cognitive tasks and the associative, pattern recognition machinery provided by neural networks.},
  Doi                      = {http://dx.doi.org/10.1016/0004-3702(90)90005-K},
  ISSN                     = {0004-3702},
  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.sciencedirect.com/science/article/pii/000437029090005K}
}

@Article{pourdamghanialigning,
  Title                    = {Aligning English Strings with Abstract Meaning Representation Graphs},
  Author                   = {Pourdamghani, Nima and Gao, Yang and Hermjakob, Ulf and Knight, Kevin},

  Annote                   = {Seaks to align English strings with words in the AMR.

(Also speaks in introduction about how it would be good just to use a generative model between AMR and Strings, but we don't got one for Strings<-> Graphs).

Has a preprocessing precedure to linearise the AMR, depths first.},

  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.isi.edu/natural-language/mt/amr_eng_align.pdf}
}

@Inproceedings{2009DBM,
  Title                    = {Deep boltzmann machines},
  Author                   = {Salakhutdinov, Ruslan and Hinton, Geoffrey E},
  Booktitle                = {International Conference on Artificial Intelligence and Statistics},
  Year                     = {2009},
  Annote                   = {Has a Boltzman Machine Learning procedure nicely defined.


In terms of Fantasy Particles and Sample
Boltsman machiens have 3 weight matrixes: visible-visible, visible-hidden, and vhidden-hidden. (Unlioke RBM which has the first and last set to zero).

Fiocus is however on deep boltman machines (not just plan regular ones).

"... unlike deep belief networks, the approxi-
mate inference procedure, in addition to an initial bottom-
up pass, can incorporate top-down feedback, allowing deep
Boltzmann machines to better propagate uncertainty about,
and hence deal more robustly with, ambiguous inputs."


Shows ghow to use a modifed RBM for pretraining},
  Pages                    = {448--455},

  Owner                    = {20361362},
  Timestamp                = {2015.04.17},
  Url                      = {http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS09_SalakhutdinovH.pdf}
}

@Article{564201,
  Title                    = {Game theory approach to discrete H infin; filter design},
  Author                   = {Xuemin Shen and Li Deng},
  Journal                  = {Signal Processing, IEEE Transactions on},
  Year                     = {1997},
  Month                    = {Apr},
  Number                   = {4},
  Pages                    = {1092-1095},
  Volume                   = {45},

  Abstract                 = {A finite-horizon discrete H∞ filter design with a linear quadratic (LQ) game approach is presented. The exogenous inputs composed of the “hostile” noise signals and system initial condition are assumed to be finite energy signals with unknown statistics. The design criterion is to minimize the worst possible amplification of the estimation error signals in terms of the exogenous inputs, which is different from the classical minimum variance estimation error criterion for the modified Wiener or Kalman filter design. The approach can show how far the estimation error can be reduced under an existence condition on the solution to a corresponding Riccati equation. A numerical example is given to compare the performance of the H∞ filter with that of the conventional Kalman filter},
  Doi                      = {10.1109/78.564201},
  ISSN                     = {1053-587X},
  Keywords                 = {H∞ optimisation;Kalman filters;Riccati equations;difference equations;discrete time filters;error analysis;filtering theory;game theory;noise;parameter estimation;signal processing;Kalman filter;amplification;difference Riccati equation;discrete H∞ filter design;estimation error signals;exogenous inputs;finite energy signals;finite-horizon discrete H∞ filter;game theory;hostile noise signals;linear quadratic game;modified Wiener filter;system initial condition;unknown statistics;Estimation error;Filtering theory;Game theory;H infinity control;Noise measurement;Nonlinear filters;Riccati equations;Signal design;Signal processing;Statistics},
  Owner                    = {20361362},
  Timestamp                = {2015.05.05}
}

@Article{shenHinftySpeachEnhancement,
  Title                    = {H∞ Filtering for Speech Ehancement},
  Author                   = {Shen, Xuemin and Deng, Li and Yasmin, Anisa},

  Annote                   = {I feel there must be a similar paper to this that has actually been published.

It is woth noting the state matrix, is [x_{n}, x_{n-1}, x_{n-2}]
and the state transition matrix is just the shift of these plus scaling each.
The Observation matrix, simply picks out the current.

This is a pretty great idea, and you see it in many places.},

  Abstract                 = {In this paper, a new approach based on the H∞
filtering is presented
for speech enhancement. This approach differs from the trad
itional
modified Wiener/Kalman filtering approach in the following two as-
pects: 1) no a priori knowledge of the noise statistics is required;
instead the noise signals are only assumed to have finite energy; 2)
the estimation criterion for the filter design is to minimize the worst
possible amplification of the estimation error signal in terms of the
modeling errors and additive noises. Since most additive noises in
speech are not Gaussian, this approach is highly robust and is more
appropriate in practical speech enhancement. The global signal-to-
noise ratio (SNR), time domain speech representation and listening
evaluations are used to verify the performance of the H∞ filtering
algorithm. Experimented results show that the filtering performance
is better than other speech enhancement approaches in the
literature under similar experimental conditions.},
  Owner                    = {20361362},
  Timestamp                = {2015.05.07},
  Url                      = {http://research.microsoft.com/pubs/194586/ShenDengYasmin1996.pdf}
}

@Book{BookOptimalStateEstimation,
  Title                    = {Optimal state estimation: Kalman, H infinity, and nonlinear approaches},
  Author                   = {Simon, Dan},
  Publisher                = {John Wiley \& Sons},
  Year                     = {2006},
  Annote                   = {Nice book.
In the science library.

Covers most things on Kalman, EKF, UKF, H infinity, and particle filters.},

  Owner                    = {20361362},
  Timestamp                = {2015.05.05}
}

@Article{SimonsFeatureArticleHInfinity,
  Title                    = {From Here to Infinity},
  Author                   = {Dan Simon},
  Journal                  = {Embedded Systems Programming},
  Year                     = {2001},

  Annote                   = {Nice.
Written in Feature Article Style.

Some of the math formatting is messed up.

Similar to the description of the filter from his book: \cite{BookOptimalStateEstimation}.

The last section in the article talks about how to handle poor knowledge of the system function.},
  Note                     = {Is a Feature article},

  Owner                    = {20361362},
  Timestamp                = {2015.05.05},
  Url                      = {http://academic.csuohio.edu/simond/courses/eec641/hinfinity.pdf}
}

@Phdthesis{socher2014recursive,
  Title                    = {Recursive Deep Learning for Natural Language Processing and Computer Vision},
  Author                   = {Socher, Richard},
  School                   = {Stanford University},
  Year                     = {2014},
  Annote                   = {see also: http://lxmls.it.pt/2014/socher-lxmls.pdf


Currently up to chapter 3.},

  File                     = {:..\\annotated_documents\\socher_thesis.pdf:PDF},
  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://nlp.stanford.edu/~socherr/thesis.pdf}
}

@Incollection{SocherEtAl2013:CVG,
  Title                    = {Parsing With Compositional Vector Grammars},
  Author                   = {Richard Socher and John Bauer and Christopher D. Manning and Andrew Y. Ng},
  Booktitle                = {ACL},
  Year                     = {2013},
  Annote                   = {This is the improved RvNN parser that combines it with Grammar,
This is what is in the Stanford Parser.},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.socher.org/uploads/Main/SocherBauerManningNg_ACL2013.pdf}
}

@Incollection{Socher2013TensorReasoning,
  Title                    = {Reasoning With Neural Tensor Networks For Knowledge Base Completion},
  Author                   = {Richard Socher and Danqi Chen and Christopher D. Manning and Andrew Y. Ng},
  Booktitle                = {Advances in Neural Information Processing Systems 26},
  Year                     = {2013},
  Annote                   = {Simarlar to mikolov2013linguisticsubstructures
Instead of , A is to B as C is to D (as in Mikolov et al)

This reasons in triples of a knowledge base,
trying to create more triples.

Going form a large number of (A,R,B) where R is a relationship between A and B.
To having even more of these.},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.15},
  Url                      = {http://nlp.stanford.edu/~socherr/SocherChenManningNg_NIPS2013.pdf}
}

@Inproceedings{Socher2010,
  Title                    = {Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora},
  Author                   = {Socher, Richard and Fei-Fei, Li},
  Booktitle                = {Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on},
  Year                     = {2010},
  Annote                   = {This is heavily inspired by machine translation.
Image is mapped to Visual Words, which are then moved to a common vector space that both visual words and textural words are mapped to .

Does Not Use Word Embedding, I think??},
  Organization             = {IEEE},
  Pages                    = {966--973},

  __markedentry            = {[20361362:]},
  Owner                    = {20361362},
  Timestamp                = {2015.04.23},
  Url                      = {http://www.socher.org/uploads/Main/SocherFeiFei_CVPR2010.pdf}
}

@Incollection{SocherEtAl2011:PoolRAE,
  Title                    = {Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection},
  Author                   = {Richard Socher and Eric H. Huang and Jeffrey Pennington and Andrew Y. Ng and Christopher D. Manning},
  Booktitle                = {{Advances in Neural Information Processing Systems 24}},
  Year                     = {2011},

  Owner                    = {20361362},
  Timestamp                = {2015.04.24},
  Url                      = {http://www.socher.org/uploads/Main/SocherHuangPenningtonNgManning_NIPS2011.pdf}
}

@Inproceedings{Socher2011ParsingPhrases,
  Title                    = {Parsing natural scenes and natural language with recursive neural networks},
  Author                   = {Socher, Richard and Lin, Cliff C and Manning, Chris and Ng, Andrew Y},
  Booktitle                = {Proceedings of the 28th international conference on machine learning (ICML-11)},
  Year                     = {2011},
  Annote                   = {Defines a tree of recurrsively applied neural networks for parsing.

Each network has:
2 split inputs, $c_i$, $c_j$ each 1 hot word vector
each goto $L$: the word embeeding matrix, with the output then concatenated which goes
to $W$: the hidden layer 

output of $W$ is $p_{i,j}$ -- the merged word vector
which also goes to two places:
To $W_{score}$ -- the score of how good the merge was
and To $W_{label}$ -- the softmaxed Part of Speech (POS) tagging layer


The paper describes how the parse trees are selected.
It also (poorly) decribes how the learning is carried out (to understand will need to chase up references).

-----
This set of slides: http://lxmls.it.pt/2014/socher-lxmls.pdf
Describes all things quiet well},
  Pages                    = {129--136},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://nlp.stanford.edu/pubs/SocherLinNgManning_ICML2011.pdf}
}

@Inproceedings{socher2010PhraseEmbedding,
  Title                    = {Learning continuous phrase representations and syntactic parsing with recursive neural networks},
  Author                   = {Socher, Richard and Manning, Christopher D and Ng, Andrew Y},
  Booktitle                = {Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop},
  Year                     = {2010},
  Annote                   = {Sorcher and Mannings earlier work.
Predecessor to \cite{socher2010PhraseEmbedding} Likely a better paper to understand that due to its earlier nature.


Embeeds Phrases via Parse of speach 


Does Assess Nerest Neighbour Phrases, which is interesting.
This is done by Embeeding a chunk of Wall Street Journal, then finding nearest neighbours on using a undsiclosed metric (Possibly equclidean)

Claims: "parser can accurately recover tree structures using only the distributed phrase representations. Furthermore, it provides semantic information even" But I can't see how.},
  Pages                    = {1--9},

  Keywords                 = {RvNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://wuawua.googlecode.com/files/Learning%20Continuous%20Phrase%20Representations%20and%20Syntactic%20Parsing%20with%20Recursive%20Neural%20Networks.pdf}
}

@Inproceedings{SocherEtAl2011:RAE,
  Title                    = {Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions},
  Author                   = {Richard Socher and Jeffrey Pennington and Eric H. Huang and Andrew Y. Ng and Christopher D. Manning},
  Booktitle                = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  Year                     = {2011},
  Annote                   = {Similar to the RvNN, the Recursive Auto Encoder applies an Auto Encoder while generating the tree, in the place of the neural network shown in[fig:The-Neural-Net]. Roughly replacing the Max-Margin derived scoring matrix with a reconstruction error.

The tree selected is selecting much the say way as for the RvNN -- by merge resulting in best reconstruction.

It is noted that if the left child (for example) is a node made by merging 5 children and the right a node by merging only 2, then it is more important to correctly reconstruct the left child than the right. The weighting function in the RAE in that paper is adjusted as such. 

It can be extended in a similar way to the RvNN by adding additional trained outputs at each node.

It is not purely greedy, on simply training the to minimize the error of the merge at each node. Like the RvNN, it uses BPTS, and attempt to minimize the sum of all errors across the tree. Unlike the RvNN, there is no target structure -- it is free to use what ever structure minimizes total error.},

  Keywords                 = {RAE},
  Owner                    = {20361362},
  Timestamp                = {2015.04.24},
  Url                      = {http://www.socher.org/uploads/Main/SocherPenningtonHuangNgManning_EMNLP2011.pdf}
}

@Techreport{export:226586,
  Title                    = {Unsupervised Learning of Word Semantic Embedding using the Deep Structured
 Semantic Model},
  Author                   = {Xinying Song and Xiaodong He and Jianfeng Gao and Li Deng},
  Year                     = {2014},
  Annote                   = {A method for learning Word Embeddings, using DNNs.

Bag of words based.
Embed the whole bag of words, in a document.
Use cosine similarity to embedding of bag of wards in query.},
  Month                    = {August},
  Number                   = {MSR-TR-2014-109},

  Owner                    = {20361362},
  Publisher                = {Microsoft Research},
  Timestamp                = {2015.05.26},
  Url                      = {http://research.microsoft.com/apps/pubs/default.aspx?id=226586}
}

@Article{CulturomicOpenProblems,
  Title                    = {Visions and open challenges for a knowledge-based culturomics},
  Author                   = {Tahmasebi, Nina and Borin, Lars and Capannini, Gabriele and Dubhashi, Devdatt and Exner, Peter and Forsberg, Markus and Gossen, Gerhard and Johansson, Fredrik D and Johansson, Richard and K{\aa}geb{\"a}ck, Mikael and others},
  Journal                  = {International Journal on Digital Libraries},
  Year                     = {2015},

  Annote                   = {A collection of opentasks and motivations for the field of culturomics,
which involves doign large text processing, taking into acount development of culture over time.},
  Number                   = {2-4},
  Pages                    = {169--187},
  Volume                   = {15},

  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2015.04.17},
  Url                      = {http://link.springer.com/article/10.1007/s00799-015-0139-1/fulltext.html}
}

@Inproceedings{Taskar2004,
  Title                    = {Max-Margin Parsing.},
  Author                   = {Taskar, Ben and Klein, Dan and Collins, Michael and Koller, Daphne and Manning, Christopher D},
  Booktitle                = {EMNLP},
  Year                     = {2004},
  Annote                   = {This is NOT a good paper to read to help undersstand Socher2011.
It is a predecessor to it, but not direct enough.
It does not example the max-margin method well (but reference a baper that does)},
  Number                   = {1.1},
  Organization             = {Citeseer},
  Pages                    = {3},
  Volume                   = {1},

  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://ai.stanford.edu/~koller/Papers/Taskar+al:EMNLP04.pdf}
}

@Article{webber2006short,
  Title                    = {A short introduction to the Penn Discourse Tree Bank},
  Author                   = {Webber, Bonnie and Joshi, Aravind and Miltsakaki, Eleni and Prasad, Rashmi and Dinesh, Nikhil and Lee, Alan and Forbes, Katherine},
  Journal                  = {COPENHAGEN STUDIES IN LANGUAGE},
  Year                     = {2006},

  Annote                   = {Explains why we have binary trees.


base-NPs 
CCG combinational Categorical Grammar},
  Pages                    = {9},
  Volume                   = {32},

  Owner                    = {20361362},
  Publisher                = {Samfundslitteratur},
  Timestamp                = {2015.05.08},
  Url                      = {http://sydney.edu.au/engineering/it/~dvadas1/papers/vadas07_np_data_penn_treebank.pdf}
}

@Article{MemoryNN,
  Title                    = {Memory Networks},
  Author                   = {Jason Weston and
 Sumit Chopra and
 Antoine Bordes},
  Journal                  = {CoRR},
  Year                     = {2014},

  Annote                   = {A generalised model for a memory network.
Application in question answering.
Controls its own knowlege base with neurons.

Interestingly has a leaned tokeniser (segementor as it is called in text) as part of its write operation.

Example of expiment:
"Joe went to the kitchen. Fred went to the kitchen. Joe picked u
p the milk.
Joe travelled to the office. Joe left the milk. Joe went to the b
athroom.
Where is the milk now?
A: office
Where is Joe?
A: bathroom
Where was Joe before the office?
A: kitchen"},
  Volume                   = {abs/1410.3916},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/WestonCB14},
  Owner                    = {20361362},
  Timestamp                = {2015.04.12},
  Url                      = {http://arxiv.org/abs/1410.3916}
}

@Article{RNC:RNC234,
  Title                    = {Robust filtering for a class of discrete-time uncertain nonlinear systems: An H∞ approach},
  Author                   = {Xie, Lihua and De Souza, Carlos E. and Wang, Youyi},
  Journal                  = {International Journal of Robust and Nonlinear Control},
  Year                     = {1996},
  Number                   = {4},
  Pages                    = {297--312},
  Volume                   = {6},

  Abstract                 = {This paper deals with the H∞ filtering problem for a class of discrete-time nonlinear systems with or without real time-varying parameter uncertainty and unknown initial state. For the case when there is no parametric uncertainty in the system, we are concerned with designing a nonlinear H∞ filter such that the induced l2 norm of the mapping from the noise signal to the estimation error is within a specified bound. It is shown that this problem can be solved via one Riccati equation. We also consider the design of nonlinear filters which guarantee a prescribed H∞ performance in the presence of parametric uncertainties. In this situation, a solution is obtained in terms of two Riccati equations.},
  Doi                      = {10.1002/(SICI)1099-1239(199605)6:4<297::AID-RNC234>3.0.CO;2-V},
  ISSN                     = {1099-1239},
  Keywords                 = {robust filtering, H∞ filtering, uncertain systems, discrete-time systems, nonlinear systems},
  Owner                    = {20361362},
  Publisher                = {Wiley Subscription Services, Inc., A Wiley Company},
  Timestamp                = {2015.05.07},
  Url                      = {http://dx.doi.org/10.1002/(SICI)1099-1239(199605)6:4<297::AID-RNC234>3.0.CO;2-V}
}

@Article{1159121,
  Title                    = {Robust H infin; filtering for a class of discrete-time uncertain nonlinear systems with state delay},
  Author                   = {Shengyuan Xu},
  Journal                  = {Circuits and Systems I: Fundamental Theory and Applications, IEEE Transactions on},
  Year                     = {2002},

  Annote                   = {Robust $H/infty$ filters are of interst to me.
This however covers the more general case of Robut H \inftyu filters with time lag in state.},
  Month                    = {Dec},
  Number                   = {12},
  Pages                    = {1853-1859},
  Volume                   = {49},

  Doi                      = {10.1109/TCSI.2002.805736},
  ISSN                     = {1057-7122},
  Keywords                 = {Delay effects;Delay systems;Filtering;Gaussian noise;Nonlinear equations;Nonlinear systems;Riccati equations;Robustness;Sufficient conditions;Uncertain systems},
  Owner                    = {20361362},
  Timestamp                = {2015.05.07}
}

@Article{younes1989parametric,
  Title                    = {Parametric inference for imperfectly observed Gibbsian fields},
  Author                   = {Younes, Laurent},
  Journal                  = {Probability theory and related fields},
  Year                     = {1989},

  Annote                   = {Part of the reason why \cite{2009DBM} became possible.
Not certasin why though.

A very heavy mathematical paper.
I have not got through it all yet.},
  Number                   = {4},
  Pages                    = {625--645},
  Volume                   = {82},

  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2015.04.17},
  Url                      = {http://www.researchgate.net/profile/Laurent_Younes/publication/227121666_Parametric_Inference_for_imperfectly_observed_Gibbsian_fields/links/00b7d5268355a8fdb7000000.pdf}
}

@Article{DBLP:journals/corr/abs-1212-5701,
  Title                    = {{ADADELTA:} An Adaptive Learning Rate Method},
  Author                   = {Matthew D. Zeiler},
  Journal                  = {CoRR},
  Year                     = {2012},

  Annote                   = {Improved over AdaGrad by not depending on a set global learning rate hyper parameter. As well as not having the gradient always decrease overtime.},
  Volume                   = {abs/1212.5701},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1212-5701},
  Keywords                 = {optimisation},
  Owner                    = {20361362},
  Timestamp                = {Wed, 02 Jan 2013 09:49:04 +0100},
  Url                      = {http://arxiv.org/abs/1212.5701}
}

@Inproceedings{zhang2014chinese,
  Title                    = {Chinese poetry generation with recurrent neural networks},
  Author                   = {Zhang, Xingxing and Lapata, Mirella},
  Booktitle                = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  Year                     = {2014},
  Pages                    = {670--680},

  Abstract                 = {We propose a model for Chinese poem
generation based on recurrent neural net-
works which we argue is ideally suited to
capturing poetic content and form. Our
generator
jointly
performs content selec-
tion (“what to say”) and surface realization
(“how to say”) by learning representations
of individual characters, and their com-
binations into one or more lines as well
as how these mutually reinforce and con-
strain each other. Poem lines are gener-
ated incrementally by taking into account
the entire history of what has been gen-
erated so far rather than the limited hori-
zon imposed by the previous line or lexical
n
-grams. Experimental results show that
our model outperforms competitive Chi-
nese poetry generation systems using both
automatic and manual evaluation methods.},
  Keywords                 = {RNN},
  Owner                    = {20361362},
  Timestamp                = {2015.04.21},
  Url                      = {http://emnlp2014.org/papers/pdf/EMNLP2014074.pdf}
}

@Inproceedings{zou2013bilingual,
  Title                    = {Bilingual Word Embeddings for Phrase-Based Machine Translation.},
  Author                   = {Zou, Will Y and Socher, Richard and Cer, Daniel M and Manning, Christopher D},
  Booktitle                = {EMNLP},
  Year                     = {2013},
  Annote                   = {Basic idea is to create bilingual word embeddings, (from unsupervised data??)
then use them with a standard phrasal translator.


Makes use of /cite{liang2006alignment} to align the Embeddings.
Normal Embedding is done 
which are then},
  Pages                    = {1393--1398},

  Owner                    = {20361362},
  Timestamp                = {2015.04.20},
  Url                      = {http://jan.stanford.edu/pubs/emnlp2013_ZouSocherCerManning.pdf}
}

