% This file was created with JabRef 2.10.
% Encoding: Cp1252


@Misc{BlogMaxLikeMaxEnt,
  Title                    = {maximum-likelihood-and-entropy},

  Annote                   = {Uncitable Blog, but nicely written into to max entropy and the relationship between -ve log likelyhood, entropy rate (true entropy), and Kullback-Leiber diverence (relitive entropy, between true and estimate) 

Likelyhood is the joint pdf for all the events ($x_i$) occuring, parameterised by $\theta$, we assume that events are IID.

Arg Max the likelyhood of the emperical samples, to get $\theta$
Equivelently can Argmin the -ve log of the likelyhood.

$\theta_0$ is the true parameter
$\hat{\theta}$ is out estimated parameteri

"mean negative log-likelihood converges to the differential entropy under the true distribution plus the Kullback-Leibler divergence between the true distribution and the distribution we guess at"},
  Author                   = {David Darmon},
  HowPublished             = {blog},

  Keywords                 = {max-entropy},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://thirdorderscientist.org/homoclinic-orbit/2013/4/1/maximum-likelihood-and-entropy}
}

@Inproceedings{goller1996learning,
  Title                    = {Learning task-dependent distributed representations by backpropagation through structure},
  Author                   = {Goller, Christoph and Kuchler, Andreas},
  Booktitle                = {Neural Networks, 1996., IEEE International Conference on},
  Year                     = {1996},
  Annote                   = {This is not a fun paper to read or understand.
It is however cruicial for the understanding of Socher2011

It deals with how to apply Back-propergation Through Structure to Directed Acyclic Graphs (notables including trees).
Which is like Back-propergation Though Time.},
  Organization             = {IEEE},
  Pages                    = {347--352},
  Volume                   = {1},

  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.1968&rep=rep1&type=pdf}
}

@Article{DBLP:journals/corr/GravesWD14,
  Title                    = {Neural Turing Machines},
  Author                   = {Alex Graves and
 Greg Wayne and
 Ivo Danihelka},
  Journal                  = {CoRR},
  Year                     = {2014},

  Annote                   = {Not well typeset. Figures miss-placed -- check they are lining up with text before over thinking it.

Several experiments are using a Neural network which control a memory bank.
Notably every part of the system is differntable allowing graident desent to be used.

Head "Head nearons" that have  read and a write weight matrix to contol how they  interact with the memory store.},
  Volume                   = {abs/1410.5401},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/GravesWD14},
  Owner                    = {20361362},
  Timestamp                = {Sun, 02 Nov 2014 11:25:59 +0100},
  Url                      = {http://arxiv.org/pdf/1410.5401v2.pdf}
}

@Article{Gupta2010,
  Title                    = {A survey of text summarization extractive techniques},
  Author                   = {Gupta, Vishal and Lehal, Gurpreet Singh},
  Journal                  = {Journal of Emerging Technologies in Web Intelligence},
  Year                     = {2010},

  Annote                   = {(Its not not published in a preditory journal, I checked. JETWI is a real thing and Academy Publisher too. It doesn't look nice)
Seems like quiet a nice review of text sumarisation.
focus on extractive, but does define abstractive..},
  Number                   = {3},
  Pages                    = {258--268},
  Volume                   = {2},
  Owner                    = {20361362},
  Timestamp                = {2015.04.17},
  Url                      = {http://learnpunjabi.org/pdf/survey-paper.pdf}
}

@Inproceedings{Kagebaeck2014,
  Title                    = {Extractive summarization using continuous vector space models},
  Author                   = {K{\aa}geb{\"a}ck, Mikael and Mogren, Olof and Tahmasebi, Nina and Dubhashi, Devdatt},
  Booktitle                = {Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)@ EACL},
  Year                     = {2014},
  Annote                   = {Focus in on using word and phrase embeedings to do multidocument extractive summary.

Quote: "To the best of our knowledge, continuous vector
space models have not previously been used in
summarization tasks"


Has a nice summery of the medthods used to produce word embeddings.
Discusses two methos for phrase embeedings: Unfolder Recussive Autoencers (Socher)
And simple addition (done by Mikolov)

Usign the Opinosis dataset, which might be good to look into, it contaiend human gnerated summerys.},
  Pages                    = {31--39},

  __markedentry            = {[20361362:]},
  Owner                    = {20361362},
  Timestamp                = {2015.04.17},
  Url                      = {http://www.aclweb.org/anthology/W14-1504}
}

@Inproceedings{Malouf2002,
  Title                    = {A comparison of algorithms for maximum entropy parameter estimation},
  Author                   = {Malouf, Robert},
  Booktitle                = {proceedings of the 6th conference on Natural language learning-Volume 20},
  Year                     = {2002},
  Annote                   = {This is not useful.
It is a review of different algorithms for maximum entropy estimation,but does not explain how the more compelx ones work.
Does present some data on the comparative speed etc.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {1--7},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://www.coli.uni-saarland.de/groups/HU/HUwm4/Slides/malouf.pdf}
}

@Article{mikolov2013efficient,
  Title                    = {Efficient estimation of word representations in vector space},
  Author                   = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  Journal                  = {arXiv preprint arXiv:1301.3781},
  Year                     = {2013},

  Annote                   = {Compares several methods for finding single word vector repressentations.
A "You shall know a word by the company it keeps" type paper.},

  Owner                    = {20361362},
  Timestamp                = {2015.04.15},
  Url                      = {http://arxiv.org/pdf/1301.3781v3}
}

@Inproceedings{mikolov2013linguisticsubstructures,
  Title                    = {Linguistic Regularities in Continuous Space Word Representations.},
  Author                   = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  Booktitle                = {HLT-NAACL},
  Year                     = {2013},
  Annote                   = {notes the existence of linear substructures in single-word embeddings for example: v("king")-v("man")+v("woman") has nearest neighbor v("queen")
Where nearest neighber was defined as the cosign similarity.


Experiements were carried out on the use of vector offsets to solve the:
A is to B as C is to ? sytle question.
Best 40% correct.
(Not good enough for use for resythisis)

*May* have been overly difficult since a closely related word might end up nearer.
Eg: Australia is to Canberra as USA is to ?
May have answered New York, instead of Washington -- Washinton and New York both being quiet close (for example) in Vector space.
So having overly dense vocabulary *may* have made the task harder. (Need to check up on Cosign Simliarity)},
  Pages                    = {746--751},

  Owner                    = {20361362},
  Timestamp                = {2015.04.15},
  Url                      = {http://www.msr-waypoint.com/en-us/um/people/gzweig/Pubs/NAACL2013Regularities.pdf}
}

@Inproceedings{2009DBM,
  Title                    = {Deep boltzmann machines},
  Author                   = {Salakhutdinov, Ruslan and Hinton, Geoffrey E},
  Booktitle                = {International Conference on Artificial Intelligence and Statistics},
  Year                     = {2009},
  Annote                   = {Has a Boltzman Machine Learning procedure nicely defined.


In terms of Fantasy Particles and Sample
Boltsman machiens have 3 weight matrixes: visible-visible, visible-hidden, and vhidden-hidden. (Unlioke RBM which has the first and last set to zero).

Fiocus is however on deep boltman machines (not just plan regular ones).

"... unlike deep belief networks, the approxi-
mate inference procedure, in addition to an initial bottom-
up pass, can incorporate top-down feedback, allowing deep
Boltzmann machines to better propagate uncertainty about,
and hence deal more robustly with, ambiguous inputs."


Shows ghow to use a modifed RBM for pretraining},
  Pages                    = {448--455},
  Owner                    = {20361362},
  Timestamp                = {2015.04.17},
  Url                      = {http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS09_SalakhutdinovH.pdf}
}

@Phdthesis{socher2014recursive,
  Title                    = {Recursive Deep Learning for Natural Language Processing and Computer Vision},
  Author                   = {Socher, Richard},
  School                   = {Stanford University},
  Year                     = {2014},
  Annote                   = {see also: http://lxmls.it.pt/2014/socher-lxmls.pdf},

  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://nlp.stanford.edu/~socherr/thesis.pdf}
}

@Incollection{Socher2013TensorReasoning,
  Title                    = {Reasoning With Neural Tensor Networks For Knowledge Base Completion},
  Author                   = {Richard Socher and Danqi Chen and Christopher D. Manning and Andrew Y. Ng},
  Booktitle                = {Advances in Neural Information Processing Systems 26},
  Year                     = {2013},
  Annote                   = {Simarlar to mikolov2013linguisticsubstructures
Instead of , A is to B as C is to D (as in Mikolov et al)

This reasons in triples of a knowledge base,
trying to create more tipples.

Going form a large number of (A,R,B) where},

  Owner                    = {20361362},
  Timestamp                = {2015.04.15},
  Url                      = {http://nlp.stanford.edu/~socherr/SocherChenManningNg_NIPS2013.pdf}
}

@Inproceedings{Socher2011,
  Title                    = {Parsing natural scenes and natural language with recursive neural networks},
  Author                   = {Socher, Richard and Lin, Cliff C and Manning, Chris and Ng, Andrew Y},
  Booktitle                = {Proceedings of the 28th international conference on machine learning (ICML-11)},
  Year                     = {2011},
  Annote                   = {Defines a tree of recurrsively applied neural networks for parsing.

Each network has:
2 split inputs, $c_i$, $c_j$ each 1 hot word vector
each goto $L$: the word embeeding matrix, with the output then concatenated which goes
to $W$: the hidden layer 

output of $W$ is $p_{i,j}$ -- the merged word vector
which also goes to two places:
To $W_{score}$ -- the score of how good the merge was
and To $W_{label}$ -- the softmaxed Part of Speech (POS) tagging layer


The paper describes how the parse trees are selected.
It also (poorly) decribes how the learning is carried out (to understand will need to chase up references).

-----
This set of slides: http://lxmls.it.pt/2014/socher-lxmls.pdf
Describes all things quiet well},
  Pages                    = {129--136},

  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://nlp.stanford.edu/pubs/SocherLinNgManning_ICML2011.pdf}
}

@Article{CulturomicOpenProblems,
  Title                    = {Visions and open challenges for a knowledge-based culturomics},
  Author                   = {Tahmasebi, Nina and Borin, Lars and Capannini, Gabriele and Dubhashi, Devdatt and Exner, Peter and Forsberg, Markus and Gossen, Gerhard and Johansson, Fredrik D and Johansson, Richard and K{\aa}geb{\"a}ck, Mikael and others},
  Journal                  = {International Journal on Digital Libraries},
  Year                     = {2015},

  Annote                   = {A collection of opentasks and motivations for the field of culturomics,
which involves doign large text processing, taking into acount development of culture over time.},
  Number                   = {2-4},
  Pages                    = {169--187},
  Volume                   = {15},

  __markedentry            = {[20361362:6]},
  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2015.04.17},
  Url                      = {http://link.springer.com/article/10.1007/s00799-015-0139-1/fulltext.html}
}

@Inproceedings{Taskar2004,
  Title                    = {Max-Margin Parsing.},
  Author                   = {Taskar, Ben and Klein, Dan and Collins, Michael and Koller, Daphne and Manning, Christopher D},
  Booktitle                = {EMNLP},
  Year                     = {2004},
  Annote                   = {This is NOT a good paper to read to help undersstand Socher2011.
It is a predecessor to it, but not direct enough.
It does not example the max-margin method well (but reference a baper that does)},
  Number                   = {1.1},
  Organization             = {Citeseer},
  Pages                    = {3},
  Volume                   = {1},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14}
}

@Article{DBLP:journals/corr/WestonCB14,
  Title                    = {Memory Networks},
  Author                   = {Jason Weston and
 Sumit Chopra and
 Antoine Bordes},
  Journal                  = {CoRR},
  Year                     = {2014},

  Annote                   = {A generalised model for a memory network.
Application in question answering.
Controls its own knowlege base with neurons.

Interestingly has a leaned tokeniser (segementor as it is called in text) as part of its write operation.

Example of expiment:
"Joe went to the kitchen. Fred went to the kitchen. Joe picked u
p the milk.
Joe travelled to the office. Joe left the milk. Joe went to the b
athroom.
Where is the milk now?
A: office
Where is Joe?
A: bathroom
Where was Joe before the office?
A: kitchen"},
  Volume                   = {abs/1410.3916},

  Bibsource                = {dblp computer science bibliography, http://dblp.org},
  Biburl                   = {http://dblp.uni-trier.de/rec/bib/journals/corr/WestonCB14},
  Owner                    = {20361362},
  Timestamp                = {Sun, 02 Nov 2014 11:25:59 +0100},
  Url                      = {http://arxiv.org/abs/1410.3916}
}

@Article{younes1989parametric,
  Title                    = {Parametric inference for imperfectly observed Gibbsian fields},
  Author                   = {Younes, Laurent},
  Journal                  = {Probability theory and related fields},
  Year                     = {1989},

  Annote                   = {Part of the reason why \cite{2009DBM} became possible.
Not certasin why though.

A very heavy mathematical paper.
I have not got through it all yet.},
  Number                   = {4},
  Pages                    = {625--645},
  Volume                   = {82},

  Owner                    = {20361362},
  Publisher                = {Springer},
  Timestamp                = {2015.04.17},
  Url                      = {http://www.researchgate.net/profile/Laurent_Younes/publication/227121666_Parametric_Inference_for_imperfectly_observed_Gibbsian_fields/links/00b7d5268355a8fdb7000000.pdf}
}

