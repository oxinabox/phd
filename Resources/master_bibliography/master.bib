% This file was created with JabRef 2.10.
% Encoding: Cp1252


@Misc{Darmon,
  Title                    = {maximum-likelihood-and-entropy},

  Annote                   = {Uncitable Blog, but nicely written into to max entropy and the relationship between -ve log likelyhood, entropy rate (true entropy), and Kullback-Leiber diverence (relitive entropy, between true and estimate) 

Likelyhood is the joint pdf for all the events ($x_i$) occuring, parameterised by $\theta$, we assume that events are IID.

Arg Max the likelyhood of the emperical samples, to get $\theta$
Equivelently can Argmin the -ve log of the likelyhood.

$\theta_0$ is the true parameter
$\hat{\theta}$ is out estimated parameteri

"mean negative log-likelihood converges to the differential entropy under the true distribution plus the Kullback-Leibler divergence between the true distribution and the distribution we guess at"},
  Author                   = {David Darmon},
  HowPublished             = {blog},

  Keywords                 = {max-entropy},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://thirdorderscientist.org/homoclinic-orbit/2013/4/1/maximum-likelihood-and-entropy}
}

@Inproceedings{goller1996learning,
  Title                    = {Learning task-dependent distributed representations by backpropagation through structure},
  Author                   = {Goller, Christoph and Kuchler, Andreas},
  Booktitle                = {Neural Networks, 1996., IEEE International Conference on},
  Year                     = {1996},
  Annote                   = {This is not a fun paper to read or understand.
It is however cruicial for the understanding of Socher2011

It deals with how to apply Back-propergation Through Structure to Directed Acyclic Graphs (notables including trees).
Which is like Back-propergation Though Time.},
  Organization             = {IEEE},
  Pages                    = {347--352},
  Volume                   = {1},

  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.1968&rep=rep1&type=pdf}
}

@Inproceedings{Malouf2002,
  Title                    = {A comparison of algorithms for maximum entropy parameter estimation},
  Author                   = {Malouf, Robert},
  Booktitle                = {proceedings of the 6th conference on Natural language learning-Volume 20},
  Year                     = {2002},
  Annote                   = {This is not useful.
It is a review of different algorithms for maximum entropy estimation,but does not explain how the more compelx ones work.
Does present some data on the comparative speed etc.},
  Organization             = {Association for Computational Linguistics},
  Pages                    = {1--7},

  __markedentry            = {[20361362:6]},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://www.coli.uni-saarland.de/groups/HU/HUwm4/Slides/malouf.pdf}
}

@Article{mikolov2013efficient,
  Title                    = {Efficient estimation of word representations in vector space},
  Author                   = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  Journal                  = {arXiv preprint arXiv:1301.3781},
  Year                     = {2013},

  Annote                   = {Compares several methods for finding single word vector repressentations.
A "You shall know a word by the company it keeps" type paper.},

  Owner                    = {20361362},
  Timestamp                = {2015.04.15},
  Url                      = {http://arxiv.org/pdf/1301.3781v3}
}

@Inproceedings{mikolov2013linguisticsubstructures,
  Title                    = {Linguistic Regularities in Continuous Space Word Representations.},
  Author                   = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  Booktitle                = {HLT-NAACL},
  Year                     = {2013},
  Annote                   = {notes the existence of linear substructures in single-word embeddings for example: v("king")-v("man")+v("woman")   has nearest neighbor v("queen")
Where nearest neighber was defined as the cosign similarity.


Experiements were carried out on the use of vector offsets to solve the:
A is to B as C is to ? sytle question.
Best 40% correct.
(Not good enough for use for resythisis)

*May* have been overly difficult since a closely related word might end up nearer.
Eg: Australia is to Canberra as USA is to ?
May have answered New York, instead of Washington -- Washinton and New York both being quiet close (for example) in Vector space.
So having overly dense vocabulary *may* have made the task harder. (Need to check up on Cosign Simliarity)},
  Pages                    = {746--751},

  Owner                    = {20361362},
  Timestamp                = {2015.04.15},
  Url                      = {http://www.msr-waypoint.com/en-us/um/people/gzweig/Pubs/NAACL2013Regularities.pdf}
}

@Phdthesis{socher2014recursive,
  Title                    = {Recursive Deep Learning for Natural Language Processing and Computer Vision},
  Author                   = {Socher, Richard},
  School                   = {Stanford University},
  Year                     = {2014},
  Annote                   = {see also: http://lxmls.it.pt/2014/socher-lxmls.pdf},

  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://nlp.stanford.edu/~socherr/thesis.pdf}
}

@Incollection{Socher2013TensorReasoning,
  Title                    = {Reasoning With Neural Tensor Networks For Knowledge Base Completion},
  Author                   = {Richard Socher and Danqi Chen and Christopher D. Manning and Andrew Y. Ng},
  Booktitle                = {Advances in Neural Information Processing Systems 26},
  Year                     = {2013},
  Annote                   = {Simarlar to mikolov2013linguisticsubstructures
Instead of , A is to B as C is to D (as in Mikolov et al)

This reasons in triples of a knowledge base,
trying to create more tipples.

Going form a large number of (A,R,B) where},

  Owner                    = {20361362},
  Timestamp                = {2015.04.15},
  Url                      = {http://nlp.stanford.edu/~socherr/SocherChenManningNg_NIPS2013.pdf}
}

@Inproceedings{Socher2011,
  Title                    = {Parsing natural scenes and natural language with recursive neural networks},
  Author                   = {Socher, Richard and Lin, Cliff C and Manning, Chris and Ng, Andrew Y},
  Booktitle                = {Proceedings of the 28th international conference on machine learning (ICML-11)},
  Year                     = {2011},
  Annote                   = {Defines a tree of recurrsively applied neural networks for parsing.

Each network has:
2 split inputs, $c_i$, $c_j$ each 1 hot word vector
each goto $L$: the word embeeding matrix, with the output then concatenated which goes
to $W$: the hidden layer 

output of $W$ is $p_{i,j}$ -- the merged word vector
which also goes to two places:
To $W_{score}$ -- the score of how good the merge was
and To $W_{label}$ -- the softmaxed Part of Speech (POS) tagging layer


The paper describes how the parse trees are selected.
It also (poorly) decribes how the learning is carried out (to understand will need to chase up references).

-----
This set of slides: http://lxmls.it.pt/2014/socher-lxmls.pdf
Describes all things quiet well},
  Pages                    = {129--136},

  Owner                    = {20361362},
  Timestamp                = {2015.04.14},
  Url                      = {http://nlp.stanford.edu/pubs/SocherLinNgManning_ICML2011.pdf}
}

@Inproceedings{Taskar2004,
  Title                    = {Max-Margin Parsing.},
  Author                   = {Taskar, Ben and Klein, Dan and Collins, Michael and Koller, Daphne and Manning, Christopher D},
  Booktitle                = {EMNLP},
  Year                     = {2004},
  Annote                   = {This is NOT a good paper to read to help undersstand Socher2011.
It is a predecessor to it, but not direct enough.
It does not example the max-margin method well (but reference a baper that does)},
  Number                   = {1.1},
  Organization             = {Citeseer},
  Pages                    = {3},
  Volume                   = {1},

  __markedentry            = {[20361362:]},
  Owner                    = {20361362},
  Timestamp                = {2015.04.14}
}

