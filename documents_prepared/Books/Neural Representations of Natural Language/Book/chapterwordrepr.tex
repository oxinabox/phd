\documentclass[parskip]{komatufte}
\include{preamble}

\graphicspath{{./figs/chapterwordrepr}, {./}}


\begin{document}
\chapter{Word Representations}\label{sec:word-representations}
\begin{abstract}
The core work that began to draw a lot of attention to this area.
This will cover crucial works such as skip-grams, CBOW, and of course the original neural probabilistic language model.
As well as the links to traditional matrix factorisations.
\end{abstract}

\aside[Word Vector or Word Embedding?]{In some older literature in the area will use the term \emph{word~vector} to refer to representations from LDA and LSA etc. Whereas in a lot of newer work the term is used synonymously with \emph{word~embedding}.}
We begin the consideration of the representation of words using neural networks with work on language modeling
This is not the only place one could begin the consideration: the information retrieval (IR) models such as LSA \pcite{dumais1988using} and LDA \pcite{blei2003latent} based on word co-location with documents would be the other obvious starting point.
However, as will be discussed there models are closer to the end point of of the chapter than the begin.
For the language modeling works, comes the contextual language model works such as skip-gram, which in turn lead to the post-neural network co-occurrence based works.
These model fundamentally are in fact more similar to the information retrieval co-location works than the language modeling from which we began.


\section{Representations for Language Modeling}

\aside[Random Variable Naming Convention]{We follow convention that capitalised $W_i$ is a random variable, and $w_i$ is a particular value which $W_i$ may take. Writing $P(\ldots W_i{=}w_i)$.
	Often though we will make uses of the abridged (abuse-of) notation and simply write $P(\ldots w_i)$, allowing the random variable to be implicitly given by the name of its value.
}

The language modeling task it to predict the next word given the words that came prior \pcite{rosenfeld2000two}.
For example, if a sentence begins \natlang{For lunch I will have a hot}, then there is a high probability that the next word will be \natlang{dog} or \natlang{meal},
and lower probabilities of words such as \natlang{day} or \natlang{are}.
Mathematically it is formulated: 
\begin{equation}\label{equ:langmodel}
	P(W_n{=}w_i \mid W_{i-1}{=}w_{i-1}, \ldots, W_1{=}w_1)
\end{equation}
or to use the compact notation 
\begin{equation}\label{equ:langmodelcompact}
P(w_i \mid w_{i-1}, \ldots, w_1)
\end{equation}
$P(w_i \mid \natlang{hot}, \natlang{a}, \natlang{want}, \natlang{I}, \natlang{lunch}, \natlang{For})$.
The task is to find the probabilities for the various words that $w_i$ could represent.

\aside[Google n-gram corpora]{Google has created several very large scale corpora of 1,2,3,4, and 5-grams from over $10^{11}$ words.
Including the 2006 Web corpus from the public internet (Available for fees at \url{https://catalog.ldc.upenn.edu/LDC2006T13}),
and the 2010-2012 Books corpus made words from the Google Books project (available freely from \url{https://books.google.com/ngrams/datasets}), as described in \textcite{lin2012syntactic}.
These very large corpora also used outside of statistical language modeling by corpus linguists investigating the use of language.
}

  
The classical approach is trigram statistical language modeling.
In this, the number of word triples in a corpus is counted,
then given the previous two words, the next is predicted based on those counts.
This is making the Markov assumption that the next state depends only on the current state,
and that that state can be described by previous two words.
Under this assumption \Cref{equ:langmodelcompact} becomes:
\begin{equation}\label{equ:trigramlangmodel}
P(w_i \mid w_{i-1}, \ldots, w_1) = P(w_i \mid w_{i-1}, w_{i-2})
\end{equation}
More generally, one can use $n$-gram language model where for any value if $n$,
this is simply a matter of defining the Markov state to contain fewer or greater numbers of previous words than in the trigram case.

This Markov assumption is, of-course, an approximation.
In the previous example, a trigram language model is finding $P(w_i \mid \natlang{hot}, \natlang{a})$.
It can be seen that the approximation has lost key information.
Based only on the previous 2 words the next word $w_i$ could now reasonably be \natlang{day}, but the sentence: \natlang{For lunch I will have a hot day} makes no sense.
However, the Markov assumption in using $n$-grams is required in order to make the problem tractable -- otherwise an unbound amount of information would need to be stored. 

They key issue with n-gram language models in that in training them there is a data-sparsity problem.
Most combinations of words occur very rarely \pcite{ha2009extending}.
It is thus hard to estimate their occurrence probability.
Combinations of words that do not occur in the model is derived from as naturally given a probability of zero.
This is unlikely to be true though -- it is simply a matter of rare words never occurring together in a finite corpus.
Several approaches have been taken to handle this.
Simplest is add-one smoothing which adds an extra pseudo-observation of every combination of terms.
In common use is are various back-off methods \pcite{katz1987estimation,kneser1995improved} which use the bigram probabilities to estimate the probabilities of unseen trigrams.
%
\aside{An extended look at classical techniques in statistical language modelling can be found in \textcite{DBLP:journals/corr/cs-CL-0108005}}
%
However, these methods are merely clever statistical tricks -- ways to reassign probability mass after the fact to leave some left-over for unseen cases.
Back-off is smarter than add-one smoothing, as it portions the probability fairly based of the $n{-}1$-gram probability.
Better still would be a method which can learn to see the common-role of words.
By looking at the fragment: \natlang{For lunch I want a hot}, any reader knows that the next word is most likely going to be a food.
We know this for the same reason we know the next word in \natlang{For elevenses  I had a cold ...} is also going to be a food.
Even though \natlang{elevenses} is a vary rare word, we know from the context that it is a meal (more on this later), and we know it shares other traits with meals, and similarly \natlang{have} / \natlang{had}, and \natlang{hot} / \natlang{cold}.
These traits influence the words that can occur after them.
Thus the motivation is for a language modeling method that use these common properties of the words in the language modeling.
We need representation that holds this information.

\subsection{The Neural Probabilistic Language Model}

\tcite{NPLM} presents a method that uses neural network representations in a language model, where this representations implicitly learn the crucial traits of words.
Capturing the trait information in the representation of a word for use in language modeling is not an idea exclusive to neural network approaches.
Almost a decade earlier was the clustering work of  \tcite{brown1992class}.
The method of \tcite{NPLM} however, implicitly learns this information into a neural network bases representation in during the training for the language modeling task.

To understand the neural probabilistic language model, first consider a simplified neural trigram language model.
This model is a simplification of that introduces by \tcite{NPLM}, it forms the basis of several models and introduces the most important idea in neural language representations: that of training a vector representation of a word using a lookup table to map a discrete word to a vector that becomes the first layer of the network.



\aside[Lookup word embeddings: Hashmap or Array?]{
The question is purely one of implementation.
Theoretically we do not care if the implementation using a String to Vector dictionary (eg a Hashmap), or an 2D Array which is indexed into via an integer index representing the word, to retrieve a column.
In tokenization of the source text it is common to transform all the words into integers, so as to save memory, especially if the tokenization does not make use of string interning.
At that point it makes sense to work with a Array.
For our purposes we will treat the word $w_i$ as if it were an integer index, though thinking of it as a string index into a hashmap changes little in the logic.
}

\aside[One-hot product or Indexed-lookup]{
In some works you may see the process of retrieving the word vector from an Matrix of word vectors described as a one-hot multiplication.
That is to say for a word represented by the index $i$,
for $e_i$ the one-hot vector with a 1 in the $i$th position,
and for $C$ the table of word embeddings.
one can write  $C\,e_i$.
We will write $C_i$ and refer to this as looking up the word vector from the $i$th column.
Of-course $C_i=C\,e_i$, however in practical implementation the performance ramification are huge.
A matrix column indexing is a $O(1)$ operation (for column major languages),
where as a dense matrix-vector product is $O(n^2)$.
%
The one-hot product can be used in a pinch to support using embeddings in neural network toolkits that do not support lookup/embedding layers,
however we strongly suggest that if your toolkit does not support lookup/embedding layers then it is unsuitable for use in NLP applications.
}

The trigram neural probabilistic network is defined by:
%
\begin{multline}
P(w_i \mid w_{i-1}, w_{i-2}) = \\
s_{max}\left(V \: \varphi\left(U\left[C_{w_{i-1}}; C_{w_{i-2}}\right] + b_1\right)+b_2\right)
\end{multline}
%
where $U,\: V,\: b_1,\: b_2$ are the weight matrices and biases of the network, and $C_{w_{i-1}}$ and $C_{w_{i-2}}$ are the looked-up word embedding for the previous two words, from the embedding table $C$.
We write $\left[v_1; v_2\right]$ as a notational convenience for the concatenation of vectors $v_1$ and $v_2$.
The network is shown diagrammatically in \Cref{fig:trigram-neural-language-model}

\begin{figure}
	\centering
	\input{figs/chapterwordrepr/trigram-neural-language-model.tex} 
	\caption{\label{fig:trigram-neural-language-model} Neural Trigram Language Model}
\end{figure}

In the neural trigram language model, each of the previous two words is used to look-up a vector from the Embedding Table matrix.
These are then concatenated to give a dense, continuous-space input to the above hidden layer.
The output layer is a softmax layer, it gives the probabilities for each word in the vocabulary, one of which corresponds to $w_i$ the word being evaluated.

The word embeddings are trained, via the normal neural network method, along with the network weights and biases.
This allows the embeddings of words which predict the same future word to move to be near each other in vector space.
It also allows the hidden layer to learn to associate information with regions of the embedding space.
This thus allows for information sharing between words.
If two words vectors are close together because they mostly predict the same future words, then that area of space is a associated with predicting those words.
Thus for cases where there are some words which never co-occur in the training set, but for which the nearby word does co-occur with, then because the knowledge is associated with that region of space, the model predicts that there is a higher-than-zero chance of the first word co-occurring with the unseen word.
This is a fuzzy and nonlinear relationship, with varying degrees of closeness resulting in (and from) varying levels of exception of the words sharing co-occurring predicted words.


\tcite{NPLM} is a more advanced version of the neural language model discussed.
Rather than being a trigram language model, it is an $n$-gram language model, where $n$ is a hyper-parameter of the model.
The knowledge sharing allows the data-sparsity issues to be ameliorated, thus allowing for $n>3$.
%
\aside{\textcite{schwenk2004efficient} suggests using only a subset of the vocabulary as options for the output, while allowing the full vocabulary in the input space -- with a fallback to classical language models for the missed words.
	This decreases the size of the softmax output layer, substantially decreasing the network evaluation and training time.
	This technique is now largely eclipsed by hierarchical softmax etc. discussed later in this chapter.
}
%
Bengio et. al. investigates values for $n$ of 3, 5 and 6.
The network used in their work also was marginally more complex, as shown in \Cref{fig:neural-language-model} and in the following equation:
\begin{align}
P(w_i & \mid w_{i-1}, w_{i-2}) = s_{max}( \nonumber
\\  & \quad U_3 \left[ C_{w_{i-1}};...; C_{w_{i-n}}\right] \nonumber
\\  & + V \: \varphi\left( U\left[C_{w_{i-1}};...; C_{w_{i-n}} \right] + b_1\right) \nonumber
\\  & +b_2)
\end{align}

It includes a layer-bypass, which is primarily present only as connivance to aid in the learning.
It allowing the input to directly effect the output without being mediated by the shared hidden layer.
This layer-bypass is a unusual feature, not present in future works deriving from this, such as \tcite{schwenk2004efficient} which presents a bag of tricks to minimize training time. 





This is the network which begins the notions of using neural networks with vector representations of words.
Bengio et. al. focused on the use of the of sliding window of previous words -- much like the traditional n-grams.
At each time-step the window is advanced forward and the next word predicted based on that shifted context of prior words.
They do very briefly mention that an RNN could be used in its place.

\begin{figure}
	\centering
	\input{figs/chapterwordrepr/neural-language-model.tex} 
	\caption{\label{fig:neural-language-model} Neural Probabilistic Language Model}
\end{figure}


\subsection{RNN Language Models}

\aside[Differences in formulation]{
The network and equations shown here may seem different to those presented in \textcite{mikolov2010recurrent}.
However they are mathematically identical.
The apparent difference is simply the choice between considering embeddings as a look-up, or as a one-hot multiplication.
}

In \tcite{mikolov2010recurrent} an RNN is for a language model.
To be precise, an Elman network is used \tcite(elman1990finding).
In this form of RNN, the output of hidden layer in the previous time-step is appended as an additional input.
Using an RNN eliminates the sliding window of earlier language models.
Instead, the context information is stored in the state that is passed forward.

This state $Z_{j}$ being the hidden state output from the first (only) hidden layer when input the $j$th word, 
is given by 

\begin{equation}
	Z_{j} = \varphi\left( U\,Z_{j-1} + C_{w_{i-1}} \right)
\end{equation}


This hidden layer was the an input to the hidden-layer at the next time-step, as well as to the output softmax.

\begin{equation}
	P(w_i \mid w_{i-1}, ... w_{1}) = s_{max}\left(V \, Z_{i-1} \right)
\end{equation}



Rather than using an Elman network, a more advanced RNN such as Long-Short-Term-Memory (LSTM), or a Gated Recurrent Unit (GRU) \pcite{cho2014properties}, based network could be used.
The structure would remain very similar,
and additional advantages may be found from the improved stability and performance.

\tcite{sundermeyer2012lstm} investigated using LSTM.
This did indeed offer an improvement over the Elman network used in \textcite{mikolov2010recurrent}.
\tcite{jozefowicz2015empirical} conducted an extensive search of hyper-parameters for LSTM and GRU networks for language modeling.
For the best performing networks of both types the results found were significantly better than \textcite{sundermeyer2012lstm} and \textcite{mikolov2010recurrent}.
They found the best was the LSTM network, particularly if the forget gate was biased to a 1.0 or more.

\begin{figure}
	\centering
	\input{figs/chapterwordrepr/rnn-language-model2010.tex} 
	\caption{\label{fig:neural-language-model} RNN Language Model}
\end{figure}

This ends our discussion of standard language models.


\section{Acausal Language Modeling}
The step beyond a normal language model,
which uses the prior words to predict the next word,
is what we will term acausal language modelling.
Here we use the word acausal in the signal processing sense.
The task here is to predict a missing word, using the words the preceed it, as well as the words that come after it.

As it is acausal it can not be done in a real-time system, and is not useful for many tasks directly.
However, what it is very useful for is as a task to learn a good representation for words.


\subsection{Context RNN}


\aside[Are CBOW \& Skip-Gram Neural Networks]{As these models do not have any non-linearities  (some would say they have no hidden layer, though is is incorrect: the projection layer of the embedding is functionally a hidden layer) it may be asserted that they are not in-fact neural networks at all.
This distinction is purely academic though.
Any toolkit that can well handle the prior discussed neural network models, can be very well used to implement CBOW and Skip-Gram.
}


\pdfcomment{I think CBOW and Skip-Gram don't feature Biases because they are written in Design Matrix form with a column of ones. If so I need to fix that up to something consistent with the remainder of this chapter or if not then }

\subsection{CBOW}
The continuous bag of words (CBOW) method was introduced by \tcite{mikolov2013efficient}.
In truth this is not particularly similar to bag of words at all, no more so than any other word representation that does not have regard for order of the context words (e.g. skip-gram, GloVE below).


For a context window of width $n$ words -- i.e. $\frac{n}{2}$ words to either side, of the target word,
the CBOW model is defined by:
\begin{align}
P(w_i & \mid w_{i-\frac{n}{2}},..., w_{i-1}, w_{i+1},...,w_{i+\frac{n}{2}})  \nonumber
\\  & = s_{max}(U \sum_{j=i+1}^{j=\frac{n}{2}} \left( C_{w_{i-j}}+C_{w_{i+j}} \right))
\end{align}

\pdfcomment{Need to consider that this is always done with a Hierarchical softmax, and check this formulation is correct}


\tcite{mikolovSkip}

\subsection{Skip-gram}
\tcite{mikolovSkip}

\aside[Skip-gram naming]{In different publications this model may be called skipgram skip-gram, skip-ngram, skip-gram etc. Further, it may be called \texttt{word2vec} after the publicly released implementation of the algorithm. Though that software also can be used for CBOW.}


The converse of CBOW is the skip-grams model \tcite{mikolov2013efficient}.
In this model, the central word is used to predict the words in the context.
While in CBOW each training case of context and target word presents one correct answer,
in the Skip-gram each training case has a number of correct answers, the probability of all of which in the softmax must be increased.

\begin{equation}
P(w_j \mid w_{i}) = \left[ s_{max}(V\,C_{w_{i}}) \right]_{w_j} 
\end{equation}


The goal, is to maximise the probabilities of all the observed outputs from the window's in the training set ($i-\frac{n}{2},...,i-1, i+i,...,i+\frac{n}{2}$).


This can be done using softmax, or hierarchical softmax (as in CBOW and other models).
Or an alternative option is suggested, negative sampling.

Note that Skip-Grams only hidden layer is the embedding layer.
If we take the earlier discussed notion of considering the output matrix $V$ as being an embedding layer/
This becomes:

\begin{align}
P(w_j \mid w_{i}) & = \left[ s_{max}(V\,C_{w_{i}}) \right]_{w_j} \\
P(w_j \mid w_{i}) & = \frac{\exp(V_{w_j}^\prime\,C_{w_{i}})}{\sum_{k=1}^{k=N} \exp(V_k^\prime\,C_{k})}
\end{align}

The key term here is the $V_{w_j}^\prime\,C_{w_{i}}$,
the remainder of the expression is normalising this into a probability.
Maximising the probability $P(w_j \mid w_{i})$ is equivalent to maximising the dot produce between $V_{w_j}^\prime$, the output embedding for $w_j$ and  $C_{w_i}$ the input embedding for $w_i$.
Which is to say that the skip-gram probability is maximises when the angular difference between the input embedding for a word, and the output embeddings for words that it co-occurs with is minimised, and the embeddings themselves are maximised.
\pdfcomment{Does maximising the embedding cancel out?}.


While skip-gram and CBOW were introduces in the same set of papers, subsequent work has found the skip-gram is almost always the preferred system for any task. \pdfcomment{Citation for this?}

\subsection{Analogy Tasks}
\aside[Using Analogy Tasks to discover subtle prejudice in corpora]{
}

It is on these systems that evaluation the analogy tasks became well known, though they were considered earlier on RNN neural language models \pcite{mikolov2013linguisticsubstructures}.
These tasks are keyed around answering the question: What is  \emph{c} and \emph{b} is to \emph{a}.
For example, a semantic analogy would be answering that \natlang{Aunt} is to \natlang{Uncle} as \natlang{King} is to \natlang{Queen}.
For example, a syntactic analogy would be answering that \natlang{Kings} is to \natlang{King} as \natlang{Queens} is to \natlang{Queen}.
The latest and largest analogy test set is presented by \tcite{gladkova2016analogy},
which evaluates embedding on 40 subcategories of knowledge.

The analogies work by relating similarities of offsets.
Word Similarity can be evaluated using word embeddings using a number of metrics.
By far the cosine similarity is the most common.
This is given by $sim(C_u, C_v)=\frac{C_{u}\cdot C_{v}}{\left\Vert u\right\Vert \left\Vert v\right\Vert }$.
A smaller distance in vector space, means the words are more similar -- in some sense.
Depending on the system this similarity could be syntactic, semantic or otherwise, the analogy tasks can help identify what kinds of similarities the embeddings are capturing.

Using the similarity scores a ranking of words to complete the analogy is found.
To find the correct word for \emph{d} in \emph{d} is to \emph{c} and \emph{b} is to \emph{a}
the following is computed for the table of embeddings $C$:
\begin{equation}
\argmax_{\forall d} sim(C_d, C_a-C_b+C_c)
\end{equation}

\pdfcomment{Insert Vector diagram here it will clarify reason for the math}

Initial results were relatively poor, but the surprising finding was that this worked at all. \pdfcomment{Citations here}.
Subsequent results found in \tcite{pennington2014glove} were much better.

\section{Co-location Factorisation}

\subsection{GloVe}

Skip-grams are intrinsically prediction based methods, effectively predicting what words will co-occur in the context of a local window and optimising as such.
In \tcite{pennington2014glove}the authors show that if one were to change that optimisation to be global over all co-occurrences,
then the optimisation critia becomes minimising the cross-entropy between the true co-occurrence probabilities given by $P(w_j\mid w_j)$, and the value of the embedding product: the cross entropy measure.,
weighted by the frequency of the occurrence of the word $X_i$.
That is to say if skip-gram were optimised globally it would be equivalent to minimising:
\begin{equation}
J = - \sum_{\forall w_i} \sum_{\forall w_j} X_{ij} P(w_j\mid w_j) \log (V_{w_j}^\prime\,C_{w_{i}})
\end{equation}

Minimising this cross-entropy efficiently means factorising the true co-occurrence matrix,
into the input and output embedding matrices $C$ and $V$, under a particular set of weightings given by the cross entropy measure.

\tcite{pennington2014glove} consider instead factorizing the logarithm of co-occurrence matrix directly,
the logarithm giving an different, but similar, weighting to the cross entropy measure.
Thas is for each word co-occurrence of $w_i$ and $w_j$: Attempting to find optimal values for 
the embeddings $C_{w_{i}}$, and $V_{w_j}^\prime$, while also allowing constant offsets $b_i$ and $k_j$
such that:
$V_{w_j}^\prime\,C_{w_{i}} + b_i + k_j \approx \log(X_{ij})$
as near as is possible.

\aside[Implementing GloVe]{To implement GloVe in any technical programming language with good support for matrix factorisation via least-squares optimisation is trivial. As is implementing any other matrix factorisation/dimensionality reduction based method.}

The precise measure used to do this, is a weighted the least squares minimisation of 
\begin{equation}
J = - \sum_{\forall w_i}  \sum_{\forall w_j} f(X_{ij})\,V_{w_j}^\prime\,C_{w_{i}}+b_i+k_j-\log (X_ij)
\end{equation}


using the weighting between 0 and 1 given by $f(x)$
\begin{equation}
f(x)=\begin{cases}
\left(\frac{x}{100}\right)^{0.75} & x<100\\
1 & otherwise
\end{cases}
\end{equation}
though they suggest other weightings may be suitable.
This can be contrasted as a saturating variant of the effective weighing of skip-gram being $X_{ij}$.
Though the variation in measure away from cross-entropy also significant and has related effects on the weightings of rare vs common words in the minimisation.

While in initial tests it was found that Glove out-performed  skip-gram,
subsequent more extensive testing in \tcite(levy2015) with more tuned parameters,
found that skip-gram out-performed GloVE on all tasks, though the performance is relatively comparable.



\aside[Key Factors Mentioned as Minor Tweaks]{
There is an interesting pattern of important factors being considered as not part of the core algorithm.
GloVe explicitly weights all co-occurrences inversely by the distance from each other -- though this is considered part of finding the co-occurrence matrix rather than part of the algorithm itself.
Skip-Gram accomplishes something similar with dynamic window sizing tweak -- the true size of the window is uniformly distributed between 1 and the value given.

GloVe explicitly scales the weight of terms to be optimised -- using the weighting function, which saturates for frequent terms.
Skip-Gram accomplishing something similar again using the sub-sampling tweak -- common words are randomly skipped from the contexts during training, based on how common they are.

While the authors papers consider these as unimportant to the main thrust of the algorithms \textcite{levy2015} found them to be crucial hyper-parameters.
}

However, GloVe loosely highlights the relationship between the co-located word prediction neural network models,
and the more traditional matrix factorization of collocation counts used in topic modeling.
Very similar properties were also explored for skip-grams with negative sampling in \tcite{levy2014neural} and then in \tcite{li2015wordemedingasEMF} with more direct mathematical equivalence to weighed co-occurrence matrix factorisation;
Later in \tcite{cotterell2017SkipgramisEPCA} showed the equivalence to exponential principal component analysis also.



\subsection{Conclusion}
As we have now concluded that neural predictive co-location models are functionally very similar to matrix factorisation of co-location counts with suitable weightings, and suitable similarity metrics.
One might suggest a variety of word embeddings to be created from a variety of different matrix factorisations with different weightings and constraints; and indeed this has been done.
Traditionally large matrix factorisations have significant problems in terms of computational time and memory complexity.
A common solution to this is to handle it an iterative optimisation procedure.
Training a neural network such as skip-gram is an iterative optimisation procedure.











\section{Natural Language Applications -- beyond language modeling}
While statistical language models are useful, they are of-course in no way the be-all and end-all of natural language processing.
Simultaneously with the developments around representations for the language modelling tasks, work was being done on solving other NLP problems using similar techniques.

\tcite{collobert2008unified}


\subsection{Using Word Embeddings as Features}

\aside[Pretrained Word-Embeddings]{
	Pretrained Word Embeddings are available for most models discussed here.
	There trained on a lot more data than most people have access too.
	It can be useful to substitute word embeddings are a representation in most systems,
	or to use them as intitial value for neural network systems that will learn them as they train the system as a whole.
\pdfcomment{Insert links}
}

\tcite{turian2010word} discusses what is now perhaps the most important use of word embeddings.
The use of the embeddings as features, in unrelated feature driven models.
One can find word embeddings using any of the methods discussed above, for example language modelling.
These embeddings can be then used as features instead of, for example bag of words or hand-crafted feature sets.
\tcite{turian2010word} found improvements on the state of the art for chunking and Named Entity Recognition (NER), using the word embedding methods of that time.
Since then, these results have been superseded again using newer methods.



\section{Aligning Vector Spaces Across Languages}
Given two vocabulary vector spaces, for example one for German and one for English,
a natural and common question is if they can aligned such that one has a single vector space for both.
Using canonical correlation analysis (CCA) one can do exactly that.
Here, we will discuss normal CCA for aligning two vectors spaces,
though there also exists generalised CCA for any number of vector spaces \pcite{gcca}.

The inputs to CCA, are two sets of vectors, normally expressed as matrices,
Which we will call for consistency $C \subseteq \mathbb{R}^{n_1}$ and $V \subseteq \mathbb{R}^{n_2}$.
These are both some set of vector representations for words, not necessarily of same dimensionality.
These could be the output of any of the embeddings dicusses earlier,
or even a sparse (non-embedding) representations such as the point-wise mutual information of the co-occurrence counts.
The other input is an a subset of pairs from those sets that are to be aligned, which we will call, $C^\star \times V^\star$.
This subset contains only pairs with known translations -- this does not have to be the whole vocabulary of either language.


%
By performing CCA (the details of which are well out of scope for this text),
one solves to find a series of vectors, $S=\left[ \tilde{s}_1, ..., \tilde{s}_d \right]$ and $T= \left[ \tilde{t}_1, ..., \tilde{t}_d\ \right]$,
such that the correlation between $\tilde{s}_i^\prime C^\star$ and $\tilde{t}_i^\prime V^\star$ is maximised,
while constraining with the requirement for all $j<i$ that $\tilde{s}_i^\prime C^\star$ is uncorrelated with $\tilde{s}_j^\prime C^\star$  and that  $\tilde{t}_i^\prime V^\star$ is uncorrelated with $\tilde{t}_j^\prime V^\star$.
This is very similar to principal component analysis(PCA), and like PCA number components to use $d$ is a variable that can be deceased to achieve dimensionality reduction.
When complete, taking $S^\prime$ and $T^\prime$ as matrices gives projection matrixes that project $C$ and $V$ to a space where aligned elements are as correlated as possible.
That is to say the final new common vector space embeddings are given by:
$S^\prime C$ and $T^\prime V$.
\pdfcomment{Check this, both for correctness in decription, and for correctness in notation.}

\textcite{faruqui2014improving} investigated this primarily as a means to use additional data to improve performance on monoligual tasks.
In this they found small and inconsistent improvement.
However
However, we suggest it is much more interesting as a multi-lingual tool.
It allows similarity measures to be made between words of different languages.
\tcite{translating-unknown-words-2016} use this on as part of a hybrid system translate out of vocabulary words.






It may be apparent that since this method does produce a representation maximising similarity between two vectors -- just like the methods discussed for learning word embeddings,
that given representations for two words from the same context, initialised randomly,
CCA could be used repeatedly to optimise a towards good word embedding capturing shared meaning from contexts.
Such a method was investigated in \tcite{dhillon2011multi}; their final process is quiet complex.




There are also means to directly train embeddings on multiple languages concurrently including the works of \tcite{zou2013bilingual} and \tcite{shi2015learningbiligualcofactorisation}, amongst others.
A survey paper was recently published by \textcite{Ruder17crosslingreview}.





\clearnotecolumn[notes]

\end{document}
