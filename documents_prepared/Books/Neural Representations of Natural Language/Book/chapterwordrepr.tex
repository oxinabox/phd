\documentclass[12pt,parskip]{komatufte}
\include{preamble}




\begin{document}
\chapter{Word Representations}\label{sec:word-representations}
%\begin{addmargin}[0pt]{.3\textwidth}
%â€¢	Chapter 4: Word Representations (10 pages)
%o	The core work that began to draw a lot of attention to this area.
%o	This will cover crucial works such as Skip-Grams, CBOW, GLoVe, and of course the original neural probabilistic language model.
%o	As well as more recent techniques based on matrix factorisations.


\aside{Note that older literature in the area will use the term \emph{word~vector} to refer to representations from LDA and LSA etc. Whereas in a lot of newer work the term is used synonymously with \emph{word~embedding}.}
We begin the consideration of the representation of words using neural networks with work on language modeling.
This is not the only place one could begin the consideration: the information retrieval (IR) models such as LDA and LSA based on word co-location would be the other obvious starting point.
However, as will be discussed there models are closer to the end point of of the chapter than the begin.
For the language modeling works, comes the contextual language model works such as skip-gram, which in turn lead to the post-neural network co-occurrence based works.
These model fundamentally are in fact more similar to the information retrieval co-location works than the language modeling from which we began.


\section{Representations for Language Modeling}
\tcite{NPLM} 
BROWN CLUSTERS

\tcite{collobert2008unified}



\subsection{RNN Language Models}
\tcite{mikolov2011RnnLM,mikolov2010recurrent,6163930}

\section{Contextual Language Modeling}

\tcite{mikolov2012contextRNNLM}

\subsection{CBOW}
\tcite{mikolov2013linguisticsubstructures,mikolovSkip}

\subsection{SkipGram}
\tcite{mikolov2013linguisticsubstructures,mikolovSkip}

\aside{%\paragraph{Pretrained Word-Embeddings}
Pretrained Word Embeddings are available for most models discussed here.

There trained on a lot more data than most people have access too.

It can be useful to subsitute word embeddings are a repressentation in most systems. 

}

\subsection{Glove}

General concessus has emerged that Glove performs marginally worse than skip-grams

\subsection{What ever it is that people do instead now that is basically SVD on colloations}



%\end{addmagin}

\end{document}
