\documentclass[12pt,parskip]{komatufte}
\include{preamble}
\begin{document}

\setchapterpreamble{%
	\dictum[Jeff Hawkins, 2012]
	{
The key to artificial intelligence has always been the representation.
You and I are streaming data engines.
}}

\chapter{Introduction}\label{sec:introduction}
\begin{abstract}
	Introduces the book, and the utility of using machine learning for natural language processing.
\end{abstract}

Some might wonder why one would be concerned with finding good representations for natural language.
In general, all problem solving is significantly simplified by finding a good representation.
Whether that is reducing an real world problem into a system of inequalities to be solved by a constrained optimiser; or using domain specific jargon to communicate with experts in the area.


There is a truly immense amount of existent information, designed for consumption by humans.
Much of it is in text form.


One of the core attractions of a deep learning system is that it functions by learning increasingly abstract representations of it inputs.
These abstract representations include useful features for the task.
Importantly, they also include useful features even for more distantly related tasks.




It is the general assumption of this book that it is not being read in isolation.
That one is not bereft of all other sources of knowledge as it is being consumed.
We assume not only can the reader access the primary sources for the papers we cite,
but also that they are able to discover and access other suitable materials going in to more detail on related areas.

The core focus of this book is to summarize the technique that have been developed over the past 1-2 decades.
In doing so we reference the works of several dozen papers.
Significant effort has gone into describing these works clearly and consistently.
In doing so, the notation used does not normally line up with the original notations use in said papers; however it is always mathematically equivalent.
For some techniques the explanation is short and simple.
For other more challenging ideas, our explanation may be longer than the original very concise formulation that is common in some papers.





For brevity we have had to limit discussion of some aspect of natural language that one might like to represent.
In particular we have neglected all discussion on the notion that a word may be made up of multiple tokens.
For example \natlang{made up of}.
Phrases do receive some discussion in \Cref{sec:sentence-representations-and-beyond}.
However works such as \tcite{yin2014exploration} deserve more attention that we have space to give them.

Similarly, we do not have space to cover character based models.
character RNNs, and other deep character models such as \tcite{DBLP:journals/corr/ZhangL15}.
These models have relevance both as sources from which word representations can be derived \pcite{bojanowski2016enriching},
but more generally can be used for end-to-end systems.
Using a purely character based approach forces the model to learn tokenizing, parsing, and any other feature extraction internally.


\aside[Helpful hint]{If you find nonlinear reading unpleasant, then you may find the use of this sidebar unpleasant As such you may not enjoy this book.}

This book makes extensive use of marginalia, to provide side information.
This is used to provide reminders of definitions, and comments on potentially unfamiliar notations.
To highlight non-technical aspects of the works discussed.
As well as to highlight overly-technical aspects of the works discussed, such as implementation details.
They are also used to give the titles to citations (to save flicking to the huge list of references at the back of the book), and for the captions to the figures.
And more generally to provide non-essential  but potentially helpful information without interrupting the flow of the main text.
One could read the whole book without ever reading any of the marginalia,
as they are not part of the main text.
However it would be missing out on a portion of, perhaps the most interesting, content.








\end{document}