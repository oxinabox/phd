\documentclass[12pt,parskip]{komatufte}
\include{preamble}
\begin{document}

\setchapterpreamble{%
	\dictum[Jeff Hawkins, 2012]
	{
The key to artificial intelligence has always been the representation.
You and I are streaming data engines.
}}

\chapter{Preface}\label{sec:introduction}
%\begin{abstract}
%	ntroduces the book, and the utility of using machine learning for natural language processing.
%\end{abstract}

Some might wonder why one would be concerned with finding good representations for natural language.
To answer that simply, all problem solving is significantly simplified by finding a good representation.
Whether that is reducing an real world problem into a system of inequalities to be solved by a constrained optimiser; or using domain specific jargon to communicate with experts in the area.


There is a truly immense amount of existent information, designed for consumption by humans.
Much of it is in text form.
People prefer to create, and consume information in natural language (such as prose) format,
rather than in some more computationally accessible format (such as a directed graph).
For example, doctors prefer to dictate clinical notes over filling in forms for a database.



One of the core attractions of a deep learning system is that it functions by learning increasingly abstract representations of it inputs.
These abstract representations include useful features for the task.
Importantly, they also include features applicable even for more distantly related tasks.

\aside[A wealth of other materials]{
Wikipedia articles on ML and NLP tend to be of reasonable quality,
Coursera offers several courses on the topics,
Cross Validated Stack Exchange has thousands of QAs,
and the documentation for many machine learning libraries often contains high quality tutorials.
Finally, preprints of the majority of the papers in the fields are available on arXiv.
}


It is the general assumption of this book that it is not being read in isolation.
That the reader is not bereft of all other sources of knowledge as it is being consumed.
We assume not only can the reader access the primary sources for the papers we cite,
but also that they are able to discover and access other suitable materials 
in order to go deeper on related areas.
There exist a wealth of blog posts, video tutorials, encyclopaedic article etc. on machine learning and on the mathematics involved.

In general we do assume the reader is familiar with matrix multiplication.
In general we will define networks in terms of matrices (rather than sums),
as this is more concise, and better reflects real world applications,
in terms of code that a programmer would write.
We also assume a basic knowledge of probability, and an even more basic knowledge of English linguistics.
Though they are not the intended audience, very little of the book should be beyond someone with a high-school education.



The core focus of this book is to summarize the technique that have been developed over the past 1-2 decades.
In doing so we detail the works of several dozen papers (and cite well over a hundred).
Significant effort has gone into describing these works clearly and consistently.
In doing so, the notation used does not normally line up with the original notations used in those papers; however we ensure it is always mathematically equivalent.
For some techniques the explanation is short and simple.
For other more challenging ideas, our explanation may be much longer than the original very concise formulation that is common in some papers.



For brevity we have had to limit discussion of some aspect of natural language.
In particular we have neglected all discussion on the notion that a word may be made up of multiple tokens.
For example \natlang{made up of}.
Phrases do receive some discussion in \Cref{sec:sentence-representations-and-beyond}.
Works such as \tcite{yin2014exploration} deserve more attention than we have space to give them.

Similarly, we do not have space to cover character based models,
mainly character RNNs, and other deep character models such as \tcite{DBLP:journals/corr/ZhangL15}.
These models have relevance both as sources from which word representations can be derived \pcite{bojanowski2016enriching},
but more generally can be used for end-to-end systems.
Using a purely character based approach forces the model to learn tokenizing, parsing, and any other feature extraction internally.
We focus instead on models that work within larger systems that accomplish such preprocessing.

Finally, we omit all discussion of attention models in recurrent neural networks \pcite{DBLP:journals/corr/BahdanauCB14}.
This would be of relevance to \Cref{sec:rnn} and \Cref{sec:sentence-representations-and-beyond}.
They are some of the state of the art models.


This book makes extensive use of marginalia, to provide side information.
This is used to provide reminders of definitions, and comments on potentially unfamiliar notations.
To highlight non-technical aspects of the works discussed.
As well as to highlight overly-technical aspects of the works discussed, such as implementation details.
They are also used to give the titles to citations (to save flicking to the huge list of references at the back of the book), and for the captions to the figures.
And more generally to provide non-essential  but potentially helpful information without interrupting the flow of the main text.
One could read the whole book without ever reading any of the marginalia, as they are not required to understand the main text.
However one would be missing out on a portion of, some very interesting, content.



\end{document}