\documentclass[12pt,parskip]{komatufte}
\include{preamble}


\begin{document}

\loadgeometry{basegeo}
\title{Neural Representations \\of\\ Natural Language}
\author{Lyndon White\\ Roberto Togneri\\ Wei Liu\\ Mohammed Bennamoun}
\publishers{SpringerBriefs in Computer Science}

\maketitle
\tableofcontents

\loadgeometry{contentgeo}
\chapter{Introduction}\label{sec:introduction}
%•	Chapter 1: Introduction: 2-3 pages
%o	Introduce the book, and the utility of using machine learning for natural language processing

%\part{Introduction to the Fields}\label{sec:introduction-to-the-fields}
%Part A: Introductory

\include{chapterintrotomachinelearning}


\chapter{Current Challenges in Natural Language Processing}\label{sec:current-challenges-in-natural-language-processing}
%•	Chapter 3: Current Challenges in Natural Language Processing (10 pages)
%o	Whereas chapter 2 introduces the Machine Learning domain, this chapter introduces the Natural Language Processing domain.
%o	This chapter will cover the current tasks that natural language processing is being deployed for
%o	It provide forward references to the works (in Part B) that use machine learnt representation to accomplish these tasks
%o	Particular tasks to be discussed include
%	Language Modelling
%	Sentiment Analysis
%	Image Captioning
%	Image generation
%	Machine Translation
%	Paraphrase and plagiarism detection
%o	Limited discussion will be presented here on prior, non-machine learning techniques.

%\part{Representations}\label{sec:representations}
%Part B: Representations

\include{chapterwordrepr}



\chapter{Word Sense Representations}\label{sec:word-sense-representations}
%•	Chapter 5: Word Sense Representations (5-10 pages)
%o	In this chapter, technologies for representing the multiple meanings of a single word can have will be discussed.
%o	This is a growing area, and is particularly important in languages (including English, but other languages even more so), where polysemous and homonymous words are common.
%o	It leads naturally to the next section on phrase representation. Rather than a single word having many meanings, the next chapter will discuss how a single meaning may take multiple words to express.
\chapter{Phrase Representations}\label{sec:phrase-representations}
%•	Chapter 7: Phrase Representations (5-10 pages)
%o	This will cover phrases, 
%o	Phrases range from:
%	multi-token words: for example: “et al”, “word sense”;
%	to collocations: “young adult”, “5 year old”
%	to longer sentence clauses: “the fast train”, “once upon a time”
\chapter{Sentence Representations and Beyond}\label{sec:sentence-representations-and-beyond}
%•	Chapter 8: Sentence representations and beyond (5-10 pages)
%o	This chapter takes the previous discussion of phrases to the next level: sentences.
%o	This will include discussions of works on recursive structure
%o	As well work leveraging recurrent neural networks.
%o	Methods that do not strongly consider order (including Sum of Word Embeddings; paragraph vectors) will also be discussed here.
%o	Many of these techniques extent to arbitrary length sequences of words.
\chapter{Character-Based Representations}\label{sec:character-based-representations}
%•	Chapter 9: Character-Based Representations (5 pages)
%o	This short chapter will discuss some of the recent works which directly modelling only the characters (letter), but using this to accomplish tasks from much larger structures.
%o	Starting from Zhang and LeCun’s “Text Understanding from Scratch” (2015), and concluding with the very most recent works.
%o	It will draw the book to a close by retouching on many of the tasks more commonly associated with prior sections and will discuss how they are attempted from a fully uninformed system that is learning only from letters.
%o	This is a challenging area with fewer works to be discussed.

\part{Conclusion}\label{part:conclusion}
%Part C: Conclusion
\chapter{Conclusion}\label{sec:conclusion}
%•	Chapter 8: Conclusion (10 pages)
%o	This will conclude the book. 
%o	It will summarise the prior charters
%o	Discuss the progression of the field.
%o	It will discuss the role machine learnt representations have within larger systems.
%o	It will conclude with an outlook on the future.


\end{document}
