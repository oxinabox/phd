\documentclass[12pt,parskip]{scrbook}
\usepackage[author={Lyndon}]{pdfcomment}

\usepackage{booktabs}
\usepackage{tikz}
\usetikzlibrary{positioning, fit,  shapes.geometric}
\usepackage{graphicx}

\graphicspath{{./figs/}, {./}}

\usepackage[subpreambles=false]{standalone}

\usepackage{amsmath}
\usepackage{amssym}
\usepackage{mathtools}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\usepackage{microtype}
\usepackage{cleveref}

\usepackage{biblatex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}

\title{Neural Representations \\of\\ Natural Language}
\author{Lyndon White, Roberto Togneri, Wei Liu, Mohammed Bennamoun}
\publishers{SpringerBriefs in Computer Science}

\maketitle

\tableofcontents

\chapter{Introduction}\label{sec:introduction}
%•	Chapter 1: Introduction: 2-3 pages
%o	Introduce the book, and the utility of using machine learning for natural language processing

\part{Introduction to the Fields}\label{sec:introduction-to-the-fields}
%Part A: Introductory

\chapter{Machine Learning for Representations}\label{sec:machine-learning-for-representations}
%•	Chapter 2: Introduction to machine learning for representations (10 pages)
%o	This chapter can be skipped by readers already familiar with machine learning
%o	This will not be a full introduction to machine learning, which of-course could be an entire book on its own.
%o	It will cover the crucial basic techniques used in the works discussed in part B. The main focus will be on introducing neural networks, going into reasonable detail on hyper-parameters such as activation functions and layer sizes, as well as covering training techniques. 
%o	It will not cover techniques that are special to natural language processing – those will be discussed in chapters 4,5, and 6.
\chapter{Current Challenges in Natural Language Processing}\label{sec:current-challenges-in-natural-language-processing}
%•	Chapter 3: Current Challenges in Natural Language Processing (10 pages)
%o	Whereas chapter 2 introduces the Machine Learning domain, this chapter introduces the Natural Language Processing domain.
%o	This chapter will cover the current tasks that natural language processing is being deployed for
%o	It provide forward references to the works (in Part B) that use machine learnt representation to accomplish these tasks
%o	Particular tasks to be discussed include
%	Language Modelling
%	Sentiment Analysis
%	Image Captioning
%	Image generation
%	Machine Translation
%	Paraphrase and plagiarism detection
%o	Limited discussion will be presented here on prior, non-machine learning techniques.

\part{Representations}\label{sec:representations}
%Part B: Representations
\chapter{Word Representations}\label{sec:word-representations}
%•	Chapter 4: Word Representations (10 pages)
%o	The core work that began to draw a lot of attention to this area.
%o	This will cover crucial works such as Skip-Grams, CBOW, GLoVe, and of course the original neural probabilistic language model.
%o	As well as more recent techniques based on matrix factorisations.

\textcite{NPLM}


\chapter{Word Sense Representations}\label{sec:word-sense-representations}
%•	Chapter 5: Word Sense Representations (5-10 pages)
%o	In this chapter, technologies for representing the multiple meanings of a single word can have will be discussed.
%o	This is a growing area, and is particularly important in languages (including English, but other languages even more so), where polysemous and homonymous words are common.
%o	It leads naturally to the next section on phrase representation. Rather than a single word having many meanings, the next chapter will discuss how a single meaning may take multiple words to express.
\chapter{Phrase Representations}\label{sec:phrase-representations}
%•	Chapter 7: Phrase Representations (5-10 pages)
%o	This will cover phrases, 
%o	Phrases range from:
%	multi-token words: for example: “et al”, “word sense”;
%	to collocations: “young adult”, “5 year old”
%	to longer sentence clauses: “the fast train”, “once upon a time”
\chapter{Sentence Representations and Beyond}\label{sec:sentence-representations-and-beyond}
%•	Chapter 8: Sentence representations and beyond (5-10 pages)
%o	This chapter takes the previous discussion of phrases to the next level: sentences.
%o	This will include discussions of works on recursive structure
%o	As well work leveraging recurrent neural networks.
%o	Methods that do not strongly consider order (including Sum of Word Embeddings; paragraph vectors) will also be discussed here.
%o	Many of these techniques extent to arbitrary length sequences of words.
\chapter{Character-Based Representations}\label{sec:character-based-representations}
%•	Chapter 9: Character-Based Representations (5 pages)
%o	This short chapter will discuss some of the recent works which directly modelling only the characters (letter), but using this to accomplish tasks from much larger structures.
%o	Starting from Zhang and LeCun’s “Text Understanding from Scratch” (2015), and concluding with the very most recent works.
%o	It will draw the book to a close by retouching on many of the tasks more commonly associated with prior sections and will discuss how they are attempted from a fully uninformed system that is learning only from letters.
%o	This is a challenging area with fewer works to be discussed.

\part{Conclusion}\label{part:conclusion}
%Part C: Conclusion
\chapter{Conclusion}\label{sec:conclusion}
%•	Chapter 8: Conclusion (10 pages)
%o	This will conclude the book. 
%o	It will summarise the prior charters
%o	Discuss the progression of the field.
%o	It will discuss the role machine learnt representations have within larger systems.
%o	It will conclude with an outlook on the future.





%\bibliography{master}
%\bibliographystyle{plain}

\end{document}
