\documentclass[12pt,parskip]{komatufte}
\include{preamble}

\begin{document}
	

\setchapterpreamble{%
	\dictum[J. M. G. ele Lammens, PhD Dissertation,
	\textit{A~Computational Model of Color Perception and Color Naming},  State University of New York, 1994]{%
Whether one sees the artificial neural network technique described below as learning or as optimization depends largely on one's background and one's theoretical likes and dislikes. I will freely use ``learning'' in the remainder of this section because that term is traditionally used in the neural networks literature, but the reader should feel free to substitute ``optimization'' if (s)he finds the other term offensive. Please contact the author for an Emacs lisp function to enforce the usage of choice.%
}}
\chapter{Introduction to neural networks for machine learning}\label{sec:machine-learning-for-representations}
\begin{abstract}
This chapter is a introduction to machine learning via neural networks.
It is 
Chapter 2: Introduction to machine learning for representations (10 pages)
This chapter can be skipped by readers already familiar with machine learning.
It takes a much more tutorial tone than the remainder of the book.
This will not cover all aspects of machine learning, which of-course could be an entire book on its own.
It will cover the crucial basic techniques used in the works discussed in part B. The main focus will be on introducing neural networks, going into reasonable detail on hyper-parameters such as activation functions and layer sizes, as well as covering training techniques. 
It will not cover techniques that are special to natural language processing â€“ those will be discussed in chapters 4,5, and 6.
\end{abstract}

Machine learning, particularly neural networks has become a hot topic of late.
This chapter does not serve as a complete foundational text on neural networks,
but it does cover the basics and then some as is required to understand the remainder of this book.
Readers already familiar with neural networks can skip over this chapter.
For a more in-depth introduction, we recommend consulting a book focusing solely on the topic such as \tcite{WebBookBackprop}.

The core notion of machine learning is to learn a to perform an function based on examples.
This is in-contrast to ``regular programming'' where code is written to accomplish the function directly, and the examples may be used to check the human implementation.
Machine learning is at its most useful when it is difficult to articulate explicitly the rules for finding an output for an input; but for which there are many examples of the correct output.
For example, the rule of English spelling ``I before E, except after C'', is known to be often incorrect.
It is hard to write a rule about letter order.
However, a dictionary will have thousands of words showing the correct order.
By applying suitable machine learning techniques,
a system could learn to determine the correct spelling of words containing \natlang{ie} and \natlang{ei},
and inform the users as to if an input word is correct, even if the word never occurred in the training dictionary.
This is because the machine learning algorithm discovers generalisable rules,
from the training examples.
For most machine learning methods, including neural networks these rules are not in a readily interpretable form, but are stored as numerical parameters of the model.

\section{Neural Networks}
\aside[Multilayer perception or Artificial Neural Network?]{A artificial neural network is sometimes called a multilayer perception.
	This is in reference to the related perception learning algorithm.
	This term has fallen out of favour in newer works,
	with Hinton, who originally coined the term, expressing his own regret at the naming.
}

A neural network is one particular family of machine learning algorithms.
It should be remember that neural networks are not emulations of the brain,
they are algorithms that were inspired by the ideas about how the brain worked \pcite{hebb1949organization}.
Some advancements have also been inspired by the functioning of the brain,
but many others have come from statistical methods or other areas of mathematics.
A neural network is no more similar to a brain, than it is to a Fourier transform.

The core idea behind a neural network is to represent the transformation of the input to the output as links between neurons in a sequence of layers.
Each neuron has a weighted connection the the neurons in the layer below,
and it applies a nonlinear function to this weighted sum, 
to determine its own activation level.
This activation level is can be connected to the the neurons in the layer above.




\aside[Don't implement neurons in your neural network]{
Typical object orientated programming teaches one to look for objects.
A obvious object in a neural network would be a neuron.
Neural networks are made of layers,
and layers are made of neurons.
A neuron has a weights field and a bias field, and maybe there is a subclass for each different activation function.
Don't do this.
It will be incredibly slow in many languages -- where objects are always stored by reference, thus requiring a pointer redirection for each neuron used.
Object orientated is not a good paradigm for this kind of technical computing.
Instead program in terms of matrixs and vectors.
potentially using objects for layers.
}





\section{Function Approximation}


The universal approximation theorem (UAT) tells us that a neural network can be used to approximate any continuous bounded function.
Here a function should be understood in the general sense -- classification is a function from observations to classes, regression is a function from observed values to target values.
More precisely it states that for a network with sufficient width
\pdfcomment{Finish this paragraph with math and citations}

A network without any hidden layers is only able to approximate linear functions.
It is equivalent to a perception, or a linear SVM. 


However, the UAT does not tell us the values for those weight parameters,
nor does it promise that any method of training will ever reach them in finite time.
More significantly it does not inform as to how wide \emph{sufficiently wide} is,
nor if deeper is more efficient than wider.


\section{Network Hyper-parameters}
\aside[Hyper-parameters vs Parameters]{
	The weights and biases of the network are called its parameters.
	They are optimised during training.
	The other features of the network, including its topology, activation functions, and the training method (which often has its own set of options), are called the hyper-parameters. 
}

The key determination in applying neural networks to any problem,
is the choice of the hyper-parameters.
Neural networks can largely be thought-off as black-boxes,
that can accomplish a, potentially poorly specified, task if given as set of examples in the form of correct outputs for given inputs.
Where the expertise in deploying a neural network solution comes in, is in the choice of the hyper-parameters of the network.

\subsection{Width and Depth}

Early neural networks often had many layers.
With the UAT showing that only one hidden layer was required then tend to become more shallow, and
with difficulties in training deeper networks.

In the last decade, deep nets have come back into fashion.
The reasons for this are out of scope for this work.
In brief it is a combination of a better techniques (e.g. RELU, unsupervised pretraining),
better hardware (e.g. GPUs and Xeon Phis), and more labeled data (e.g. from crowd-sourcing platforms such as Amazon Mechanical Turk.)

\aside[What happened to Deep Belief Network Pretraining?]{
	\textcite{hinton2006fastDBN,bengio2007greedylayerwise} lead to a new resurgence of interest in neural networks.
	Pretraining with Deep Belief Networks allowed for deep networks to be trained.
	It was held for many years that it was necessary to use layer-wise unsupervised pretraining,
	even when one has only supervised data. 
	\textcite{glorot2011deepRELUsparse} showed that with ReLU units, a deep network could be trained directly and archive the same performance.
	As such unsupervised pretraining is now a more niche technique used primarily when there is an excess of unsupervised data.
}
It has been generally found that deeper networks can have significantly few total neurons than for the same performance as wider shallower networks.

The choice of the number of hidden layers (depth),
and the sizes (width) is a key parameter in designing a neural network.
It is arguably \emph{the} key parameter.
It can be assessed using a hyper-parameter sweep on a validation or development dataset.
It is a particularly relevant parameter for our purposes, as hidden layer width forms the basis of our representations.



\subsection{Activation functions}

\aside[Notation: Generic Activation Function $\varphi$:]{Though-out this book, when the activation function is not significant to the problem at hand, we will represent it with the \mbox{symbol $\varphi$.}}
The universal approximation theorem requires that the hidden layer non-linearity be non-constant, monotonic and bounded.
In practice one does not even require that, using the unbounded RELU activation functions.
There are a wide variety of activations functions in use.
For the hidden layers these generally perform relatively similarly.
Sigmoid, tanh, and ReLU are the most common activation functions used for hidden layers.
We suggest consulting a more in-depth work on neural networks with a focus on the machine learning aspects to gain a understanding of their pros and cons.

More significantly, the final activation function or the output function does always matter.
The output function should be chosen for the task at hand.

In the following sections $y$ is the predicted final output of the network it is normally a vector, $z$ is the output from the final hidden layer again normally a vector.

\subsubsection{Linear/Affine}
A linear or affine output-layer is the most general output layer.
It is the normal output layer for use on regression or function approximation tasks.
\begin{equation}
y=Wz + b
\end{equation}
In this equation $W$ and $b$ are a trained matrix and vector respectively.
This allows the output to be any value, within some set of bounds learned during training.
However, when this is not needed it increases the difficulty of training for no benefit.
One can consider the Linear output layer as being a learned scaling and shift of the output of any of the other activation functions.
Conversely, one can look at all other activation functions as being a non-linear transformation of an linear layer, as in a traditional neural network a affine transformation is done before all other activation functions.

Linear layers, cannot on their own, be used as hidden layer activation functions.
A stack of linear layers simplifies to be equivalent to a single linear layer, which means that the network can only approximate linear functions.
A non-linearity is required.
This is what the other activation functions provide


\subsubsection{Sigmoid}
This is the classic neural network activation function.
\begin{equation}
y=\sigma{z}=\frac{1}{1+e^{-z}}
\end{equation}
Sigmoid output values are between zero and one.
If the network has a single output then this is a fuzzy boolean.
It can be used to classify into 2 categories.
It is sometimes call a logistic output.

\subsubsection{Softmax}
Softmax is the standard output layer for categorical classification.
\begin{equation}
y=s_m(z)=\left( \frac{e^{z_i}}{\sum_{j=1}^{j=N} e^{z_j}} \right)_{i=1}^{i=N}
\end{equation}
For $N$ the number of outputs equal to the number of categories attempting to be classified into.
These outputs have  values are between zero and one, and sum to one.
They are taken as a probabilistic description of the likelihood of each class.
i.e. a discrete, probability mass function (pmf).


Softmax is a \emph{soft}, i.e. differentiable,  version of what could be called \emph{hard-max}.
In this conceptual hard-max function, the largest output is set to one, and the others set to zero.
It is non-differentiable and thus not suitable for use in a neural network.
In softmax the largest values is proportionally increased to be closest to one,
and the smallest to be closest to zero.

A neural network classifier with a softmax output layer is sometimes called a multinomial logistic regression network (especially if there is no hidden layer).
This can lead to some confusion with a sigmoid outputting network.
However, the similarity can be understood by considering a softmax output layer with 2 classes.
This reduces to $s_m(z)= \left( \sigma(z), 1-\sigma(z) \right)$.

Further consideration of the softmax is given in \Cref{sec:softmax-and-bayes-theorem}.

\subsubsection{Tanh}
The hyperbolic tangent function is a  scaled and shifted sigmoid function.

\begin{equation}
y=\tanh(z)=\frac{e^{2z}-1}{e^{2z}+1}=2\sigma(2z)+1
\end{equation}

The notable difference is that outputs are restricted to be between $-1$ and $+1$

\subsubsection{RELU}
The Rectified Linear Unit (ReLU) is a more recently popularized activation function \pcite{dahl2013reludropout}.
It comes from much earlier work on by neuroscience \pcite{hahnloser1998piecewise}.
\begin{equation}
y=ReLU(z)=\max \left( 0, z \right)=\begin{cases}
z & 0\le z\\
0 & z<0
\end{cases}
\end{equation}
Values are restricted to be at non-negative.
As this function has derivative zero for $z<0$, once a unit is turned off, it is not turned back-on.
Further to this, using random initialization this ensures that half of all neurons in the layer will be never turned on -- commonly called the neuron dying.
Thus resulting in sparse connections between the layers.
This has been found to be a good thing \pcite{glorot2011deepRELUsparse}.
ReLU is very commonly used as a hidden layer activation function for deep networks.



\subsubsection{RELU6}
\aside[Why are we bringing up ReLU6?]{ReLU6 is one of the more obsure activation functions. As such it may seem a bizare inclusion in this shorter introduction. However, we have found it surprisingly successful as an activation function, e.g. in the Auto-encoder example. Further to that, as a function bounded about and below it is much more theoretically nice than the ReLU unit.}

Closely related to ReLU is ReLU6.
This is another linear unit the saturates at 0, but also at 6.
\begin{equation}
y=ReLU6(z)=\max \left(0, \min\left(6, z\right) \right) =  \begin{cases}
6 & 6<z\\
z & 0\le z\le6\\
0 & z<0
\end{cases}
\end{equation}
This has similar advantages to ReLU.
however, as well as units being able to die to zero, it can also die to positive.
Unlike ReLU, it fully meets the requirements for the UAT.
The bounding at 6, rather than any other number is rather arbitrary -- particularly given scaling with the intervening affine layers.
The important point is that it is bound both above and below.




\subsubsection{Other activation functions}

There are numerous other activation functions.
Most are of interest for use in hidden layers such as Leaky RELU, Softplus, ELU and many others.
For specialized tasks, other functions might be used, such as atan2 \pcite{WhiteRepresentingAnglesSE}.




\section{Training}
The process of training the network is the process of solving a very high dimensional, nonlinear, nonconvex global optimization problem.

\aside[Evaluation function vs Loss function]{
	In this discussion we have distinguished between two types of \emph{error function}.
	The \emph{evaluation function} is the metric we are truly trying to improve, this could be accuracy for classification, BLEU score for translation, F1 score for retrieval etc. It is applied to the whole system (which may be greater than just the neural network), and the system may be evaluated in different ways using different evaluations.
	As the evaluation is often not differentiable, a proxy for it which we call the \textbf{loss function} is used in training.
	For example, squared error for regression, or cross-entropy for classification.
	The loss function is such that minimising the loss function also results in the true evaluation function being optimised.
	The loss function is applied the output of the network during training to calculate the error of a between the network output for a given input, and the target output from the example.
}

Typically, such a problem would be considered very difficult to solve.
Nonlinear and nonconvex problems can have local minima that are not global minima.
This means they can not be guaranteed to be solved  a global optima by gradient descent.
However, this does not pose an issue for more neural networks as finding the global optima is not required, merely finding a \emph{good enough} set of weights and values.

To define the training problems as an optimisation problem one first considers the loss function for a single training case.
For a target outputs $y_i$ and an actual outputs $\hat{y_i}$, a loss function is defined $loss(y, \hat{y})$.
For example the sum of squared errors loss used in regression is defined by 
$SSE(y, \hat{y})=\sum_{\forall i} (y_i-\hat{y_i})^2$.
The cross-entropy loss used in binary classification is defined by
$CE(y, \hat{y})=-\sum_{\forall i} (1-y_i)(\log (1-\hat{y_i}))) + y_i(\log (\hat{y_i})))$.
The choice of loss function depends on the purpose of the network.

The network function is composed into the loss function.
to the per training case loss, for a training case loss: $loss(y, N(x,\tilde{P}))$.
For $N(x,\tilde{P})$ being the function that executes some neural network, with weight and bias parameters given by $\tilde{P}$, upon an input $x$ where the target output is $y$ and the actual output is $\hat{y}$.

The total loss is defined by taking the mean over the whole train batch:
$loss_{total} = \frac{1}{K}\sum_{j=1}^{j=K} loss(y^{(j)}, N(x^{(j)},\tilde{P}))$.
This is now nothing but a nonlinear function to be minimised by adjusted the values of the weights and biases in $\tilde{P}$.
This can be given to off-the-shelf  unconstrained nonlinear optimisation algorithms \pcite{Ngiam2011} such as L-BFGS  \parencite{nocedal1980updating}.
Alternatively, and more commonly, the loss and updated can be processed iteratively on sub-sets (minibatches) of the training data at a time -- often using optimisation algorithms specifically targeted at machine learning applications.
In either case, almost all optimizers used for neural networks rely on gradient descent.

\aside[AdaGrad, AdaDelta, and Adam]{
AdaGrad \parencite{AdaGrad}, AdaDelta \parencite{DBLP:journals/corr/abs-1212-5701} and Adam \parencite{kingma2014adam}
 are effectively successive iterations of an optimisation algorithm.
In all cases these algorithms dynamically adapt the learning rate during training.
There are several other algorithms in the family, both older and newer.
Adam is currently the most commonly used optimiser in recent works.
We suggest there is an interesting theoretical space about the relationship between adapting the learning rate, and implicitly finding the second derivative (the Hessian), as in the more traditional quasi-netwon methods.}


To train the network one must find the values for the networks weight and bias parameters, such that that total loss function is minimized over the training set.
$\argmin{\tilde{P}} loss_{total}(\tilde{P}, X_{train}, Y_{train})$.
As the overall loss function, including the network function, is differentiable
gradient descent can be used.
If one considers a plot with the loss given on the vertical axis,
and the values of the parameters as describing a position on the horizontal axes,
then gradient descent is moving that the parameter-point downward on that surface based on local estimate of the slope.
\pdfcomment{Insert Plot Here}
In gradient descent based methods the derivatives are calculated and then used to update the parameters.
Many advanced methods exist but the core is, for each $W \in \tilde{P}$ updating its value according to the local gradient
$W \leftarrow W - \alpha \frac{\partial loss_{total}}{\partial{W}}$, where $\alpha$ is a update step size, commonly called the learning rate.
The method of calculating the gradients: $\frac{\partial loss_{total}}{\partial{W}}$
is known as back-propagation.
Back-propagation is simply a method of applying the well-known chain-rule of calculus to the neural network loss functions \pcite{backprop}.
However, this innovation was core to allowing neural networks as we know them to be trained.





\aside[What is Backpropergation?]{%
	It is worth emphasizing that back-propagation is not a method for training neural networks.
	It is a method of applying calculus to determine the error (loss) gradient, with respect to the network parameters (weights and biases), so that gradient descent based methods can be use for optimizing those parameters.
	Some works incorrectly say ``Trained with back-propagation'', when they mean to say ``trained with gradient descent''.
}





\section{Some Examples of Common Neural Network Architectures}
We will discuss more linguistically relevant neural networks in the following chapters.
However, to introduce the topic, we will give some basic examples.

\aside[These examples are available online]{For a practical introduction to the networks discussed here, worked examples of these network are available in the accompanying blog post at \url{http://white.ucc.asn.au/NNforNLPBook/NNexamples}.
Examples are in the julia programming language making use of the TensorFlow.jl package.
}

\subsection{Classifier}\label{sec:classifier}
As one of the most basic networks, consider a classifier on the classic MNIST challenge.

\input{figs/chapterintromachinelearning/mnist-basic.tex}

\input{figs/chapterintromachinelearning/mnist-advanced.tex}


\subsubsection{Softmax and Bayes' Theorem}\label{sec:softmax-and-bayes-theorem}
As a digression, it is worth considering the similarity between a network with an softmax output layer, and the application of Bayes' Theorem.
This will become important for understanding output embeddings, and hierarchical softmax in the future chapters.

For $y$ a class identifier
for $z$ some features vector, this could be the output of a hidden layer below, or it could be a direct input.
Our estimated probability is given by
\begin{equation}
P(Y=y\mid Z=z)=\frac{e^{[Wz+b]_{y}}}{\sum_{\forall i}e^{[Wz+b]_{i}}}
\end{equation}

Breaking these terms down we have:
\begin{equation}
P(Y=y\mid Z=z)=\frac{e^{[Wz]_{i}}\,e^{b_{y}}}{\sum_{\forall i}e^{[Wz]_{i}}\,e^{b_{i}}}
\end{equation}

One can see that $e^{[b]_{i}}$ does not depend on the value of $z$.
The bias term is analogous to the prior probability, literally it is the bias we have towards each element (i.e. each value $Y$ could take) without observing the condition.
Marginalising the probability over all $X$ values gives $P(Y=i)=e^{[b]_{i}}=e^{b_{i}}$

\pdfcomment{Is this actually a probability, or is it a score?}

The other component is $e^{[Wz]_{i}}$.
The key term here is $[Wz]_{i}$, by considering this, for each column of $y$,
we have 
\begin{equation}
[Wz]_{i}=\sum_{\forall j}W_{j,i}\,z_{j}=W_{i}^{T}z.
\end{equation}

Given one is considering the case for $Y=i$, then 
it can be seen that the row vector $W_{i}^{T}$ as a weighting map for features possessed by $z$.

i.e for some scoring function $R$:
\pdfcomment{Is this actually a postior probability, or is it just a score?}
\begin{equation}
R(Z=z\mid Y=i)=e^{[Wz]_{i}}
\end{equation}


Then reformulating the original statement we have:
\begin{equation}
P(Y=y\mid Z=z)=\frac{R(Z=z\mid Y=y)\,P(Y=y)}{\sum_{\forall i}R(Z=z\mid Y=i)\,P(Y=i)}
\end{equation}

Contrast this to Bayes' Theorem
\begin{equation}
P(Y=y\mid Z=z)=\frac{P(Z=z\mid Y=y)\,P(Y=y)}{\sum_{\forall i}P(Z=z\mid Y=i)\,P(Y=i)}
\end{equation}



\subsection{Bottle Necking Autoencoder}
An autoencoder is a tool primarily for use in finding a representation for their input.
There are many varieties of autoencoder based on neural network related techniques, including the works of \textcite{hinton2002RBM,hinton2006reducing,hinton2006fastDBN,vincent2010stacked,ICML2012Chen_416,2014VAE}.
This is itself a whole subfield.
For consideration here we look at a bottle necking autoencoder
\parencite{bourlard1988auto,japkowicz2000nonlinear}.
It has been used in a variety of tasks to attempt to find an optimal representation for an input e.g. as in \tcite{Usui:92}.

An autoencoder is a neural network tasked with outputting its input.
This in and of itself is a pointless task -- one already has the perfect reproduction of the input, in the input itself.
However, the true use of an autoencoder is to extract the output of one of the intermediary layers.
We call the intermediary layer the code layer.
To force this coded representation to have useful interesting properties,
and to prevent the network from simply learning the identity function,
all autoencoders include one or more features that increases the difficulty.
In the case of the bottlenecking autoencoder this feature is the bottleneck.
The code layer is much narrower than the input layer.
This forces the network to effectively learn to compress the data -- performing dimensionality reduction.

In this particular example we are looking at an auto-encoder for the MNIST images discussed earlier.
The original images are $28 \times 28$ pixels, i.e. 784 dimensional.
We compress it down to just 2 dimensions using the network shown in \Cref{fig-autoencoder}.

\begin{figure}
	\caption{A structure for a Bottle Necking Autoencoder for uses on MNIST.}
	\label{fig-autoencoder}
	\input{figs/chapterintromachinelearning/autoencoder.tikz}
\end{figure}


In this particular network $\varphi$ is a leaking RELU6 unit.
\begin{equation}
\varphi(z)=\begin{cases}
    0.01z+6 & 6<z \\
	1.01z & 0 \le z \le 6 \\
	0.01z & z < 0
\end{cases}
\end{equation}

The reason for this is that a sigmoidal unit does not perform well in a deep network because of the gradient-vanishing/exploding effects.
We found the effect of this to be that only the bias information of the final layer mattered -- effectively making all outputs just the average of all inputs.
The normal solution to this in deep networks is ReLU or ReLU6 units.

However, a ReLU, or ReLU6 unit is not ideal either,
as these units turn-off, and can not turn back on again.
If one of the just 2 neurons in the code layer turn-off then they can not turn back on again -- thus forcing it to be a dimensionality reduction to just one neuron, or none if that neural also dies.
In trialling this network structure, this was found to occur quiet very often.

The solution was to add a ``leak'' to the unit.
A small constant gradient even in the saturated position.
Making such a neuron possible, if difficult, to turn back on once turned off.
With this change the network always produces quality results.
An example of the recreation of an input from the test set can be seen in 
Figure \ref{fig-ae-recreation}. 
%\pdfcomment{this should be using CRef but that is currently broken.}

\asidefig[Recreation of an input from the MNIST test set. Input is on the left, output is on the right. 	{\label{fig-ae-recreation}}]{\includegraphics[scale=0.30]{figs/chapterintromachinelearning/recreation}}


More significantly, a sampling of the code layer is visually shown in \Cref{mnist-encoding}.
In this image, the positions on the X and Y axis of the input images, are given by the values on the coded layer said images correspond to.
It can be seen the the coding is capturing key information about the appearance.
The 0s appear near to the similarly rounded 6s,
the 8s near the 3s, etc.
In particular the 1s are arrayed according to the their slope.
The autoencoder has captured very useful information about the inputs, that would be hard to define with any hand written feature extractor.

It is this property of networks, to implicitly discover and define the most important latent variables and relationships to the task at hand that also makes the valuable for natural language processing.



\begin{figure}
	\caption{A sampling of MNIST images from the test-set, arranged according to the values of 2 neurons on the encoding layer.}
	\label{mnist-encoding}
	\includegraphics[width=\textwidth]{figs/chapterintromachinelearning/mnist-encoding.png}
\end{figure}

\section{Recurrent Neural Networks}
A key limitation of a neural network is that the number of inputs and outputs must be known at training time and must always be the same for all cases.
This is not true for natural language: If an over-all input is a sentence for example, then each sentence input will be made up a varying number of words.

Recurrent neural networks (RNN) over-come this by allowing the network to have a memory.
A RNN is effectively a chain of feed-forward neural networks,
each one being identical in terms of their weight and bias parameters,
but each one acting at a different time-step.





\begin{figure}
	\caption{The unrolled structure of an RNN for us in Encoding, Decoding and Encoding-Decoding (sequence-to-sequence) problems. RU is the recurrent unit -- the neural network which reoccurs at each time step.}
	
	\label{fig-rnns}
	
	\resizebox{\textwidth}{!}{\input{figs/chapterintromachinelearning/rnns.tikz}}
\end{figure}

Three general examples of the structure of RNNs are shown in \Cref{fig-rnns}.
These exampled will be considered in more later in this section below.
At each time-step the same network is used, with different inputs.
For purposes of looking at this in the big picture we will first consider each network a black-box recurrent unit (RU).

The recurrent unit takes at each time step, an input for that time-step, some representation its of state for the RU at the previous timestep; and it produces an output for that time-step, and its state representation for the next time-step.
A diagram of this is shown in \Cref{fig-ru}.
Every-time step has this 2 inputs and 2 outputs.
However, sometimes we will ignore them.
The initial state for the first time step is normally set to some zero vector,
and the final state at end of the sequence of normally discarded.
The outputs of many states are often discarded (in encoders), and the inputs are often filled deterministically with some low-meaning, or just generally useful prompt (in decoders).

In general most uses of recurrent networks can be considers as encoders, decoders or as both.
These common structures are shown in \Cref{fig-rnns}.
The motivation for using an RNN is if the size of the input and the size of the output is not consistent.

For example, if the system is attempting to learn a mapping from colors, represented as numerical triples to their names then it is not known how many words should be output to describe a color e.g. \texttt{(144,238,144)} might map to having 3 outputs: \natlang{very light green}; but \texttt{(255,165,0)} might map to just one: \natlang{orange} \pcite{2016arXiv160603821M}.
In this case, one would use a decoder type network.
One input, at the first time-step, and use the output from every time-step.
Each output would be a softmax probability for possible words -- which would be a softmax d.
The inputs at each time-step after the first in such a network, are often something like the most-likely previous output (or at train time the targeted output at the previous time-step), concatenated with the value being decoded.
In theory providing this information after the first time-step is not required -- it can call be stored in the state, and the inputs after could just be prompts of the zero vector -- in practice is it low cost and intuitively simplifies the relationships that must be learnt.

The opposed example applies for encoder networks:
if one has a series of inputs of different lengths,
but only a fixed sized output.
For example the reverse problem, going from words to the most likely point in color space \pcite{2017arXiv170909360W}.
At each time-step the inputs are are provided -- being one of the words in the compound color name.
All the outputs at all time-steps except the last can be ignored.
The output of the final time-step can be the true output.







\begin{figure}
	\caption{A recurrent unit with its 2 inputs and 2 outputs. Not shown are the internal Weight and Bias parameters which are trained to control the interactions.}	
	
	\label{fig-ru}
	
	\input{./figs/chapterintromachinelearning/rnn-unit.tikz}
\end{figure}



 

\end{document}