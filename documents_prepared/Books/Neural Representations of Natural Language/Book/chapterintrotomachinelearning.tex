\documentclass[12pt,parskip]{komatufte}
\include{preamble}
\usepackage{showframe}

\begin{document}
	

\setchapterpreamble{%
	\dictum[J. M. G. ele Lammens, PhD Dissertation,
	\textit{A~Computational Model of Color Perception and Color Naming},  State University of New York, 1994]{%
Whether one sees the artificial neural network technique described below as learning or as optimization depends largely on one's background and one's theoretical likes and dislikes. I will freely use ``learning'' in the remainder of this section because that term is traditionally used in the neural networks literature, but the reader should feel free to substitute ``optimization'' if (s)he finds the other term offensive. Please contact the author for an Emacs lisp function to enforce the usage of choice.%
}}
\chapter{Machine Learning for Representations}\label{sec:machine-learning-for-representations}

%•	Chapter 2: Introduction to machine learning for representations (10 pages)
%o	This chapter can be skipped by readers already familiar with machine learning
%o	This will not be a full introduction to machine learning, which of-course could be an entire book on its own.
%o	It will cover the crucial basic techniques used in the works discussed in part B. The main focus will be on introducing neural networks, going into reasonable detail on hyper-parameters such as activation functions and layer sizes, as well as covering training techniques. 
%o	It will not cover techniques that are special to natural language processing – those will be discussed in chapters 4,5, and 6.

Machine learning, particularly neural networks has become a hot topic of late.
This chapter does not serve as a general introduction to machine learning,
but it does cover the basics and then some as is required to understand the remainder of this book.


We begin with the consideration of a multivariates linear function.
We can see that this can project any point in $X$ to any point in $Y$ but that all points must move together along a straight line.

Consider instead the Logistic or sigmoid linear function:
This can project any point in $X$ to a point in $[0,1]^n$,
to a fuzzy yes-no.
But it all must move in a straight-ish line together.



\end{document}