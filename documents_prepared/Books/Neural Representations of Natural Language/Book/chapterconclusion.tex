\documentclass[12pt,parskip]{komatufte}
\include{preamble}
\begin{document}

%\setchapterpreamble{%
%	\dictum[Jeff Hawkins, 2012]
%	{
%The key to artificial intelligence has always been the representation.
%You and I are streaming data engines.
%}}

\chapter{Conclusion}\label{sec:conclusion}
\aside[Some final reminders]{This section has some final reminders, and some remarks not made elsewhere.}

Methods for finding representations of natural languages have been introduced.
Neural networks, elegantly produce such representations as by products, when applied to a NLP task.
We have introduced neural networks, and covered techniques for working with words, word senses, and larger structures.


\aside[Gradient Checks]{Back-propagation is a very easy algorithm to mess up, and it is hard to tell when you do mess it up, because it will often still work reasonably well.
Gradient checks, either with finite-differencing, or with more sophisticated techniques (e.g. dual-numbers), is a quick and easy way to check the correctness of your implementations.
}



\Cref{sec:machine-learning-for-representations} introduced the idea of machine learning, in particular neural networks.
Machine learning allows us to use examples of the desired outputs for given inputs
in order to train a system that can determine the output for an input that was never shown during training.
A key feature of modern machine learning is the decreased reliance on hand-engineered features to represent their inputs.
The embeddings and vector representations which are the main topic of this book, are just such automatically derived features.

\aside[Hyperparameters matter]{Neural networks have many hyper parameters, from hidden layer-size, to regularisation penalty, to number of training iterations. Choosing the right ones can significantly impact performance.}

\Cref{sec:rnn} showed how recursive neural networks allowed working with inputs of varying lengths -- such as natural language senses.
It discussed the various types of recursive networks as characterised by their recurrent units, including both GRU and LSTM.
As well as the common RNN structures, including:
the encoder for producing a fixed-size  output (such as a classification) from a varying sized input;
the decoder for generating a varying sized output from a fixed size input;
and the encoder-decoder for when the input and output both vary in size.
These models are very practical for natural language processing applications.

\aside[Preprocessing Matters]{We've not discussed this in much detail, but there are a large number of preprocessing tricks that can be employed when processing text. Options include removing stop-words, rare words, and punctuation. As well as removing or replacing numbers and dates. Different options for tokenization exist with regard to splitting up contractions and other factors. Often it is good to convert all the text to lower-case. One can even lemmatize or stem every word occurrence. What is useful depends on the task.}

\Cref{sec:word-representations} discussed how we can find and use representations of words.
This is one of the most important ideas in machine learning for NLP.
It begins with the core idea of how a word can be input into a neural network:
the input embedding via a look-up table.
The input embedding, and the complementarity output embedding from the weights of the softmax layer, capture a representation of the words.
The meaning of a word is determined by its usage, which is largely characterised by what words it co-occurs with.
Word representation models include the well known skip-gram model which use these principles to derivative high quality representations that can be reused as features in many tasks.
The skip-gram model is based around predicting which words will be co-located.
It is closely related to an iterative approximation to factorising the matrix of co-occurrence counts, which is commonly used in older methods.
Word embedding models commonly use  hierarchical softmax or negative sampling to speed-up training and evaluation.
These word embeddings allow machine learning to be used for natural language processing.

\aside[There is more to clustering than K-means]{K-means is the most well-known clustering algorithm. But it is by no means the only one.
	K-means in particular is very vulnerable to getting stuck in local optima, compared to many other clustering algorithms.
	If one does use k-means, then make sure to run it multiple times and take the best result.
}

\Cref{sec:word sense-representations} extends the idea of representing words to representing senses.
Most words have many meanings.
It is thus impossible for a single representation to characterise the correct meaning in all contexts.
Word senses can either be externally defined using a lexical resource,
or discovered (induced) from the data.
If the senses are externally defined, determining a good representation for them boils down to disambiguating a corpus to find the senses used, then creating a representation for them using the single sense word embedding methods.
Inducing the senses from the corpus is more in-line with the general goal of not needing hand-engineered features, and there are many more methods for this.

The majority of word sense induction methods are either context-clustering-based, or co-location-prediction based.
In both cases, the core idea is still that (like for word-embeddings) the surrounding words characterise what a particular word use means.
In the \emph{context clustering} methods, for each word the different contexts in which it occurs are represented and then clustered.
Each cluster represents a word sense.
In the \emph{co-location prediction} approaches the word sense is treated as a hidden (latent) variable,
which is giving influence on the observed variables that are the words which it is co-occurring with.
Classical probability methods can then be used, together with the neural network methods, in order to uncover that latent variable that is the sense.
These word sense induction methods give embeddings that can be used much like word embeddings.

\aside[Machine learning beyond neural networks]{
	Though we've barely mentioned them in this book,
	there are plenty of other machine learning algorithms beyond just neural networks.
	Many of these have great advantaged over neural networks,
	in terms of their performance with smaller amounts of data.
	%
	\begin{sloppypar}
	Some examples of some of the diversity of options (contrasted with neural networks) can be found at
	\url{https://white.ucc.asn.au/2017/12/18/7-Binary-Classifier-Libraries-in-Julia.html}.
	\end{sloppypar}
	%
	It is often ideal to take ones word-embeddings (or other neural network derived representations) and used them as features,
	in a classifier such as a support vector machine \parencite{Cortes1995},
	or a gradient boosting tree ensemble \parencite{chen2016xgboost}.	
}


\Cref{sec:sentence-representations-and-beyond} takes the idea of represent words to larger structures.
There is a great diversity of methods to represent phrases, sentences, and documents.
In three broad categories, we can consider weakly ordered models, sequential models, and structural models.
The \emph{weakly ordered models} are the most diverse category, being the catch all for various methods that do not directly consider word order.
The \emph{sequential methods} are largely the application of the RNNs from \Cref{sec:rnn} in the natural way to process text,
but also include some methods specifically for obtaining a representation.
Finally, the \emph{structural methods} allow the inputs to change the structure of the network,
allowing it to process parse trees in the natural way.


There are a great number of techniques for using natural language with machine learning.
They all revolve around the core idea of a representation.
Beyond that they range from simple to complex.




The methods used need to be sufficiently powerful to accomplish the task,
but beyond this excess capacity is both a waste of training time,
and a waste of developer effort.
Spending more time to implement one of the more complicated models may not result in a final system that even works as well as a simple baseline.
The ideal system is suitably simple.
However, the identification of what it means for an implementation to be suitably simple takes surprising amounts of research.
In part this can perhaps be attributed to our over-expectation of complexity in language.
While language can be complex, a lot of it is surprisingly simple.


The systems we have discussed have been small and self-contained.
They are suitable for use as a component in a larger system.
If one looks at the workings of a digital assistant (such as the ones found on a smart phone),
one will find many machine learning based subsystems:
for speech-recognition, for intent detection, and for accomplishing subtasks within those.
A complicated system such as Zara \pcite{siddique2017zara} contains over a dozen separate machine learning based subsystems.
\aside[Zara, and defending herself from bad influences]{
	One of the most interesting subsystems in Zara \parencite{siddique2017zara},
	is the use of a module to detect abusive language, racism and sexism.
	It is a well-known problem that when allowing a learning system to learn from the open-web,
	such systems can pick-up bad habits 
	(\url{https://www.theverge.com/2016/3/24/11297050}).
	The use of such an abuse detection module allows a system to be protected from such things.
}


There is another approach, processing huge corpora of texts, on with end-to-end deep learning systems.
For example using decades of user support logs to trained question answering systems.
Even this is based on many of the same principles we have discussed in this book.
Though, at this scale other options open up, like char-RNNs which function on a character scale rather than the word-scale and larger we've considered here.
With enough data, all the linguistic modelling concerns can be learnt into the neural network.
The data and computational requirements place such things out of reach of many developers.




Most tasks do not have this scale of data.
They may be small data, a few hundred examples.
They may be largish data, with a few million examples.
The more data you have, the more complex a model you can train,
and thus the more difficult a problem you can solve.
But good representations can help us when we do not have enough data.
By creating word embeddings (\Cref{sec:word-representations}),
word sense embeddings (\Cref{sec:word-sense representations}),
and larger embeddings (\Cref{sec:sentence-representations-and-beyond}),
we can leverage other larger data sources.
The embeddings created using systems that are trained on other data, to perform other tasks,
capture information to accomplish those tasks.
This information that will also likely be useful for our own unrelated tasks.


\end{document}