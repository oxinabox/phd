\documentclass[12pt,parskip]{komatufte}
\include{preamble}
\begin{document}

%\setchapterpreamble{%
%	\dictum[Jeff Hawkins, 2012]
%	{
%The key to artificial intelligence has always been the representation.
%You and I are streaming data engines.
%}}

\chapter{Conclusion}\label{sec:conclusion}

Methods for finding representations of natural languages have been introduced.
Neural networks, elegantly produce such representations as by products,
when applied to an NLP task.

A key take-away idea is that simple methods often work best.
The methods used need to be sufficiently powerful to accomplish the task,
but beyond this excess capacity is both a waste of training time,
and a waste of developer effort.
Spending more time to produce a system that might not even work as well as a simple baseline.
The ideal system is suitably simple.
However, the identification of what it means for an implementation to be suitably simple takes surprising amounts of research.
In part this can perhaps be attributed to our over-expectation of complexity in language.
While language can be complex, a lot of it is surprisingly simple.



The systems we have discussed have been small and self-contained.
Suitable for use as a component in a larger system.
If one looks at he workings of a digital assistant such as is found on a smart phone,
one will find many machine learning based systems subsystems:
for speech-recognition, for intent detection, for accomplishing subtasks within those.
A complicated system such as Zara \pcite{siddique2017zara} contains over a dozen machine learning based subsystems.
\aside[Zara, and defending herself from bad influences]{
	One of the most interesting subsystems in Zara \parencite{siddique2017zara},
	is the use of a module to detect abusive language, racism and sexism.
	It is a well-known problem that when allowing a learning system to learn from the open-web,
	they can pick-up bad habits 
	(\url{https://www.theverge.com/2016/3/24/11297050}).
	The use of such an abuse detection module allows a system to be protected from such things.
}


There is another approach, so called big data, processing petabytes of text on GPU farms,
with end-to-end deep learning systems such as char-RNNs.
Even this is based on the many of the same principles we've discussed in this book.
Though the data and computational requirements place such things out of reach of most readers.


Most tasks are not big data tasks.
They may be small data, a few hundred examples.
They may be largish data, with a few million examples.
The more data you have, the more complex a model you can train,
and thus the more difficult a problem you can solve.
But good representations can help us when we don't have enough data.
By creating word embeddings (\Cref{sec:word-representations}),
word sense embeddings (\Cref{sec:word-sense-representations}),
and larger embeddings (\Cref{sec:sentence-representations-and-beyond}),
we can leverage other larger data sources.
The embedding created using systems that are trained on other data, to perform other tasks,
capture information to accomplish those tasks,
information that will also likely be useful for our own tasks, whatever they may be.




\section{??Blurb??}

Popular deep neural networks for languages processing comprehensively reviewed and explained from a practical perspective.
Language modelling, vector representations, and challenging tasks such as WSD, and sentence embeddings investigated to illustrate the used of these networks.
This book is packed with valuable advice and experiences
obtained from practical development and implementations 
in the up-and-coming Julia programming language for technical computing.
It is a solid introduction to one of the most exciting new areas of natural language processing and computational linguistics.


\end{document}