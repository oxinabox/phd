\documentclass[12pt,parskip]{komatufte}
\include{preamble}
\begin{document}

\setchapterpreamble{%
	\dictum[the seven senses of \natlang{literally},
	 \textit{Oxford English Dictionary}, 3rd ed., 2011]
	{
		\begin{description}
			\setlength\itemsep{0em}
			\item[1a.] In a literal, exact, or actual sense; not figuratively, allegorically, etc.
			\item[1b.] Used to indicate that the following word or phrase must be taken in its literal sense, usually to add emphasis.
			\item[1c.] colloq. Used to indicate that some (frequently conventional) metaphorical or hyperbolical expression is to be taken in the strongest admissible sense: `virtually, as good as'; (also) `completely, utterly, absolutely' \ldots
			\item[2a ]With reference to a version of something, as a transcription, translation, etc.: in the very words, word for word.
			\item[2b.] In extended use. With exact fidelity of representation; faithfully.
			\item[3a.] With or by the letters (of a word). Obs. rare.
			\item[3b.] In or with regard to letters or literature. Obs. rare.
		\end{description}		
}}

\chapter{Word Sense Representations}\label{sec:word-sense-representations}
\begin{abstract}
	Chapter 5: Word Sense Representations (5-10 pages)
	In this chapter, technologies for representing the multiple meanings of a single word is discussed.
	This is an growing area, and is particularly important in languages where polysemous and homonymous words are common (This includes English, but it is even more prevalent in for example Chinese languages).
	It leads naturally to the next section on phrase representation. Rather than a single word having many meanings, the next chapter will discuss how a single meaning may take multiple words to express.
\end{abstract}
	
\section{Word Senses}

Words have multiple meanings.
When assigning a single representation to a word, it is impossible for that representation to truly describe the meaning in all contexts.
It may have some features applicable to some uses but not to others,
it may be an average of all features for particular contexts,
or it may only represent the most common sense,
or a combination of all.




\aside[polysemous/homonymous]{A word with multiple meanings. For purposes of all but the most linguistically driven NLP work there is no need to distinguish between a polyseme and a homonym. Polysemous and homonymous are synonymous.}

\aside[Part of Speech/POS]{The syntactic category a word belongs to. Different POS tags come from different tag sets.
Can be simple as the WordNet: noun, adjective, verb, etc. Or complex as in the Brown: VBG-\emph{verb, gerund/present participle}, NN-\emph{	noun, singular or mass}.
 }
\aside[word use]{An occurrence of a word in a text, such as a training corpus. Each word will have multiple uses in a text. Each word use will only have one particular meaning and will thus belong to one synset.}
\aside[lemma]{the base form of the word as defined by the lexicographical resource. It is normally closely related to (often identical to) the stem which is defined as the root form of the word with all morphological inflections (e.g. tenses) removed. The words are sometimes used interchangeably, though this is not truly correct.}
\aside[Lexeme]{The set of words that share a common lemma: \natlang{go}, \natlang{going}, \natlang{goes}, and \natlang{went} all belong to the lexeme headed by the lemma \natlang{go}}
\aside[synset]{A synset is a set of synonymous words: that is words that have the same meaning}
\aside[gloss]{A gloss is the dictionary entry for a word-sense, it normally includes both the definition and an example of use.
In WordNet each synset shares a common gloss.}

%\aside[Sensekey]{This is not a commonly used phrase by linquists, but it is an important implementation detail when using WordNet for WSD tasks. It repressents the mapping between lemma+pos and synset, and is needed because that is a many-many relationship. It take the basic form \texttt{lemma\%pos\_id:lex\_filenum}}
\aside[Lemmatization]{WordNet is indexed by lemmas. This in essence means with tense and plurality information removed. WordNet comes with a lemmatized called morphy to do this.(c.f. stemming)}
\aside[Unlemmatization]{Given lemma (as one can extract from WordNet) and a full POS tag (Such as a Brown-style tag) for a word, it is possible to undo the Lemmatization with a high degree of reliability using relatively simple rules. The POS tag encodes the key inflectional features that are lost. Patten.en \pcite{de2012pattern} is a python library encoding such rules (pluralisation, verb conjugation, etc.); though combining them with the POS tag to drive them is a task left for the reader. This can can be used to find substitute words using WordNet's features, for finding synonyms, antonyms and other lemmas from lexically related categories.}


The standard way to assign word senses is via some lexicographical resource.
i.e. a dictionary, or a thesaurus.
There is not a canonical list of word senses that are truly real and consistently defined in English.
Every dictionary is unique, with different definitions and numbers of word senses.
The most commonly used lexicographical resource is WordNet \pcite{miller1995wordnet},
there are several WordNets equivalents in other languages,
as well as the multi-lingual  BabelNet \pcite{navigli2010babelnet}.



\begin{figure}
	\caption{The relationship between terms used to discuss word sense problems.
		The Lemma stands in for the lexeme, for WordNet's purposes when indexing.
		The core task of WSD is to go from a word-use to the corresponding synset.
		For WSD contests the word-use is normally pretagged with its POS tag, and its corresponding lemma,
		as these can be found with very high reliability by a POS tagger and a stemmer respectively.
	}
	\begin{tikzpicture}[circle/.append style={draw, font=\footnotesize, rounded rectangle}]
		\node[circle](worduse){\normalsize Word-use\\ e.g.\\ \natlang{Fred \underline{goes} shopping}};
		\node[circle, above right = 2 of worduse](pos){\normalsize POS\\ e.g.\\ \emph{verb}};
		\node[circle, below right = 2 of worduse](lexeme){\normalsize Lexeme\\  e.g. \\ \{\natlang{go}, \natlang{going},\\ \natlang{goes}, \natlang{went}\}};
		\node[circle, draw, below right = 2 of pos](synset){\normalsize Synset\\ e.g. \\ \{\natlang{go}, \natlang{move}, \\ \natlang{locomote}\}};

		
		
		\draw[->] (worduse) edge node[labe] {has 1} (synset);
		\draw[->] (worduse) edge node[labe] {has 1} (pos);
		\draw[->] (worduse) edge node[labe] {has 1} (lexeme);
		
		\draw[->] (synset) edge node[labe] {has 1} (pos);
		\draw[<->] (synset) edge node[labe] {has many} (lexeme);
		
		\node[circle, above right = 2 of synset](gloss){\normalsize Gloss\\ e.g. \\ \emph{change location \ldots}};
		\draw[<->] (synset) edge node[labe] {has 1} (gloss);
		
		\node[circle, right = 2 of lexeme](lemma){\normalsize Lemma\\ e.g. \\  \natlang{go}};
		\draw[<->] (lemma) edge node[labe] {has 1} (lexeme);
	\end{tikzpicture}
	
	
\end{figure}



\subsection{Word Sense Disambiguation}


\aside[Semantic Syllepsis]{
	Also known as pathological sentences that kill almost all WSD systems.
	Consider the sentence: \natlang{John used to work for the \emph{newspaper} that you are carrying.},
	In this sentence the word-use newspaper simultaneously have two different meanings: it is both the company, and the object.
	This violates out earlier statement that every word-use belongs to exactly one synset.
	As word sense disambiguation systems normally attempt to assign a single sense  to an each word they are unable to handle these sentences.
	Most word sense induction systems can not much better: at best a new sense could be allocated for the join use, which does not correspond to the human notion of the word having two senses for different parts of the sentence.
	Most works on word sense disambiguation outright ignore these sentences, or consider them to be ungrammatical, or incorrect.
	However, they are readily understood and used without thought by most native speakers.
	These constructions are also known as zeugma, although zeugma is itself as highly polysemous word, so its usage varies. 
}



Word sense disambiguation is one of the hardest problems in NLP.
Very few systems significantly out perform the baseline most common sense (MCS).

Progress on the problem is made difficult by several factors.

There are problems with the data available.
The lack of very large scale training corpora rendering fully supervised methods difficult.
The limited testing corpora can result in systems that allow 
Lack of human agreement on the correct sense, resulting in weak ground truth.

There are also issues inherent in the task.
Determining the sense may require very long range information:
for example the information on context may not even be in the same sentence.
It may require knowing the domain of the text, where word sense uses vary between domain.
It may in-fact be intentionally unclear, with multiple correct interpretations, as in a pun.
Or be unintentionally unknowable, due to poor writing style, such that it would confuse any human reader also.


It can also be said that word senses are highly artificial and do not adequately represent meaning.
However, WSD is required to interface with  lexicographical resources,
such as translation dictionaries, ontologies (e.g. OpenCyc), and other datasets (e.g. ImageNet ).


It may be interesting to note, that the number of meanings a word has is anti-logarithmically related to its rank \tcite{zipf1945meaning}.
\pdfcomment{idk if this is the right way to talk about it, can I just say Zipfian, or is that not true in this case?}
That is to say the most common words have exponentially more meanings than rarer words.
This aligns well with our notion that precise (e.g. technical) words exist but are used only infrequently -- since they are only explaining a single situation
This also means that by far most word uses are potentially very ambiguous.

However, also the most commonly used word sense (for a given word ) is also overwhelmingly more frequent than it's less common brethren -- also being Zipfian distributed in there usage \cite{Kilgarriff2004}.

For this reason the most frequent sense (MFS) baseline is often the best performing result in any WSD task.

\subsubsection{Most Frequent Word Sense}
It is easy to count how often each sense of a word occurs,
once one has a sense annotated corpus.
Further, for frequent senses, due to there over-whelming commonness this counting is likely to have them win-out even from small training corpora.

For the $i$th word use being the word $w_i$, having some sense $h_j$
then without any further context the 
probability of that sense being the correct sense is $P(h_j \mid w_i)$.
As POS tagging is almost a solved problem, one can use the part of speech tag $p_i$ (for the $i$ word use) as an additional condition, and thus find $P(h_j \mid w_i, p_i)$.
As WordNet encodes this information for lemma-sense pair using the SemCor corpus counts (this is also used for sense ordering, which is why this is sometimes called First Sense.).
This is a readily available and practical method for getting a baseline probability of each sense.
By applying argmax, to always select the most common sense, this can be used for word sense disambiguation:  $\argmax_{\forall h_j} P(h_j \mid w_i, p_i)$.

In the most recent SemEval WSD task \pcite{moro2015semeval},
MFS beat all submitted entries for English, both overall, and on almost all cuts of the data.
(The results for other languages were not as good, however in other languages true corpus derived sense counts were not used).



\section{Word Sense Representation}
\pdfcomment{Rewrite this stuff about wordnet being a poor moral baseline to fit the books intended audience better}
\aside[WordNet is not a strong moral baseline]{
	
	WordNet is the standard "dictionary/thesaurus/source of lexicographical information" for algorithms. (ML and otherwise)
	(It is also the seed for some online dictionaries)
	
	The definitions in WordNet were written in a large part by undergrads at Princeton in 1990. That is why it has things like "S: (v) nag, peck, hen-peck (bother persistently with trivial complaints) "She nags her husband all day long"".
	
	Further, the dataset used to determine frequency of Synsets (that is the total count of all words) comes from a dataset now known as SemCore.
	Which is a sense annotated subset of the Brown Corpus.
	
	The Brown Corpus is a sampling of American texts from 1961.
	The norms of 1961 were not the norms of today.
	(Remember the US didn't pass the Civil Rights act to end segregation until 1964).
	
	Now here is a fact I will return to later. The word Wheelchair is only mentioned once in the Brown corpus.
	
	Moving on:
	ImageNet is a common resource for image recognition.
	It labels images with their WordNet synset.
	More generally, using a high performing ImageNet network minus its final classification layer, is commonly used as a feature extractor on any computer vision machine learning problem.
	
	ImageNet only uses the most common synsets.
	(in 2009 it has 8,000. It is now up to  22,000)
	Where most common is determined using the counts reported by WordNet (at least originally it was).
	Which were extracted from SemCore, i.e. The Brown Corpus.
	Which means that it did not include wheelchairs for example (it does now).
	
	And this is why automatic captioning systems recognize wheelchairs as skateboards.
	
	Because of the fact that they are based on what people were writing about in 1961.}

\subsection{Supervised Methods}
The simple and direct method, is to take a dataset that is annotated with word-senses,
and then treat each tagged word as if it were a word, then apply any of the methods for word representation discussed in \Cref{sec:word-representations}.
\tcite{iacobacci2015sensembed} use a Continuous Bag of Word language model \parencite{mikolov2013efficient}, doing exactly this.
This does however run into the aforementioned problem, that there is relatively little training data that has been manually sense annotated.
Iacobacci et al. use a 3rd party WSD tool, BabelFly \cite{Moro2014}, to annotate the corpus with senses.
This allows for existing word representation techniques to be applied, but being limited to the use of an existing WSD tool makes it unsuitable for use in WSD tasks.


\subsection{Pseudo-semi-supervised Method}\label{sec:pseudo-semi-supervised-method}
\tcite{Chen2014} use an almost semi-supervised approach to train sense vectors.
They partially disambiguate their training corpus, using initial word sense vectors and WordNet.
They then discard these original sense-vectors, and use the partially disambiguated corpus to train sense-vectors via a skip-gram variant.
\pdfcomment{Don't call things Pseudo-semi-supervised Method}

%
\aside[Cosine distance]{
As a refresher:
here we talk of cosine distance, as it is more reasonable as smaller implies closer.

(Though it is still not a true metric as $d_{cos}(v,kv)=0$ for all $k\in \mathbb{R}_+$).
Other times you may see cosine similarity, it ranged between -1 (most different) and 1 (most similar).
Cosine similarity is given by $sim(a,b)=\frac{a\cdot b}{\left\Vert a\right\Vert _{2}\left\Vert b\right\Vert _{2}}=\cos(\angle a b )$
i.e. the unit-length normalised dot product of the vectors.
Cosine distance is usually defined at $d_{cos}(a,b)=\frac{1-sim(a,b)}{2}$.
It ranged between 0 (most similar) and 1 (most different).
}

The first phase of there method is in essence a word-embedding based WSD system.
When assessed as such a WSD result they report that it only marginally exceeds the MFS baseline,
though that is not at all unusual for WSD algorithms as discussed above.

In phase 1 they assign every word-sense in WordNet, a sense vector.
This sense vector is the average of word-embeddings of a subset if words in the gloss,
as determined using pretrained skip-grams \parencite{mikolov2013efficient}.
For the word $w$ with word-sense $w_{s_i}$,
they select a set of candidate words, $cand(W_{s_i})$, from the gloss to be included in the average
based on the follow set of requirements.
First the word must be a content word: that is a verb, noun, adverb or adjective.
Secondly, its cosine distance to $w$ must be below some threshold $\delta$.
Finally, it must not be the word itself (One can assume this is in terms of identical lexemes).
Or written mathematically, where $C_v$ is the skip-gram word vector for $v$
\begin{equation}
cand(w_{s_{i}})=\left\{ u\left|\begin{array}{c}
u\in gloss(w_{s_{i}})\\
\wedge\, d_{cos}(C_w,C_u)<\delta\\
\wedge\, pos(u)\in\left\{ verb,adv,noun,adj\right\} \\
\wedge\, u\ne w
\end{array}\right.\right\} 
\end{equation}
The phase 1 sense vector for $w_{s_i}$ is the mean of the word vectors for all the words in $cand(w_{s_{i}})$.
The effect of this is that most words in the same synset will have similar but not necessarily identical initial vectors.
\pdfcomment{Consider what this does to rare senses of a word, i.e. the base wordsense will be distant}.



They then use the phase 1 sense vectors to disambiguate the words in their unlabelled training corpus.
For each sentence in the corpus, they define an initial content vector.
This is done by taking the mean of the skip-gram word embedding for all content words in the sentence.
For each word in the sentence, they then compare each sense vector to the context vector.
If the closes sense vector is below a given threshold,
then it that word is tagged with that word-sense, and the context vector is updated to use the sense-vector instead of the word vector.
Words that do not come within the threshold are not tagged.
This is an important features, as it means that words without clear senses do not get a sense given to them.
Thus avoiding any dubious sense tags for the next training step.


Once the corpus is (partially) tagged Chen et. al. employ the skip-gram word-embedding method with a variation to predict the word-senses.
The original sense vectors are discarded.
Rather than the model being tasked only to predict the surrounding words,
it is tasked to predict surrounding words and their sense-tags (where present).
For purposes of the loss function predicting tags and words is weighted equally.
Note that the input the the skip-gram is the central word, not the central word-tag.
In this method the word-sense embeddings are output embeddings.

The phase 1 sense vectors have not been assesses as their representational quality.
It can be assumed as it was not reported that it was worse than those found in phase 2.
The phase 2 sense vectors were not assessed for their capacity to be used for word sense disambiguation.
An extension to the method of \textcite{Chen2014}, to use the phase 2 vectors for WSD, would allow this method to be used to disambiguate its own training data. Allowing the method to become self-supervised.


\section{Word Sense Induction}

\aside[Can we go from induced senses to lexical senses]{
	A natural question given the many WSI systems,
	and the existing wealth of lexically annotated resources,
	is if we can induce sense, then align them to the lexically defined senses.
	\textcite{agirre2006} proposed a method for doing this using a weighted mapping based on the probabilities found using induced sense WSD on a labelled ``mapping'' corpus.
	This is only been used on relitively small datasets with only hundreds of words,
	being SenseEval 3 \parencite{mihalcea2004senseval} and SemEval-2007 Task 02 \parencite{SemEval2007WSIandWSD}.
	It is unclear as to how well it scales to real-word WSD tasks.
	Our own limited investigations with the larger SemEval 2007 Task 7 \parencite{Navigli:2007:STC:1621474.1621480} suggest that it may not.
	Finding good methods to link unsupervised representations,
	to human defined representations remains a topic worthy of research.
}


In this section we will discuss methods for finding a word-sense, without reference to a standard set of senses.
Such systems must discover the word senses at the same time as they find their representations.
One strong advantage of these methods is that they do not require a labelled dataset.
As discussed there are relatively few high-quality word-sense labelled datasets.
The other key advantage is not relying on fixed senses determined by a lexicographer.
This is particular useful if the words senses are highly domain specific;
or in a language without strong lexicographical resources.
This allows the data to inform on what word-senses exist.

\aside[Why do skipgrams perform so well on SCWS?]{
	SCWS is a corpus designed for evaluating word sense embeddings.
	Single sense embeddings (e.g. skipgrams) can not take advantage of the context information in the SCWS, however, they do often perform comparably to the word-sense embeddings.
	Sometimes even outperforming them.
	It is unclear as to if this highlights the difficulty of the task -- that the impact of context is hard to gauge.
	Or if it merely shows just how well tuned the more mature single sense embedding methods are.
}

Most vector WSI and WSR approaches are evaluated on similarity tests.
Like WordSim-353 \textcite{WordSim353}, for context-less, or Stanford's Contextual Word Similarities (SCWS) \textcite{Huang2012}.
This evaluation is also suitable for evaluating single sense word-embeddings, e.g. skipgrams.


We can divide these into collocation clustering based approaches,
and collocation prediction based approaches.
This division is very similar to the separation of collocation matrix factorisation,
and collocation prediction based approaches discussed in the Word Representation chapter.
It can be assumed thus that at their heart, like for word embeddings,
they are fundamentally very similar.


\subsection{Collocation Clustering Based Approaches}
The fundamental method for most clustering based approaches is as per \tcite{Schutze:1998wordsenseclustering}.
That original work is not a neural word-sense embedding, however the approach has propagated forward.
\tcite{pantel2002WSI} and \tcite{Reisinger2010} are also not strictly neural word embedding approaches, however the overall method is also identical so we discuss their work also here so the similarities can be seen.


\aside[On context representations]{
These collocation clustering methods require finding a representation for the the context, which can then by combined with cosine similarity (or another metric), to give the similarities for clustering.
More generally, this can be related to the next chapter: \Cref{sec:sentence-representations-and-beyond}, as any of these methods could be used to derive a vector representation of a context, which could be used with cosine distance to define a usage similarity.
In most words comparatively simple representations of the contexts are used.
}


First consider all word uses, with their context.
Define some method of measuring similarity between contexts.
Contexts can be defines using windows (like in many word-embedding models),
 using sentences, or defined using some rules.
In vector representational works, this is ubiquitously done by assigning each context a vector, and then using cosine similarity between those vectors as cluster similarity.
\textcite{Schutze:1998wordsenseclustering} investigated using various on subset of log inverse document frequency (idf) weighted bags of words for the contexts, and dimensionality reduced versions of these.
In \textcite{pantel2002WSI} the context vectors are the mutual information vectors found by considering the frequency of the word and its contexts occurrences.
\pdfcomment{At somepoint I have to explain what td-idf is}
In \textcite{Reisinger2010}, this was td-idf or $\chi^2$ weighted bag of words vectors.
In \tcite{Huang2012} this was td-idf weighted averages of (a custom) single sense vectors for their contexts.
In \tcite{kaageback2015neural} this was again a weighted average of single sense vectors, this time using a factor based on how close the word was, and a factor based on how likely the co-occurrence was according to the word embedding skip-gram models.
It is interesting to note that idf, td-idf, mutual information, skip-gram co-occurrence probabilities (being a proxy for point-wise mutual information \textcite{levy2014neural}), are all closely related measures.
\pdfcomment{Check if Pointwise mutual information and Mutual information are the same thing}

\aside[On clustering]{
	Clustering can be defined as a (mixed integer) optimisation task, of assigning points to clusters so as satisfy some loss function based e.g. on minimising intra-cluster variance while maximising inter-cluster variance.
	As this is NP-hard, most clustering methods are approximate.
	k-means is very popular because of its simplicity,
	however it easily falls into local minima,
	and so normally is run dozens (or more, even thousands) or times to obtain the optimal results.
	K-means also has the issue of having to select the number of clusters ($k$).
	It should be remembers that there exist many other clustering methods than  k-means and k-means variants.
	These use different loss functions, and different strategies to overcome the NP-hard nature of the problem.
	In particular there mixture model methods, hierarchical methods, spectral methods, and others.
	I personally favour Affinity Propagation \parencite{frey2007clustering}.
	On any clustering task (word sense or otherwise) it is worth investigating several clustering algorithms.
	A serious of interesting and very easy reading articles on clustering can be found at:
	\url{http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/}, \url{http://alexhwilliams.info/itsneuronalblog/2015/10/01/clustering2/}, and \url{http://alexhwilliams.info/itsneuronalblog/2015/11/18/clustering-is-easy/}
}


Secondly, some method is then applied to cluster the word uses based on the calculated similarity of the contexts.
\textcite{Schutze:1998wordsenseclustering} used group average agglomerative clustering.
\textcite{pantel2002WSI} used a custom hierarchical clustering method.
\textcite{Reisinger2010} used mixtures of von-Mises-Fisher distributions.
\textcite{Huang2012} used spherical k-means.
\textcite{kaageback2015neural} uses k-means.

Finally some method is used find an embedding for that cluster.
For non-neural embedding methods this step is not always done, as defining a representation is not the goal, though in general it can be derived from most clustering techniques.
\textcite{Schutze:1998wordsenseclustering} use the centroids of their clusters.
\textcite{Huang2012} use a method of relabelling the word uses with the cluster ID,
then retrain their single sense word embedding method.
This relabelling technique is similar to the method later used by \textcite{Chen2014} for learning lexical sense representations, as discussed in \Cref{sec:pseudo-semi-supervised-method}.
\textcite{kaageback2015neural} when using k-means uses the clusters centroid: which is the weighted mean of all context vectors in the cluster.


\pdfcomment{Maybe use a table to summarise them all}

\subsubsection{Online clustering}
The methods discussed above all use off-line clustering.
That is to say the clustering is performed after the embedding is trained.
\tcite{neelakantan2015efficient} perform the clustering during training.
They use a modified skipgram formulation.
They start with a fixed number of randomly initialised sense vectors for each context.
These sense vectors are used as input embeddings for the skipgram context prediction task, over single sense output word embeddings.
Each sense also has linked to it a context cluster centroid, which is the average of all output embeddings for contexts that that sense is assigned to.
Each time a training instance is presented, the average of the context output embeddings is compared to each sense's context cluster centroid.
The context is assigned to the cluster with the closest centroid, updating the centroid value.
This can be seen as similar to performing a single k-means update step for each training instance.
Optionally, if the closest centroid is further from the context vector than some threshold hyper-parameter a new sense can be created using that context vector as the initial centroid.
After the assignment of the content to a cluster, the corresponding sense vector is selected for use as the input vector in the skip-gram context prediction task.

\textcite{kaageback2015neural} investigated using their weighting function with the online clustering used by \textcite{neelakantan2015efficient}.
More generally any such weighting function could be used.



\subsection{Collocation Prediction Based Approaches}
\aside[Probability]{
One may wish to brush up on basic probability notions for this section.
In particular joint, conditional and marginal probabilities definitions;
as well as Bayes law and the probability chain rule which come from those.
The derivation of the math used for \textcite{tian2014probabilistic} latent senses is as follows and should be understood as clear.
{\tiny
\begin{align*}
P(w_o\mid w_i) &= \sum_{\forall h_k \in \mathcal{H}(w_i)} P(w_o \mid h_k, w_i) P(h_k \mid w_i) \\
&= \sum_{\forall h_k \in \mathcal{H}(w_i)} \frac{P(w_o, h_k, w_i)}{P(h_k, w_i)} \, 
\frac{P(h_k, w_i)}{P(w_i)} \\
&= \sum_{\forall h_k \in \mathcal{H}(w_i)} \frac{P(w_o, h_k, w_i)}{P(w_i)} \\
&= \sum_{\forall h_k \in \mathcal{H}(w_i)} P(w_o, h_k \mid w_i) \\
&= P(w_o \mid w_i)
\end{align*}}
}


\tcite{tian2014probabilistic} observed that word collocation, as used for skip-gram word embeddings could be considered as having a latent variable of the sense.
That is to say for input word $w_i$ having senses $\mathcal{H}(w_i)$,
the probability of output word $w_o$ occurring near $w_i$ can be given as:

\begin{align}
	P(w_o\mid w_i) &= \sum_{\forall h_k \in \mathcal{H}(w_i)} P(w_o \mid h_k, w_i) P(h_k \mid w_i) \label{equ:tianmm} \\
\end{align}

Given that a sense $h_k$ only belongs to one word $w_i$, as Tian et. al. are not doing any form of sense sharing via synsets etc,
we know that $k$th sense of the $i$th word only occurs when the $i$th word occurs.
we have that the intersection probability $P(w_i,h_k) = P(h_k)$.
%and $P(w_i) = \sum_{\forall h_k \in \mathcal{H}(w_i) P(h_k)}$;

We can thus write \Cref{equ:tianmm} as:
\pdfcomment{I am not happy with how this math looks.}
\begin{align}
P(w_o\mid w_i) &= \sum_{\forall h_k \in \mathcal{H}(w_i)} P(w_o \mid h_k) P(h_k \mid w_i) \label{equ:tianmm2}
\end{align}

$P(w_o \mid h_k)$ is in a sensible form for writing as a softmax type probability.
With output embeddings for the words $w_o$, and input embeddings for the word senses $h_k$.
This softmax can be sped-up using negative sampling or heirachical softmax.
The later was done by Tian et. al.

\Cref{equ:tianmm2} is in the form of a mixture model form with a latent variable.
Such a class of problems are often solved using the Expectation Maximisation (EM) method.

The EM procedure functions by performing two alternating steps.
The E-step calculates the expected chance of assigning word-sense for each training case ($\hat{P}(h_l \mid w_o)$) in the training set $\mathcal{X}$.
Where a training case is a pairing of a word use $w_i$ paired with a single context word $w_o$
\begin{align}
\hat{P}(h_l \mid w_o) = \frac{\hat{P}(h_l \mid w_i) P(w_o \mid h_l)}{\sum_{\forall h_k \in \mathcal{H}(w_i)} \hat{P}(w_o \mid h_k) P(h_k \mid w_i)}
\end{align}

The M-step updates the prior likelihood of each sense (that is without context) using the expected assignments from the E-step.

\begin{align}
\hat{P}(h_l \mid w_i) = \frac{1}{|\mathcal{X}|} \sum_{\forall (w_o,w_i)\in \mathcal{X}} \hat{P}(h_l \mid w_o)
\end{align}

During this step the likelihood of the $P(w_o \mid w_i)$ can be optimised via gradient decent in the neural network parameters of the softmax component: $P(w_o \mid h_k)$,
to maximise the likelihood of the observations.

By using this EM optimisation the network can fit values for the embeddings in that softmax component.
The similarities to fitting a Gaussian Mixture Model being used for clustering are notable.


A limitation of the method used by \tcite{tian2014probabilistic}, is that the number of each sense must be known in advance.
While there are ration methods to approach this,
such as using some multiple of number of senses assigned by a lexicographical resource (e.g. WordNet).
Situations where such resources are  is not available/suitable or suitable are one of the main circumstances in which WSI is desirable  (e.g. domain specific terminology, or under-resourced languages).
In these case one could apply a heuristic based on the distribution of senses based on the distribution of words \parencite{zipf1945meaning}.
However, this would be unable to correctly determine the number of senses for exceptional words.
Better would be to allow senses to be determined based how the words are used -- if they are used in two different ways, then they should have two different senses.
How a word is being used can be determined by the contexts in which it appears.


\tcite{tian2014probabilistic} presented a fairly heavy classical statistical model.
The next step beyond this down a Bayesian modelling path,
its to make number of senses for each word itself a fit-able parameter of the model.
This is the approach taken by \tcite{AdaGrams}

Considering again the form of \Cref{equ:tianmm2}
\begin{align}
P(w_o\mid w_i) &= \sum_{\forall h_k \in \mathcal{H}(w_i)} P(w_o \mid h_k) P(h_k \mid w_i) 
\end{align}
The prior probability of a sense given a word but no context is 
$P(h_k \mid w_i)$ is Dirichlet distributed.
The prior probability of any categorical classification task is a Dirichlet distributed, as this is how a Dirichlet distribution is defined.
When considering that the sense my be one from an unlimited collection of possible senses,
then that prior becomes a Dirichlet process.

In essence the prior over a potentially unlimited number of possible sense become another parameter of the model (along with the embeddings for the output words and input embeddings).
The fitting of such a model is beyond the scope of this book;
it is not entirely dissimilar to the fitting via EM and gradient descent.
The final output of \tcite{AdaGrams} is much the same as \Cref{equ:tianmm},
a set of embeddings for induced senses,
and a language model that is able to predict how likely a word is to occur near that word sense ($P(w_o \mid h_k)$).

By application of Bayes theorem, this can be inverted to take a word's context,
and predict the probability of each word sense.

\begin{align}
P(h_l \mid w_o) = \frac{P(w_o \mid h_l) P(h_l \mid w_i)}{\sum_{\forall h_k \in \mathcal{H}(w_i)} P(w_o \mid h_k) P(h_k \mid w_i)}
\end{align}

With the common (but technically incorrect) assumption that all words in the context are independent (technically only requires conditionally independent on on the word in question $w_i$, never the less incorrect.)
For a context window of width $j$ given by $\mathcal{W}_i = \lbrace w_{i-\frac{j}{2}} \ldots w_{i-1} \ldots w_{i-1} \dots w_{i+\frac{j}{2}} \rbrace$

\begin{align}
P(h_l \mid \mathcal{W}_i) = 
\frac{\prod_{\forall w_{j}\in \mathcal{W}_i} P(w_{j} \mid h_l) P(h_l | w_i)}{\sum_{h_k \in \mathcal{H}(w_i)} \prod_{\forall w_{j} \in \mathcal{W}_i} P(w_{j} \mid h_k) P(h_k | w_i)}
\end{align}

\pdfcomment{Check this equation}

\section{Implementations}
Of the methods discussed in this section the following implementations exist:
\begin{enumerate}
	\item WordNet \parencite{tengi1998design} should have a binding in all modern programming languages.
	\item BabelNet \parencite{navigli2010babelnet} is intended to be accessed as an online resource, via a RESTful APImost commonly accessed. Users receive 1000 free queries per day. Which isn't much for these kinds of tasks. Academic users can request an upgrade to 50,000 queries per day, or to download a copy of the database (My own experience was the request was handled in <24 hours).
	\item AdaGrams \parencite{AdaGrams} has a julia implementation at \url{https://github.com/sbos/AdaGram.jl}. This was working well for Julia v0.5. At time of publication it was not working in Julia v0.6. Though updating is reasonable.
\end{enumerate}


\section{Conclusion}
\aside[Finding the nearest neighbours (Nearest Neighbour Trees)]{
	A common task with any representation (once trained) is to find its nearest neighbours, given a particular point.
	The na\"ive solution is to check the distance to all points.
	For $n$ points this is for all cases $O(n)$ operations.
	For word embeddings $n$ is the size of the vocabulary, perhaps $100,000$ words.
	$100,000$ operations per check, is not entirely unreasonable on modern computers (even when the operations are on 300 dimensional representations).
	However, for word sense embeddings, which have many senses per word in the vocabulary,
	this means many more points to check. With fine-grained senses 30 senses per word is not unusual.
	Having a total $n=3,000,000$ causes a noticeable delay, when performing thousands of checks for a similarity or analogy task.
	All is not lost however,
	data structures exist for fast nearest neighbour querying.
	A k-d tree takes at worst $O(n \log^2(n))$ time to construct.
	Once constructed on average it takes $O(\log (n))$ to find the nearest neighbour to any point.
	This makes checking the nearest neighbour nearly instantaneous for even the largest vocabularies.
}




\end{document}