\documentclass[12pt,parskip]{komatufte}
\include{preamble}
\begin{document}

\setchapterpreamble{%
	\dictum[the seven senses of \natlang{literally},
	 \textit{Oxford English Dictionary}, 3rd ed., 2011]
	{
		\begin{description}
			\setlength\itemsep{0em}
			\item[1a.] In a literal, exact, or actual sense; not figuratively, allegorically, etc.
			\item[1b.] Used to indicate that the following word or phrase must be taken in its literal sense, usually to add emphasis.
			\item[1c.] colloq. Used to indicate that some (frequently conventional) metaphorical or hyperbolical expression is to be taken in the strongest admissible sense: `virtually, as good as'; (also) `completely, utterly, absolutely' \ldots
			\item[2a ]With reference to a version of something, as a transcription, translation, etc.: in the very words, word for word.
			\item[2b.] In extended use. With exact fidelity of representation; faithfully.
			\item[3a.] With or by the letters (of a word). Obs. rare.
			\item[3b.] In or with regard to letters or literature. Obs. rare.
		\end{description}		
}}

\chapter{Word Sense Representations}\label{sec:word-sense-representations}
\begin{abstract}
	In this chapter, techniques for representing the multiple meanings of a single word are discussed.
	This is a growing area, and is particularly important in languages where polysemous and homonymous words are common.
	This includes English, but it is even more prevalent in Mandarin for example.
	The techniques discussed can be broadly classified as lexical word sense representation,  and as word sense induction.
	The inductive techniques can be sub-classified as clustering based or as prediction-based.
\end{abstract}
	
\section{Word Senses}

Words have multiple meanings.
A single representation for a word cannot truly describe the meaning in all contexts.
It may have some features applicable to some uses but not to others,
it may be an average of all features for all uses,
or it may only represent the most common sense.
For most word-embeddings it will be an unclear combination of all of the above.
Word sense embeddings attempt to find representations not of words, but of particular senses of words.




\aside[Polysemous/Homonymous]{A word with multiple meanings i.e. senses. For NLP representational purposes polysemous and homonymous are synonymous.}

\aside[Part of Speech/POS]{The syntactic category a word belongs to. Different POS tags come from different tag sets.
Can be simple as the WordNet tag set: noun, adjective, verb, etc. or complex as in the Brown tag set: VBG-\emph{verb, gerund/present participle}, NN-\emph{	noun, singular or mass}.
 }
\aside[Word-use]{An occurrence of a word in a text, such as a training corpus. Each word will have multiple uses in a text. Each word-use will only have one particular meaning and will thus belong to one synset.}
\aside[Lemma]{The base form of the word as defined by the lexicographical resource. It is normally closely related to (often identical to) the \emph{stem} which is the word's root form with all morphological inflections (e.g. tenses) removed.}
\aside[Lexeme]{The set of words that share a common lemma: \natlang{go}, \natlang{going}, \natlang{goes}, and \natlang{went} all belong to the lexeme headed by the lemma \natlang{go}}
\aside[Synset]{A synset is a set of synonymous words: that is words that have the same meaning.}
\aside[Gloss]{A gloss is the dictionary entry for a word-sense, it normally includes both the definition and an example of use.
In WordNet each synset shares a common gloss.}

%\aside[Sensekey]{This is not a commonly used phrase by linquists, but it is an important implementation detail when using WordNet for WSD tasks. It repressents the mapping between lemma+pos and synset, and is needed because that is a many-many relationship. It take the basic form \texttt{lemma\%pos\_id:lex\_filenum}}
\aside[Lemmatization]{WordNet is indexed by lemmas. This in essence means with tense and plurality information removed. WordNet comes with a lemmatizer called morphy to do this.}
\aside[Unlemmatization]{Given lemma (as one can extract from WordNet) and a full POS tag (such as a Brown-style tag) for a word, it is possible to undo the lemmatization with a high degree of reliability using relatively simple rules. The POS tag encodes the key inflectional features that are lost. Patten.en \pcite{de2012pattern} is a python library encoding such rules (pluralisation, verb conjugation, etc.); though combining them with the POS tag to drive them is a task left for the reader. This can can be used to find substitute words using WordNet's features, for finding synonyms, antonyms and other lemmas from lexically related categories.}


The standard way to assign word senses is via some lexicographical resource, such as a dictionary, or a thesaurus.
There is not a canonical list of word senses that are consistently defined in English.
Every dictionary is unique, with different definitions and numbers of word senses.
The most commonly used lexicographical resource is WordNet \pcite{miller1995wordnet}, and the multi-lingual  BabelNet \pcite{navigli2010babelnet}.
The relationship between terminology used in word sense problems is shown in \Cref{fig:wordsenseterms}


\begin{figure}
	\caption{The relationship between terms used to discuss word sense problems.
		The Lemma stands in for the lexeme, for WordNet's purposes when indexing.
		The core task of WSD is to go from a word-use to the corresponding synset.
		For WSD tasks the word-use is normally pre-tagged with its lemma and POS tag,
		as these can be found with high reliability using standard tools.
		Note that the arrows in this diagram are directional -- if you reverse the single directional \emph{has 1} relationships, they are \emph{has many} relationships.
	}
	\label{fig:wordsenseterms}

	\resizebox{\textwidth}{!}{\begin{tikzpicture}[circle/.append style={draw, font=\footnotesize, rounded rectangle}]
		\node[circle](worduse){\normalsize Word-use\\ e.g.\\ \natlang{Fred \underline{goes} shopping}};
		\node[circle, above right = 2 of worduse](pos){\normalsize POS\\ e.g.\\ \emph{verb}};
		\node[circle, below right = 2 of worduse](lexeme){\normalsize Lexeme\\  e.g. \\ \{\natlang{go}, \natlang{going},\\ \natlang{goes}, \natlang{went}\}};
		\node[circle, draw, below right = 2 of pos](synset){\normalsize Synset\\ e.g. \\ \{\natlang{go}, \natlang{move}, \\ \natlang{locomote}\}};

		
		
		\draw[->] (worduse) edge node[labe] {has 1} (synset);
		\draw[->] (worduse) edge node[labe] {has 1} (pos);
		\draw[->] (worduse) edge node[labe] {has 1} (lexeme);
		
		\draw[->] (synset) edge node[labe] {has 1} (pos);
		\draw[<->] (synset) edge node[labe] {has many} (lexeme);
		
		\node[circle, above right = 2 of synset](gloss){\normalsize Gloss\\ e.g. \\ \natlang{to change location \ldots}};
		\draw[<->] (synset) edge node[labe] {has 1} (gloss);
		
		\node[circle, right = 2 of lexeme](lemma){\normalsize Lemma\\ e.g. \\  \natlang{go}};
		\draw[<->] (lemma) edge node[labe] {has 1} (lexeme);
	\end{tikzpicture}}
	
	
\end{figure}



\subsection{Word Sense Disambiguation}





Word sense disambiguation is one of the hardest problems in NLP.
Very few systems significantly out perform the baseline most frequent sense (MFS) technique.

Progress on the problem is made difficult by several factors.

The sense is hard to identify from the context.
Determining the sense may require very long range information:
for example the information on context may not even be in the same sentence.
It may require knowing the domain of the text, where word sense uses vary between domain, this information is external to the text itself.
It may in-fact be intentionally unclear, with multiple correct interpretations, as in a pun.
It maybe unintentionally unknowable, due to poor writing style, such that it would confuse any human reader also.
These difficulties are compounded by the limited amount of data available.

There is only a relatively small amount of labelled data for word sense problems.
It is the general virtue of machine learning that given enough data almost any input-output mapping problem can be solved.
Such an amount of word sense annotated data is not available -- compared to finding unsupervised word embeddings which can use anything that has ever been written.
The lack of very large scale training corpora renders fully supervised methods difficult.
It also results in small sized testing corpora; which leads to systems that may appear to perform well, but do not generalise to real world uses.
Lack of human agreement on the correct sense, resulting in weak ground truth, further makes creating new resources harder.
This limited amount of data compounds the problem's inherent difficulties.


It can also be said that word senses are highly artificial and do not adequately represent meaning.
However, WSD is required to interface with  lexicographical resources,
such as translation dictionaries (e.g. BabelNet), ontologies (e.g. OpenCyc), and other datasets (e.g. ImageNet \pcite{imagenet_cvpr09}).


It may be interesting to note, that the number of meanings a word has is anti-logarithmically related to its rank \pcite{zipf1945meaning}.
\pdfcomment{idk if this is the right way to talk about it, can I just say Zipfian, or is that not true in this case?}
That is to say the most common words have exponentially more meanings than rarer words.
This aligns well with our notion that precise (e.g. technical) words exist but are used only infrequently -- since they are only explaining a single situation
This also means that by far most word uses are potentially very ambiguous.

The most commonly used word sense (for a given word) is also overwhelmingly more frequent than it's less common brethren -- also being Zipfian distributed in there usage \pcite{Kilgarriff2004}.
For this reason the most frequent sense (MFS) baseline is often the best performing result in any WSD task.
\aside[Semantic Syllepsis]{
	(Also known as pathological sentences that kill almost all WSD systems.)
	Consider the sentence: \natlang{John used to work for the \emph{newspaper} that you are carrying.},
	In this sentence the word-use newspaper simultaneously have two different meanings: it is both the company, and the object.
	This violates out earlier statement that every word-use belongs to exactly one synset.
	As word sense disambiguation systems normally attempt to assign a single sense they are unable to handle these sentences.
	Most word sense induction systems can not much better: at best a new sense could be allocated for the joint use, which does not correspond to the linguistic notion of the word having two senses for different parts of the sentence.
	Most works on word sense disambiguation outright ignore these sentences, or consider them to be ungrammatical, or incorrect.
	However, they are readily understood and used without thought by most native speakers.
	These constructions are also known as zeugma, although zeugma is itself as highly polysemous word, so its usage varies. 
}
\subsubsection{Most Frequent Sense}\label{sec:most-frequent-sense}
Given a sense annotated corpus, it is easy to count how often each sense of a word occurs.
Due to the over-whelming frequency of the most frequent sense, it is unlikely for even a small training corpus to have the most frequent sense differing from the use in the language as a whole.

The Most Frequent Sense (MFS) method of word sense disambiguation is defined by counting the frequency of a particular word-sense for a particular part of speech tagged word.
For the $i$th word use being the word $\n w_i$, having some sense $\n s_j$
then without any further context the 
probability of that sense being the correct sense is $P(\n s_j \mid \n w_i)$.
One can use the part of speech tag $p_i$ (for the $i$ word use) as an additional condition, and thus find $P(\n s_j \mid \n w_i, p_i)$.
WordNet encodes this information for lemma-sense pair using the SemCor corpus counts (this is also used for sense ordering, which is why this is sometimes called First Sense.).
This is a readily available and practical method for getting a baseline probability of each sense.
Most frequent sense can be applied for word sense disambiguation using this frequency based probability estimate:  $\argmax_{\forall \n s_j} P(\n s_j \mid \n w_i, p_i)$.


In the most recent SemEval WSD task \pcite{moro2015semeval},
MFS beat all submitted entries for English, both overall, and on almost all cuts of the data.
(The results for other languages were not as good, however in other languages true true corpus derived sense counts were not used).



\section{Word Sense Representation}


\aside[WordNet is not a strong moral baseline]{
	WordNet, as a resource based in part on the work of Princeton undergraduate students in the early 1990's,
	and on the literature of 1961,
	is not the kind of resource one might hope for from a AI information perspective.
	
	
	The glosses include a number of biases.
	These biases are reflective of language use, but are not necessarily ideal to be encoding into a system.
	For example: \natlang{"S: (v) nag, peck, hen-peck (bother persistently with trivial complaints) `She nags her husband all day long' ''}.
	Other dictionaries regularly show up in the News for similar content.
	
	Another problem is the source of the word sense counts.
	As discussed in in the main text, sense counts are important in WSD systems.
	The counts come from SemCor, a sense annotated subset of the Brown Corpus.
	The Brown Corpus is a sampling of American texts from 1961.
	The cultural norms of 1961 were not the norms of today.
	(For context, note that the US didn't pass the Civil Rights act to end segregation until 1964).
	As such, one should not trust WordNet (or SemCor) to reflect current sense counts,
	for words which have undergone usage change since 1961.
	
	Further more, when creating down stream resources based on WordNet,
	one should not use the sense counts to determine how important it is to include a concept.
	One can imagine the issues that would be caused had ImageNet \parencite{imagenet_cvpr09} used those counts to determine the synsets for which images would be provided.
	(Do not fear: even the initial release of ImageNet contains hundreds of images of \natlang{wheelchairs}, and \natlang{prosthesis} -- synsets that almost do not occur in the Brown Corpus).
	Unintentional biasing of data can have on-going effects on the behaviour of machine learning based systems far beyond the original conception.
}

\subsection{Directly supervised method}
The simple and direct method is to take a dataset that is annotated with word-senses,
and then treat each tagged word as if it were a word, then apply any of the methods for word representation discussed in \Cref{sec:word-representations}.
\tcite{iacobacci2015sensembed} use a CBOW language model \parencite{mikolov2013efficient} to do this.
This does however run into the aforementioned problem, that there is relatively little training data that has been manually sense annotated.
Iacobacci et al. use a 3rd party WSD tool, BabelFly \pcite{Moro2014}, to annotate the corpus with senses.
This allows for existing word representation techniques to be applied, but being limited to the use of an existing WSD tool makes the use of the resulting word sense embeddings questionable in applications to WSD tasks.

\tcite{Chen2014} applies a similar technique, but using a word-embedding based partial WSD system of their own devising.


\subsection{Word embedding based disambiguation method}\label{sec:pseudo-semi-supervised-method}
\aside[WSD with embeddings]{
It is beyond the scope of this work to discuss WSD systems in general,
we discussed the baseline Most Frequent Sense in \Cref{sec:most-frequent-sense},
and a word-embedding based is detailed as a part of the method discussed in \Cref{sec:pseudo-semi-supervised-method}.
This can be seen as similar to the LSI enhanced Lesk WSD system of \textcite{basile-caputo-semeraro:2014:Coling}.
}

\textcite{Chen2014} uses an almost semi-supervised approach to train sense vectors.
They partially disambiguate their training corpus, using initial word sense vectors and WordNet.
They then discard these original sense-vectors, and use the partially disambiguated corpus to train sense-vectors via a skip-gram variant.


%
\aside[Cosine distance]{
As a refresher:
here we talk of cosine distance, as it is more reasonable as smaller implies closer.

(Though it is still not a true metric as $\d d_{cos}(v,kv)=0$ for all $k\in \mathbb{R}_+$).
Other times you may see cosine similarity, ranging between -1 (most different) and 1 (most similar).
Cosine similarity is given by $sim(a,b)=\frac{\v a\cdot \v b}{\left\Vert \v a\right\Vert _{2}\left\Vert \v b\right\Vert _{2}}=\cos(\angle \v a \v b )$
i.e. the unit-length normalised dot product of the vectors.
Cosine distance is usually defined as $\d d_{cos}(\v a,\v b)=\frac{1-sim(\v a,\v b)}{2}$.
Ranging between 0 (most similar) and 1 (most different).
}

The first phase of this method is in essence a word-embedding based WSD system.
When assessed as such, they report that it only marginally exceeds the MFS baseline,
though that is not at all unusual for WSD algorithms as discussed above.

They assign every word-sense in WordNet a sense vector.
This sense vector is the average of word-embeddings of a subset of words in the gloss,
as determined using pretrained skip-grams \parencite{mikolov2013efficient}.
For the word $w$ with word-sense $\d w_{\n s_i}$,
a set of candidate words, $cands(\d w_{\n s_i})$, is selected from the gloss to be included in the average
based on the follow set of requirements.
First, the word must be a content word: that is a verb, noun, adverb or adjective.
Secondly, its cosine distance to $w$ must be below some threshold $\delta$.
Finally, it must not be the word itself (one can assume this is in terms of identical lexemes).
Or written mathematically, where $C_v$ is the skip-gram word vector for $v$
\begin{equation}
cands(\d w_{\n s_{i}})=\left\{ u\left|\begin{array}{c}
u \in gloss(\d w_{\n s_{i}})\\
\wedge\, d_{cos}(\i C_w, \i C_u)<\delta\\
\wedge\, pos(u)\in\left\{ verb,adv,noun,adj\right\} \\
\wedge\, u\ne w
\end{array}\right.\right\} 
\end{equation}
The phase one sense vector for $\d w_{\n s_i}$ is the mean of the word vectors for all the words in $cands(\d w_{\n s_{i}})$.
The effect of this is that most words in the same synset will have similar but not necessarily identical initial vectors.
\pdfcomment{Consider what this does to rare senses of a word, i.e. the base wordsense will be distant}

The phase one sense vectors are used to disambiguate the words in their unlabelled training corpus.
For each sentence in the corpus, an initial content vector is defined by taking the mean of the skip-gram word embedding (not word sense) for all content words in the sentence.
For each word in the sentence, each possible sense-embedding is compared to the context vector.
If the closest sense vector is below a given threshold,
then it that word is tagged with that word-sense, and the context vector is updated to use the sense-vector instead of the word vector.
Words that do not come within the threshold are not tagged, and the context vector is not updated.
This is an important feature, as it ensures that words without clear senses do not get a sense ascribed to them.
Thus avoiding any dubious sense tags for the next training step.

In phase two of training
Chen et. al. employ the skip-gram word-embedding method with a variation to predict the word-senses.
They train it on the partially disambiguated corpus produces in phase one.
The original sense vectors are discarded.
Rather than the model being tasked only to predict the surrounding words, it is tasked to predict surrounding words and their sense-tags (where present).
In the loss function the prediction of tags and words is weighted equally.
Note that the input the the skip-gram is the central word, not the central word and sense-tag.
In this method the word-sense embeddings are output embeddings, only.

The phase one sense vectors have not been assessed on their representational quality.
It could be assumed that as results for these were not reported that it was worse than those found in phase two.
The phase two sense vectors were not assessed for their capacity to be used for word sense disambiguation.
An extension to the method of \textcite{Chen2014}, to use the phase two vectors for WSD, would allow this method to be used to disambiguate its own training data.
Which would allowing the method to become self-supervised.


\section{Word Sense Induction (WSI)}

\aside[Can we go from induced senses to lexical senses]{
	A natural question given the many WSI systems,
	and the existing wealth of lexically indexed resources,
	is if we can induce sense, then align them to the lexically defined senses.
	\textcite{agirre2006} proposed a method for doing this using a weighted mapping based on the probabilities found using induced sense WSD on a labelled ``mapping'' corpus.
	This is only been used on relatively small datasets with only hundreds of words
	(SenseEval 3 \parencite{mihalcea2004senseval} and SemEval-2007 Task 02 \parencite{SemEval2007WSIandWSD}).
	It is unclear as to how well it scales to real-word WSD tasks.
	Our own limited investigations with the larger SemEval 2007 Task 7 \parencite{Navigli:2007:STC:1621474.1621480} suggest that it may not scale very well.
	Finding suitable methods to link unsupervised representations,
	to human defined senses remains a topic worthy of research.
}


In this section we will discuss methods for finding a word-sense without reference to a standard set of senses.
Such systems must discover the word senses at the same time as they find their representations.
One strong advantage of these methods is that they do not require a labelled dataset.
As discussed there are relatively few high-quality word-sense labelled datasets.
The other key advantage is not relying on fixed senses determined by a lexicographer.
This is particular useful if the words senses are highly domain specific;
or in a language without strong lexicographical resources.
This allows the data to inform on what word-senses exist.

\aside[Why do skipgrams perform so well on SCWS?]{
	SCWS is a corpus designed for evaluating word sense embeddings.
	Single sense embeddings (e.g. skipgrams) can not take advantage of the context information in the SCWS, however, they do often perform comparably to the word-sense embeddings.
	Sometimes even outperforming them.
	It is unclear as to if this highlights the difficulty of the task -- that the impact of context is hard to gauge.
	Or if it merely shows just how well tuned the more mature single sense embedding methods are.
}

Most vector WSI and WSR approaches are evaluated on similarity tests.
Like WordSim-353 \textcite{WordSim353}, for context-less, or Stanford's Contextual Word Similarities (SCWS) \textcite{Huang2012}.
This evaluation is also suitable for evaluating single sense word-embeddings, e.g. skipgrams.


We can divide these into context clustering based approaches,
and collocation prediction based approaches.
This division is similar to the separation of collocation matrix factorisation,
and collocation prediction based approaches discussed in \Cref{sec:word-representations}.
It can be assumed thus that at their heart, like for word embeddings,
they are fundamentally very similar.
One could think of prediction of collocated words as a soft indirect clustering of contexts that can have those words.


\subsection{Context Clustering Based Approaches}
As the meaning of a word according to a word embedding is determined by the contexts in which it occurs, different meanings (senses) of the same words should occur in different contexts.
If we cluster the contexts that a word occurs in, one would expect to find distinct clusters for each sense of the word.
It is on this principle that the context clustering based approaches function.



\subsubsection{Offline clustering}
The fundamental method for most clustering based approaches is as per \tcite{Schutze:1998wordsenseclustering}.
That original work is not a neural word-sense embedding, however the approach remains the same.
\tcite{pantel2002WSI} and \tcite{Reisinger2010} are also not strictly neural word embedding approaches, however the overall method is also identical.


\aside[On context representations]{
These collocation clustering methods require finding a representation for the the context, which can then by combined with cosine similarity (or another metric), to give the similarities for clustering.
More generally, this can be related to the next chapter: \Cref{sec:sentence-representations-and-beyond}, as any of these methods could be used to derive a vector representation of a context, which could be used with cosine distance to define a usage similarity.
In most works discussed here comparatively simple representations of the contexts are used.
}

The clustering process is done by considering all word uses, with their contexts.
The contexts can a window words (like in many word-embedding models), the sentence, or defined using some other rules.
Given a pair of contexts, some method of measuring their similarity must be defined
In vector representational works, this is ubiquitously done by assigning each context a vector, and then using cosine similarity between those vectors.
Different methods define the context vectors differently
\begin{itemize}
\item \textcite{Schutze:1998wordsenseclustering} uses variations of inverse-document-frequency (idf) weighted bags of words, including applying dimensionality to find a dense representation.
\item \textcite{pantel2002WSI} uses the mutual information vectors between words and their contexts.
\pdfcomment{At somepoint I have to explain what td-idf is}
\item \textcite{Reisinger2010}, uses td-idf or $\chi^2$ weighted bag of words.
\item \tcite{Huang2012} uses td-idf weighted averages of (their own) single sense word embeddings for all words in the context.
\item  \tcite{kaageback2015neural} also uses a weighted average of single sense word embeddings, using a factor based on how close the word was, and a factor based on how likely the co-occurrence was according to the word embedding skip-gram models.

\end{itemize}
It is interesting to note that idf, td-idf, mutual information, skip-gram co-occurrence probabilities (being a proxy for point-wise mutual information \parencite{levy2014neural}), are all closely related measures.

\aside[On clustering]{
	Clustering can be defined as a (mixed integer) optimisation task, of assigning points to clusters so as satisfy some loss function based e.g. on minimising intra-cluster variance while maximising inter-cluster variance.
	As this is NP-hard, most clustering methods are approximate.
	k-means is very popular because of its simplicity,
	however it easily falls into local minima,
	and so normally is run dozens times (at least) to obtain more optimal results.
	K-means also has the issue of having to select the number of clusters ($k$).
	It should be remembers that there exist many other clustering methods than  k-means and k-means variants.
	These use different loss functions, and different strategies to overcome the NP-hard nature of the problem.
	In particular there mixture model methods, hierarchical methods, spectral methods, and others.
	We personally favour affinity propagation \parencite{frey2007clustering},
	though there is provably no ideal clustering algorithm, even in the non-heuristic case \parencite{kleinberg2003impossibility}.
	On any clustering task (word sense or otherwise) it is worth investigating several clustering algorithms, and not just settling for k-means (particularly not setting for k-means run once.).
	A series of interesting and very easy reading articles on clustering can be found at:
	\url{http://alexhwilliams.info/itsneuronalblog/2015/09/11/clustering1/}, \url{http://alexhwilliams.info/itsneuronalblog/2015/10/01/clustering2/}, and \url{http://alexhwilliams.info/itsneuronalblog/2015/11/18/clustering-is-easy/}
}


Secondly, some method is then applied to cluster the word uses based on the calculated similarity of the contexts.
\begin{itemize}
	\item \textcite{Schutze:1998wordsenseclustering} used group average agglomerative clustering.
	\item \textcite{pantel2002WSI} used a custom hierarchical clustering method.
	\item \textcite{Reisinger2010} used mixtures of von-Mises-Fisher distributions.
	\item \textcite{Huang2012} used spherical k-means.
	\item \textcite{kaageback2015neural} uses k-means.
\end{itemize}

Finally some method is used find an embedding for that cluster.
For non-neural embedding methods this step is not always done, as defining a representation is not the goal, though in general it can be derived from most clustering techniques.
\textcite{Schutze:1998wordsenseclustering} and 
\textcite{kaageback2015neural} use the centroids of their clusters.
\textcite{Huang2012} use a method of relabelling the word uses with the cluster ID,
then retrain their single sense word embedding method.
This relabelling technique is similar to the method later used by \textcite{Chen2014} for learning lexical sense representations, as discussed in \Cref{sec:pseudo-semi-supervised-method}.
As each cluster of contexts represents a sense, those cluster embeddings are thus also suitable word sense embeddings.

\pdfcomment{Maybe use a table to display this instread of lists?}

Inducing word sense embeddings via off-line clustering is all very similar in its functioning.
Represent the contexts: generally via some weighted measure.
Measure the similarity of the contexts: generally by cosine similarity
Cluster the contexts via some means.
Extract an embedding to represent the cluster: e.g. using the cluster centroid.
Use this embedding to represent the sense.

\subsubsection{Online clustering}
The methods discussed above all use off-line clustering.
That is to say the clustering is performed after the embedding is trained.
\tcite{neelakantan2015efficient} perform the clustering during training.
To do this they use a modified skip-gram based method.
They start with a fixed number of randomly initialised sense vectors for each context.
These sense vectors are used as input embeddings for the skip-gram context prediction task, over single sense output word embeddings.
Each sense also has linked to it a context cluster centroid, which is the average of all output embeddings for the contexts that that sense is assigned to.
Each time a training instance is presented, the average of the context output embeddings is compared to each sense's context cluster centroid.
The context is assigned to the cluster with the closest centroid, updating the centroid value.
This can be seen as similar to performing a single k-means update step for each training instance.
Optionally, if the closest centroid is further from the context vector than some threshold  a new sense can be created using that context vector as the initial centroid.
After the assignment of the content to a cluster, the corresponding sense vector is selected for use as the input vector in the skip-gram context prediction task.

\textcite{kaageback2015neural} investigated using their weighting function (discussed above) with the online clustering used by \textcite{neelakantan2015efficient}.
More generally any such weighting function could be used.
This online clustering approach is loosely similar to the co-location prediction based approaches.


\subsection{Collocation Prediction Based Approaches}
\aside[Probability]{
One may wish to brush up on basic probability notions for this section.
In particular joint, conditional and marginal probabilities definitions;
as well as Bayes law and the probability chain rule which come from those.
}
Rather than clustering the contexts, and using those clusters to determine embeddings for different senses, one could consider the sense as a latent variable in the task used to find word embeddings -- normally a language modelling task.
The principle is that it is not the word that determined it's collocated context words,
but rather a word sense.
So the word sense can be modelled as a hidden variable, where the word, and the context words are being observed.

\tcite{tian2014probabilistic} used this to define a  skip-gram based method for word sense embeddings.
For input word $\n w_i$ having senses given by $\seq S(\n w_i)$,
the probability of output word $\n w_o$ occurring near $\n w_i$ can be given as:
\begin{equation}
	P(\n w_o\mid \n w_i) = \sum_{\forall \n s_k \in \seq S(\n w_i)} P(\n w_o \mid \n s_k, \n w_i) P(\n s_k \mid \n w_i) \label{equ:tianmm}
\end{equation}

Given that a sense $\n s_k$ only belongs to one word $\n w_i$ (as Tian et. al. are not doing any form of sense sharing via synsets etc.),
we know that $k$th sense of the $i$th word only occurs when the $i$th word occurs.
we have that the intersection probability $P(\n w_i,\n s_k) = P(\n s_k)$.
%and $P(\n w_i) = \sum_{\forall \n s_k \in \seq S(\n w_i) P(\n s_k)}$;

We can thus write \Cref{equ:tianmm} as:
\pdfcomment{I am not happy with how this math looks.}
\begin{align}
P(\n w_o\mid \n w_i) &= \sum_{\forall \n s_k \in \seq S(\n w_i)} P(\n w_o \mid \n s_k) P(\n s_k \mid \n w_i) \label{equ:tianmm2}
\end{align}

A softmax classifier can be used to define $P(\n w_o \mid \n s_k)$, just like in normal language modelling.
With output embeddings for the words $\n w_o$, and input embeddings for the word senses $\n s_k$.
This softmax can be sped-up using negative sampling or hierarchical softmax.
The later was done by Tian et. al.

\Cref{equ:tianmm2} is in the form of a mixture model with a latent variable.
Such a class of problems are often solved using the Expectation Maximisation (EM) method.
In short, the EM procedure functions by performing two alternating steps.
The E-step calculates the expected chance of assigning word-sense for each training case ($\hat{P}(\n s_l \mid \n w_o)$) in the training set $\seq X$.
Where a training case is a pairing of a word use $\n w_i$, and context word $\n w_o$, with $\n s_l\in \seq S(\n w_i)$
\begin{align}
\hat{P}(\n s_l \mid \n w_o) = \frac{\hat{P}(\n s_l \mid \n w_i) P(\n w_o \mid \n s_l)}{\sum_{\forall \n s_k \in \seq S(\n w_i)} \hat{P}(\n w_o \mid \n s_k) P(\n s_k \mid \n w_i)}
\end{align}

The M-step updates the prior likelihood of each sense (that is without context) using the expected assignments from the E-step.

\begin{align}
\hat{P}(\n s_l \mid \n w_i) = \frac{1}{\left|\seq X\right|} \sum_{\forall (\n w_o,\n w_i)\in \seq X} \hat{P}(\n s_l \mid \n w_o)
\end{align}

During this step the likelihood of the $P(\n w_o \mid \n w_i)$ can be optimised to maximise the likelihood of the observations.
This is done via gradient descent on the neural network parameters of the softmax component: $P(\n w_o \mid \n s_k)$.

By using this EM optimisation the network can fit values for the embeddings in that softmax component.


A limitation of the method used by \tcite{tian2014probabilistic}, is that the number of each sense must be known in advance.
One could attempt to solve this by using for example the number of senses assigned by a lexicographical resource (e.g. WordNet).
However, situations where such resources are not available or not suitable are one of the main circumstances in which WSI is desirable  (for example in work using domain specific terminology, or under-resourced languages).
In these case one could apply a heuristic based on the distribution of senses based on the distribution of words \parencite{zipf1945meaning}.
An attractive alternative would be to allow senses to be determined based how the words are used. If they are used in two different ways, then they should have two different senses.
How a word is being used can be determined by the contexts in which it appears.


\tcite{AdaGrams} extends on this work by making the the number of senses for each word itself a fit-able parameter of the model.
This is a rather Bayesian modelling approach, where one considers the distribution of the prior

Considering again the form of \Cref{equ:tianmm2}
\begin{align}
P(\n w_o\mid \n w_i) &= \sum_{\forall \n s_k \in \seq S(\n w_i)} P(\n w_o \mid \n s_k) P(\n s_k \mid \n w_i) 
\end{align}
The prior probability of a sense given a word, but no context, is 
$P(\n s_k \mid \n w_i)$.
This is Dirichlet distributed.
The prior probability of any categorical classification task is a Dirichlet distributed, by  definition.
When considering that the sense my be one from an unlimited collection of possible senses,
then that prior becomes a Dirichlet process.

In essence, this prior over a potentially unlimited number of possible senses becomes another parameter of the model (along with the input sense embeddings and output word embeddings).
The fitting of the parameters of such a model is beyond the scope of this book;
it is not entirely dissimilar to the fitting via EM and gradient descent used by Tian et. al.
The final output of \tcite{AdaGrams} is as desired:
a set of induced sense embeddings, 
and a language model that is able to predict how likely a word is to occur near that word sense ($P(\n w_o \mid \n s_k)$).

By application of Bayes' theorem, the sense language model can be inverted to take a word's context,
and predict the probability of each word sense.

\begin{equation}
P(\n s_l \mid \n w_o) = \frac{P(\n w_o \mid \n s_l) P(\n s_l \mid \n w_i)}{\sum_{\forall \n s_k \in \seq S(\n w_i)} P(\n w_o \mid \n s_k) P(\n s_k \mid \n w_i)}
\end{equation}

With the common (but technically incorrect) assumption that all words in the context are independent \footnote{Technically, this use of Bayes theorem only requires that the context words be conditionally independent on on the word in question $\n w_i$, never the less incorrect.}
Given a sequence of context words:
\begin{equation}
\n \seq{W}_i = \n w_{i-\frac{n}{2}},\ldots, \n w_{i-1}, \n w_{i+1},\ldots, \n w_{i+\frac{n}{2}}
\end{equation}
being a context window (as in \Cref{sec:acausal-language-modeling})

\begin{equation}
P(\n s_l \mid \n \seq{W}_i) = 
\frac{\prod_{\forall \n w_{j}\in \n \seq{W}_i} P(\n w_{j} \mid \n s_l) P(\n s_l | \n w_i)}{\sum_{\n s_k \in \seq S(\n w_i)} \prod_{\forall \n w_{j} \in \n \seq{W}_i} P(\n w_{j} \mid \n s_k) P(\n s_k | \n w_i)}
\end{equation}
For $\n \seq{W}_i$ giving a context window of width $n$. i.e. $\n \seq{W}_i = \lbrace \n w_{i-\frac{n}{2}} \ldots \n w_{i-1} \ldots \n w_{i-1} \ldots \n w_{i+\frac{n}{2}} \rbrace$

\pdfcomment{Check this equation}

\section{Implementations}
Of the methods discussed in this section the following implementations exist:
\begin{enumerate}
	\item WordNet \parencite{tengi1998design} should have a binding in all modern programming languages.
	\item BabelNet \parencite{navigli2010babelnet} is intended to be accessed as an online resource, via a RESTful APImost commonly accessed. Users receive 1000 free queries per day. Which isn't much for these kinds of tasks. Academic users can request an upgrade to 50,000 queries per day, or to download a copy of the database (My own experience was the request was handled in <24 hours).
	\item AdaGrams \parencite{AdaGrams} has a julia implementation at \url{https://github.com/sbos/AdaGram.jl}. This was working well for Julia v0.5. At time of publication it was not working in Julia v0.6. Though updating is reasonable.
\end{enumerate}


\section{Conclusion}
\aside[Finding the nearest neighbours (Nearest Neighbour Trees)]{
	A common task with any representation (once trained) is to find its nearest neighbours, given a particular point.
	The na\"ive solution is to check the distance to all points.
	For $n$ points this is for all cases $O(n)$ operations.
	For word embeddings $n$ is the size of the vocabulary, perhaps $100,000$ words.
	$100,000$ operations per check, is not entirely unreasonable on modern computers (even when the operations are on 300 dimensional representations).
	However, for word sense embeddings, which have many senses per word in the vocabulary,
	this means many more points to check.
	30 senses per word is not unusual for fine-grained word sense induction.
	Having a total $n=3,000,000$ representations to check causes a noticeable delay, when performing thousands of checks for a similarity or analogy task.
	To solve this we can use  data structures exist for fast nearest neighbour querying.
	A k-d tree takes at worst $O(n \log_2(n))$ time to construct.
	Once constructed on average it takes $O(\log (n))$ to find the nearest neighbour to any point.
	This makes checking the nearest neighbour nearly instantaneous for even the largest vocabularies.
}
Word sense representations allow the representations of the senses of words.
When one word has multiple meanings.
This increases the expressiveness of the representation.
They can in general be applied anywhere word embeddings can.
They are particularly useful for translation,
and in languages with large numbers of homonyms.

It leads naturally to the next section on phrase representation. Rather than a single word having many meanings, the next chapter will discuss how a single meaning may take multiple words to express.


\end{document}