\documentclass[12pt,parskip]{komatufte}
\include{preamble}
\begin{document}

\setchapterpreamble{%
	\dictum[the seven senses of \natlang{literally},
	 \textit{Oxford English Dictionary}, 3rd ed., 2011]
	{
		\begin{description}
			\setlength\itemsep{0em}
			\item[1a.] In a literal, exact, or actual sense; not figuratively, allegorically, etc.
			\item[1b.] Used to indicate that the following word or phrase must be taken in its literal sense, usually to add emphasis.
			\item[1c.] colloq. Used to indicate that some (frequently conventional) metaphorical or hyperbolical expression is to be taken in the strongest admissible sense: `virtually, as good as'; (also) `completely, utterly, absolutely' \ldots
			\item[2a ]With reference to a version of something, as a transcription, translation, etc.: in the very words, word for word.
			\item[2b.] In extended use. With exact fidelity of representation; faithfully.
			\item[3a.] With or by the letters (of a word). Obs. rare.
			\item[3b.] In or with regard to letters or literature. Obs. rare.
		\end{description}		
}}

\chapter{Word Sense Representations}\label{sec:word-sense-representations}
\begin{abstract}
	Chapter 5: Word Sense Representations (5-10 pages)
	In this chapter, technologies for representing the multiple meanings of a single word is discussed.
	This is an growing area, and is particularly important in languages where polysemous and homonymous words are common (This includes English, but it is even more prevalent in for example Chinese languages).
	It leads naturally to the next section on phrase representation. Rather than a single word having many meanings, the next chapter will discuss how a single meaning may take multiple words to express.
\end{abstract}
	
\section{Word Senses}

Words have multiple meanings.
When assigning a single representation to a word, it is impossible for that representation to truly describe the meaning in all contexts.
It may have some features applicable to some uses but not to others,
it may be an average of all features for particular contexts,
or it may only represent the most common sense,
or a combination of all.




\aside[polysemous/homonymous]{A word with multiple meanings. For purposes of all but the most linguistically driven NLP work there is no need to distinguish between a polyseme and a homonym. Polysemous and homonymous are synonymous.}

\aside[Part of Speech/POS]{The syntactic category a word belongs to. Different POS tags come from different tag sets.
Can be simple as the WordNet: noun, adjective, verb, etc. Or complex as in the Brown: VBG-\emph{verb, gerund/present participle}, NN-\emph{	noun, singular or mass}.
 }
\aside[word use]{An occurrence of a word in a text, such as a training corpus. Each word will have multiple uses in a text. Each word use will only have one particular meaning and will thus belong to one synset.}
\aside[lemma]{the base form of the word as defined by the lexicographical resource. It is normally closely related to (often identical to) the stem which is defined as the root form of the word with all morphological inflections (e.g. tenses) removed. The words are sometimes used interchangeably, though this is not truly correct.}
\aside[Lexeme]{The set of words that share a common lemma: \natlang{go}, \natlang{going}, \natlang{goes}, and \natlang{went} all belong to the lexeme headed by the lemma \natlang{go}}
\aside[synset]{A synset is a set of synonymous words: that is words that have the same meaning}
\aside[gloss]{A gloss is the dictionary entry for a word-sense, it normally includes both the definition and an example of use.
In WordNet each synset shares a common gloss.}

%\aside[Sensekey]{This is not a commonly used phrase by linquists, but it is an important implementation detail when using WordNet for WSD tasks. It repressents the mapping between lemma+pos and synset, and is needed because that is a many-many relationship. It take the basic form \texttt{lemma\%pos\_id:lex\_filenum}}
\aside[Lemmatization]{WordNet is indexed by lemmas. This in essence means with tense and plurality information removed. WordNet comes with a lemmatized called morphy to do this.(c.f. stemming)}
\aside[Unlemmatization]{Given lemma (as one can extract from WordNet) and a full POS tag (Such as a Brown-style tag) for a word, it is possible to undo the Lemmatization with a high degree of reliability using relatively simple rules. The POS tag encodes the key inflectional features that are lost. Patten.en \pcite{de2012pattern} is a python library encoding such rules (pluralisation, verb conjugation, etc.); though combining them with the POS tag to drive them is a task left for the reader. This can can be used to find substitute words using WordNet's features, for finding synonyms, antonyms and other lemmas from lexically related categories.}


The standard way to assign word senses is via some lexicographical resource.
i.e. a dictionary, or a thesaurus.
There is not a canonical list of word senses that are truly real and consistently defined in English.
Every dictionary is unique, with different definitions and numbers of word senses.
The most commonly used lexicographical resource is WordNet \pcite{miller1995wordnet},
there are several WordNets equivalents in other languages,
as well as the multi-lingual  BabelNet \pcite{navigli2010babelnet}.



\begin{figure}
	\caption{The relationship between terms used to discuss word sense problems.
		The Lemma stands in for the lexeme, for WordNet's purposes when indexing.
		The core task of WSD is to go from a word-use to the corresponding synset.
		For WSD contests the word-use is normally pretagged with its POS tag, and its corresponding lemma,
		as these can be found with very high reliability by a POS tagger and a stemmer respectively.
	}
	\begin{tikzpicture}[circle/.append style={draw, font=\footnotesize, rounded rectangle}]
		\node[circle](worduse){\normalsize Word-use\\ e.g.\\ \natlang{Fred \underline{goes} shopping}};
		\node[circle, above right = 2 of worduse](pos){\normalsize POS\\ e.g.\\ \emph{verb}};
		\node[circle, below right = 2 of worduse](lexeme){\normalsize Lexeme\\  e.g. \\ \{\natlang{go}, \natlang{going},\\ \natlang{goes}, \natlang{went}\}};
		\node[circle, draw, below right = 2 of pos](synset){\normalsize Synset\\ e.g. \\ \{\natlang{go}, \natlang{move}, \\ \natlang{locomote}\}};

		
		
		\draw[->] (worduse) edge node[labe] {has 1} (synset);
		\draw[->] (worduse) edge node[labe] {has 1} (pos);
		\draw[->] (worduse) edge node[labe] {has 1} (lexeme);
		
		\draw[->] (synset) edge node[labe] {has 1} (pos);
		\draw[<->] (synset) edge node[labe] {has many} (lexeme);
		
		\node[circle, above right = 2 of synset](gloss){\normalsize Gloss\\ e.g. \\ \emph{change location \ldots}};
		\draw[<->] (synset) edge node[labe] {has 1} (gloss);
		
		\node[circle, right = 2 of lexeme](lemma){\normalsize Lemma\\ e.g. \\  \natlang{go}};
		\draw[<->] (lemma) edge node[labe] {has 1} (lexeme);
	\end{tikzpicture}
	
	
\end{figure}



\subsection{Word Sense Disambiguation}


\aside[Semantic Syllepsis]{
	Also known as pathological sentences that kill almost all WSD systems.
	Consider the sentence: \natlang{John used to work for the \emph{newspaper} that you are carrying.},
	In this sentence the word-use newspaper simultaneously have two different meanings: it is both the company, and the object.
	This violates out earlier statement that every word-use belongs to exactly one synset.
	As word sense disambiguation systems normally attempt to assign a single sense  to an each word they are unable to handle these sentences.
	Most word sense induction systems can not much better: at best a new sense could be allocated for the join use, which does not correspond to the human notion of the word having two senses for different parts of the sentence.
	Most works on word sense disambiguation outright ignore these sentences, or consider them to be ungrammatical, or incorrect.
	However, they are readily understood and used without thought by most native speakers.
	These constructions are also known as zeugma, although zeugma is itself as highly polysemous word, so its usage varies. 
}



Word sense disambiguation is one of the hardest problems in NLP.
Very few systems significantly out perform the baseline most common sense (MCS).

Progress on the problem is made difficult by several factors.

There are problems with the data available.
The lack of very large scale training corpora rendering fully supervised methods difficult.
The limited testing corpora can result in systems that allow 
Lack of human agreement on the correct sense, resulting in weak ground truth.

There are also issues inherent in the task.
Determining the sense may require very long range information:
for example the information on context may not even be in the same sentence.
It may require knowing the domain of the text, where word sense uses vary between domain.
It may in-fact be intentionally unclear, with multiple correct interpretations, as in a pun.
Or be unintentionally unknowable, due to poor writing style, such that it would confuse any human reader also.


It can also be said that word senses are highly artificial and do not adequately represent meaning.
However, WSD is required to interface with  lexicographical resources,
such as translation dictionaries, ontologies (e.g. OpenCyc), and other datasets (e.g. ImageNet ).


It may be interesting to note, that the number of meanings a word has is anti-logarithmically related to its rank \tcite{zipf1945meaning}.
\pdfcomment{idk if this is the right way to talk about it, can I just say Zipfian, or is that not true in this case?}
That is to say the most common words have exponentially more meanings than rarer words.
This aligns well with our notion that precise (e.g. technical) words exist but are used only infrequently -- since they are only explaining a single situation
This also means that by far most word uses are potentially very ambiguous.

However, also the most commonly used word sense (for a given word ) is also overwhelmingly more frequent than it's less common brethren -- also being Zipfian distributed in there usage \cite{Kilgarriff2004}.

For this reason the most frequent sense (MFS) baseline is often the best performing result in any WSD task.

\subsubsection{Most Frequent Word Sense}
\pdfcomment{Talk abow MFS baseline and why it always wins}
For a word use $w_i$, having some sense $h_j$
then without any further context the 
probability of that sense being the correct sense is $P(h_j \mid w_i)$.



\section{Word Sense Representation}
\pdfcomment{Rewrite this stuff about wordnet being a poor moral baseline to fit the books intended audience better}
\aside[WordNet is not a strong moral baseline]{
	
	WordNet is the standard "dictionary/thesaurus/source of lexicographical information" for algorithms. (ML and otherwise)
	(It is also the seed for some online dictionaries)
	
	The definitions in WordNet were written in a large part by undergrads at Princeton in 1990. That is why it has things like "S: (v) nag, peck, hen-peck (bother persistently with trivial complaints) "She nags her husband all day long"".
	
	Further, the dataset used to determine frequency of Synsets (that is the total count of all words) comes from a dataset now known as SemCore.
	Which is a sense annotated subset of the Brown Corpus.
	
	The Brown Corpus is a sampling of American texts from 1961.
	The norms of 1961 were not the norms of today.
	(Remember the US didn't pass the Civil Rights act to end segregation until 1964).
	
	Now here is a fact I will return to later. The word Wheelchair is only mentioned once in the Brown corpus.
	
	Moving on:
	ImageNet is a common resource for image recognition.
	It labels images with their WordNet synset.
	More generally, using a high performing ImageNet network minus its final classification layer, is commonly used as a feature extractor on any computer vision machine learning problem.
	
	ImageNet only uses the most common synsets.
	(in 2009 it has 8,000. It is now up to  22,000)
	Where most common is determined using the counts reported by WordNet (at least originally it was).
	Which were extracted from SemCore, i.e. The Brown Corpus.
	Which means that it did not include wheelchairs for example (it does now).
	
	And this is why automatic captioning systems recognize wheelchairs as skateboards.
	
	Because of the fact that they are based on what people were writing about in 1961.}

\subsection{Supervised Methods}
The simple and direct method, is to take a dataset that is annotated with word-senses,
and then treat each tagged word as if it were a word, then apply any of the methods for word representation discussed in \Cref{sec:word-representations}.
\tcite{iacobacci2015sensembed} use a Continuous Bag of Word language model \parencite{mikolov2013efficient}, doing exactly this.
This does however run into the aforementioned problem, that there is relatively little training data that has been manually sense annotated.
Iacobacci et al. use a 3rd party WSD tool, BabelFly \cite{Moroetal:14tacl}, to annotate the corpus with senses.
This allows for existing word representation techniques to be applied, but being limited to the use of an existing WSD tool makes it unsuitable for use in WSD tasks.


\subsection{Pseudo-semi-supervised Method}
\tcite{Chen2014} use an almost semi-supervised approach to train sense vectors.
They partially disambiguate their training corpus, using initial word sense vectors and WordNet.
They then discard these original sense-vectors, and use the partially disambiguated corpus to train sense-vectors via a skip-gram variant.
\pdfcomment{Don't call things Pseudo-semi-supervised Method}

%
\aside[Cosine distance]{
As a refresher:
here we talk of cosine distance, as it is more reasonable as smaller implies closer.

(Though it is still not a true metric as $d_{cos}(v,kv)=0$ for all $k\in \mathbb{R}_+$).
Other times you may see cosine similarity, it ranged between -1 (most different) and 1 (most similar).
Cosine similarity is given by $sim(a,b)=\frac{a\cdot b}{\left\Vert a\right\Vert _{2}\left\Vert b\right\Vert _{2}}=\cos(\angle a b )$
i.e. the unit-length normalised dot product of the vectors.
Cosine distance is usually defined at $d_{cos}(a,b)=\frac{1-sim(a,b)}{2}$.
It ranged between 0 (most similar) and 1 (most different).
}

The first phase of there method is in essence a word-embedding based WSD system.
When assessed as such a WSD result they report that it only marginally exceeds the MFS baseline,
though that is not at all unusual for WSD algorithms as discussed above.

In phase 1 they assign every word-sense in WordNet, a sense vector.
This sense vector is the average of word-embeddings of a subset if words in the gloss,
as determined using pretrained skip-grams \parencite{mikolov2013efficient}.
For the word $w$ with word-sense $w_{s_i}$,
they select a set of candidate words, $cand(W_{s_i})$, from the gloss to be included in the average
based on the follow set of requirements.
First the word must be a content word: that is a verb, noun, adverb or adjective.
Secondly, its cosine distance to $w$ must be below some threshold $\delta$.
Finally, it must not be the word itself (One can assume this is in terms of identical lexemes).
Or written mathematically, where $C_v$ is the skip-gram word vector for $v$
\begin{equation}
cand(w_{s_{i}})=\left\{ u\left|\begin{array}{c}
u\in gloss(w_{s_{i}})\\
\wedge d_{cos}(C_w,C_u)<\delta\\
\wedge pos(u)\in\left\{ verb,adv,noun,adj\right\} \\
\wedge u\ne w
\end{array}\right.\right\} 
\end{equation}
The phase 1 sense vector for $w_{s_i}$ is the mean of the word vectors for all the words in $cand(w_{s_{i}})$.
The effect of this is that most words in the same synset will have similar but not necessarily identical initial vectors.
\pdfcomment{Consider what this does to rare senses of a word, i.e. the base wordsense will be distant}.



They then use the phase 1 sense vectors to disambiguate the words in their unlabelled training corpus.
For each sentence in the corpus, they define an initial content vector.
This is done by taking the mean of the skip-gram word embedding for all content words in the sentence.
For each word in the sentence, they then compare each sense vector to the context vector.
If the closes sense vector is below a given threshold,
then it that word is tagged with that word-sense, and the context vector is updated to use the sense-vector instead of the word vector.
Words that do not come within the threshold are not tagged.
This is an important features, as it means that words without clear senses do not get a sense given to them.
Thus avoiding any dubious sense tags for the next training step.


Once the corpus is (partially) tagged Chen et. al. employ the skip-gram word-embedding method with a variation to predict the word-senses.
The original sense vectors are discarded.
Rather than the model being tasked only to predict the surrounding words,
it is tasked to predict surrounding words and their sense-tags (where present).
For purposes of the loss function predicting tags and words is weighted equally.
Note that the input the the skip-gram is the central word, not the central word-tag.
In this method the word-sense embeddings are output embeddings.

The phase 1 sense vectors have not been assesses as their representational quality.
It can be assumed as it was not reported that it was worse than those found in phase 2.
The phase 2 sense vectors were not assessed for their capacity to be used for word sense disambiguation.
An extension to the method of \textcite{bibid}, to use the phase 2 vectors for WSD, would allow this method to be used to disambiguate its own training data. Allowing the method to become self-supervised.


\section{Word Sense Induction}

\aside[Can we go from induced senses to lexical senses]{
	A natural question given the many WSI systems,
	and the existing wealth of lexically annotated resources,
	is if we can induce sense, then align them to the lexically defined senses.
	\textcite{agirre2006} proposed a method for doing this using a weighted mapping based on the probabilities found using induced sense WSD on a labelled ``mapping'' corpus.
	This is only been used on relitively small datasets with only hundreds of words,
	being SenseEval 3 \parencite{mihalcea2004senseval} and SemEval-2007 Task 02 \parencite{SemEval2007WSIandWSD}.
	It is unclear as to how well it scales to real-word WSD tasks.
	Our own limited investigations with the larger SemEval 2007 Task 7 \parencite{Navigli:2007:STC:1621474.1621480} suggest that it may not.
	Finding good methods to link unsupervised representations,
	to human defined representations remains a topic worthy of research.
}


In this section we will discuss methods for finding a word-sense, without reference to a standard set of senses.
Such systems must discover the word senses at the same time as they find their representations.
One strong advantage of these methods is that they do not require a labelled dataset.
As discussed there are relatively few high-quality word-sense labelled datasets.
The other key advantage is not relying on fixed senses determined by a lexicographer.
This is particular useful if the words senses are highly domain specific;
or in a language without strong lexicographical resources.
This allows the data to inform on what word-senses exist.

\aside[Why do skipgrams perform so well on SCWS?]{
	SCWS is a corpus designed for evaluating word sense embeddings.
	Single sense embeddings (e.g. skipgrams) can not take advantage of the context information in the SCWS, however, they do often perform comparably to the word-sense embeddings.
	Sometimes even outperforming them.
	It is unclear as to if this highlights the difficulty of the task -- that the impact of context is hard to gauge.
	Or if it merely shows just how well tuned the more mature single sense embedding methods are.
}

Most vector WSI and WSR approaches are evaluated on similarity tests.
Like WordSim-353 \textcite{WordSim353}, for contextless, or Stanford's Contextual Word Similarities (SCWS) \textcite{Huang2012}.
This evaluation is also suitable for evaluating single sense word-embeddings, e.g. skipgrams.


We can divide these into collocation clustering based approaches,
and collocation prediction based approaches.
This division is very similar to the separation of collocation matrix factorisation,
and collocation prediction based approaches discussed in the Word Representation chapter.
It can be assumed thus that at their heart, like for word embeddings,
they are fundamentally very similar.


\subsection{Collocation Clustering Based Approaches}
The fundamental method for most clustering based approaches is as per \tcite{pantel2002WSI}.
\textcite{pantel2002WSI} is not a neural word embedding.

\subsection{Collocation Prediction Based Approaches}

\tcite{tian2014probabilistic} 





\section{Word sense induction (WSI)}












\end{document}