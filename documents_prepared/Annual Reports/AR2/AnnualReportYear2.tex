%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage{standalone}
\usepackage{times}
\usepackage[utf8]{luainputenc}
\usepackage{microtype}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2cm,lmargin=1.5cm,rmargin=1.5cm}
\usepackage[english]{babel}
\usepackage{verbatim}

\usepackage{tikz}
\usetikzlibrary{positioning, fit,arrows,shapes.geometric}
\usetikzlibrary{backgrounds}
\usetikzlibrary{shadows}
\usepgflibrary{shadings}

\usepackage{xcolor}
\usepackage{pgfgantt}


\usepackage[backend=bibtex,
style=verbose,
bibencoding=ascii,
maxcitenames=99,
url=true,
hyperref=true
]{biblatex}
\bibliography{master}
\DeclareFieldFormat{abstract}{\par Abstract: \emph{#1}}
\renewbibmacro*{finentry}{\printfield{abstract}\finentry}

\usepackage{cleveref}

\begin{document}

\title{Annual Report 2016-2017}
\author{Lyndon White}

\maketitle

\section{Summary of Research Progress to Date  (including any change in focus, and list of publications)}

Work has proceeded, with some deviation from the Project Proposal. 
The work has yield several publications and pending publications.
All this work has yielded publication, or is pending publication; so in the following sections, I will allow the publications abstracts to describe the work.


\subsection{Sentence Generation from Sum of Word Embeddings}
This work was largely carried out in late 2015/ early 2016.
The publications were finalised and presented in 2016.
The works were mentioned in prior annual report.

\subsubsection*{Publications Arising}
This covers the first half of going from Sum of word embedding, to a bag of words as a conference paper. This received the Best Student Paper award at CICLing 2016. 

\vspace{1cm}
\fullcite{White2015BOWgen}
\vspace{1cm}

The second publication, covers the remaining step to go from the bag of words to an ordered sentence; and the evaluation of the overall systems.
This was presented as a workshop paper at ICDM.
\vspace{1cm}
\fullcite{White2016SOWE2Sent}
\vspace{1cm}

\subsection{Characteristic Vector Autoregression}

It was intended that in this time, 
the next project to be worked on was Characteristic Vector Autoregression.
Early investigations were not promising.
At the Big Data Masterclass that was around this time,
I had the opportunity to speak to the guest presenter Dr Hien Nguyen.
He has had some experience using these models, and confirmed that there was little chance of them proving useful for anywhere near the number of dimensions commonly used for word embeddings.
On this basis, that line of inquiry was terminated.

\subsection{Word Sense Embeddings}
It had become clear that word embeddings were the key building block of the larger expression of which the core inquiry focused upon.
A key limitation of them, for this use, was using a single representation for a word which can (and normally will) have multiple meanings.
Without considering this, it seems that any larger work would be bounded in it upper limits.
Towards improving understanding of this area, and building the tools required for further work multiple sense word embeddings.

Initial efforts towards defining a new method for word sense induction were met with frustration, due to numerical instability.
A side avenue of how to leverage pretrained embeddings  toward new uses proved more fruitful.
This has been prepared for publication, and is currently under review.

\vspace{1cm}
\fullcite{WhiteRefittingSenses}
\vspace{1cm}

\subsection{Meaning of Water Usage}
There was intent to extend the work on determining the multiple meanings of words from context to the domain of time series utilities usage.
The reason for a person using a particular amount of water at a particular time can be at least partially inferred from the context.
For example steady high water usage during business hours, could mean the person was operating a linen washing business
The same amount of water at 6:00 am, would have a different sense -- in this case likely an automated reticulation system.

However, this work had to be put on hold, as it relied on the expertise of Dr Liu with this kind of data.
Dr Liu at this time went on maternity leave.

\subsection{Colour Generation}
To extend the work on words with multiple meanings into longer phrases,
colour descriptions have been chosen as a constrained domain for that investigation.
The description of colours for example ``bluish green'' vs ``dark teal`` is a subset of language with many of the interesting features of the language as a whole -- it is a microcosm of language.
In particular it has large amounts of multiple sense words such as ``tan'' which can be any number of shades from where it overlaps with the senses of ``terracotta``, to where it overlaps with ``beige''.
It also features significant structural patterns, such as the use of ``ish'' as a term to combine two colours.
The intent is to investigate both the generation of colour from its description,
and of the description from the colour.

As well as having theoretic interest as a demonstration of the concepts;
such research has application as a complainant of in user input systems (such as intent recognition), and in natural language generation systems (such as captioning). It also has direct applications as a teaching tool for English language learners, and as an accessibility add for the visually impaired.

\subsection{Problems}


There were several difficulties on installing the required OS, software and drivers for the new GPU work station.
Initially the system took longer than expected to be set up ('INC0867929');
to to incompatibilities with the RedHat SOE.
This issue was work around by using the earlier used CPU base workstations/NeCTAR,
until it could be solved by faculty IT.
This did have some effect of increasing time taken in investigations.

When moving over to the new set up once it was complete, there were further issues with having the correct versions of software (Incident 'INC0966100').
This took much longer than expected to be resolved, as its timing coincided with the new student intake; which faculty IT had to prioritise.
It has now been largely overcome, as I have now built and installed some of the software myself.


As mentioned above Dr Liu has taken maternity leave, in December 2016.
She is not expected to fully return until 2018.
The loss of ready access to Dr Liu's advice and expertise is an issue.
Currently, it is planned to begin seeing Dr Liu at meetings again in the coming weeks; of course her availability will remain very restricted.
As a step toward handling this problem, research projects where her specific expertise is requires, such as the Water Meaning work have had to be put on hold.



\newpage

\section{Completion Plan}
The Gantt Chart showing in \Cref{gantt} is updated from that submitted with the first annual report. It shows the tasks remaining to be completed; and the tasks completed. 


\subsection{Tasks Completed (2016-2017)}
UPDATE THIS SECTION
As the thesis is by publication, the key progress indicator towards textual completion, in the publications. As mentioned above 1 publication has been completed, 1 accepted, and a third is in the final stages of editing. The numerals in the following section align with the Gannt Chart in  \Cref{gantt}. These publications will form key chapters in the thesis.

\subsubsection*{Semantic Evaluation}
\begin{description}
	\item[1:] \cite{White2015SentVecMeaning} (\textbf{Published}: \citefield{White2015SentVecMeaning}{booktitle}, \citeurl{White2015SentVecMeaning})
\end{description}
\subsubsection*{Sum of Word Embeddings}
\begin{description}
	\item[2:] \cite{White2015BOWgen} (\textbf{Accepted}: 06--Mar--2016 \citefield{White2015BOWgen}{booktitle})
	\item[3:] \cite{SOWEgen} (\textbf{In preparation})
\end{description}


\subsection{Tasks Remaining (2016-2017)}
One change is the removal of the "semisupervised semantics constraints" research task, to  be replaced with a "characteristic vector autoregression" task. This new task has additional clarity on the method being used. As part of the investigations with this method, semi-supervised semantic constraints may be included. Similarly Tree search has been replaced with Structure search -- this is merely a clarification that there may be non-tree structures that form the best representation of sentences.
\subsubsection*{Characteristic Vector Autoregression (Apr 2016--Sep 2016)}
An vector autoregressive (VAR) model is potentially able to model sentence dynamics, as a time series.
Rather than trying to fit a model that described all sentences, one VAR will be trained per sentence.
The VAR parameters (matrices) are then themselves used as a vector space representation -- the characteristic VAR for the sentence.
Potential issues include the question of if sentences have a linear structure, and if they have enough words to be modelled this way.
Solutions such as more advanced VAR (non-linear VAR, structured VAR) may required. Potentially unfixing the word vectors may also useful.
This has some potential, not just as a representation method but as a generative method.
These issues, solutions and uses will be progressively investigated.

\subsubsection*{Structure Search (Oct 2016--Mar 2017)}
A sentence is words arranged into structure. 
Only certain structures are grammatically permitted.
Only certain words can go into each structural slot.
A generative sentence model could be created,
by heuristically ordering the possible structures,
then by assessing the structures by how well words filling that structural slot can be found.
This method would be applied on top of another technique.
Such as that produced by the characteristic vector autoregression.

\begin{figure}[h!]
	\centering
	\resizebox{0.9\textwidth}{!}{\centering
		\input{gantt.tex}
	}
	\caption{\label{gantt} an Updated Gnatt Chart}
\end{figure}




\end{document}
