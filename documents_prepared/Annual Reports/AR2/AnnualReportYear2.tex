%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage{standalone}
\usepackage{times}
\usepackage[utf8]{luainputenc}
\usepackage{microtype}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2cm,lmargin=1.5cm,rmargin=1.5cm}
\usepackage[english]{babel}
\usepackage{verbatim}

\usepackage{tikz}
\usetikzlibrary{positioning, fit,arrows,shapes.geometric}
\usetikzlibrary{backgrounds}
\usetikzlibrary{shadows}
\usepgflibrary{shadings}

\usepackage{xcolor}
\usepackage{pgfgantt}


\usepackage[backend=bibtex,
style=verbose,
bibencoding=ascii,
maxcitenames=99,
url=true,
hyperref=true
]{biblatex}
\bibliography{master}
\DeclareFieldFormat{abstract}{\par Abstract: \emph{#1}}
\renewbibmacro*{finentry}{\printfield{abstract}\finentry}

\usepackage{cleveref}

\begin{document}

\title{Annual Report 2016-2017}
\author{Lyndon White}

\maketitle

\section{Summary of Research Progress to Date}

Work has proceeded, with some deviation from the Project Proposal. 
The work has yield several publications and pending publications.
All this work has yielded publication, or is pending publication; so in the following sections, I will allow the publications abstracts to describe the work.


\subsection{Sentence Generation from Sum of Word Embeddings}
This work was largely carried out in late 2015/ early 2016.
The publications were finalised and presented in 2016.
The works were mentioned in prior annual report.

\subsubsection*{Publications Arising}
This covers the first half of going from Sum of word embedding, to a bag of words as a conference paper. This received the Best Student Paper award at CICLing 2016. 


\fullcite{White2015BOWgen}
\vspace{1cm}

The second publication, covers the remaining step to go from the bag of words to an ordered sentence; and the evaluation of the overall systems.
This was presented as a workshop paper at ICDM.

\fullcite{White2016SOWE2Sent}
\vspace{1cm}

\subsection{Characteristic Vector Autoregression}

It was intended that in this time, 
the next project to be worked on was Characteristic Vector Autoregression.
Early investigations were not promising.
At the Big Data Masterclass that was around this time,
I had the opportunity to speak to the guest presenter Dr Hien Nguyen.
He has had some experience using these models, and confirmed that there was little chance of them proving useful for anywhere near the number of dimensions commonly used for word embeddings.
On this basis, that line of inquiry was terminated.

\subsection{Word Sense Embeddings}
It had become clear that word embeddings were the key building block of the larger expression of which the core inquiry focused upon.
A key limitation of them, for this use, was using a single representation for a word which can (and normally will) have multiple meanings.
Without considering this, it seems that any larger work would be bounded in it upper limits.
Towards improving understanding of this area, and building the tools required for further work multiple sense word embeddings.

Initial efforts towards defining a new method for word sense induction were met with frustration, due to numerical instability.
A side avenue of how to leverage pretrained embeddings  toward new uses proved more fruitful.
This has been prepared for publication, and is currently under review.

\subsection{Meaning of Water Usage}
There was intent to extend the work on determining the multiple meanings of words from context to the domain of time series utilities usage.
The reason for a person using a particular amount of water at a particular time can be at least partially inferred from the context.
For example steady high water usage during bussiness hours, could mean the person was operating a linen washing bussiness.
The same amount of water at 6:00 am, would have a different sense -- in this case likely an automated reticulation system.

However, this work had to be terminated, as it relied on the expertise of Dr Liu with this kind of data.
Dr Liu at this time went on maternity leave.




\subsection{Problems (including any change in focus)}


\newpage

\section{Completion Plan}
The Gantt Chart showing in \Cref{gantt} is largely unchanged from the that in the project proposal. It shows the tasks remaining to be completed; and the tasks completed. 


\subsection{Tasks Completed (2015-2016)}
As the thesis is by publication, the key progress indicator towards textual completion, in the publications. As mentioned above 1 publication has been completed, 1 accepted, and a third is in the final stages of editing. The numerals in the following section align with the Gannt Chart in  \Cref{gantt}. These publications will form key chapters in the thesis.

\subsubsection*{Semantic Evaluation}
\begin{description}
	\item[1:] \cite{White2015SentVecMeaning} (\textbf{Published}: \citefield{White2015SentVecMeaning}{booktitle}, \citeurl{White2015SentVecMeaning})
\end{description}
\subsubsection*{Sum of Word Embeddings}
\begin{description}
	\item[2:] \cite{White2015BOWgen} (\textbf{Accepted}: 06--Mar--2016 \citefield{White2015BOWgen}{booktitle})
	\item[3:] \cite{SOWEgen} (\textbf{In preparation})
\end{description}


\subsection{Tasks Remaining (2016-2017)}
One change is the removal of the "semisupervised semantics constraints" research task, to  be replaced with a "characteristic vector autoregression" task. This new task has additional clarity on the method being used. As part of the investigations with this method, semi-supervised semantic constraints may be included. Similarly Tree search has been replaced with Structure search -- this is merely a clarification that there may be non-tree structures that form the best representation of sentences.
\subsubsection*{Characteristic Vector Autoregression (Apr 2016--Sep 2016)}
An vector autoregressive (VAR) model is potentially able to model sentence dynamics, as a time series.
Rather than trying to fit a model that described all sentences, one VAR will be trained per sentence.
The VAR parameters (matrices) are then themselves used as a vector space representation -- the characteristic VAR for the sentence.
Potential issues include the question of if sentences have a linear structure, and if they have enough words to be modelled this way.
Solutions such as more advanced VAR (non-linear VAR, structured VAR) may required. Potentially unfixing the word vectors may also useful.
This has some potential, not just as a representation method but as a generative method.
These issues, solutions and uses will be progressively investigated.

\subsubsection*{Structure Search (Oct 2016--Mar 2017)}
A sentence is words arranged into structure. 
Only certain structures are grammatically permitted.
Only certain words can go into each structural slot.
A generative sentence model could be created,
by heuristically ordering the possible structures,
then by assessing the structures by how well words filling that structural slot can be found.
This method would be applied on top of another technique.
Such as that produced by the characteristic vector autoregression.

\begin{figure}[h!]
	\centering
	\resizebox{0.9\textwidth}{!}{\centering
		\input{gantt.tex}
	}
	\caption{\label{gantt} an Updated Gnatt Chart}
\end{figure}




\end{document}
