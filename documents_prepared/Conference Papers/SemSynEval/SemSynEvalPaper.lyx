#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass acm-sigs
\begin_preamble
\usepackage{tikz}
\usetikzlibrary{positioning, fit,arrows,chains,shapes.geometric}
\usepackage{verbatim}
\usepackage{pdfcomment}
\usepackage{environ}
\RenewEnviron{comment}{\pdfcomment[author={Lyndon}]{\BODY}}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
A Method for the Evaluation of the Semantic Localization of Sentence Embeddings
\end_layout

\begin_layout Author
Lyndon White
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Who are the authors? Should sort that out at some point
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\entails}{\vDash}
\end_inset


\end_layout

\begin_layout Abstract
Several approaches for embedding a sentence into a vector space have been
 developed recently.
 It is unclear as to the information contained into these embeddings.
 To what extent the position in vector space reflect semantic meaning, rather
 than other factors such as syntactic structure.
 For many applications it is most desirably to be encoding the meaning in
 the embedding.
 Currently the semantic localization is assessed indirectly through practical
 benchmarks on these applications.
 Here a new method is presented to directly assess if the embeddings produced
 by a model are semantically consistent.
 It is demonstrated on 3 existing models and on a bag of words benchmark.
\end_layout

\begin_layout Abstract
This new method uses as synthetic corpus constructed from a paraphrase corpus,
 by using word replacement to generate sentences which are semantically
 equivalent, or not, to the original sentences.
 The embedding distances between the original and the synthetic sentences
 can then be compared ot the distances between the original and the semantically
 equivalent paraphrase.
 This thus allows the assessment as to strength of the link between semantic
 closeness and the embedding closeness.
\end_layout

\begin_layout Terms
General terms from the ACM classification.
 See 
\begin_inset CommandInset href
LatexCommand href
target "http://www.acm.org/about/class/"

\end_inset


\end_layout

\begin_layout Keywords
Free key words
\end_layout

\begin_layout Section
Introduction
\begin_inset Note Note
status open

\begin_layout Plain Layout
1 page
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Various sentence embeddings are often called semantic vector space representatio
ns, such as in 
\begin_inset CommandInset citation
LatexCommand cite
key "iyyer2014generating"

\end_inset

.
 The embedding the meaning of a sentence into a vector space is highly desirable.
 A wide array of tools exist which can handle vector representations, but
 not natural language discourse.
 Vector embeddings of a sentence may encode a number of factors including
 semantic meaning, syntactic structure or topic.
 Since many of these embeddings are learned unsupervised on textual corpora,
 it is not entirely clear the emphasis placed on each factor in the encoding.
 For certain applications encoding semantic meaning is particularly desirable.
 These applications include, machine translation, automatic summarization,
 and to a lesser extent sentiment analysis.
 For successful application to these areas it is required that the embeddings
 generated by the models correctly encode meaning such that sentences with
 the same meaning are co-located in the vector space, and sentences with
 significantly differing meanings are further away.
\end_layout

\begin_layout Standard
Current methods to produce these embeddings generate them as a byproduct
 of unsupervised tasks.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
All the papers in this section to an extent suggest they are capturing semantics
, except perhaps the structure one
\end_layout

\end_inset

Several underlying have been used, including: word prediction, as in the
 work of 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

; recreation of input, as in the auto-encoders of 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:RAE,SocherEtAl2011:PoolRAE"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "iyyer2014generating"

\end_inset

; alignment of sentences embeddings across a parallel mulch-lingual corpus
 as in 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/HermannB13"

\end_inset

; and structural classification as in 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2013:CVG,socher2010PhraseEmbedding"

\end_inset

.
 These methods learn the vector representations of their inputs which is
 best for the optimization function of their choice -- one for which sufficient
 training data exists.
 The methods are then used as a feature detectors step in other tasks.
 The information captured by the methods has proved to be very useful in
 these other tasks approach or exceeding previous state-of-the-art results
 in sentiment analysis and paraphrase detection.
 It is not entirely clear, though, the precise nature of the information
 captured, is it semantic, syntactic, topical or some other factor.
\end_layout

\begin_layout Standard
This paper aims to provide a method to assess if the models are capturing
 semantic information.
 The strict definition of semantic equivalence for sentences is that each
 shall entail the other.
 This method will allow us to directly assess as to whether an give models
 embedding of the sentence is a encoding of meaning, consistent with the
 encodings for other sentences with similar meanings.
 The method functions in rough terms, by checking if sentence with meaning
 the same thing are located closely; and whether sentences of different
 meanings are located distantly.
 To do this a synthetic evaluation corpus is developed from existing resources
 though word substitution.
 Correctly separating sentences with similar word content and structure
 but different meaning, is a challenging task and proves a models semantic
 encoding strength.
 Assessment on this task will allow a better understanding of how these
 models work, and suggest new directions for the development in this area.
\end_layout

\begin_layout Standard
The paper shall be laid out as in 5 sections.
 The Related Works section discusses the linguistic underpinnings of this
 work as well as various models and how they are currently assessed.
 The Methodology section describes the development of the evaluation corpus
 and how it is used to evaluated the semantic consistency of a model.
 The Results and Discussion section applies the method to 4 existing models
 and discusses the implications for their accuracy.
 The Conclusion closes the paper and suggests new directions for development.
\end_layout

\begin_layout Section
Literature Review
\end_layout

\begin_layout Subsection
Evaluation Methods
\end_layout

\begin_layout Standard
As discussed in the introduction, current methods of evaluating the quality
 of embedding are on direct practical applications.
 The evaluation methods are not directly link to the methods used for training.
\end_layout

\begin_layout Subsubsection
Paraphrase Detection
\end_layout

\begin_layout Standard
Closely related to the corpus used for evaluation here are paraphrase corpora,
 such as the Microsoft Research Parapharase (MSRP) Corpus 
\begin_inset CommandInset citation
LatexCommand cite
key "msrParapharaCorpus"

\end_inset

.
 Such corpora present pairs of sentences and have the associated task of
 determining if the sentences are paraphrases, or not.
 This pairwise check is valuable, and does indicate to an extent if the
 embeddings are capturing the meaning.
 However, by looking at groups of paraphrases, a greater intuition can be
 gained as to the arrangement of meaning within the vector space.
 
\end_layout

\begin_layout Subsubsection
Sentiment Analysis
\end_layout

\begin_layout Standard
Sentiment Analysis is another often used technique for evaluating the quality
 of embedding.
 It was used both for Recussive Autoencoder in 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:RAE"

\end_inset

and for the paragraph vector models in 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

.
 Sentiment Analysis is normally tasked with classifying as positive or negative,
 though some variations exist as in the Sentiment Treebank
\begin_inset CommandInset citation
LatexCommand cite
key "socher2013recursive"

\end_inset

.
 Determining the sentiment of a sentence is partially a semantic task, but
 it is lacking in several areas that would be required for meaning.
 There is only an indirect requirement for the model to encode the subject
 at all: 
\begin_inset Quotes eld
\end_inset

The concert was quiet
\begin_inset Quotes erd
\end_inset

 does differ in sentiment from 
\begin_inset Quotes eld
\end_inset

The engine was quiet
\begin_inset Quotes erd
\end_inset

 but this is a limited affect.
 Sentiment Analysis is certainly a key task in natural language processing,
 but it is very distinct from semantic meaning.
\end_layout

\begin_layout Subsubsection
Information Retrieval
\end_layout

\begin_layout Standard
Where as the Sentiment Analysis task does not strongly depend on the topic,
 Information Retrieval tasks depend almost entirely on topic.
\end_layout

\begin_layout Subsubsection
Summarisation
\end_layout

\begin_layout Subsubsection
Textual Entailment
\end_layout

\begin_layout Section
Methodology
\end_layout

\begin_layout Subsection
Base Corpus
\end_layout

\begin_layout Standard
The Opinosis Corpus
\begin_inset CommandInset citation
LatexCommand cite
key "ganesan2010opinosis"

\end_inset

 was used as a source of original natural text.
 It is sourced from several online review sites: Tripadvisor, Edmunds.com,
 and Amazon.com, and contains single sentence statements about hotels, cars
 and electronics.
 The advantage of this as a source for texts is that comments on the quality
 of services and products tend to be along similar lines.
 The review sentences are syntactically simpler than sentences from a news-wire
 corpus, and also contain less named entities.
 However as they are from more casual communications, the adherence to grammar
 and spelling may be less formal.
 
\end_layout

\begin_layout Subsection
Identification of Paraphrases
\end_layout

\begin_layout Standard
Paraphrases were identified using the standard criterion: bidirectional
 entailment.
 For a paraphrase group 
\begin_inset Formula $\mathcal{S}$
\end_inset

 of sentences: 
\begin_inset Formula $\forall s_{1},\,s_{2}\in\mathcal{S},\,s_{1}\entails s_{2}\;\wedge\;s_{2}\entails s_{1}$
\end_inset

, every sentence in the group entails the every other sentence in the group.
 A stricter interpretation of bidirectional entailment was used, as compared
 to the 
\begin_inset Quotes eld
\end_inset

exhibiting mostly bidirectional entailment
\begin_inset Quotes erd
\end_inset

 used in the MSRP corpus.
 The manual grouping was carried out, grouping paraphrases by human comprehensio
n.
 The general guidelines were as follows.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
I could group thi follwoing to Factors that were assumed make sentence the
 same, and factors that were assumed to make them different
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Tense
\end_layout

\begin_layout Standard
Tense was ignored.
 All sentences were in past or present tense.
 As
\end_layout

\begin_layout Subsubsection
Transitional Phrases, and Discourse Markers
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Hedges are a form of Pragmatic Marker.
 Are Pragmatic Markers the same as discourse markers?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Transitional phrases and discourse markers were ignored for purposes of
 selecting paraphrases.
 This means parts of the sentences, such as 
\begin_inset Quotes eld
\end_inset

However, ...
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

On the plus side, ...
\begin_inset Quotes erd
\end_inset

 were ignored.
 Hedging statements were treated similarly.
 For example: 
\begin_inset Quotes eld
\end_inset

I feel the hotel is perfect.
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

The hotel is perfect.
\begin_inset Quotes erd
\end_inset

 are considered semantically equivalent.
\end_layout

\begin_layout Subsubsection
Intensity
\end_layout

\begin_layout Standard
Reasonable levels of intensity were also coarsely grouped.
 For example: 
\begin_inset Quotes eld
\end_inset

Service was poor
\begin_inset Quotes erd
\end_inset

 with 
\begin_inset Quotes eld
\end_inset

Service was terrible
\begin_inset Quotes erd
\end_inset

; 
\begin_inset Quotes eld
\end_inset

Service was fabulous
\begin_inset Quotes erd
\end_inset

 with 
\begin_inset Quotes eld
\end_inset

Service was amazing
\begin_inset Quotes erd
\end_inset

; and 
\begin_inset Quotes eld
\end_inset

Service was reasonable
\begin_inset Quotes erd
\end_inset

 with 
\begin_inset Quotes eld
\end_inset

Service was adequate
\begin_inset Quotes erd
\end_inset

.
 Other approximations were handled similarly to intensities.
\end_layout

\begin_layout Subsubsection
Approximations
\end_layout

\begin_layout Standard
When context suggested values to be very similar, the sentences were considered
 to be the same.
 For example: 
\begin_inset Quotes eld
\end_inset

The car gets 20 miles/gal.
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

The car gets 21miles/gal.
\begin_inset Quotes erd
\end_inset

 would be considered paraphrases.
 More complex approximations are involved with distance and location.
 For example: 
\begin_inset Quotes eld
\end_inset

It was in the center of town
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

It was near the center of town.
\begin_inset Quotes erd
\end_inset

 are equivalent, as are 
\begin_inset Quotes eld
\end_inset

It was very close to...
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

It was a 5 minute walk from...
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

It was just a few blocks away from...
\begin_inset Quotes erd
\end_inset

.
 Recognizing these approximations as all being equivalent is a requirement
 for natural language understanding.
 The ability to interpret other implications and indirect speech was also
 assumed.
\end_layout

\begin_layout Subsubsection
Indirect Speech and Euphemisms
\end_layout

\begin_layout Standard
Many real world statements are indirect, metaphorical or euphemistic and
 these were groups according to their meaning.
 For example: 
\begin_inset Quotes eld
\end_inset

The service needed improvement.
\begin_inset Quotes erd
\end_inset

 was grouped with 
\begin_inset Quotes eld
\end_inset

The service was poor.
\begin_inset Quotes erd
\end_inset

.
 Other implications service being synonymous with customer service also
 occur within paraphrase groups where sentence context warrants.
 Conversely, without context, the subject was not assumed to be equivalent.
\end_layout

\begin_layout Subsubsection
Entity Reference
\end_layout

\begin_layout Standard
When the entity was referenced, or implied, it was not groups with entities
 that were directly stated.
 Thus: 
\begin_inset Quotes eld
\end_inset

The Tuskan Inn has a great location
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

It has a great location
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

The hotel has a great location.
\begin_inset Quotes erd
\end_inset

 are not paraphrases.
 Without greater context than a single sentence, none of them have enough
 information to entail the others.
 On the other hand, when a entity has multiple common names, for example
 
\begin_inset Quotes eld
\end_inset

San Francisco
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

SF
\begin_inset Quotes erd
\end_inset

, the synonym is sufficient.
 Certain groupings of entities can also be synonymous with entries as a
 whole, for example 
\begin_inset Quotes eld
\end_inset

The front desk staff and all the other staff...
\begin_inset Quotes erd
\end_inset

 is synonymous with 
\begin_inset Quotes eld
\end_inset

All the staff...
\begin_inset Quotes erd
\end_inset

.
 While the addition of redundant information, does not change the meaning,
 addition of new information does.
\end_layout

\begin_layout Subsubsection
Additional Information
\end_layout

\begin_layout Standard
Any differing information, in a phrase is enough to cause it not to be grouped
 with phrases without that information.
 This comes from the definition of bidirectional entailment.
 If sentence 
\begin_inset Formula $A$
\end_inset

 entails two facts, which are equivalent to stand-alone sentences 
\begin_inset Formula $C$
\end_inset

 and 
\begin_inset Formula $D$
\end_inset

, and sentence 
\begin_inset Formula $B$
\end_inset

 does not entail 
\begin_inset Formula $C$
\end_inset

, then 
\begin_inset Formula $B$
\end_inset

 can not entail 
\begin_inset Formula $A$
\end_inset

 as entailment is a transitive operation.
 For example: 
\begin_inset Quotes eld
\end_inset

The rooms were clean, also the housekeeping was polite.
\begin_inset Quotes erd
\end_inset

 is not a paraphrase of 
\begin_inset Quotes eld
\end_inset

The rooms were clean.
\begin_inset Quotes erd
\end_inset

 or of 
\begin_inset Quotes eld
\end_inset

The housekeeping was polite.
\begin_inset Quotes erd
\end_inset

.
 This is a notable difference from the MSRP Corpus where the creators noted
\begin_inset CommandInset citation
LatexCommand cite
key "msrParapharaCorpus"

\end_inset

 
\begin_inset Quotes eld
\end_inset

...the majority of the 'equivalent' pairs in this dataset exhibit 'mostly bidirecti
onal entailments', with one sentence containing information that differs
 from or is not contained in the other.
\begin_inset Quotes erd
\end_inset

.
 While this approach does lead to more varied paraphrasings, it ties the
 meaning of a sentence to be a subset of its entailments.
 In the two pairwise classification task of paraphrase, or not, this adds
 a subtask of detecting the significance of information overlap, a reasonable
 measure for a system.
 However when examining the space of the embeddings and the relationship
 to meaning it is more problematic.
 For example, three of the largest paraphrase groupings in the corpus are
 around the facts: 
\begin_inset Quotes eld
\end_inset

The staff were helpful
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

The staff were friendly
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

The staff were friendly and helpful.
\begin_inset Quotes erd
\end_inset

.
 If 
\begin_inset Quotes eld
\end_inset

mostly bidirectional entailment
\begin_inset Quotes erd
\end_inset

 were used then either the three groups much be combined though helpful
 and friendly are not synonymous.
 The is scope for further analysis in this area, which considers quantified
 relationships between near paraphrases such as these.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
If for sentences 
\begin_inset Formula $A,B,C$
\end_inset

 such that 
\begin_inset Formula $A\entails C$
\end_inset

, but 
\begin_inset Formula $B\not\entails C$
\end_inset

, then 
\begin_inset Formula $B\not\entails A$
\end_inset

: Proof by contractition: assume 
\begin_inset Formula $B\entails A$
\end_inset

, by transitivity of entailment operator, then 
\begin_inset Formula $B\entails C$
\end_inset

 which is a contradiction.
 In p
\end_layout

\end_inset


\end_layout

\begin_layout Section
Results and Discussion
\end_layout

\begin_layout Subsection
The Models
\end_layout

\begin_layout Standard
For demonstration purposes, several models are evaluated below.
\end_layout

\begin_layout Paragraph
Distance
\end_layout

\begin_layout Standard
Distance may be calculated though any method the model specifies.
 The All the model chosen below, use the cosine distance (or equivalently
 cosign similarity).
 This is given by 
\begin_inset Formula $d(\tilde{u},\tilde{v})=1-\dfrac{\tilde{u}\cdot\tilde{v}}{\left\Vert \tilde{u}\right\Vert _{2}\left\Vert \tilde{v}\right\Vert _{2}}$
\end_inset

, this is between 0, and 2 and is proportional to the cosine of the angle
 between the embeddings 
\begin_inset Formula $\tilde{u}$
\end_inset

 and 
\begin_inset Formula $\tilde{v}$
\end_inset


\begin_inset Note Comment
status open

\begin_layout Plain Layout
Double check this
\end_layout

\end_inset

.
\end_layout

\begin_layout Subsubsection
Unfolding Recursive Auto-Encoder (U-RAE)
\end_layout

\begin_layout Standard
The Unfolding Recursive Auto-Encoder is a autoencoder based method.
 It functions by using the same network to recursively pairwise combine
 embedded representations, following the parse tree
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

.
 It's optimization target is to be be able to reverse (unfold) the merges
 and produce the original sentence.
 The central folding layer - where the whole sentence is collapsed to a
 single embedding vector is the representation.
\end_layout

\begin_layout Standard
In this evaluation we make use of the pretrained network the authors of
 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

 have graciously made available
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
name "http://www.socher.org/index.php/Main/DynamicPoolingAndUnfoldingRecursiveAutoencodersForParaphraseDetection"
target "http://www.socher.org/index.php/Main/DynamicPoolingAndUnfoldingRecursiveAutoencodersForParaphraseDetection"

\end_inset


\end_layout

\end_inset

, full information is available in that paper.
 It is initialized on the unsupervised Collobert and Weston word embeddings
\begin_inset CommandInset citation
LatexCommand cite
key "collobert2008unified"

\end_inset

, and training on a subset of 150,000 sentences from the gigaword corpus.
 In the evaluation below the dynamic pooling layer is not used.
\end_layout

\begin_layout Subsubsection
Doc2Vec Models
\end_layout

\begin_layout Standard
Two new methods, commonly refereed to a doc2vec are described in 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

.
 For both, we evaluate using the GenSim implementation
\begin_inset CommandInset citation
LatexCommand cite
key "rehurek_lrec"

\end_inset

 from the current develop branch.
\end_layout

\begin_layout Standard
Both are trained on approximately 12 million sentences from 500 randomly
 selected wikipedia articles.
 In both the window size was set to 8 words, and the vectors were of 300
 dimensions.
\end_layout

\begin_layout Paragraph
PV-DM
\end_layout

\begin_layout Standard
Distributed Memory Paragraph Vectors (PV-DM) Doc2Vec document embeddings
 are based on an extension of Continuous Bag-of-Words word-embedding model
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013efficient"

\end_inset

.
 It is trained using a sliding window of words to predict the next word.
 The softmax predictor network is feed a word-embedding for each word in
 the window, and an additional embedding vector which is reused for all
 words in the sentence (called the paragraph vector in original paper).
 These input embeddings can be concatenated or averaged, in the results
 show below they were concatenated.
 During training both word and sentence vectors are allows to vary, in evaluatio
n, the word vectors are locked and the sentence vector trained until convergence.
 
\end_layout

\begin_layout Paragraph
PV-DBOW
\end_layout

\begin_layout Standard
Distributed Bag of Words version of Paragraph Vectors (PV-DBOW), is based
 on the Skip-gram model for word-embeddings, also from 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013efficient"

\end_inset

.
 In PV-DBOW a sentence vector is used as the sole input to a neural net.
 That network is tasked with predicting the words in the sentence.
\end_layout

\begin_layout Subsubsection
Baseline: Bag of Words
\end_layout

\begin_layout Standard
The traditional bag of words model is presented as a baseline.
 There is a dimension in each vector for the count of each token, including
 punctuation.
 In bag of words, there is a direct relationship between the number of words
 in-common, the sentence length, and the distance.
\end_layout

\begin_layout Subsection
Model Results and Discussion
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
A method what presented, to evaluate the semantic localization of sentence
 embedding models.
 Semantically equivalent sentences are those which exhibit bidirectional
 entailment -- they each imply the truth of the other.
 Paraphrases are mostly semantically equivalent.
 Replacing a noun with its synonym creates another sentence which is semanticall
y equivalent to the original.
 Replacing a verb with its antonym creates a new sentence which is not.
 By comparing the distances of the generated sentences, and paraphrases
 from the original sentence, the relationship between semantic closeness
 and embedding distance can be seen.
\end_layout

\begin_layout Standard
The models evaluated using this method show that they are substantially
 more permanent the a naive bag of words approach there is still significant
 room for improvement.
 While these models perform very well at related practical tasks, this new
 method highlights some of their limitations.
 It suggests that calling the vector space the models embed into a syntactic
 space is misleading.
 The space clearly incorporates elements of syntax and word choice, as well
 as meaning.
 This result is not surprising and indeed some papers (including 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

) do refer to the space this way.
 The new method does make its truth substantially clearer.
\end_layout

\begin_layout Subsection
Motivating better use of Semantic Resources in embedding creation
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "yu2014improving"

\end_inset

 is a improved method over word2vec for determining word embeddings making
 use of semantic knowledge, potential exists to extend in into the document
 domain through the doc2vec models presented in 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

.
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand cite
key "iyyer2014generating"

\end_inset

, dependency trees are used instead of the constituency tree used in the
 original URAE, because of its improved invariance to syntactical changes.
 This may, by decreasing the impact of syntax create models with a greater
 emphasis on semantic placement.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "../../../Resources/master_bibliography/master"
options "ieeetr"

\end_inset


\end_layout

\end_body
\end_document
