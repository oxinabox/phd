#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass acm-sigs
\begin_preamble
\usepackage{tikz}
\usetikzlibrary{positioning, fit,arrows,chains,shapes.geometric}
\usetikzlibrary{graphs,graphdrawing}
\usegdlibrary{force, layered, trees}


\renewcommand{\tabref}{\Tabref}
\renewcommand{\figref}{\Figref}
\renewcommand{\secref}{\Secref}


\usepackage{verbatim}
\usepackage{pdfcomment}
\usepackage{environ}
\RenewEnviron{comment}{\pdfcomment[author={Lyndon}]{\BODY}}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Evaluating Semantic Localization of Sentence Embeddings through a Semantic
 Classification Task
\end_layout

\begin_layout Author
L.
 White, R.
 Togneri., W.
 Liu, M.
 Bennamoun
\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\entails}{\vDash}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Guide can be found at https://www.acm.org/sigs/publications/sigguide-v2.2sp
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
Several approaches for embedding a sentence into a vector space have been
 developed.
 However, it is unclear to what extent the sentence's position in a vector
 space reflect semantic meaning, rather than other factors such as syntactic
 structure.
 Depending on the model used for the embeddings this will vary -- different
 models are suited for different down-stream applications.
 For applications such as machine translation and automated summarization,
 it is highly desirable to have semantic meaning encoded in the embedding.
 We consider the to be the quality of 
\emph on
semantic localization
\emph default
 for the model -- how well sentences meaning coincides with their embedding's
 position in vector space.
 Currently the semantic localization is assessed indirectly through practical
 benchmarks for specific applications.
 
\end_layout

\begin_layout Abstract
In this paper, we ground the semantic localization problem by assessing
 the localization and separability of paraphrase groups in the embedding
 space.
 Two corpora, namely, a subset of the Microsoft Research Paraphrase Corpus
 and and a subset of the Opinosis corpus were grouped into classes of sentences
 with the same meaning.
 Several existing models, including URAE, PV--DM and PV--DBOW, were assessed
 against a bag of words benchmark.
\end_layout

\begin_layout Terms
General terms from the ACM classification.
 See 
\begin_inset CommandInset href
LatexCommand href
target "http://www.acm.org/about/class/"

\end_inset


\end_layout

\begin_layout Keywords
Free key words
\end_layout

\begin_layout Section
Introduction
\begin_inset Note Note
status open

\begin_layout Plain Layout
1 page
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Sentence embeddings are often referent to as semantic vector space representatio
ns
\begin_inset CommandInset citation
LatexCommand cite
key "iyyer2014generating"

\end_inset

.
 Embedding the meaning of a sentence into a vector space is expected to
 be very useful for natural language understanding tasks.
 Such vector representation of natural languages enables discourse analysis
 to take advantage to the array of tools available for computation in vector
 spaces.
 However, the embeddings of a sentence may encode a number of factors including
 semantic meaning, syntactic structure and topic.
 Since many of these embeddings are learned unsupervised on textual corpora
 using various models with different training objectives, it is not entirely
 clear the emphasis placed on each factor in the encoding.
 For applications where encoding semantic meaning is particularly desirable,
 such as machine translation and automatic summarization, it is crucial
 to be able to assess how well the embeddings capture the sentences semantics.
 In other words, for successful application to these areas it is required
 that the embeddings generated by the models correctly encode meaning such
 that sentences with the same meaning are co-located in the vector space,
 and sentences with differing meanings are further away.
 However, few current models are directly trained to optimize for this criteria.
\end_layout

\begin_layout Standard
Currently sentence embeddings are often generated as a byproduct of unsupervised
, or semi-supervised, tasks.
 These tasks include: word prediction 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

; recreation of input, as in the auto-encoders of 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:RAE,SocherEtAl2011:PoolRAE"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "iyyer2014generating"

\end_inset

; alignment of sentence embeddings across a parallel multilingual corpus
 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/HermannB13"

\end_inset

; and syntactic structural classification 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2013:CVG,socher2010PhraseEmbedding"

\end_inset

.
 As a result the vector representations of the input sentences learned by
 these models are tuned towards the chosen optimization task.
 When employing the embeddings produced as a features for other tasks, the
 information captured by the embeddings often proved to be very useful:
 e.g.
 approaching or exceeding previous state-of-the-art results, in sentiment
 analysis
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:RAE,SocherMVRNN,le2014distributed"

\end_inset

 and paraphrase detection
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

.
 However these practical applications do not directly show how well meaning
 is captured by the embeddings.
\end_layout

\begin_layout Standard
This paper aims to provide a method to assess how well the models are capturing
 semantic information.
 A strict definition of sentences semantic equivalence is: that each sentence
 shall entail the other.
 Such mutually entailing sentences are called 
\emph on
paraphrases
\emph default
.
 In this paper we propose to use paraphrases to assess how well the true
 semantic space aligns with the vector space the models embed into.
 It thus assesses whether projecting a sentence via the models in to the
 vector space preserves meaning.
\end_layout

\begin_layout Standard
The evaluation corpora were prepared by grouping paraphrases from the Microsoft
 Research Paraphrase (MSRP) 
\begin_inset CommandInset citation
LatexCommand cite
key "msrParapharaCorpus"

\end_inset

 and Opinosis 
\begin_inset CommandInset citation
LatexCommand cite
key "ganesan2010opinosis"

\end_inset

 corpora.
 A semantic classification task was defined which assesses if the model's
 embeddings could be used to correctly classify sentences as belonging to
 the paraphrase group with semantically equivalent sentences.
 Ensuring that the many sentences of common meaning, but differing form
 are located in vector space together, is a challenging task and shows a
 model's semantic encoding strength.
 This assessment allows for a better understanding of how these models work,
 and suggest new directions for the development in this area.
\end_layout

\begin_layout Standard
The rest of the paper is organized into the following sections.
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Background"

\end_inset

 discusses the existing models being assessed, the methods traditionally
 used to assess them, and the more recent work on methods to assess their
 semantic correctness.
 The 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Methodology"

\end_inset

 describes the processes by which the models are evaluated using our new
 method, and the parameters used in the evaluation.
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Corpus-Construction"

\end_inset

 continues into more details on the development of the evaluation corpora
 for the semantic classification evaluation task.
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Results-and-Discussion"

\end_inset

 section details the results from evaluating the models and discusses the
 implications for their semantic consistency.
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Conclusion"

\end_inset

 closes the paper and suggests new directions for development.
\end_layout

\begin_layout Section
Background
\begin_inset CommandInset label
LatexCommand label
name "sec:Background"

\end_inset


\end_layout

\begin_layout Subsection
Models
\end_layout

\begin_layout Standard
Three well known sentence embedding methods are evaluated in this work.
 The compositional distributed model of the Unfoldering Recussive Autoencoder
 (URAE) from the work of 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

; and the two word content predictive models, Distributed Memory (PV-DM)
 and Distributed Bag of Words from 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

.
 As well as these advanced sentence embedding models, a simple mean of word
 embeddings is also assessed.
 These models and their variant forms have been applied to numerous natural
 language processing tasks in the past, but not to a real-sentence semantic
 classification task as described here..
 
\end_layout

\begin_layout Subsubsection
Unfolding Recursive Auto-Encoder (URAE)
\end_layout

\begin_layout Standard
The Unfolding Recursive Autoencoder (URAE)
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

 is an autoencoder based method.
 It functions by using the same single layer neural-network to recursively
 pairwise combine embedded representations, following the parse tree.
 It's optimization target is to be be able to reverse (unfold) the merges
 and produce the original sentence.
 The central folding layer - where the whole sentence is collapsed to a
 single embedding vector is the representation.
\end_layout

\begin_layout Subsubsection
PV-DM
\end_layout

\begin_layout Standard
Distributed Memory Paragraph Vectors (PV-DM)
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

 document embeddings are based on an extension of Continuous Bag-of-Words
 word-embedding model
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013efficient"

\end_inset

.
 It is trained using a sliding window of words to predict the next word.
 The softmax predictor network is feed a word-embedding for each word in
 the window, and an additional embedding vector which is reused for all
 words in the sentence -- called the paragraph vector in 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

.
 These input embeddings can be concatenated or averaged, in the results
 show below they were concatenated.
 During training both word and sentence vectors are allows to vary, in evaluatio
n (ie inference), the word vectors are locked and the sentence vector trained
 until convergence at the prediction task.
 
\end_layout

\begin_layout Subsubsection
PV-DBOW
\end_layout

\begin_layout Standard
Distributed Bag of Words Paragraph Vectors (PV-DBOW)
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

, is based on the Skip-gram model for word-embeddings, also from 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013efficient"

\end_inset

.
 In PV-DBOW a sentence vector is used as the sole input to a neural net.
 That network is tasked with predicting the words in the sentence.
 Each training iteration a number of words to predict, given by the window
 size parameter are selected from the sentence and the network tasked with
 predicting them using the sentence vector as input.
 As in PV-DM to infer embedding the rest of the network is locked, and only
 the sentence vector input allowed to vary and it is trained to convergence.
\end_layout

\begin_layout Subsubsection
Mean of Word Embeddings (MOWE)
\end_layout

\begin_layout Standard
Taking the element-wise mean of the word embeddings over all words in the
 sentence also produces a vector with the potential to encode meaning.
 Like bag of words no order information is encoded, but the model can take
 into word relations such as synonymity.
 This method was used as baseline in 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

.
 The closely related sum of word embeddings was first considered in 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolovSkip"

\end_inset

 for short phrases.
 It was found to be an effective model for summarization in 
\begin_inset CommandInset citation
LatexCommand cite
key "KaagebExtractiveSummaristation"

\end_inset

.
 It is a computationally cheap model, as pretrained word embeddings are
 available.
\end_layout

\begin_layout Subsection
General Evaluation Methods
\end_layout

\begin_layout Standard
As discussed in the introduction, current methods of evaluating the quality
 of embedding are on direct practical applications.
 The evaluation methods are not directly link to the methods used for training.
 One of the more semantically focused is paraphrase detection.
\end_layout

\begin_layout Standard
Evaluation on a Paraphrase detection task takes the form of being presented
 with pairs of sentences and tasked with determining if the sentences are
 paraphrases or not.
 The MSRP Corpus 
\begin_inset CommandInset citation
LatexCommand cite
key "msrParapharaCorpus"

\end_inset

 which we used in the semantic classification task is intended for such
 use.
 This pairwise check is valuable, and does indicate to an extent if the
 embeddings are capturing the meaning.
 However, by looking at groups of paraphrases, a greater intuition can be
 gained as to the arrangement of meaning within the vector space.
 
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Need Linking sentence?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Sentiment Analysis is very commonly used technique for evaluating the quality
 of embedding.
 It was used both for recursive autoencoder in 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:RAE"

\end_inset

and for the paragraph vector models in 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

.
 Sentiment Analysis is normally tasked with classifying as positive or negative,
 or with assigning a score, such as in the Sentiment Treebank
\begin_inset CommandInset citation
LatexCommand cite
key "socher2013recursive"

\end_inset

.
 Determining the sentiment of a sentence is partially a semantic task, but
 it is lacking in several areas that would be required for meaning.
 For example, there is only an indirect requirement for the model to process
 the subject at all.
  
\begin_inset Quotes eld
\end_inset

The concert was quiet
\begin_inset Quotes erd
\end_inset

 does differ in sentiment from 
\begin_inset Quotes eld
\end_inset

The engine was quiet
\begin_inset Quotes erd
\end_inset

 but this is a limited affect.
 Sentiment Analysis is a key task in natural language processing, but it
 is very distinct from semantic meaning.
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Need Linking sentence?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Document Classification is a classic natural language processing task.
 A particular case of this is topic categorization, where the task is to
 classify the texts based on topic.
 Early work in the area include 
\begin_inset CommandInset citation
LatexCommand cite
key "maron1961automatic"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "borko1963automatic"

\end_inset

.
 Much more recently it has been used to assess the convolution neural networks
 of 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/ZhangL15"

\end_inset

, where the articles of several News corpora were classified into categories
 such as 
\begin_inset Quotes eld
\end_inset

Sports
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

Business
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

Entertainment
\begin_inset Quotes erd
\end_inset

, they also classified a subset of the Yahoo! Answers corpus.
 The topics for classification tend to be broad.
 A huge spectrum of different sentences are assigned to the same topic.
 It is thus insufficiently narrow to evaluate the consistency of meanings.
 Information retrieval can be seen as the inverse of the document classification
 task.
\end_layout

\begin_layout Standard
Information Retrieval is the task of identifying the documents which most
 fit a query.
 Such document selection depends almost entirely on topic.
 Suitable results for information retrieval have no requirement to agree
 on meaning, though text with the same meaning are will be fit the same
 queries.
 As with the Sentiment Analysis and Document Classification, the Information
 Retrieval task, correct semantic understanding contributes only partially
 to a models success at the task.
 Thus the requirement for new tasks to directly evaluate semantic consistency.
\end_layout

\begin_layout Subsection
Evaluations of Semantic Consistency
\begin_inset CommandInset label
LatexCommand label
name "sub:Evaluations-of-Semantic"

\end_inset


\end_layout

\begin_layout Standard
Semantic consistency for word embeddings is often measured using the analogy
 task.
 In an analogy the meta-relation: 
\begin_inset Quotes eld
\end_inset

A is to B as C is to D
\begin_inset Quotes erd
\end_inset

, for A,B,C,D words.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013linguisticsubstructures"

\end_inset

, the authors show that the word-embedding models are semantically consistent
 by showing that the semantic relations between words were reflected as
 a linear offset in the vector space.
 That is to say, for embeddings 
\begin_inset Formula $\tilde{x}_{a},\,\tilde{x}_{b},\,\tilde{x}_{c},\,\tilde{x}_{d}$
\end_inset

 corresponding to words A,B,C and D, respectively; it was tested that if
 for a strong relationship matching between A/B and C/D, then the offset
 vector would be approximately equal: 
\begin_inset Formula $\tilde{x}_{b}-\tilde{x}_{a}\approxeq\,\tilde{x}_{d}-\tilde{x}_{c}$
\end_inset

.
 Rearranging this in word space gets the oft-quoted example of 
\begin_inset Formula $"King"-"Man"+"Woman"\approxeq"Queen"$
\end_inset

, As man is to woman, king is to queen.
 In the rating task as described by 
\begin_inset CommandInset citation
LatexCommand cite
key "jurgens2012semeval"

\end_inset

, the goal is to rank such analogous word pairs based on the degree the
 relation matches.
 Thus to evaluate the word-embedding model using this task, it was a matter
 of sorting closeness of the corresponding offset vectors.
 Surprisingly strong results were found by the authors 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013linguisticsubstructures"

\end_inset

.
 It was thus demonstrated that word embeddings were not simply semantically
 consistent, but more so that this consistency was displayed as local linearity.
 This result gives strong confidence in the capacity of word embeddings
 to perform semantic tasks.
 However this relationship analogy test can not be easily performed for
 sentence embeddings.
\end_layout

\begin_layout Standard
The work of 
\begin_inset CommandInset citation
LatexCommand cite
key "gershmanphrase"

\end_inset

, compares the distances of modified sentences in vector space, to the semantic
 distances ascribed to them by human raters.
 Like the analogy task for word vectors, this task requires ranking the
 targets based on the vector distance, however instead of rating on strength
 of relationship is is simply on similarities of the sentences to an original
 base sentence for each group.
 In that evaluation 30 simple base sentences of the form 
\emph on

\begin_inset Quotes eld
\end_inset

A [adjective1][noun1] [prepositional phrase][adjective2][noun2]
\begin_inset Quotes erd
\end_inset


\emph default
 were modified to produce 4 difference derived sentences.
 The derived sentences were produced by swapping the nouns, swapping the
 adjectives, reversing the positional phrase (so
\emph on
 
\begin_inset Quotes eld
\end_inset

behind
\begin_inset Quotes erd
\end_inset


\emph default
 becomes 
\emph on

\begin_inset Quotes eld
\end_inset

in front of
\begin_inset Quotes erd
\end_inset


\emph default
), and a paraphrase by doing all of the aforementioned changes.
 Human raters were tasked with sorting the transformed sentences in similarity
 to the base sentence.
 This evaluation found that the embedding models considered did not agree
 with the semantic similarity rankings placed by humans.
 While the sentence embedding models performed poorly on the distance ranking
 measure, it is also worth considering how they perform on a meaning classificat
ion task.
\end_layout

\begin_layout Standard
A meaning classification task was recently proposed in 
\begin_inset CommandInset citation
LatexCommand cite
key "RitterPosition"

\end_inset

, to classify sentences based on which spatial relationship was described.
 The task was to classify the sentence as describing: Adhesion to Vertical
 Surface, Support by Horizontal Surface, Full Containment, Partial Containment,
 or Support from Above.
 In this evaluation also, the sentences took a very structured form: 
\emph on

\begin_inset Quotes eld
\end_inset

There is a [noun1] [on/in] the [noun2]
\begin_inset Quotes erd
\end_inset


\emph default
.
 These highly structured sentences, take advantage of the disconnection
 between word content and the positional relationship described to form
 a task that must be solved by a compositional understanding combining the
 understanding of the words.
 
\emph on

\begin_inset Quotes eld
\end_inset

The apples is on the refrigerator
\emph default

\begin_inset Quotes erd
\end_inset

 and 
\emph on

\begin_inset Quotes eld
\end_inset

The magnet is on the refrigerator
\begin_inset Quotes erd
\end_inset


\emph default
 belong to two separate spatial categories, even though the word content
 is very similar.
 Surprisingly, the simple model of adding word vectors outperformed compositiona
l models such as the recursive autoencoder.
 The result does have some limitation due to the highly artificial nature
 of the sentences, and the restriction to categorizing into a small number
 of classes based only on the meaning in terms of positional relationship.
 To generalist this task, we consider real world sentences being classed
 into groups according to their full semantic meaning.
\end_layout

\begin_layout Section
Methodology
\begin_inset CommandInset label
LatexCommand label
name "sec:Methodology"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement t
wide true
sideways false
status open

\begin_layout Plain Layout

\size footnotesize
\begin_inset CommandInset include
LatexCommand include
filename "figs/block_overview.pgf"

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Process-Diagram"

\end_inset

Process Diagram for the Evaluation of Semantic Consistency via our method
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
To evaluate how well a model vectors capture the meaning of a sentence,
 a semantic classification task was defined.
 The task is to classify paraphrases into classes which all share the same
 meaning.
 This is a far finer-grained task than topic classification.
 It is a multiclass problem, rather than the binary decision problem of
 paraphrase detection.
 A model which produces embeddings which are easily classifiable according
 to their meaning can been thus seen to have good semantic localization.
 
\end_layout

\begin_layout Standard
This semantic classification does not have direct practical application
 -- it is rare that the need will be to quantify sentences into groups with
 the same prior known meaning.
 Rather it serves as a measure to assess the models general suitability
 for other tasks requiring a model with consistency between meaning and
 embedding.
\end_layout

\begin_layout Standard
To evaluate the success at the task three processes are involved, as shown
 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:Process-Diagram"

\end_inset

: Corpus Preparation, Model Preparation, and the Semantic Classification
 task itself.
\end_layout

\begin_layout Subsection
Corpus Preparation
\end_layout

\begin_layout Standard
The construction of the corpora is detailed more fully in then next section.
 In brief: Two corpora were constructed by selection subsets of the Opinosis
 
\begin_inset CommandInset citation
LatexCommand cite
key "ganesan2010opinosis"

\end_inset

 and Microsoft Research Paraphrase (MSRP) 
\begin_inset CommandInset citation
LatexCommand cite
key "msrParapharaCorpus"

\end_inset

Corpora.
 The corpora were partitions into groups of paraphrases -- sentences with
 the same meaning.
 Any Paraphrase groups with less than three sentences were discarded.
 The paraphrase grouping was carried out manually for Opinosis, and automaticall
y for the MSRP corpus using the existing paraphrase pairings.
 The paraphrase groups divide the total semantic space of the corpora into
 discrete classes.
 It is by comparing the ability of the models to produce embedding which
 can be classified back into these classes, that we can compare the real
 semantic space partitions to their corresponding vector embedding spare
 regions.
\end_layout

\begin_layout Subsection
Model Preparation and Inferring Vectors
\end_layout

\begin_layout Standard
Prior to application to semantic classification task, as with any task the
 models had to be pretrained.
 Here we are using the term pretraining to differentiate the model training
 from the classifier training.
 The pretraining is not done using the evaluation corpus as it is very small.
 Instead other data is used, and the inference/evaluation procedure given
 for each method was then used to produce the vectors for each sentence.
 The model parameters used are detailed below.
\end_layout

\begin_layout Subsubsection
Unfolding Recursive Auto-Encoder (URAE)
\end_layout

\begin_layout Standard
In this evaluation we make use of the pretrained network the authors of
 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

 have graciously made available
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
name "http://www.socher.org/index.php/Main/DynamicPoolingAndUnfoldingRecursiveAutoencodersForParaphraseDetection"
target "http://www.socher.org/index.php/Main/DynamicPoolingAndUnfoldingRecursiveAutoencodersForParaphraseDetection"

\end_inset


\end_layout

\end_inset

, full information is available in that paper.
 It is initialized on the unsupervised Collobert and Weston word embeddings
\begin_inset CommandInset citation
LatexCommand cite
key "collobert2008unified"

\end_inset

, and training on a subset of 150,000 sentences from the gigaword corpus.
 This pretrained model when used with dynamic pooling and other word based
 features performed very well on the MSRP corpus paraphrase dectection.
 In the evaluation below the dynamic pooling layer is not used.
\end_layout

\begin_layout Subsubsection
Paragraph Vector Methods (PV-DM and PV-DBOW)
\end_layout

\begin_layout Standard
Both PV-DM and PV-DBOW, were evaluated using the GenSim implementation
\begin_inset CommandInset citation
LatexCommand cite
key "rehurek_lrec"

\end_inset

 from the current 
\emph on
develop
\emph default
 branch.
 Both were trained on approximately 1.2 million sentences from randomly selected
 Wikipedia articles, and the window size was set to 8 words, and the vectors
 were of 300 dimensions.
\end_layout

\begin_layout Subsubsection
Mean of Word Embeddings (MOWE)
\end_layout

\begin_layout Standard
The word embeddings used for MOWE were taken from the Google News pretrained
 model
\begin_inset Foot
status open

\begin_layout Plain Layout
Available at 
\begin_inset CommandInset href
LatexCommand href
name "https://code.google.com/p/word2vec/"
target "https://code.google.com/p/word2vec/"

\end_inset


\end_layout

\end_inset

 based on the method described in 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolovSkip"

\end_inset

.
 This has been trained on 100 million sentences from Google News.
 A small portion of the evaluation corpus did not have embeddings in the
 Google News model.
 These tokens were largely numerals, punctuation symbols, proper nouns and
 unusual spellings, as well as the stop-words: 
\begin_inset Quotes eld
\end_inset

and
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

a
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

of
\begin_inset Quotes erd
\end_inset

.
 These words were simply skipped.
 The resulting embeddings have 300 dimensions, like the word embeddings
 they were based on.
\end_layout

\begin_layout Subsubsection
Bag of Words (BOW)
\end_layout

\begin_layout Standard
A bag of words (BOW) model is also presented as a baseline.
 There is a dimension in each vector embedding for the count of each token,
 including punctuation, in the sentence.
 In the Opinosis and MSRP subcorpora there were a total of 1,085 and 2,976
 unique tokens respectively, leading to BOW embeddings of corresponding
 dimensionality.
 This model does not have any pretraining step.
 For comparison to the lower dimensional models Principle Component Analysis
 (PCA) was used to produce a additional baseline set of embeddings of 300
 dimensions.
 It does not quiet follow the steps shown in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:Process-Diagram"

\end_inset

.
 The PCA training step is performed just during the SVM classification process,
 and it is used to infer the PCA BOW embeddings during the testing step.
 This avoids unfair information transfer where the PCA would otherwise be
 about to chose optimal representation for the whole set, including the
 test data.
 It was found that where the PCA model was allowed to cheat in this way
 it performed a few percentage points better.
 The bag of words models do not have any outside knowledge.
\end_layout

\begin_layout Subsection
Semantic Classification 
\end_layout

\begin_layout Standard
The core of this evaluation procedure is in the semantic classification
 step.
 A support vector machine (SVM), with a linear kernel, and class weighting
 was applied to the task of predicting which paraphrase group each sentence
 belongs to.
 Classification was verified using threefold cross-validation across different
 splits of the testing/training data, the average results are shown in this
 section.
 The splits were in proportion to the class size.
 For the smallest groups this means there were two training cases and one
 test case to classify.
 Only as linear kernal was used as more powerful classifier would be able
 to compensate for irregularities in the vector space, thus making model
 comparison more difficult.
 Scikit-learn 
\begin_inset CommandInset citation
LatexCommand cite
key "scikit-learn"

\end_inset

 was used to orchestrate the cross-validation and to interface with the
 LIBLINEAR SVM implementation
\begin_inset CommandInset citation
LatexCommand cite
key "LIBLIBEAR"

\end_inset

.
 As the linear SVM's classification success depends on how linearly separable
 the input data is, thus this assessed the quality of the localization of
 the paraphrase groupings embeddings.
\end_layout

\begin_layout Section
Corpus Construction
\begin_inset CommandInset label
LatexCommand label
name "sec:Corpus-Construction"

\end_inset


\end_layout

\begin_layout Subsection
Microsoft Research Paraphrased Grouped Subcorpus
\end_layout

\begin_layout Standard
The MSRP corpus is a very well established data set for the paraphrase detection
 task 
\begin_inset CommandInset citation
LatexCommand cite
key "msrParapharaCorpus"

\end_inset

.
 Sentences are presented as pairs which are either paraphrases, or not.
 A significant number of paraphrases appear in multiple different pairings.
 Using this information, groups of paraphrases can be formed.
\end_layout

\begin_layout Standard
The corpus was partitioned according to sentence meaning by taking the symmetric
 and transitive closure of set of paraphrase pairs.
 For example if sentences 
\emph on
A
\emph default
,
\emph on
B
\emph default
,
\emph on
C
\emph default
 and 
\emph on
D
\emph default
 were present in the original corpus as paraphrase pairs: 
\begin_inset Formula $A,B$
\end_inset

, 
\begin_inset Formula $D,\,A$
\end_inset

 and 
\begin_inset Formula $B,C$
\end_inset

 then the paraphrase group 
\begin_inset Formula $\{A,B,C,D\}$
\end_inset

 is found.
 Again any paraphrase groups than 3 phrases were discarded.
 The resulting sub-corpus has the breakdown as shown in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:msrp_corpus_hist"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/msrp_corpus_stats.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:msrp_corpus_hist"

\end_inset

 Break down of how many paraphrases groups are present in the MSRP subcorpus
 of which sizes.It contains a total of 859 unique sentences, broken up into
 273 paraphrase groups.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/opinosis_corpus_stats.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:opinosis_corpus_hist"

\end_inset

Break down of how many paraphrases groups are present in the Opinosis subcorpus
 of which sizes.
 It contains a total of 521 unique sentences, broken up into 89 paraphrase
 groups.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Opinosis Paraphrase Grouped Subcorpus
\end_layout

\begin_layout Standard
The Opinosis Corpus
\begin_inset CommandInset citation
LatexCommand cite
key "ganesan2010opinosis"

\end_inset

 was used as secondary source of original real-world text.
 It is sourced from several online review sites: Tripadvisor, Edmunds.com,
 and Amazon.com, and contains single sentence statements about hotels, cars
 and electronics.
 The advantage of this as a source for texts is that comments on the quality
 of services and products tend to be along similar lines.
 The review sentences are syntactically simpler than sentences from a news-wire
 corpus, and also contain less named entities.
 However, as they are from more casual communications, the adherence to
 grammar and spelling may be less formal.
 
\end_layout

\begin_layout Standard
Paraphrases were identified using the standard criterion: bidirectional
 entailment.
 For a paraphrase group 
\begin_inset Formula $\mathcal{S}$
\end_inset

 of sentences: 
\begin_inset Formula $\forall s_{1},\,s_{2}\in\mathcal{S},\quad s_{1}\entails s_{2}\quad\wedge\;s_{2}\entails s_{1}$
\end_inset

, every sentence in the group entails the every other sentence in the group.
 A stricter interpretation of bidirectional entailment was used, as compared
 to the 
\begin_inset Quotes eld
\end_inset

mostly bidirectional entailment
\begin_inset Quotes erd
\end_inset

 used in the MSRP corpus.
 The grouping was carried out manually.
 The general guidelines were as follows.
\end_layout

\begin_layout Itemize
Tense, Transitional Phrases, and Discourse and Pragmatic Markers were ignored.
\end_layout

\begin_layout Itemize
Statement intensity was coarsely quantized.
 
\end_layout

\begin_layout Itemize
Approximately equal quantitative and qualitative values were treated as
 synonymous.
\end_layout

\begin_layout Itemize
Sentences with entities mentioned explicitly were grouped separately from
 similar statements where they were implied.
\end_layout

\begin_layout Itemize
Sentences with additional information were grouped separately from those
 without that information.
\end_layout

\begin_layout Standard
The final point is the most significant change from the practices apparent
 in the construction of the MSRP corpus.
 Sentences with differing or additional information were placed in classed
 has non-paraphrases.
 This requirement comes from the definition of bidirectional entailment.
 For example, 
\emph on

\begin_inset Quotes eld
\end_inset

The staff were friendly and polite.
\begin_inset Quotes erd
\end_inset


\emph default
, 
\emph on

\begin_inset Quotes eld
\end_inset

The staff were polite.
\begin_inset Quotes erd
\end_inset


\emph default
 and 
\emph on

\begin_inset Quotes eld
\end_inset

The staff were friendly.
\begin_inset Quotes erd
\end_inset


\emph default
 are in three separate paraphrase groups.
 The creators of the MSRP corpus, however, note 
\begin_inset Quotes eld
\end_inset

...the majority of the equivalent pairs in this dataset exhibit 'mostly bidirection
al entailments', with one sentence containing information 'that differs'
 from or is not contained in the other.
\begin_inset Quotes erd
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "msrParapharaCorpus"

\end_inset

.
 While this does lead to more varied paraphrases, this straying from the
 strict linguistic definition of a paraphrase complicates the evaluation
 of the semantic space attempted here.
 This stricter adherence to bidirectional entailment resulted in finer separatio
n of groups, which makes this a more challenging corpus.
\end_layout

\begin_layout Standard
After the corpus had been broken into paraphrase groups some simple post-process
ing was done.
 Several artifacts present in the original corpus were removed, such as
 substituting the ampersand symbol for 
\emph on

\begin_inset Quotes eld
\end_inset

&amp
\begin_inset Quotes erd
\end_inset


\emph default
.
 Any paraphrase groups with containing identical sentences were merged,
 and duplicated removed.
 Finally, any group with less than three phrases was discarded.
 With this complete the breakdown is as in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:opinosis_corpus_hist"

\end_inset

.
\end_layout

\begin_layout Standard
Further information on the construction of the corpora in this section,
 and download links are available online
\begin_inset Note Note
status open

\begin_layout Plain Layout
INSERT LINK HERE
\end_layout

\end_inset


\end_layout

\begin_layout Section
Results and Discussion
\begin_inset CommandInset label
LatexCommand label
name "sec:Results-and-Discussion"

\end_inset


\end_layout

\begin_layout Subsection
Classification Results and Discussion
\end_layout

\begin_layout Standard
The results of performing the evaluation method described in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Methodology"

\end_inset

 are shown in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "tab:results"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="3">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
MSRP Subcorpus
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Opinosis Subcorpus
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 PV-DM
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
78.00%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
38.26%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 PV-DBOW
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
89.93%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
32.19%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 URAE
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
51.14%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
20.86%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 MOWE
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
97.91%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
69.30%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 BOW
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
98.37%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
65.23%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 PCA BOW
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
97.96%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
54.43%
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:results"

\end_inset

The semantic classification accuracy of the various models across the two
 evaluation corpora.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Difference in Performance Between the Corpora
\end_layout

\begin_layout Standard
While the relative performance of the models is similar between the corpora,
 the absolute performance differs.
 All the models perform much better on the MSRP subcorpus than on the Opinosis
 subcorpus.
 This can be attributed to the significantly more distinct classes in the
 MSRP subcorpus.
 The Opinosis subcorpus draws a finer line between sentences with similar
 meanings.
 For example, there is a paraphrase group for 
\emph on

\begin_inset Quotes eld
\end_inset

The staff were polite.
\begin_inset Quotes erd
\end_inset


\emph default
, another for 
\emph on

\begin_inset Quotes eld
\end_inset

The staff were friendly.
\begin_inset Quotes erd
\end_inset

,
\emph default
 and a third for 
\emph on

\begin_inset Quotes eld
\end_inset

The staff were friendly and polite.
\begin_inset Quotes erd
\end_inset


\emph default
.
 In MSRP, as discussed earlier, these would all have been considered the
 same group.
 Secondly, there is a much wider range of topics in the MSRP.
 Thus the paraphrase groups with different meanings in MSRP corpus are also
 more likely to have different topic entirely than those from Opinosis.
 Thus the the ground truth of the semantics separability of phrases from
 the MSRP corpus is higher than for Opinosis.
 
\end_layout

\begin_layout Subsubsection
The performance of the URAE
\end_layout

\begin_layout Standard
The URAE model performs substantially worse than the other models.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "KaagebExtractiveSummaristation"

\end_inset

 is was suggested that the URAE's poor performance at summarizing the Opinosis
 corpus could perhaps be attributed to the less formally structured product
 reviews -- the URAE being a highly structured compositional model.
 However its poor performance on the MSRP subcorpus suggests otherwise.
 The pretrained URAE model used here performed well on paraphrase detection
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

, when used in concert with other features including the word embedding
 distances.
 This suggests the URAE may work best when used to supplement other features.
 There are several more advanced versions of the RAE not evaluated here.
\end_layout

\begin_layout Subsection
Agreement in Difficulty
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/msrp_difficulty_agreement.png
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:msrp_difficulty_agreement"

\end_inset

 The misclassification agreement between each of the models for the MSRP
 subcorpus.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/opinosis_difficulty_agreement.png
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:opinosis_difficulty_agreement"

\end_inset

 The misclassification agreement between each of the models for the Opinosis
 subcorpus.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Model Agreement
\end_layout

\begin_layout Standard
None of the models perform perfectly.
 The misclassifications of the models can be compared.
 By selecting one of the test/train folds from the classification task above,
 and comparing the predicted classifications for each test-set sentence,
 the similarities of the models were assessed.
 The heatmaps in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:msrp_misclass_agreement"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:msrp_misclass_agreement"

\end_inset

 show the agreement in errors.
 Here misclassification agreement is: out of all sentences which both models
 failed to classify, for which portion were their predicted classes the
 same.
 This is equivalent to the mean Jacquard index of their sets of mutually
 incorrect classes.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/msrp_misclassification_agreement.png
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:msrp_misclass_agreement"

\end_inset

 The misclassification agreement between each of the models for the MSRP
 subcorpus.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/opinosis_misclassification_agreement.png
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:opinosis_misclass_agreement"

\end_inset

 The misclassification agreement between each of the models for the Opinosis
 subcorpus.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
This is speculation.
 I can't think of a way to prove it yet.
\end_layout

\end_inset

It can be seen that all misclassification is more in agreement in the MSRP
 subcorpus than in Opinosis.
 This may be attributed to the same reasoning that the MSRP classifications
 perform better -- the increased real semantic difference between statements.
 Reasonable misclassification occurs when there are multiple phrases that
 are semantically close together.
 As there are less statements truly semantically close to any other statements,
 the chance of there being 3 or more statements all close enough to cause
 confusion is much lower.
 Thus when this reasonable confusion occurs, it occurs in the same way across
 all models.
 
\end_layout

\begin_layout Standard
Strong similarity can be seen between the word-based models: MOWE, BOW and
 PCA BOW.
 This confirms the expectation that when the word content is misleading
 as to the meaning, it is misleading consistently.
 However PV-DBOW, while also another word order ignorant model, makes similar
 errors in MSRP to the word-based models, that similarity is not seen in
 Opinosis.
\end_layout

\begin_layout Standard
It can be noted that the 3 more advanced models: PV-DM, PV-DBOW and URAE
 are all much more similar to the word-based models than to each other.
 This seems to indicate that they do still make word based mistakes, but
 they make different 
\end_layout

\begin_layout Subsection
Limitations
\end_layout

\begin_layout Standard
This evaluation has some limitations.
 As with all such empirical evaluations of machine learning models, a more
 optimal choice of hyper-parameters and training data will have a impact
 on the performance.
 In particular, if the model training was on the evaluation data the models
 would be expected to be better able to place correct embedding for them.
 This was however unfeasible due to the small sizes of the datasets used
 for evaluation, and would not reflect real word application of the models
 to data not prior seen.
 Beyond the limitation of the data set is the contents of the corpora themselves.
\end_layout

\begin_layout Standard
The paraphrase groups were not selected to be independent of the word content
 overlap -- they were simply collected on commonality of meaning from real
 world sourced corpora.
 This is a distinct contrast to 
\begin_inset CommandInset citation
LatexCommand cite
key "RitterPosition"

\end_inset

 discussed in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sub:Evaluations-of-Semantic"

\end_inset

 where the classes were chosen to not have meaningful word overlap.
 However our work is complementary to work, and our findings are well aligned.
 The key difference in performance is the magnitude of the performance of
 the sum of word embeddings (comparable to the mean of word embeddings evaluated
 here).
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "RitterPosition"

\end_inset

 the word embedding model performed similarly to the best of the more complex
 models.
 In the results presented above we find that the word embedding based model
 performs significantly beyond the more complex models.
 This can be attributed to the word overlap in the paraphrase groups --
 in real-world speech people trying to say the same thing do infact use
 the same words very often.
\end_layout

\begin_layout Section
Conclusion
\begin_inset CommandInset label
LatexCommand label
name "sec:Conclusion"

\end_inset


\end_layout

\begin_layout Standard
A method was presented, to evaluate the semantic localization of sentence
 embedding models.
 Semantically equivalent sentences are those which exhibit bidirectional
 entailment -- they each imply the truth of the other.
 Paraphrases are semantically equivalent.
 The evaluation method is a semantic classification task -- to classify
 sentences as belonging to a paraphrase group of semantically equivalent
 sentences.
 This classification was performed across two subcorpora derived from existing
 sources, the closure of the MRSP corpus, and a manually grouped subset
 of the Opinosis corpus.
 The relative performance of various models was consistent across the two
 tasks, though differed on an absolute scale.
\end_layout

\begin_layout Standard
The MOWE and BOW models perform best, followed by the paragraph vector models,
 with the URAE trailing in both tests.
 The strong performance of the mean of word embeddings (MOWE) compared to
 the more advanced models aligned with the results of 
\begin_inset CommandInset citation
LatexCommand cite
key "RitterPosition"

\end_inset

 for sum of word embeddings.
 The difference in performance presented here for real-word sentences, were
 more marked.
 This may be attributed to real-world sentences often having meaning overlap
 correspondent to word overlap.
 It can be concluded that adding word vector representations is a practical
 and surprisingly effective method for encoding the meaning of a sentence.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "../../../Resources/master_bibliography/master"
options "abbrv"

\end_inset


\end_layout

\end_body
\end_document
