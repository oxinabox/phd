#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass acm-sigs
\begin_preamble
\usepackage{tikz}
\usetikzlibrary{positioning, fit,arrows,chains,shapes.geometric}
\usetikzlibrary{graphs,graphdrawing}
\usegdlibrary{force, layered, trees}

\usepackage{verbatim}
\usepackage{pdfcomment}
\usepackage{environ}
\RenewEnviron{comment}{\pdfcomment[author={Lyndon}]{\BODY}}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
The Evaluation of the Sentence Embeddings on the Semantic Localization of
 Groups of Paraphrases
\end_layout

\begin_layout Author
Lyndon White
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Who are the authors? Should sort that out at some point
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\entails}{\vDash}
\end_inset


\end_layout

\begin_layout Abstract
Several approaches for embedding a sentence into a vector space have been
 developed.
 It is unclear as to the information contained into these embeddings.
 To what extent the position in vector space reflect semantic meaning, rather
 than other factors such as syntactic structure.
 For many applications such as machine translation and automated summrization,
 it is highly desirable to have the meaning encoded in the embedding.
 Currently the semantic localization is assessed indirectly through practical
 benchmarks on certain applications.
 Here a new method is presented to more directly assess if the embeddings
 produced by a model are semantically consistent.
 It is demonstrated on several existing models and on a bag of words benchmark.
\end_layout

\begin_layout Abstract
A subset of the Opinosis corpus was annotated into groups of paraphrases
 -- sentences with the same meaning.
 A subset of Microsoft Research Paraphrase Corpus was also grouped into
 paraphrases.
 By assessing the localization and separability of these paraphrase groups
 in the models vector space the models semantic strength can be evaluated.
 The results found on these evaluations highlight some limitations of current
 models.
\end_layout

\begin_layout Terms
General terms from the ACM classification.
 See 
\begin_inset CommandInset href
LatexCommand href
target "http://www.acm.org/about/class/"

\end_inset


\end_layout

\begin_layout Keywords
Free key words
\end_layout

\begin_layout Section
Introduction
\begin_inset Note Note
status open

\begin_layout Plain Layout
1 page
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Various sentence embeddings are often called semantic vector space representatio
ns, such as in 
\begin_inset CommandInset citation
LatexCommand cite
key "iyyer2014generating"

\end_inset

.
 Embedding the meaning of a sentence into a vector space is expected to
 be very useful.
 A wide array of tools exist which can handle vector representations, but
 not natural language discourse.
 Vector embeddings of a sentence may encode a number of factors including
 semantic meaning, syntactic structure and topic.
 Since many of these embeddings are learned unsupervised on textual corpora,
 it is not entirely clear the emphasis placed on each factor in the encoding.
 For certain applications encoding semantic meaning is particularly desirable.
 These applications include machine translation, automatic summarization,
 and to a lesser extent sentiment analysis.
 For successful application to these areas it is required that the embeddings
 generated by the models correctly encode meaning such that sentences with
 the same meaning are co-located in the vector space, and sentences with
 differing meanings are further away.
 However, current models are not directly trained to optimize for this criteria.
\end_layout

\begin_layout Standard
Current methods to produce sentence embeddings generate them as a byproduct
 of unsupervised, or semi-supervised, tasks.
 
\begin_inset Note Comment
status collapsed

\begin_layout Plain Layout
All the papers in this section to an extent suggest they are capturing semantics
, except perhaps the structure one
\end_layout

\end_inset

Several underlying tasks have been used, including: word prediction, as
 in the work of 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

; recreation of input, as in the auto-encoders of 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:RAE,SocherEtAl2011:PoolRAE"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "iyyer2014generating"

\end_inset

; alignment of sentences embeddings across a parallel mulch-lingual corpus
 as in 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/HermannB13"

\end_inset

; and syntactic structural classification as in 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2013:CVG,socher2010PhraseEmbedding"

\end_inset

.
 These methods learn vector representations of their inputs which are best
 for the optimization task of their choice -- one for which sufficient training
 data exists.
 The methods are then used as a feature detectors step in other tasks.
 The information captured by the methods has proved to be very useful in
 these other tasks approach or exceeding previous state-of-the-art results
 in sentiment analysis and paraphrase detection.
 It is not entirely clear, though, the precise nature of the information
 captured.
 It is not known if the representations are predominantly semantic, syntactic,
 topical or for some other factor.
\end_layout

\begin_layout Standard
This paper aims to provide a method to assess how well the models are capturing
 semantic information.
 The strict definition of sentences semantic equivalence is: that each shall
 sentence entail the other.
 Such mutually entailing sentences are called paraphrases.
 The method proposed in this paper allows us to directly assess as to whether
 an give models embedding of the sentence is consistent with the embeddings
 for other synonymous sentences.
 It thus assesses whether projecting a sentence through the model in to
 the vector space preserves meaning.
\end_layout

\begin_layout Standard
To do this a evaluation corpus was prepared by grouping paraphrases from
 the Opinosis and Microsoft Researcg Paraphrase Corpora.
 Ensuring that the many sentenced of common meaning, but differing form
 are located in vector space together, is a challenging task and shows a
 models semantic encoding strength.
 Assessing that quality is performed as a Semantic Classification task --
 classifying sentences according to their meaning.
 This assessment will allow a better understanding of how these models work,
 and suggest new directions for the development in this area.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Layout paragraph needs rewriting
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The paper shall be laid out as in 5 sections.
 The Related Works section discusses the linguistic underpinnings of this
 work as well as various models and how they are currently assessed.
 The Methodology section describes the development of the evaluation corpus
 and how it is used to evaluated the semantic consistency of a model.
 The Results and Discussion section applies the method to existing models
 and discusses the implications for their accuracy.
 The Conclusion closes the paper and suggests new directions for development.
\end_layout

\begin_layout Section
Related Works
\end_layout

\begin_layout Subsection
General Evaluation Methods
\end_layout

\begin_layout Standard
As discussed in the introduction, current methods of evaluating the quality
 of embedding are on direct practical applications.
 The evaluation methods are not directly link to the methods used for training.
\begin_inset Note Note
status open

\begin_layout Plain Layout
The below need to become running paragraphs
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Paraphrase Detection
\end_layout

\begin_layout Standard
Closely related to the evaluation here is 
\end_layout

\begin_layout Standard
evaluation here are paraphrase corpora, such as the Microsoft Research Paraphara
se (MSRP) Corpus 
\begin_inset CommandInset citation
LatexCommand cite
key "msrParapharaCorpus"

\end_inset

.
 Such corpora present pairs of sentences and have the associated task of
 determining if the sentences are paraphrases, or not.
 This pairwise check is valuable, and does indicate to an extent if the
 embeddings are capturing the meaning.
 However, by looking at groups of paraphrases, a greater intuition can be
 gained as to the arrangement of meaning within the vector space.
 
\end_layout

\begin_layout Subsubsection
Sentiment Analysis
\end_layout

\begin_layout Standard
Sentiment Analysis is very commonly used technique for evaluating the quality
 of embedding.
 It was used both for Recussive Autoencoder in 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:RAE"

\end_inset

and for the paragraph vector models in 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

.
 Sentiment Analysis is normally tasked with classifying as positive or negative,
 or with assigning a score, such as in the Sentiment Treebank
\begin_inset CommandInset citation
LatexCommand cite
key "socher2013recursive"

\end_inset

.
 Determining the sentiment of a sentence is partially a semantic task, but
 it is lacking in several areas that would be required for meaning.
 For example, there is only an indirect requirement for the model to process
 the subject at all.
 
\begin_inset Quotes eld
\end_inset

The concert was quiet
\begin_inset Quotes erd
\end_inset

 does differ in sentiment from 
\begin_inset Quotes eld
\end_inset

The engine was quiet
\begin_inset Quotes erd
\end_inset

 but this is a limited affect.
 Sentiment Analysis is a key task in natural language processing, but it
 is very distinct from semantic meaning.
\end_layout

\begin_layout Subsubsection
Document Classification
\end_layout

\begin_layout Standard
Document Classification is a classic natural language processing task.
 A particular case of this is topic categorization, where the task is to
 classify the texts based on topic.
 Early work in the area include 
\begin_inset CommandInset citation
LatexCommand cite
key "maron1961automatic"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "borko1963automatic"

\end_inset

.
 Much more recently it has been used to assess the convolution neural networks
 of 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/ZhangL15"

\end_inset

, where the articles of several News corpora were classified into categories
 such as 
\begin_inset Quotes eld
\end_inset

Sports
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

Business
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

Entertainment
\begin_inset Quotes erd
\end_inset

, they also classified a subset of the Yahoo! Answers corpus.
 The topics for classification tend to be broad.
 A huge spectrum of different sentences are assigned to the same topic.
 It is thus insufficiently narrow to evaluate the consistency of meanings.
 Information retrieval can be seen as the inverse of the document classification
 task.
\end_layout

\begin_layout Subsubsection
Information Retrieval
\end_layout

\begin_layout Standard
Information Retrieval is the task of identifying the documents which most
 fit a query.
 Such document selection depends almost entirely on topic.
 Suitable results for information retrieval have no requirement to agree
 on meaning, though text with the same meaning are will be fit the same
 queries.
 As with the Sentiment Analysis and Document Classification, the Information
 Retrieval task, correct semantic understanding contributes only partially
 to a models success at the task.
 Thus the new task defined in the paper of Semantic Classification.
\end_layout

\begin_layout Subsection
Evaluations of Semantic Consistency
\end_layout

\begin_layout Standard
Semantic consistency for word embeddings is often measured using the analogy
 task.
 In an analogy the meta-relation: 
\begin_inset Quotes eld
\end_inset

A is to B as C is to D
\begin_inset Quotes erd
\end_inset

, for A,B,C,D words.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013linguisticsubstructures"

\end_inset

, the authors show that the word-embedding models are semantically consistent
 by showing that the semantic relations between words were reflected as
 a linear offset in the vector space.
 That is to say, for embeddings 
\begin_inset Formula $\tilde{x}_{a},\,\tilde{x}_{b},\,\tilde{x}_{c},\,\tilde{x}_{d}$
\end_inset

 corresponding to words A,B,C and D, respectively; it was tested that if
 for a strong relationship matching between A/B and C/D, then the offset
 vector would be approximately equal: 
\begin_inset Formula $\tilde{x}_{b}-\tilde{x}_{a}\approxeq\,\tilde{x}_{d}-\tilde{x}_{c}$
\end_inset

.
 Rearranging this in word space gets the oft-quoted example of 
\begin_inset Formula $"King"-"Man"+"Woman"\approxeq"Queen"$
\end_inset

, As man is to woman, king is to queen.
 In the rating task as described by 
\begin_inset CommandInset citation
LatexCommand cite
key "jurgens2012semeval"

\end_inset

, the goal is to rank such analogous word pairs based on the degree the
 relation matches.
 Thus to evaluate the word-embedding model using this task, it was a matter
 of sorting closeness of the corresponding offset vectors.
 Surprisingly strong results were found by the authors 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013linguisticsubstructures"

\end_inset

.
 It was thus demonstrated that word embeddings were not simply semantically
 consistent, but more so that this consistency was displayed as local linearity.
 This result gives strong confidant in the capacity of word embeddings to
 perform semantic tasks.
 However this relationship analogy test can not be easily performed for
 sentence embeddings.
\end_layout

\begin_layout Standard
The work of 
\begin_inset CommandInset citation
LatexCommand cite
key "gershmanphrase"

\end_inset

, compares the distances of modified sentences in vector space, to the semantic
 distances ascribed to them by human raters.
 Like the analogy task for word vectors, this task requires ranking the
 targets based on the vector distance, however instead of rating on strength
 of relationship is is simply on similarities of the sentences to an original
 base sentence for each group.
 In that evaluation 30 simple base sentences of the form 
\begin_inset Quotes eld
\end_inset

A [adjective1][noun1] [prepositional phrase][adjective2][noun2]
\begin_inset Quotes erd
\end_inset

 were modified to produce 4 difference derived sentences.
 The derived sentences were produced by swapping the nouns, swapping the
 adjectives, reversing the positional phrase (so 
\begin_inset Quotes eld
\end_inset

behind
\begin_inset Quotes erd
\end_inset

 becomes 
\begin_inset Quotes eld
\end_inset

in front of
\begin_inset Quotes erd
\end_inset

), and a paraphrase by doing all of the aforementioned changes.
 Human raters were tasked with sorting the transformed sentences in similarity
 to the base sentence.
 This evaluation found that the embedding models considered did not agree
 with the semantic similarity rankings placed by humans.
 While the sentence embedding models performed poorly on the distance ranking
 measure, it is also worth considering how they perform on a meaning classificat
ion task.
\end_layout

\begin_layout Standard
A meaning classification task was recently proposed in 
\begin_inset CommandInset citation
LatexCommand cite
key "RitterPosition"

\end_inset

, to classify sentences based on which spacial relationship was described.
 The task was to classify the sentence as describing: Adhesion to Vertical
 Surface, Support by Horizontal Surface, Full Containment, Partial Containment,
 or Support from Above.
 In this evaluation also, the sentences took a very structured form: 
\begin_inset Quotes eld
\end_inset

There is a [noun1] [on/in] the [noun2]
\begin_inset Quotes erd
\end_inset

.
 These highly structured sentences, take advantage of the disconnection
 between word content and the spacial relationship described can form a
 task that must be solved by a compositional understanding combining the
 understanding of the words.
 
\begin_inset Quotes eld
\end_inset

The apples is on the refrigerator
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

The magnet is on the refrigerator
\begin_inset Quotes erd
\end_inset

 belong to two separate spacial categories, even though the word content
 is very similar.
 Surprisingly, the simple model of adding word vectors outperformed compositiona
l models such as the recursive autoencoder.
 The result does have some limitation due to the highly artificial nature
 of the sentences, and the restriction to categorizing into a small number
 of classes based only on the meaning in terms of positional relationship.
 To generalist this task, we consider real world sentences being classed
 into groups according to their full semantic meaning.
\end_layout

\begin_layout Section
Methodology
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement t
wide true
sideways false
status open

\begin_layout Plain Layout
\begin_inset CommandInset include
LatexCommand include
filename "figs/block_overview.pgf"

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Process 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Model Pretraining
\end_layout

\begin_layout Standard
Prior to application to semantic classification task, as with any task the
 models had to be pretrained.
\end_layout

\begin_layout Subsection
Semantic Classification Task
\end_layout

\begin_layout Standard
To evaluate how well a model vectors capture the meaning of a sentence,
 a semantic classification task was defined.
 The task is to classify paraphrases into classes which all share the same
 meaning.
 This is a far finer-grained task than topic classification.
 It is a multiclass problem, rather than the binary decision problem of
 paraphrase detection.
 As is normal for classification tasks, the classifiers are given several
 examples of each class, and the required to classify unseen examples.
 As the goal here to to evaluate the embeddings and not the classifiers,
 only a simple linear classifier is used.
 The classifier is given the embeddings for the sentences as its inputs.
 If the sentence embeddings for each class form distinct and well separated
 groups, then the classification is a simple task and the classifier will
 do well.
 If, on the other hand, the sentence embeddings for a group of paraphrases
 are very spread-out, or overlap with other groups of different meaning,
 then the classification problem becomes hard.
 A model which produces embeddings which are easily classifiable according
 to their meaning can been thus seen to have good semantic localization.
 
\end_layout

\begin_layout Standard
Unlike the tasks discussed in the previous section, semantic classification
 does not have direct practical application.
 Rather it serves as a measure to assess the models general suitability
 for other tasks requiring a model with consistency between meaning and
 embedding.
\end_layout

\begin_layout Section
Corpus Construction
\end_layout

\begin_layout Subsection
Opinosis Paraphrase Grouped Subcorpus (Opinosis Subcorpus)
\end_layout

\begin_layout Subsubsection
Corpus
\end_layout

\begin_layout Standard
The Opinosis Corpus
\begin_inset CommandInset citation
LatexCommand cite
key "ganesan2010opinosis"

\end_inset

 was used as a source of original natural text.
 It is sourced from several online review sites: Tripadvisor, Edmunds.com,
 and Amazon.com, and contains single sentence statements about hotels, cars
 and electronics.
 The advantage of this as a source for texts is that comments on the quality
 of services and products tend to be along similar lines.
 The review sentences are syntactically simpler than sentences from a news-wire
 corpus, and also contain less named entities.
 However, as they are from more casual communications, the adherence to
 grammar and spelling may be less formal.
 
\end_layout

\begin_layout Subsubsection
Identification of Paraphrases
\end_layout

\begin_layout Standard
Paraphrases were identified using the standard criterion: bidirectional
 entailment.
 For a paraphrase group 
\begin_inset Formula $\mathcal{S}$
\end_inset

 of sentences: 
\begin_inset Formula $\forall s_{1},\, s_{2}\in\mathcal{S},\quad s_{1}\entails s_{2}\quad\wedge\; s_{2}\entails s_{1}$
\end_inset

, every sentence in the group entails the every other sentence in the group.
 A stricter interpretation of bidirectional entailment was used, as compared
 to the 
\begin_inset Quotes eld
\end_inset

mostly bidirectional entailment
\begin_inset Quotes erd
\end_inset

 used in the MSRP corpus.
 The grouping was carried out manually.
 The general guidelines were as follows.
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
I could group thi follwoing to Factors that were assumed make sentence the
 same, and factors that were assumed to make them different
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Tense
\end_layout

\begin_layout Standard
Tense was ignored.
 All sentences were in past or present tense.
\end_layout

\begin_layout Paragraph
Transitional Phrases, and Discourse Markers
\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
Hedges are a form of Pragmatic Marker.
 Are Pragmatic Markers the same as discourse markers?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Transitional phrases, as well as pragmatic and discourse markers were ignored
 for purposes of selecting paraphrases.
 This means parts of the sentences, such as 
\begin_inset Quotes eld
\end_inset

However, ...
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

On the plus side, ...
\begin_inset Quotes erd
\end_inset

, as well as 
\begin_inset Quotes eld
\end_inset

I feel ...
\begin_inset Quotes erd
\end_inset

 were ignored, and treated as semantically equivalent to the remainder of
 the sentence.
 
\end_layout

\begin_layout Paragraph
Intensity
\end_layout

\begin_layout Standard
Levels of intensity were coarsely quantized.
 For example: 
\begin_inset Quotes eld
\end_inset

Service was poor
\begin_inset Quotes erd
\end_inset

 with 
\begin_inset Quotes eld
\end_inset

Service was terrible
\begin_inset Quotes erd
\end_inset

; 
\begin_inset Quotes eld
\end_inset

Service was fabulous
\begin_inset Quotes erd
\end_inset

 with 
\begin_inset Quotes eld
\end_inset

Service was amazing
\begin_inset Quotes erd
\end_inset

; and 
\begin_inset Quotes eld
\end_inset

Service was reasonable
\begin_inset Quotes erd
\end_inset

 with 
\begin_inset Quotes eld
\end_inset

Service was adequate
\begin_inset Quotes erd
\end_inset

.
 Other approximations were handled similarly to intensities.
\end_layout

\begin_layout Paragraph
Approximations
\end_layout

\begin_layout Standard
When context suggested values to be very similar, the sentences were considered
 to be the same.
 For example: 
\begin_inset Quotes eld
\end_inset

The car gets 20 miles/gal.
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

The car gets 21miles/gal.
\begin_inset Quotes erd
\end_inset

 would be considered paraphrases.
 More complex approximations are involved with distance and location.
 For example: 
\begin_inset Quotes eld
\end_inset

It was in the center of town
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

It was near the center of town.
\begin_inset Quotes erd
\end_inset

 are equivalent, as are 
\begin_inset Quotes eld
\end_inset

It was very close to...
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

It was a 5 minute walk from...
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

It was just a few blocks away from...
\begin_inset Quotes erd
\end_inset

.
 Recognizing these approximations as all being equivalent is a requirement
 for natural language understanding.
 The ability to interpret other implications and indirect speech was also
 assumed.
\end_layout

\begin_layout Paragraph
Indirect Speech and Euphemisms
\end_layout

\begin_layout Standard
Many real world statements are indirect, metaphorical or euphemistic and
 these were grouped according to their meaning.
 For example: 
\begin_inset Quotes eld
\end_inset

The service needed improvement.
\begin_inset Quotes erd
\end_inset

 was grouped with 
\begin_inset Quotes eld
\end_inset

The service was poor.
\begin_inset Quotes erd
\end_inset

.
 Other implications service being synonymous with customer service also
 occur within paraphrase groups where sentence context warrants.
 Conversely, without context, the subject was not assumed to be equivalent.
\end_layout

\begin_layout Paragraph
Entity Reference
\end_layout

\begin_layout Standard
When the entity was referenced, or implied, it was not groups with entities
 that were directly stated.
 Thus: 
\begin_inset Quotes eld
\end_inset

The Tuskan Inn has a great location
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

It has a great location
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

The hotel has a great location.
\begin_inset Quotes erd
\end_inset

 are not paraphrases.
 Without greater context than a single sentence, none of them have enough
 information to entail the others.
 On the other hand, when a entity has multiple common names, for example
 
\begin_inset Quotes eld
\end_inset

San Francisco
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

SF
\begin_inset Quotes erd
\end_inset

, the synonym is sufficient.
 Certain groupings of entities can also be synonymous with entries as a
 whole, for example 
\begin_inset Quotes eld
\end_inset

The front desk staff and all the other staff...
\begin_inset Quotes erd
\end_inset

 is synonymous with 
\begin_inset Quotes eld
\end_inset

All the staff...
\begin_inset Quotes erd
\end_inset

.
 While the addition of redundant information, does not change the meaning,
 addition of new information does.
\end_layout

\begin_layout Paragraph
Additional Information
\end_layout

\begin_layout Standard
Any differing information, in a phrase is enough to cause it not to be grouped
 with phrases without that information.
 This comes from the definition of bidirectional entailment.
 If for sentences 
\begin_inset Formula $A,B,C$
\end_inset

 such that 
\begin_inset Formula $A\entails C$
\end_inset

, but 
\begin_inset Formula $B\not\entails C$
\end_inset

, then 
\begin_inset Formula $B\not\entails A$
\end_inset

: Proof by contradiction: assume 
\begin_inset Formula $B\entails A$
\end_inset

, by transitivity of entailment operator, then 
\begin_inset Formula $B\entails C$
\end_inset

 which is a contradiction, thus 
\series bold

\begin_inset Formula $B\not\entails A$
\end_inset


\series default
.
 This remains true regardless of whether or not 
\begin_inset Formula $A\entails B$
\end_inset

.
\begin_inset Note Comment
status open

\begin_layout Plain Layout
 If sentence 
\begin_inset Formula $A$
\end_inset

 entails two facts, which are equivalent to stand-alone sentences 
\begin_inset Formula $C$
\end_inset

 and 
\begin_inset Formula $D$
\end_inset

, and sentence 
\begin_inset Formula $B$
\end_inset

 does not entail 
\begin_inset Formula $C$
\end_inset

, then 
\begin_inset Formula $B$
\end_inset

 can not entail 
\begin_inset Formula $A$
\end_inset

 as entailment is a transitive operation.
 
\end_layout

\end_inset

For example: 
\begin_inset Quotes eld
\end_inset

The rooms were clean, also the housekeeping was polite.
\begin_inset Quotes erd
\end_inset

 is not a paraphrase of 
\begin_inset Quotes eld
\end_inset

The rooms were clean.
\begin_inset Quotes erd
\end_inset

 or of 
\begin_inset Quotes eld
\end_inset

The housekeeping was polite.
\begin_inset Quotes erd
\end_inset

.
 This is a notable difference from the MSRP Corpus where the creators noted
\begin_inset CommandInset citation
LatexCommand cite
key "msrParapharaCorpus"

\end_inset

 
\begin_inset Quotes eld
\end_inset

...the majority of the equivalent pairs in this dataset exhibit 'mostly bidirection
al entailments', with one sentence containing information 'that differs'
 from or is not contained in the other.
\begin_inset Quotes erd
\end_inset

.
 While this approach does lead to more varied paraphrases, it ties the meaning
 of a sentence to be a subset of its entailments.
 In the two pairwise classification task of paraphrase, or not, this adds
 a subtask of detecting the significance of information overlap, a reasonable
 measure for a system.
 However when examining the space of the embeddings and the relationship
 to meaning it is more problematic.
 For example, three of the largest paraphrase groupings in the corpus are
 around the facts: 
\begin_inset Quotes eld
\end_inset

The staff were helpful
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

The staff were friendly
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

The staff were friendly and helpful.
\begin_inset Quotes erd
\end_inset

.
 If 
\begin_inset Quotes eld
\end_inset

mostly bidirectional entailment
\begin_inset Quotes erd
\end_inset

 were used then either the three groups much be combined though helpful
 and friendly are not synonymous.
 The is scope for further analysis in this area, which considers quantified
 relationships between near paraphrases such as these.
 This stricter adherence to bidirectional entailment resulted in finer separatio
n of groups, which makes this a more challenging corpus.
\end_layout

\begin_layout Subsubsection
Post-Processing
\end_layout

\begin_layout Standard
The subcorpus was post-processed to remove several artifacts present in
 the original corpus.
 Digits and punctuation were removed from the start of sentences unless
 they were required by the context.
 These were present in several sentences which were harvested from numbered
 lists.
 Repeated commas were removed.
 
\begin_inset Quotes eld
\end_inset

&amp
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

&quot
\begin_inset Quotes erd
\end_inset

 were replaced with the ampersand and quote symbols respectively.
 These artifacts are presumably from a HTML transcription issue.
 A less expected issue was also corrected where some hyphens were replaced
 with commas.
 These were fixed in the words 
\begin_inset Quotes eld
\end_inset

non-smoking
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

e-ink
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

on-line
\begin_inset Quotes erd
\end_inset

.
 Finally, any paraphrase group with less than three phrases was discarded.
 With this complete the breakdown is as in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "tab:opinosis_corpus_hist"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/opinosis_corpus_stats.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:opinosis_corpus_hist"

\end_inset

Break down of how many paraphrases groups are present in the Opinosis subcorpus
 of which sizes.
 It contains a total of 521 unique sentences, broken up into 89 paraphrase
 groups.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Microsoft Research Paraphrased Grouped Subcorpus
\end_layout

\begin_layout Standard
While the MSRP Corpus is focused around the pairwise detection of paraphrases,
 or not, a subset of the sentences do occur in multiple paraphrase pairs.
 The transitive closure of the paraphrase pairings can be found, this gives
 paraphrase groups.
 Such a closure was performed across both the training and test sets of
 the corpus.
 Again any paraphrase groups with less than 3 phrases were discarded.
 The resulting sub-corpus has the breakdown as shown in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "tab:msrp_corpus_hist"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/msrp_corpus_stats.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:msrp_corpus_hist"

\end_inset

 Break down of how many paraphrases groups are present in the MSRP subcorpus
 of which sizes.It contains a total of 859 unique sentences, broken up into
 273 paraphrase groups.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Results and Discussion
\end_layout

\begin_layout Subsection
The Models
\end_layout

\begin_layout Standard
For demonstration purposes, several models are evaluated below.
\end_layout

\begin_layout Subsubsection
Unfolding Recursive Auto-Encoder (U-RAE)
\end_layout

\begin_layout Standard
The Unfolding Recursive Auto-Encoder is a autoencoder based method.
 It functions by using the same network to recursively pairwise combine
 embedded representations, following the parse tree
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

.
 It's optimization target is to be be able to reverse (unfold) the merges
 and produce the original sentence.
 The central folding layer - where the whole sentence is collapsed to a
 single embedding vector is the representation.
\end_layout

\begin_layout Standard
In this evaluation we make use of the pretrained network the authors of
 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

 have graciously made available
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
name "http://www.socher.org/index.php/Main/DynamicPoolingAndUnfoldingRecursiveAutoencodersForParaphraseDetection"
target "http://www.socher.org/index.php/Main/DynamicPoolingAndUnfoldingRecursiveAutoencodersForParaphraseDetection"

\end_inset


\end_layout

\end_inset

, full information is available in that paper.
 It is initialized on the unsupervised Collobert and Weston word embeddings
\begin_inset CommandInset citation
LatexCommand cite
key "collobert2008unified"

\end_inset

, and training on a subset of 150,000 sentences from the gigaword corpus.
 In the evaluation below the dynamic pooling layer is not used.
\end_layout

\begin_layout Subsubsection
Doc2Vec Models
\end_layout

\begin_layout Standard
Two new methods, commonly referred to as doc2vec are described in 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

.
 For both, we evaluate using the GenSim implementation
\begin_inset CommandInset citation
LatexCommand cite
key "rehurek_lrec"

\end_inset

 from the current develop branch.
\end_layout

\begin_layout Standard
Both are trained on approximately 12 million sentences from randomly selected
 wikipedia articles.
 In both the window size was set to 8 words, and the vectors were of 300
 dimensions.
\end_layout

\begin_layout Paragraph
PV-DM
\end_layout

\begin_layout Standard
Distributed Memory Paragraph Vectors (PV-DM) Doc2Vec document embeddings
 are based on an extension of Continuous Bag-of-Words word-embedding model
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013efficient"

\end_inset

.
 It is trained using a sliding window of words to predict the next word.
 The softmax predictor network is feed a word-embedding for each word in
 the window, and an additional embedding vector which is reused for all
 words in the sentence (called the paragraph vector in original paper).
 These input embeddings can be concatenated or averaged, in the results
 show below they were concatenated.
 During training both word and sentence vectors are allows to vary, in evaluatio
n, the word vectors are locked and the sentence vector trained until convergence.
 
\end_layout

\begin_layout Paragraph
PV-DBOW
\end_layout

\begin_layout Standard
Distributed Bag of Words version of Paragraph Vectors (PV-DBOW), is based
 on the Skip-gram model for word-embeddings, also from 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013efficient"

\end_inset

.
 In PV-DBOW a sentence vector is used as the sole input to a neural net.
 That network is tasked with predicting the words in the sentence.
\end_layout

\begin_layout Subsubsection
Bag of Words
\end_layout

\begin_layout Standard
A bag of words (BOW) model is presented as a baseline.
 There is a dimension in each vector embedding for the count of each token,
 including punctuation, in the sentence.
 In the corpus there were a total of 1085 unique tokens, thus a BOW embedding
 has 1085 dimensions.
 For comparison to the lower dimensional models Principle Component Analysis
 (PCA) was used to produce a additional baseline set of embeddings.
 In the evaluations below the PCA was computed only on the training portions
 of the data, and used to project both the testing and training BOW representati
ons down to 300 dimensions.
 These bag of words models do not have any outside knowledge.
\end_layout

\begin_layout Subsubsection
Mean of Word Embeddings 
\end_layout

\begin_layout Standard
Taking the element-wise mean of the word embeddings over all words in the
 sentence also produces a vector with the potential to encode meaning.
 Like bag of words no order information is encoded, but the model can take
 into word relations such as synonymity.
 This method was used as baseline in 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

.
 The closely related sum of word embeddings was first considered in 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolovSkip"

\end_inset

 for short phrases.
 It was found to be an effective model for summarization in 
\begin_inset CommandInset citation
LatexCommand cite
key "KaagebExtractiveSummaristation"

\end_inset

.
 It is a computationally cheap model, pretrained word embeddings are available.
\end_layout

\begin_layout Standard
The word embeddings used were taken from the Google News pretrained model
\begin_inset Foot
status open

\begin_layout Plain Layout
Available at 
\begin_inset CommandInset href
LatexCommand href
name "https://code.google.com/p/word2vec/"
target "https://code.google.com/p/word2vec/"

\end_inset


\end_layout

\end_inset

 base on the method described in 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolovSkip"

\end_inset

.
 This has been trained on 100 million sentences from Google News.
 54 tokens of the 1085 unique tokens in the corpus did not have embeddings
 in the Google News model.
 These were largely numbers, punctuation characters, proper nouns and unusual
 spellings, as well as the stop-words: 
\begin_inset Quotes eld
\end_inset

and
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

a
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

of
\begin_inset Quotes erd
\end_inset

.
 These words were simply skipped.
 The resulting embeddings have 300 dimensions, like the word embeddings
 they were based on.
\end_layout

\begin_layout Subsection
Classification Into Group with SVM
\end_layout

\begin_layout Standard
A support vector machine (SVM), with a linear kernel, was applied to the
 task of predicting which paraphrase group a sentence belong to was used
 to evaluate the embeddings.
 A range of values for the margin parameter (C) were trialed for each model.
 The best results are reported on here.
 Classification was verified using threefold cross-validation, the average
 results are shown in this section.
 For the smallest groups this means there were two training cases and one
 test case to classify.
 As a linear SVM's classification success depends on how linearly separable
 the input data is, this is a good method to assess the quality of the localizat
ion of the paraphrase groupings embeddings.
\end_layout

\begin_layout Subsection
Classification Results and Discussion
\end_layout

\begin_layout Standard
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="3">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Opinosis Subcorpus
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
MSRP Subcorpus
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 PV-DM
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
38.26%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
78.00%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 PV-DBOW
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
32.19%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
89.93%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 URAE
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
20.86%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
51.14%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 MOWE
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
69.30%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
97.91%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 BOW
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
65.23%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
98.37%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 PCA BOW
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
54.43%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
97.96%
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Should this be in a bar chart?
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Opinosis Subcorpus
\end_layout

\begin_layout Subsubsection
Difference in Performance Between the Corpora
\end_layout

\begin_layout Standard
While the relative performance of the models is similar between the corpora,
 the absolute performance differs.
 All the models perform much better on the MSRP subcorpus than on the Opinosis
 subcorpus.
 This can be attributed to the significantly more distinct classes in the
 MSRP subcorpus.
 The Opinosis subcorpus draws a finer line between sentences with similar
 meanings.
 For example, there is a paraphrase group for 
\begin_inset Quotes eld
\end_inset

The staff were nice.
\begin_inset Quotes erd
\end_inset

, another for 
\begin_inset Quotes eld
\end_inset

The staff were friendly.
\begin_inset Quotes erd
\end_inset

 and a third for 
\begin_inset Quotes eld
\end_inset

The staff were friendly and polite.
\begin_inset Quotes erd
\end_inset

.
 In MSRP, as discussed earlier, these would all have been considered the
 same group.
 Secondly, there is a much wider range of topics in the MSRP.
 Thus the paraphrase groups with different meanings in MSRPC are also more
 likely to have different topic entirely than those from Opinosis.
 Thus the the ground truth of the semantics separability of phrases from
 the MSRP corpus is higher than for Opinosis.
 
\end_layout

\begin_layout Subsubsection
The performance of the URAE
\end_layout

\begin_layout Standard
The URAE model performs substantially worse than the other models.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "KaagebExtractiveSummaristation"

\end_inset

 is was suggested that the URAE's poor performance at summarizing the Opinosis
 corpus could perhaps be attributed to the less formally structured product
 reviews -- the URAE being a highly structured compositional model.
 However its poor performance on the MSRP subcorpus suggests otherwise.
 The pretrained URAE model used here performed well on paraphrase detection,
 when used in concert with other features including the word embedding distances.
 This suggests the URAE may work best when used to supplement other features.
\end_layout

\begin_layout Standard
There are several more advanced versions of the RAE not evaluated here.
\end_layout

\begin_layout Subsection
Agreement in Difficulty
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/opinosis_difficulty_agreement.png
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:opinosis_difficulty_agreement"

\end_inset

 The misclassification agreement between each of the models for the Opinosis
 subcorpus.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/msrp_difficulty_agreement.png
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:msrp_difficulty_agreement"

\end_inset

 The misclassification agreement between each of the models for the MSRP
 subcorpus.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Model Agreement
\end_layout

\begin_layout Standard
None of the models perform perfectly.
 The misclassifications of the models can be compared.
 By selecting one of the test/train folds from the classification task above,
 and comparing the predicted classifications for each test-set sentence,
 the similarities of the models were assessed.
 The heatmaps in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:msrp_misclass_agreement"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:msrp_misclass_agreement"

\end_inset

 show the agreement in errors.
 Here mistclassification agreement is: out of all sentences which both models
 failed to classify, for which portion were their predicted classes the
 same.
 This is equivalent to the mean Jacquard index of their sets of mutually
 incorrect classes.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/opinosis_misclassification_agreement.png
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:opinosis_misclass_agreement"

\end_inset

 The misclassification agreement between each of the models for the Opinosis
 subcorpus.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/msrp_misclassification_agreement.png
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:msrp_misclass_agreement"

\end_inset

 The misclassification agreement between each of the models for the MSRP
 subcorpus.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Comment
status open

\begin_layout Plain Layout
This is speculation.
 I can't think of a way to prove it yet.
\end_layout

\end_inset

It can be seen that all misclassification is more in agreement in the MSRP
 subcorpus than in Opinosis.
 This may be attributed to the same reasoning that the MSRP classifications
 perform better -- the increased real semantic difference between statements.
 Reasonable misclassification occurs when there are multiple phrases that
 are semantically close together.
 As there are less statements truly semantically close to any other statements,
 the chance of there being 3 or more statements all close enough to cause
 confusion is much lower.
 Thus when this reasonable confusion occurs, it occurs in the same way across
 all models.
 
\end_layout

\begin_layout Standard
Strong similarity can be seen between the word-based models: MOWE, BOW and
 PCA BOW.
 This confirms the expectation that when the word content is misleading
 as to the meaning, it is misleading consistently.
 However PV-DBOW, while also another word order ignorant model, makes similar
 errors in MSRP to the word-based models, that similarity is not seen in
 Opinosis.
\end_layout

\begin_layout Standard
It can be noted that the 3 more advanced models: PV-DM, PV-DBOW and URAE
 are all much more similar to the word-based models than to each other.
 This seems to indicate that they do still make word based mistakes, but
 they make different 
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
A method was presented, to evaluate the semantic localization of sentence
 embedding models.
 Semantically equivalent sentences are those which exhibit bidirectional
 entailment -- they each imply the truth of the other.
 Paraphrases are semantically equivalent.
 The evaluation method is a semantic classification task.
 This classification was performed across two subcorpora derived from existing
 sources, the closure of the MRSP corpus, and a manually groups subset of
 the Opinosis corpus.
 The relative performance of various models was consistent across the two
 models, though differed on an absolute scale.
\end_layout

\begin_layout Standard
The MOWE and BOW models perform best, followed by the paragraph vector models,
 with the RAE trailing in both tests.
 The strong performance of the mean of word embeddings (MOWE) compared to
 the more advanced models aligned with the results of 
\begin_inset CommandInset citation
LatexCommand cite
key "RitterPosition"

\end_inset

 for sum of word embeddings.
 The difference in performance presented here for real-word sentences, were
 more marked.
 This may be attributed to real-world sentences often having meaning overlap
 correspondent to word overlap.
 It can be concluded that adding word vector representations is a practical
 and surprisingly effective method for encoding the meaning of a sentence.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Need to edit next section still
\end_layout

\end_inset


\end_layout

\begin_layout Standard
By comparing the distances of the generated sentences, and paraphrases from
 the original sentence, the relationship between semantic closeness and
 embedding distance can be seen.
\end_layout

\begin_layout Standard
The models evaluated using this method show that they are substantially
 more permanent the a naive bag of words approach there is still significant
 room for improvement.
 While these models perform very well at related practical tasks, this new
 method highlights some of their limitations.
 It suggests that calling the vector space the models embed into a syntactic
 space is misleading.
 The space clearly incorporates elements of syntax and word choice, as well
 as meaning.
 This result is not surprising and indeed some papers (including 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

) do refer to the space this way.
 The new method does make its truth substantially clearer.
\end_layout

\begin_layout Standard
Currently methods using sentence embeddings for tasks such as summarization
 and translation are limited.
 Some work in summarization has been done such as 
\begin_inset CommandInset citation
LatexCommand cite
key "yogatamaextractive"

\end_inset

 and in 
\begin_inset CommandInset citation
LatexCommand cite
key "KaagebExtractiveSummaristation"

\end_inset

.
 Current applications of sentence embeddings to translation are ever more
 limited, though word and short-phrase embeddings have been used successfully,
 as in 
\begin_inset CommandInset citation
LatexCommand cite
key "zou2013bilingual"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "gao2014learning"

\end_inset

.
 Models which perform poorly at sentiment classification, are expected to
 perform poorly at these tasks also.
 This expectation seems partially confirmed by the poor performance of the
 U-RAE in the summarization of 
\begin_inset CommandInset citation
LatexCommand cite
key "KaagebExtractiveSummaristation"

\end_inset

, though the sum of word vectors did not perform significantly better in
 that task.
 Successfully encoding meaning will allow for significant advancements in
 these areas.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "../../../Resources/master_bibliography/master"
options "ieeetr"

\end_inset


\end_layout

\end_body
\end_document
