#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{tikz}
\usetikzlibrary{positioning, fit}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
On the Modeling of the Australian Hansard with Document Embedding
\end_layout

\begin_layout Author
Lyndon White
\end_layout

\begin_layout Standard
.
\end_layout

\begin_layout Section
Introduction
\begin_inset Note Note
status open

\begin_layout Plain Layout
1 page
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Document embedding are a relatively recent technique for modeling documents
 as points in a vector space.
 Here we present an application of them to the Australian Hansard -- the
 record of federal parliamentary proceedings.
 We demonstrate that document embedding capture useful information from
 the speeches given in the debates.
\end_layout

\begin_layout Section
Background and Motivation
\begin_inset Note Note
status open

\begin_layout Plain Layout
1 page
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Each year XXX pages of political debate are produced.
 
\end_layout

\begin_layout Standard
The 
\end_layout

\begin_layout Standard
This approach has application in the future automation of the recording
 of formalized procedures such as the hansard -- rather than the current
 highly manual system requiring the words of the speech to be entered by
 the clerks, and metadata of the debate to be entered as well, the text
 of the speech could be captured by voice recognition, and the annotated
 performed automatically using an algorithm such as this.
 
\end_layout

\begin_layout Standard
The document embedding also have applications in the analysis of the speakers.
 It can be noted that the various parties speeches occupy different parts
 of the document space.
\end_layout

\begin_layout Subsection
Document Embedding
\end_layout

\begin_layout Subsubsection
Word Embedding
\end_layout

\begin_layout Standard
There are many models for word-embeddings.
 
\end_layout

\begin_layout Subsubsection
LSI and LDA
\end_layout

\begin_layout Standard
Latent Dirichlet Allocation
\begin_inset CommandInset citation
LatexCommand cite
key "blei2003latent"

\end_inset

 is a hierarchical generative model for a corpus.
 Each document is modeled as a bag of words, sampled from a distribution
 defined by a mixture of topic models -- the topic models are the latent
 variables.
 The learning process learns the distribution of words within topics, and
 which topics each document belongs to.
\end_layout

\begin_layout Subsubsection
Sentence Embeddings
\end_layout

\begin_layout Standard
Several recursive neural network based embedding also exist, for producing
 an embedding of a phrase
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:RAE,SocherEtAl2013:CVG,SocherEtAl2011:PoolRAE"

\end_inset

.
 However these are restricted to functioning on single sentences.
\end_layout

\begin_layout Subsubsection
Document Embeddings
\end_layout

\begin_layout Standard
The document embedding technique we examine here was first presented in
 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

.
 While they use the term paragraph vector, the technique is, as noted, suitable
 for use on documents of any length.
\end_layout

\begin_layout Standard
Distributed Memory Paragraph Vector (PV-DM) Doc2Vec document embeddings
 are based on an extension of Continuous Bag-of-Words word-embedding model
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013efficient"

\end_inset

.
 The other model presented in the 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

 Distributed Bag of Words version of Paragraph Vector (PV-DBOW), is based
 on the Skip-gram model for word-embeddings , also from 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013efficient"

\end_inset

.
 The naming of the paragraph (or document) vector models compared to their
 word vector counter-parts may catch the unwary reader.
 We will not be discussing PV-DBOW further as studies presented in 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

 found PV-DM to perform better; though an ensemble learner using both outperform
 either alone
\begin_inset Note Note
status open

\begin_layout Plain Layout
There is a citation that states Ensemble learners basically always do this
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Section
Modeling
\end_layout

\begin_layout Subsection
The Doc2Vec Model
\end_layout

\begin_layout Standard
The document vector can be considered as encoding the additional information
 required to shift the probability distribution for the predicted word,
 given the past words, from some 
\begin_inset Quotes eld
\end_inset

typical
\begin_inset Quotes erd
\end_inset

 distribution (as one would get from a Word2Vec model) to a specialized
 distribution for this document.
 Thus the document vector encodes what is non-typical about this document,
 on a number of different dimensions or features -- the selection of what
 features should be encoded for detecting non-typicality of documents is
 learned during the training.
 The features are selected to maximize the models overall ability to predict
 words in the training corpus.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset CommandInset include
LatexCommand include
filename "figs/doc2vec.pgf"

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:doc2vec"

\end_inset

 Doc2Vec, PV-DM model for the first training step on the speech 
\begin_inset Quotes eld
\end_inset

I rise to oppose the motion ...
\begin_inset Quotes erd
\end_inset

, assuming window size 3.
 The second step would use the words: 
\emph on
rise
\emph default
, 
\emph on
to
\emph default
, 
\emph on
oppose
\emph default
 
\begin_inset Formula $\to$
\end_inset


\emph on
the
\emph default
; third step would use: 
\emph on
to
\emph default
, 
\emph on
oppose
\emph default
, 
\emph on
the
\emph default
 
\begin_inset Formula $\to$
\end_inset


\emph on
motion
\emph default
.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
The LDA model
\end_layout

\begin_layout Standard
An 
\end_layout

\begin_layout Subsection
Derivation of the Models from the Hansard XML
\end_layout

\begin_layout Standard
The hansard is the record of the proceedings of the Australia Federal Parliament
, both houses.
 It is available in XML form back to 1903
\begin_inset Foot
status open

\begin_layout Plain Layout
Online: 
\begin_inset CommandInset href
LatexCommand href
name "http://www.aph.gov.au/Parliamentary_Business/Hansard/"
target "http://www.aph.gov.au/Parliamentary_Business/Hansard/"

\end_inset


\end_layout

\end_inset

.
 We look at the subset from 2/03/1998 until 23/08/2012 as was examined by
 
\begin_inset CommandInset citation
LatexCommand cite
key "turpin2012attempt"

\end_inset

.
 From this are extracted all speeches that occurred within greater debates.
 This gives us 72,037 documents.
\end_layout

\begin_layout Standard
The text of each speech was converted to lowercase and was tokenized into
 words using the NLTK
\begin_inset CommandInset citation
LatexCommand cite
key "bird2009natural"

\end_inset

 regex tokenizer.
 This splits each word, and each punctuation symbol into separate tokens
 for processing.
 The Gensim
\begin_inset CommandInset citation
LatexCommand cite
key "rehurek_lrec"

\end_inset

 implementation of LSI, LDA, and doc2vec were used.
\end_layout

\begin_layout Subsection
Training Vs Inferring
\end_layout

\begin_layout Standard
The doc2vec model is created by training the aforementioned neural network.
 During training the document vectors from the training data are continuously
 updated, along with the predictor and the word vectors.
 This will result in the document vectors containing high quality features
 to distinguish them among the other documents from the training corpus.
 We can also train the predictor and word vectors separately
\end_layout

\begin_layout Section
Evaluation: Separation/Classification
\end_layout

\begin_layout Subsection
Political Party
\end_layout

\begin_layout Subsection
Debate Type
\end_layout

\begin_layout Subsection
Topic
\end_layout

\begin_layout Standard
We can take an first order approximation of the topic of a speech by the
 presence of a few words.
\end_layout

\begin_layout Section
Results and Discussion
\end_layout

\begin_layout Subsection
Length
\end_layout

\begin_layout Standard
One of the latent variable captured in the embedding is the length.
 Unlike some corpora, the speeches in hansard vary dramatically in length.
 Speeches vary from a few words: 
\begin_inset Quotes eld
\end_inset

Leave granted
\begin_inset Quotes erd
\end_inset

, to the most wordy of bills: 
\begin_inset Quotes eld
\end_inset

PUBLIC EMPLOYMENT (CONSEQUENTIAL AND TRANSITIONAL) AMENDMENT BILL 1999
\begin_inset Quotes erd
\end_inset

 which has roughly 14,019 words.
 Q1: 378 Words, Median: 717 words, Q3: 1549.
\end_layout

\begin_layout Standard
We note that, when using the Wikipedia model, Principle Component Analysis's
 most significant dimension is proportional to length.
 This shows that it is encoded as a feature in the document vector -- though
 not necessarily directly.
\end_layout

\begin_layout Section
Conclusion and Future Work
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "../../../Resources/master_bibliography/master"
options "ieeetr"

\end_inset


\end_layout

\end_body
\end_document
