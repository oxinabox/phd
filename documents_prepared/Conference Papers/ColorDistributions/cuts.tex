


Rather than representing each observation as a one-hot output with the hot cell being the bin it lays in, we represent it as a histogram of values for a continuous probability distribution with the expected value being the original value of the observation.
This can be seen as blurring a one-hot representation.
By adding this blur to all observations, it encourages the classifier to learn at lower level the tied value between the adjunct bins.
For representations of the Saturation and Value channels a truncated Gaussian distribution is used for the representation, restricting the value to between 0 and 1.
For the Hue channel wrapped normal distribution is used
\pdfcomment{Workout how to merge this with descetisation section}



By blurring the training samples, we inform the model about the nearness of the discretized outputs.
The outputs from softmax output layer are intrinsically unordered and purely categorical.
By adding blurring the training output, we ensure that the network learns the relationship between the nearby output neurons.
Such that when one is activated, neurons representing  adjacent points in colors space are also activated (to a lesser strength).
By adding this to all training terms this information is trained into the model
A nearly equivalent alternative would be to define the same requirement as an additional term in the loss function, similar to L2-regularization, penalizing the networks that have sharp differenced between adjacent neurons.
This would be more difficult to tune.
We suggest that modifying the training data is a more elegant way to train this information into the model.

However, there is a trade-off, in blurring.
Without any blurring, the training data is one-hot encoded, and we do not get the benefits of encoding prior knowledge about smoothness, nor do we help the model effectively learn the relative position of the neurons.
The more blur that is added the more information is added about the structure of the space,
but the less information is available from each training datum about its precise point.
For example if the blurring made the target output have 5 adjacent neurons valued at \texttt{[0.1, 0.2, 0.4, 0.2, 0.1]}, then the loss from the network outputting a distribution shifted one place to the left, is much less than it would be for a less blurred case: \texttt{[0.01, 0.04, 0.9, 0.04, 0.01]}.
Thus the level of blurring is a hyper-parameter that needs to be set.


To determine the blurring level we conducted a hyper-parameter sweep.
We checked a range of values and evaluated on the development dataset.
We evaluated both for the output resolution of 64 and 256, and both on the full distribution estimation task, and the extrapolation task.
We found that best results for was to set the standard deviation of the distributions used in the discretization process to be $\sigma=\frac{1}{2n}$, where $n$ is the output resolution.
We found an exception to this when checking the blurring hyper-parameters the full task at 256 output bins; in this case $\sigma=\frac{1}{4n}$, however the different was very small.
As such in all further investigations we used a blurring with $\sigma=\frac{1}{2n}$ in all desecration, except as noted otherwise.

Decreasing or increasing the standard deviation from this produced higher perplexity and mean squared error to peak on when evaluated on the development datasets.
The blurring of $\sigma=\frac{1}{2n}$, redistributes the probability mass assigned in the discretization, to the surrounding bins.
For a training point that would be at the center of a bin, this roughly corresponds to 68.3\% of the probably mass assigned to the central bin, 15.7\% assigned to the bin on each side, and the remaining 0.3\% distributed to the remaining bins.
However, in general points are not aligned to the center of bins, so they generate asymmetric training cases.
All results presented here are for this value of the blurring hyper parameter.



In Hue-Saturation-Value (HSV) color space; we think of the hue channel 
as being cyclic: \emph{red}, \emph{orange}, \emph{yellow}, \emph{green}, \emph{blue}, \emph{purple}, and back to \emph{red}.
One solution would be to move to a color-space which is orderable in all channels: for example Red-Green-Blue (RGB).
However, some of these space do not well align to human perception; or exhibit strong correlation between the values on different channels.


\section{Highly multimodal colors}\label{sec:highly-multimodal-colors}
One of the core motivating factors of this work is to be able to handle the color names which have multiple distinct modes.
That is to say there are distinct peaks of the most likely region in color space.
\textcite{mcmahan2015bayesian} identify ``greenish'' as a convex color, i.e. one with a multimodal distribution.
We further identify several others ``purplish grey'', ``purplish'' and ``blueish'' amongst them to varying extents.
Though this is not true for all ``-ish'' colors: ``reddish'', ``orangish'', ``yellowish'' are not.
We also note this for shades of grey -- where hue is traditionally considered not to matter.




\multimodalfig{grey}
\multimodalfig{lightgrey}
\multimodalfig{darkgrey}


\pdfcomment{TODO: once I work out which images are being kept, give them captions.}

One such color with a significant bi-modal distribution is ``grey''.
Traditionally, ``grey'' has been considered to be achromatic -- that is to say its hue component does not matter.
However, by looking at the data from the Monroe dataset \parencite{Monroe2010XKCDdataset} in \Cref{figgrey} it can be seen that that a the hue distribution significantly favors blues and reds over greens and purples.
Further, ``light grey'' increases the yellow peak (\Cref{figlightgrey}), and ``dark grey'' (\Cref{figdarkgrey}) increases the ``blue'' peak, while also changing the value dimension, as expected.
Other multimodal colors include ``greenish'' which is less likely to be a pure green, than to be on the blue or yellow side of the color; 
and ``purplish grey'' which has a dip at ``magenta``.
These colors are discussed further in \Cref{resultsdistributions}.

A core interest here is in colors with a multi-modal and asymmetric distributions; such colors can not be considered as targets for regression as they do not have a symmetric noise around their mode.
As such they can not be estimated by using distance in color-space as a proxy for the probability of intent.




\pdfcomment{TODO: Rewrite this to not assume conditional independance as part of evalation}
As a second measure, we use the mean squared error to peak (MSE).
This is useful in the mono-modal symmetric case, and allows our model to be compared to regression models.
To do this, the output bin with the maximum probability according to $p_c(v_c\mid t)$ is found the continuous space value at the center of the bin's range is selected.
The mean square error is found in the transitional way, averaging over the three channels.
That is to say, with the definitions as before:
\[
binpeak_c(t)=\argmax_{1\le i \le n}{p_c\left(\tfrac{i}{n} \mid t\right)}
\]
\[
peak_c(t)=\frac{binpeak(t)}{n} - \frac{1}{2n}
\]
\[
SE(t, (v_{H}, v_{S}, v_{V})) = \frac{1}{3}
	\sum_{\mathclap{\forall d\in{H,S,V}}} (peak_c(t) - v_d)^2
\]
\[
MSE(\tau) =\frac{1}{|\tau|}
	\sum_{\mathclap{\qquad\qquad\quad
		\forall(t,(v_{H}, v_{S}, v_{V})) \in \tau}}
	 SE(t,(v_{H}, v_{S}, v_{V}))
\]

The space that the error is measured in is the 0-1 scaled HSV space that is used in the source dataset.
This measurement of the error to peak is the error that would be obtained if a single color output was greedily chosen.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Estimating a distribution 
The mapping from color name to color could be considered a regression problem, solving to find a function that when input some text such as ``forest green'', outputs a numerical value in a color space such as HSV or RGB.
However, regression discards information about the distribution.
If the distributions in color space were mono-modal
\footnote{It should be understood that in this paper, when say \emph{mono-modal} or \emph{multimodal} it is meant in the sense of the number of peaks in the probability distribution; not in the sense of the number of modalities of the data -- e.g. multimodal audio-visual data. 
	\textcite{mcmahan2015bayesian} call this convex, we prefer the term multimodal.}
and symmetric with consistently small variance, then considering the problem as regression with noise would be adequate.
However, the distributions in color space are generally non-systematic, wide variance, and are often multi-model.
Thus regression to a single value is inadequate.

Rather than attempting to predict a single point in color space, an estimate of the distribution in color space must be produced.
To do this we divide the color space into a number of even sized regions, i.e.  bins, and then discretize color observations from the training data into thethe probability distribution into these bins.
This transforms the problem of estimating a distribution in color-space,
into the very well understood problem of classification into categories -- where each category is a region of color-space.

A classifier will output probabilities for each of the possible categories its input could belong to, thus fulfilling the goal of learning a probability distribution.
Basic classification model consider each category as being distinct and unrelated.
In discretization the notion of continuousness is lost.
To counter this, we employ a novel blurring strategy, to train the model to learn that points near in color space, should be similar in likelihood.


\textcite{Usui:92} presents a very early application of neural networks to color, where a bottle-necked autoencoder is applied the to spectral-reflectance curves of Munsell color chips as a method of defining a statistically optimal basis for color-space.

