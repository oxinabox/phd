\documentclass[11pt,letterpaper, twocolumn]{article}

\usepackage{newtxtext}
%\usepackage[author={Lyndon}]{pdfcomment}

\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}

\usepackage{url}
%\usepackage[subtle]{savetrees}


\usepackage{tikz}
\usetikzlibrary{positioning, fit,  shapes.geometric}
\usepackage{graphicx}

\graphicspath{{./figs/}, {./}}

\makeatletter
\setlength{\@fptop}{0pt}
\setlength{\@fpbot}{0pt plus 1fil}
\makeatother


\usepackage[subpreambles=false]{standalone}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}
\newcommand{\compactmath}[1]{\noindent\resizebox{\columnwidth}{!}{$#1$}}

\usepackage{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{natbib}
\bibliographystyle{apalike}

\newcommand{\parencite}{\citep}
\newcommand{\textcite}{\cite}


%opening
\title{Learning Distributions of Meant Color}
\author{Lyndon White \\ lyndon.white@research.uwa.edu.au %
	\and Roberto Togneri \\ roberto.togneri@uwa.edu.au%
	\and Wei Liu \\ wei.liu@uwa.edu.au %
	\and Mohammed Bennamoun \\ mohammed.bennamoun@uwa.edu.au %
}


\begin{document}

\maketitle

\begin{abstract}
When a speaker says the name of a color, the color they picture is not necessarily the same as the listener imagines.
Color is a grounded semantic task, but that grounding is not a single value as the most recent works on color-generation do.
Rather proper understanding of color language requires the capacity to map a sequence of words is to a distribution in color space.
To achieve this, we propose a novel GRU based model which when given a color description such as \texttt{``light greenish blue''} produces an estimated probability distribution across HSV color space.
This model learns the compositional functioning of each term: \texttt{``light''}, \texttt{``green''}, \texttt{``ish''} and \texttt{``blue''} in the color name, and show together they determine the shape of the distribution.
This allows the prediction of  distributions for color description phrases never seen in training.
Such capacity is crucial in allowing human computer interaction systems to interpret vague color statements, with particular use in recognising objects described by color.
\end{abstract}

\section{Introduction}

Color understanding is an important subtask in natural language understanding.
The color language sub-domain displays many of the same features and difficulties as in the full domain.
Due to this difficulty, recent state of the art systems for image generation has demonstrated their capacity by generating from texts containing complex color descriptions such as ``the flower has petals that are bright pinkish purple with white stigma'' \parencite{reed2016generative, 2015arXiv151102793M}.
It is a challenging domain, due to ambiguity, multiple roles taken by the same words, the many modifiers, and shades of meaning.
The core focus of this work is on the linguistic phenomena around the descriptions of the color of a single patch as characterized by a color space such as HSV \parencite{smith1978color}.
Issues of illumination and perceived color based on visual context are considered out of the scope.



Consider that the word \texttt{``tan''} may mean a number of colors for different people in different circumstances ranging from the bronze of a tanned sunbather, to the brown of tanned leather;
\texttt{``green''} may mean anything from \texttt{``aquamarine''} to \texttt{``forest green''};
 and even \texttt{``forest green''} may mean the shades of a rain-forest, or of a fir-wood.
Thus the color intended cannot be uniquely inferred from the color name.
However, based on knowledge of the population's use of the words  it is possible to find a probability distribution as to the color intended.
The primary aim of this research is to map a sequence of color description words to a probability distribution in a color space such as HSV.
The is what is required for a proper understanding of color language.
This is in contrast to most recent work mapping terms to a single point color space \parencite{DBLP:journals/corr/KawakamiDRS16}.


Estimating color distributions has a clear use as a subsystem in many systems.
For example, in a human-interfacing image-processing system, when asked to select the \texttt{``dark bluish green''} object, each object can be ranked based on how likely it's color is according to the distribution.
This way if extra information eliminates the most-likely object, the second most likely object can immediately be determined.
Further, as the probability of the color of the object being described by the user input is known, a threshold can be given to report no object found, or to ask for additional confirmation.


Proper understanding requires considering the color intended as a random variable.
One cannot simply map from a name to a single point or region in the color space, but rather one must map to a distribution.
For a given color name, any number of points in color space could be intended, with some being more or less likely than others.
Or equivalently, up to interpretation, it may intend a region but the likelihood of what points are covered is variable and uncertain.
Thus we must estimate a probability distribution for all of color space.
To qualify our estimate of the distribution we discretize the color space into a large number of patches, and produce an output much like a histogram.
This allows us to take advantage of the well known methods for estimating a probability mass distribution.


Proper understanding of color language also requires a model capable of understanding linguistic compositionality.
It must understand how modifiers such as \texttt{``dark''} modify basic colors; and  how other modifiers such as \texttt{``very''} would interact as a modifier to modifiers.
It also must understand the functioning of affixes such as \texttt{``-ish''} in \texttt{``greenish''}.
This compositional understanding is needed both as a point of theory: a model without it is not sufficiently powerful; and practically: the generalisation ability from a compositional model allows it to handle color descriptions not seen in training.
Due to the combinatorial nature of language a data-sparsity problem exists:
for a large number of word combinations there are few examples in any given corpus.
This is a well known problem in n-gram language modelling \parencite{kneser1995improved,chen1996empirical,rosenfeld2000two}.
To handle this we take inspiration from a solution used in that area: the use of a recurrent neural network \parencite{mikolov2010recurrent,mikolov2011RnnLM} to process each color description as a compositional sequence of terms.
This allows the compositional understanding that is required for color understanding.


The core contribution of this work is a method for estimating probability distributions in color space given a color name which is able to handle color descriptions that are never seen during training.
To handle distribution estimation we employ a novel discretization and blurring procedure.
To allow for the capacity to predict distributions for colors never seen in training we define a GRU-based neural network to learn the compositional relationship from the term sequences describing the colors.
We call this model the Color Distribution Estimation from Sequences of Terms (CDEST) model.

There exists a large prior body of work for determining the name of a color, as well as several works relating to determining the color of a name, however to the best of our knowledge there are no other works on determining the distribution in color-space for a name when using large scale data of millions of observations for hundreds of color names.

\section{Related Work}\label{sec:related-work}

The understanding of color names has long been a concern of psycholinguistics and anthropology  \parencite{berlin1969basic,heider1972universals,HEIDER1972337,mylonas2015use}.
From this has come the corresponding area in natural language processing.

The ISCC-NBS color system \parencite{kelly1955iscc} has been key to these developments.
It maps a vocabulary of 26 words, including modifiers, that are composed according to a context free grammar, to points in color space.
\textcite{Berk:1982:HFS:358589.358606} defines a variant which is used as a basis for the thresholding based color naming work of \textcite{conway1992experimental}.
The models of \textcite{conway1992experimental} define regions on some channels of the color space to determine if modifiers such as \texttt{``pale''} are to be added to the color name -- thus achieving a basic compositional model.
\textcite{mojsilovic2005computational} uses another variant for a non-compositional color naming system based on defining a similarity metric.

There are also many works based on learning to name the 11 basic colors of \textcite{berlin1969basic}.
\textcite{ele1994computational} proposed to use a Gaussian Categorical Model to name the colors.
This functions by defining a Gaussian distribution across color space for each named color.
They propose, but do not verify, an extension to multi-word color names similar to \textcite{conway1992experimental} based on using additional Gaussian categories to determine if additional modifiers are to be added to the color name.
\textcite{menegaz2007discrete} partitions the color space using Delaunay triangulation and assigned probabilities for each name to the vertices  based on responses of 6 subjects to 424 color patches.
Our method also uses a discretization approach, though it is not based on Delaunay triangulation, but rather even sized bins.
\textcite{van2009learning} learns a generative topic based model from real world images, rather than color patches.
They use a probabilistic latent semantic analysis based model which is restricted to be unimodal -- which is correct for the small set of 11 colors being considered.

More recent works, including our own, function with much larger number of colors, larger vocabularies, and larger pools of respondents. 

The XKCD color survey \parencite{Munroe2010XKCDdataset}, collected over 3.4 million observations, in the form of color-name pairs, from over 222,500 respondents, using this Munroe identified 954 color names.
\textcite{mcmahan2015bayesian} subset this to produce what is commonly known as the Munroe Dataset.
They restrict the responses to be from native English speakers, 
and remove color names with less than 100 uses,
this gives a total of 2,176,417 observations and 829 color names. 
McMahan and Stone also define a standard test, development and train split, which we use in this paper.

\textcite{mcmahan2015bayesian} and \textcite{meomcmahanstone:color} use the Munroe dataset to evaluate their color naming methods.
These works are based on defining fuzzy rectangular distributions in the color space to cover the distribution estimated from the data, which are used in a Bayesian system to non-compositionally determine the color name.

%\pdfcomment{During the similar time-period another online color naming survey was conducted.
%\textcite{mylonas2010online,mylonas2012colour} collected a total of 50,000 observations from 2500 respondents in 7 languages.
%In this work we use the larger, more publically available, Munroe dataset.}

\textcite{2016arXiv160603821M} extends beyond all prior discussed color naming systems to produce a color namer capable of true compositional reasoning on the Munroe dataset.
Their method uses a recurrent neural network (RNN), which takes as input a point in color space, and the previous output word, and gives a probability of the next word to be output -- this is a conditional language model.
The color space used is a novel high dimensional Fourier inspired representation.
Our proposed CDEST model can be considered as the inverse of their conditional language model -- we map a sequence of color terms to a distribution over colors.
\textcite{2016arXiv160603821M} maps a point in color space, to a sequence of distributions over color terms.


\textcite{DBLP:journals/corr/KawakamiDRS16} also produces color naming model capable of compositional reasoning.
This is a per-character RNN and variational autoencoder approach.
It is in principle very similar to \textcite{2016arXiv160603821M}, but functioning on a character, rather than a word level.
The work by Kawakami et al, also includes a method for generating colors.

There is significantly less work on generating colors from color names.
Effectively early work includes the aforementioned color lists.
\textcite{DBLP:journals/corr/KawakamiDRS16} presents a method using RNNs over characters, to predict a point in \emph{Lab} color space \cite{hunter1958photoelectric}.
Color generation is the single output version of our task of color distribution estimation.

Color generation systems outputting a single color can approximate a probability distribution by using the distance from a observed color to the predicted color in color space as a proxy for the probability of the observed color. However, this does not handle asymmetric, or multimodal distributions, nor does it take into account that variance of a color significantly varies between names.
Thus there is the need for true distribution estimation.


\textcite{DBLP:journals/corr/MonroeHGP17} presents a neural network solution to a communication game, where a speaker is presented with three color patches and ask to describe one of them, 
such that when a listener is presented with the same color patches in randomized order they select the described color.
In this game, the color descriptions has full context, for example the speaker can say ``the darker blue one, not the cyan''.
Speaker and listener models are trained, using LSTM based decoders and encoders respectively.
They present several variants of their models.
The final time-step of their model produces a 100 dimensional representation of the description provided.
From this, a Gaussian distributed score function is calculated, over the high dimensional color space of \textcite{2016arXiv160603821M}.
By normalizing the scores of the three colors the listener is to choose from, the conditional probability of each can be found.
While this method does work with a probability distribution, as a step in its goal,
this probability distribution is always Gaussian -- both symmetric and unimodal -- albeit in a high-dimensional color space.
Though this distribution is very reasonable when the speaker is trying to minimise ambiguity, it is not suitable for general use.
Thus the need for a direct distribution estimation system.

\section{Color~Distribution~Estimation Framework}\label{sec:method}

\subsection{Conditional Independence Assumption}\label{sec:conditional-independence-assumption}
We make the assumption that given the name of the color, then the distribution of the H, S and V channels are independent.
That is to say, it is assumed if the color name is known, then  knowing the value of one channel would not provide any additional information as to the value  of the other channels.
This assumption is made, though not remarked upon, in \textcite{meomcmahanstone:color} and \textcite{mcmahan2015bayesian}.
The assumption of conditional independence allows considerable saving in computational resources.
Approximating the 3D joint distribution as the product of three 1D distributions decreases the space complexity from $O(n^3)$ to $O(n)$.


Superficial checks were carried out on the accuracy of this assumption.
Spearman's correlation on the training data suggests that for over three quarters of all color names, there is only weak to zero maximum pairwise absolute correlation between the channels (\mbox{Q3 = 0.187}).
However, this measure underestimates correlation for values that have circular relative value, such as hue.
When this correlation measure was compared amongst 16 color spaces HSV was found to be the lowest by a large margin.
Full details, including the full table of correlations, are available in the \Cref{sec:corrind}.
These results are suggestive, rather than solidly indicative of the degree of correctness of the conditional independence assumption.
For the investigation here, we consider the conditional independence assumption sufficient.

The evaluation metrics chosen do not assume conditional independence.
Though both Baseline model and the CDEST model do.
Better results may be obtained by outputting a 3D joint distribution -- which implementation-wise is a trivial extension of our models, 
though practically the increased memory use renders it computationally difficult.


\subsection{Discretization and Blurring}
The core problem is estimating a continuous  probability distribution, conditional on the color name.
Estimating discrete conditional distribution is a significantly more studied application of neural networks
-- this is the basic function of any softmax classifier.
To simplify the problem we thus transform it to be a discrete distribution estimation task, by discretizing the color space.
Discretization to a resolution of 64 and 256 bins per channel is considered.
At resolution of 256 bins there is effectively no information being lost as the original data was collected using 24 bit colors (8 bits per channel).

Discretization to resolution $n$ is the process by which a scalar observation%
\footnote{In the Munroe dataset, the provided HSV values are scaled to be between 0 and 1 in all channels. This simplifies the math in this section.}
 $x$
from one of the continuous color channels hue, saturation or value is converted into an $n$-vector with the properties expected of a probability mass function.
A na{\"i}ve approach is one-hot binning:
\[\Omega_{n}^{1hot}(x)=\left(\begin{cases}
1 & \mathrm{if}\:\:\frac{i-1}{n}<x\le\frac{i}{n}\\
0 & \:\:\:\:\:\:\mathrm{otherwise}
\end{cases}\right)_{i=1}^{i=n}
\]
This gives a $n$-vector that is zero everywhere, except for in the element corresponding to the patch of color space that the value $x$ lies with-in.
Discretization in this way loses all notion of continuousness of the color space.

However, the distribution in color space is intrinsically continuous.
Points near each other in color space have similar probabilities of being the intended color for a color name.
While discretization inevitably renders the space discrete, it is desirable to bring back this notion of smoothness as prior knowledge.

The training data is thus enhanced by adding a blur during discretization.
Consider $\mathcal{D}(\mu,\sigma^{2})$ some unimodal distribution, characterised by having expected value $\mu$ and variance parameter $\sigma^{2}$.
For saturation and value this is a truncated Gaussian.
Hue can elegantly be handled using a wrap-around Gaussian%
\footnote{We initially implemented the wrap-around Gaussian using 
 a von Mises distribution, however calculating this is computationally expensive.
Instead we handle it by defining a truncated Gaussian with a support 3 times the the true range of values for hue, then during discretization alias the memory location used for the output bins for the extensions of the support.
This is indistinguishable for the small variance level used here.}.
We write $P_{\mathcal{D}}(y_{1}<Y\le y_{2}\mid M=\mu,\,\Sigma=\sigma)$ to mean the probability of a value distributed according to $\mathcal{D}(\mu,\sigma^{2})$ being in the patch bordered by $y_1$ and $y_2$.
Using this the blurred-binning function is defined: 

\compactmath{\Omega_{n}^{blur}(x,\mathcal{D},\sigma)=\left(P_{\mathcal{D}}\left(\dfrac{i-1}{n}<Y\le\dfrac{i}{n}\mid M=x,\,\Sigma=\sigma\right)\right)_{i=1}^{i=n}}
This function maps points $x$ in the continuous color space, to probability mass vectors of length $n$.
The majority of the mass will be in the bin that the value $x$ would be in,
but some will be shared with the bins either side, and further.

By using more or less blurring applied to the training data, the priority of smoothness v.s. exact matching is controlled.
Adjusting $\sigma$ can be seen as similar to adjusting a regularisation parameter.
Considering the limits:
for all $\mathcal{D}$ and values $x$: 
\mbox{$\lim_{\sigma \to 0}\, \Omega_n^{blur}(x, \mathcal{D}, \sigma) = \Omega_n^{1hot}(x)$},
and \mbox{$\lim_{\sigma \to \infty}\, \Omega_n^{blur}(x, \mathcal{D}, \sigma) =  \left(\frac{1}{n}\right)_{i=1}^{i=n}$}.
A coarse parameter sweep was carried out using the development portion of the dataset (see \Cref{sec:data-preparation}).
Best results were found for $\sigma = \frac{1}{2n}$.
For a training point that would be at the center of a bin, this roughly corresponds to 68.3\% of the probably mass assigned to the central bin, 15.7\% assigned to adjacent bins, and the remaining 0.3\% distributed to the remaining bins.
All results presented here are for this level of blurring.
Further tuning of this parameter might enable better performance -- such-as using different blurring levels for each channel.


\subsection{Baseline Model}\label{sec:Baseline-model}
While the main interest in this work is in compositionally modelling the color language,
we also define a new non-compositional Baseline model to allow for comparison.
This model loosely resembles the histogram model discussed in \textcite{meomcmahanstone:color} and \textcite{mcmahan2015bayesian}.
Existing works do not aim to estimate a general distribution so are unsuitable for comparison.
Our Baseline must be able to estimate multimodal and asymmetric color distributions.

We define the Baseline using the the element-wise mean of discretized  training observations.
During our investigations we found that a model based only taking on the mean would return a predicted probability of zero for some of observations in the development dataset.
To handle this add-one smoothing is applied to each output distribution.

For training data $V \subset \left[ 0,1 \right] ^{3}\times T$, where $\left[ 0,1 \right] ^{3} \subset \mathbb{R}^{3}$ is the scaled HSV color space,\footnote{The 0-1 scaled HSV color space is as provided in the dataset from \textcite{mcmahan2015bayesian} see \Cref{sec:data-preparation}.} and $T$ is the natural language space.
The subset of training data for the description $t \in T$ is given by
$V_{|t}=\{(\tilde{v}_i,\,t_i) \in V \: \mid \: t_{i}=t\}$.
Per channel $c$ the Baseline model is defined by: 

\compactmath{
	q_{c}(x_{c}\mid t)=\frac{\displaystyle
		\sum_{\mathrlap{\!\!\forall((v_{H},v_{S},v_{V}),t)\in V_{|t}}}
			\Omega_n^{blur}(v_{c},\mathcal{D}_c,\sigma)
			\cdot
			\Omega_n^{1hot}(x_{c})		
			+1}
		{\left|V_{|t}\right|+n}
}
%
In this equation taking the dot-product with $\Omega_n^{1hot}(x_{c})$ is simply selecting the bin containing $x_c$.
Note the distinction between $x_c$ and $v_c$: $x_c$ is the point being queried, and $v_c$ is a point from the training set.
In implementation of-course the training set is not re-evaluated for every query.
By the conditional independence assumption the overall Baseline model is given by: $q(x_H,x_S,x_V\mid t) = \prod_{c\in {\{H,S,V\}}} q_c(x_c\mid t)$


The Baseline model can be used to predict distributions for all color descriptions in the training set.
This is inferior in generalisability to the CDEST model, which can handle any combination of tokens from the training set.
Without the requirement to learn the how the compositional structure of the terms in the color name function, it is a much simpler modelling problem, as such we suggest it is a strong Baseline for evaluational.
If the compositional model can match its performance, that would show that it was capturing the information from the training data.


\subsection{CDEST Model}

\begin{figure}
	\resizebox{\columnwidth}{!}{\input{./figs/neuralnet}}
	
	\caption{\label{network}
		The CDEST model for predicting the color space probability distributions of color.
		The section in the dotted-boxes is repeated for each time step.
	}
\end{figure}

The CDEST Model is the main focus of this work.
It is an RNN based model which learns the compositional interactions of the terms making up a color description and outputs a distribution estimate in color space.
The general structure of this network, shown in \Cref{network} is similar to \textcite{2016arXiv160603821M}, or indeed to most other word sequence learning models.
Each word first is transformed to an embedding representation.
This representation is trained with the rest of the network allowing per word information to be efficiently learned.
The embedding is used as the input for a Gated Recurrent Unit (GRU)  \parencite{cho2014properties}.
The output of the final time-step is fed to a Rectified Linear Unit (ReLU)  \parencite{dahl2013reludropout}.
Finally, this is the shared input for three distinct softmax output layers -- one for each of hue, saturation and value.
These outputs are vectors $\hat{y}_{H}(t)$, $\hat{y}_{s}(t)$, and $\hat{y}_{V}(t)$.
Using the conditional independence assumption the probability estimate is given by:
\[
	\hat{p}(x_H,x_S,x_V\mid t) = \displaystyle\prod_{
		\mathclap{c\in {\{H,S,V\}}}}
	 \hat{y}_{c}(t)\cdot \Omega_{n}^{1hot}(x_c))
\]
As in the Baseline model, the dot-product with $\Omega_{n}^{1hot}(x_c)$ serves to select the bin containing $x_c$.

The distinguishing features of this model compared to other word sequence learning models, is the use of GRU, rather than Long Short Term Memory (LSTM), and the split into three output layers.


We chose GRU as the basis of our reused structure in the recurrent network.
GRU has fewer parameters to learn than the more established LSTM.
It has generally been found to preform similarly well to LSTM \parencite{chung2014empirical};
including on the color naming problem \parencite{2016arXiv160603821M}.
A component for processing per-term such as the GRU, is essential in allowing the model to learn the compositional function of each term,  and thus to learn to handle color descriptions from outside the training set.

The three output layers are used to predict the distributions for the three channels.
Separating them like this requires a conditional independence assumption (see \Cref{sec:conditional-independence-assumption}).
The network is trained to minimize to sum of the three cross-entropy losses for these output layers.
The targets for training are the color space observations from the training set discretized using $\Omega_n^{blur}$ -- thus encouraging the model to learn smooth distributions.
The multiple output layers commonly occur multitask learning \parencite{caruana1997multitask,collobert2008unified}.
The layers prior to the output are shared, allowing common patterns to be learned.


\section{Experimental Setup}\label{sec:experimental-setup}
\subsection{Data Preparation and Tokenization}\label{sec:data-preparation}
We use the Munroe dataset \parencite{Munroe2010XKCDdataset,mcmahan2015bayesian}.
Each observation in the dataset is a textual color description, paired with a point in HSV color space.
The descriptions are tokenized into separate words and affixes.
Beyond simply breaking up a description \texttt{``greenish blue''} into words: \texttt{``greenish''} and \texttt{``blue''}, the suffixes \texttt{``-ish''} and \texttt{``-y''} are also separated at their own tokens: \texttt{``green''}, \texttt{``ish''}, \texttt{``blue''}.
This tokenization is achieved through a short list of word replacement rules.
Hyphens are also treated as their own tokens: \texttt{``blue-green''} becomes \texttt{``blue''}, \texttt{``-''}, \texttt{``green''}.
The beginning and end of the color description is not demarcated with any form of marker token.
Using this tokenization, each description is split into between one and four terms.
This results in a total of 311 unique tokens used by the CDEST model.
The Baseline model does not function per term, so uses the original 829 descriptions directly.


\subsection{Extrapolation Sub-Dataset}
The key advantage of CDEST over the Baseline model is its ability to predict the distribution for never before seen descriptions of colors.
For example, based on the learned understanding of \texttt{``salmon''} and of \texttt{``bright''}, from examples like \texttt{``bright green''} and \texttt{``bright red''}, our system can suggest the distribution in the color space of \texttt{``bright salmon''}, even though that description never occurs in the training data.
Thus demonstrating proper compositional learning.

To evaluate this generalisation capacity a new dataset is derived from the Munroe dataset, which we will call the extrapolation sub-dataset.
This is defined by selecting the rarest 100 color descriptions from the dataset,
with the restriction that every token in a selected description must still have at least 8 uses in other descriptions.
The selected examples include multi-token descriptions such as: \texttt{``bright yellow green''} and also single tokens that occur more commonly as modifiers than as stand-alone descriptions: e.g. \texttt{``pale''}.
The test and development datasets are restricted to contain only observations of these selected color descriptions.
Conversely, the extrapolation training set has no observations of these color descriptions.
This produces a dataset suitable for evaluating the capacity of our model to estimate the distributions for color descriptions not seen in training.
A similar approach for testing generalisation of compositional models has been used in \textcite{DBLP:journals/corr/AtzmonBKGC16}

\subsection{CDEST Model Parameters}
All hidden layers have width 128, except the embedding layer with width 16.
These values were found on a coarse search of the hyper-parameters using the development data set with the output resolution being 64 bins.
These parameters were also used for the 256 bin output resolution, to simplify comparison, though we suggest increasing the hidden layer size which would give additional benefit for the higher output resolution case.
During the hyper-parameter search, it was noted that the accuracy continued to improve as the hidden layer width was increased,
however significantly diminishing returns in terms of training time v.s. accuracy lead us to limit the hidden layer sizes.
Dropout \parencite{srivastava2014dropout} with a probability of 0.5 was used during training, on all hidden layers, except the embedding layer.


\subsection{Perplexity in Color-Space}
The Perplexity allows us to evaluate how well our estimated distribution matches the distribution of the observations in the test set.
Perplexity is commonly used for evaluating language models, however here it is being used to evaluate the discretized distribution estimate.
It can loosely be thought of as to how well the model's distribution does in terms of the size of a equivalent uniform distribution.

For $\tau$ the test-set made up of pairs consisting of a color names $t$, and color space points $\tilde{x}$;
and for $p(\tilde{x}\mid t)$ as the output of the evaluated model.
Perplexity is defined in the normal way:
\[
 PP(\tau) = 2^-{\left(
 	\frac{1}{|\tau|} 
 	\sum_{
	 		\forall(t,(\tilde{x})) \in \tau}
 	 \log_2 p(\tilde{x}\mid t)\right)}
\]
As this varies depending on the output resolution,
we also define a standardized perplexity $\frac{PP(\tau)}{n^3}$, where $n$ is the per channel output resolution of the model.
This standardised perplexity allows us to compare models of different output resolution.
It is equivalent to comparing the relative performance of the model to that of a uniform distribution $PP_{uniform}=n^3$.
Perplexity is a measure of how well the distribution estimated by the model matches reality according to the observations in the test set.


\subsection{Implementation}
The implementation of the CDEST and Baseline models was in the julia programming language \parencite{Julia}.
The full implementation and the experiment details can be downloaded from the source repository\footnote{Implementation source is at \url{https://github.com/oxinabox/ColoringNames.jl}}.
This implementation makes heavy use of use of the MLDataUtils.jl\footnote{MLDataUtils.jl is available from \url{https://github.com/JuliaML/MLDataUtils.jl}} and TensorFlow.jl\footnote{TensorFlow.jl is available from \url{https://github.com/malmaud/TensorFlow.jl}} packages, the latter of which we enhanced significantly to allow for this work to be carried out.




\section{Results and Discussion}\label{sec:results-and-discussion}

\subsection{Qualitative Comparison of the Distribution}\label{resultsdistributions}

\newcommand{\multimodalfig}[2]{
	\begin{figure}
		\includegraphics[width=\columnwidth]{multimodal/empiri256#1}
		
		\vspace{3mm}
		
		\includegraphics[width=\columnwidth]{multimodal/gru256#1}	
		\caption{\label{fig#1} Distribution estimate for \mbox{\texttt{``#2''}}}
	\end{figure}
}

\multimodalfig{indigo}{indigo}
\multimodalfig{greenish}{greenish}
\multimodalfig{purplishgrey}{purplish grey}


Shown in \Cref{figindigo,figgreenish,figpurplishgrey} are side-by-side comparisons of the output of the CDEST and the Baseline models.
Overall, it can be seen the Baseline model is a lot sharper, with more spikes,
whereas the CDEST model tends to be much smoother, even though both use the same blurring during discretization.
Smoothness is generally desirable, the increased smoothness seen in most plots is intuitively correct.
Though not always.

It can be seen that the CDEST model fails for some multimodal colors -- such as the hue \texttt{``greenish''} (\Cref{figgreenish}) where the concave section is filled in;
but succeeds for others such as \texttt{``purplish grey''} (\Cref{figpurplishgrey}).
We suggest the reason for this may be difficulties caused by the use of greenish both as a modifier: \texttt{``greenish blue''} and as a standalone description covering the edges of the green band.

The horizontal bands in the Baseline model outputs are the result of the add-one smoothing process, they are larger for colors with fewer examples -- such as \texttt{``purplish grey''}.
In the seminal work of \textcite{NPLM} one of the motivations for employing neural networks in natural language processing was to better handle cases that do not occur in the training data, by sharing information between terms.
While \textcite{NPLM} was looking at language modelling, where the key technique for handling unseen cases was back-off, the case equally applies here for distribution estimation, where unseen cases are handled with add-one smoothing.
The neural model of CDEST can, by knowledge sharing, better estimate the values for the unseen cases in color space.
This is district from, but related to, its key capacity as a compositional model to handle unseen cases in the natural language space.
 


\subsection{Distribution Estimation}
\pgfkeys{/pgf/number format/.cd, fixed relative,precision=4}
\pgfplotstableset{resultsstyle/.append style={%
	col sep=tab,
	header=has colnames,
	ignore chars={"},
	every head row/.style={before row=\toprule,	after row=\midrule},
	columns={model,resolution,perp,perpstd},
	columns/model/.style={string type},
	columns/resolution/.style={column name=$n$},
	columns/perp/.style={column name=$PP$, column type={r}},
	%columns/perphue/.style={column name=$PP_{H}$},
	%columns/perpsat/.style={column name=$PP_{S}$},
	%columns/perpval/.style={column name=$PP_{V}$},
	columns/perpstd/.style={column name=$\frac{PP}{n^3}$, column type={l}},
	%columns/msetopeak/.style={column name=$MSE$}
	}%
}

\begin{table}
	\centering
	\resizebox{\columnwidth}{!}{
		\tiny
		\pgfplotstabletypeset[resultsstyle,
			every row 2 column perpstd/.style={
				postproc cell content/.style={@cell content/.add={$\bf}{$}}
			}
			]{results/full.tsv}
	}
	\caption{\label{tblresfull} The results of evaluation on the full Munroe  dataset. Here $n$ is the output resolution of the model in each channel, $PP$ is the perplexity.}
\end{table}

Before we can evaluate the models capacity to learn the compositional functioning of color terms, by the extrapolation dataset, we must first test that it can learn when given examples from the full dataset where training and test set both use the same terms.
The base task here is the estimation the distribution in color space for a given color description.
The results are shown in \Cref{tblresfull}.
It can be seen that all models perform similarly.
Thus confirming that the CDEST model is able to learn to estimate a color distribution
The CDEST model based on sequence of color tokens,
reflects real use of the color descriptions in the test set; 
equally well as the non-compositional Baseline, that counts the exact uses of whole descriptions.

The CDEST model matches Baseline performance, when trained on a full set of color terms with all combinations of terms present in the training data.
There is little reason to use the CDEST model in this case however, since the Baseline model is simpler.
The key advantage of the CDEST model is its ability to predict a distribution for an unseen combination of colors, this is evaluated using the extrapolation task.

\subsection{Extrapolation}

\begin{table}
	\centering
	\resizebox{\columnwidth}{!}{
	\pgfplotstabletypeset[resultsstyle,
		every row 0 column 0/.style={
			postproc cell content/.style={@cell content/.add={\it}{}}},
		every row 3 column 0/.style={
			postproc cell content/.style={@cell content/.add={\it}{}}}
		]{results/extrapo.tsv}
	}
	\caption{\label{tblresextrapo} The results of evaluation on the extrapolation sub-dataset. Here $n$ is the output resolution of the model, $PP$ is the perplexity}
\end{table}


A core motivation of using the CDEST model, over the Baseline, is its ability to learn to combine tokens in a description in ways not seen in training.
This demonstrates that the CDEST model is capable of learning the compositional effects of the tokens in the color name.
That is to say learning how each influences the final distribution -- rather than simply memorising the training data as is done in the Baseline.

When it comes to the extrapolation task, where the color descriptions in the test set do not occur in the training set the best the Baseline model can do is a uniform distribution.
This is an uninteresting comparison as it is always $\frac{PP}{n^3}=1.0$.
Thus we look to comparing the results for extrapolation to the models when they are trained without the need for extrapolation.
To evaluate how well the model does at predicting these distributions,
we compare a CDEST model trained on the extrapolation sub-dataset, to the models trained on the full dataset.
Both the non-extrapolating, and extrapolating models are evaluated on the same set of rare color descriptions,
but the non-extrapolating models are also shown these rare descriptions during training.
The extrapolating model has never been trained on these combinations of color terms,
and instead must use the knowledge of how those color terms influence the color distribution in the other cases.

The results for this evaluation are shown in \Cref{tblresextrapo}.
As expected the non-extrapolating CDEST out performs extrapolating model.
This is as expected as the non-extrapolating models have training data for the rare color descriptions that occur in the extrapolation test set.
However the loss in performance when forced to extrapolate is relatively small. The extrapolation results as similar to the overall results from \Cref{tblresfull}.
Based on these results we judge that the model is successfully the learning compositional effects of the color terms upon the distribution.

The non-extrapolating CDEST also benefits from the same knowledge sharing that allows the extrapolating CDEST model to function at all.
This can be seen from the CDEST model out performing the Baseline model.
The Baseline model cannot benefit from knowledge sharing based on term use for estimating the curve of the rare descriptions.
This is notable in the high resolution case (256 bin),
where the sparsity of training data is such that the extrapolating CDEST model out performs the non-extrapolating Baseline.


\section{Conclusion}\label{sec:conclusion}
We have presented a method for estimating the probably distribution of colors that may be ascribed to an input name.
This method uses a discretization process based on treating each training point as the center of a Gaussian, or wrap-around Gaussian distribution, and finding the probability distribution for discrete regions of the color space.
The blurring in the discretized training points helps the model's softmax output to learn a reasonable continuous probability distribution, as approximated using a discrete distribution.
Working with probability distributions, rather than regression to a single color space point on color, allows for better handling of colors with observed distributions that are asymmetric, wide variance or multimodal in the color space -- most colors.

The model learns the compositional structure of a color name, which allows it to predict distributions for color names not seen given during training.
The input terms learn separate representations, which are together used to estimate the distribution.
For example: the color \texttt{``bright salmon''} does not occur in the training data, but there are many uses of \texttt{``bright''}, and \texttt{``salmon''} in other combinations.
As the CDEST model learns how each term influences the shape of the distribution it can thus extrapolate a distribution for \texttt{``bright salmon''} based on the learnt understanding of the \texttt{``bright''} and the \texttt{``salmon''} tokens.

Tagging each token with a linguistic role as a preprocessing step
may enhance the modelling of words with multiple uses. E.g. \texttt{``greenish''} which are used both as a modifier, and as a color category.

The discretization process for representing the a continuous probability distribution is pragmatically effective, but unsatisfying.
We suggest there are avenues for advancement here by the extensions of work such as \textcite{1998NNpdfDiffCdf}, \textcite{likas2001probability} and others to handle conditional distributions.

\subsection{Acknowledgements}
The computational resources required for this work were generously provided by the Australian National eResearch Collaboration Tools and Resources project (Nectar),
as well as a GPU grant from NVIDIA.
The first author would also like to thank Ari Herman (Portland State University) with whom long-ago discussion of a related problem lead to our initial interest in this area.


\bibliography{master}

\clearpage
\appendix

\input{supp.tex}

\end{document}
