\documentclass[11pt,letterpaper]{article}
\usepackage{ijcnlp2017}
\def\ijcnlppaperid{***} %  Enter the IJCNLP Paper ID here:

%\ijcnlpfinalcopy % Uncomment this line for the final submission:

\usepackage{newtxtext}
\usepackage[author={Lyndon}]{pdfcomment}

\usepackage{booktabs}
\usepackage{pgfplotstable}



\usepackage{tikz}
\usetikzlibrary{positioning, fit,  shapes.geometric}
\usepackage{graphicx}

\graphicspath{{./figs/}, {./}}

\makeatletter
\setlength{\@fptop}{0pt}
\setlength{\@fpbot}{0pt plus 1fil}
\makeatother



\usepackage[subpreambles=false]{standalone}

\usepackage{amsmath}
\usepackage{mathtools}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

%\usepackage{microtype}
\usepackage{verbatim}
\usepackage{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\parencite}{\cite}
\newcommand{\textcite}{\newcite}


%opening
\title{Learning Distributions of Meant Color}
\author{}
\date{}


\begin{document}

\maketitle

\begin{abstract}
When a speaker says the name of a color, the color they picture is not necessarily the same color the hearer imagines.
Color is a grounded semantic task, but that grounding is not a single value, but rather a range of possible values that could be intended.
To handle this case, we propose a model that given a input color description such as ``light greenish blue'' produces an estimated probability distribution across HSV color space.
This work presents a method for estimating probability distributions, based on samples using a discretization processes.
Predicting distributions  is useful beyond regressing to a single output for handling cases where the distribution is not simply a true value plus noise, but rather is an actual feature of the population speaker varying conception of the color.
The distributions feature wide, asymmetrical variance; and are potentially multimodal in nature.
We demonstrate a GRU-based neural network learning the grounded compositional semantics of the terms used within the color description.
By learning per-term, rather than per whole description, the model is able to predict distributions for combinations of terms that are not seen in the training data.
Our results show that it is reliably able to do this,
with only a small decrease in accuracy, compared to model trained directly with those combinations of terms.
The ability to predict distributions is useful as a component in human computer interaction systems.


\end{abstract}

\section{Introduction}
When a person says ``tan'' they may mean a number of colors: From the bronze of a tanned sunbather, to the brown of tanned leather.
When they say ``green'' they may mean anything from ``aquamarine'' to ``forest green'';
 and even ``forest green'' itself may mean the shades of a ``rain-forest'', or of a ``fir-wood''.
Thus the color can not be deterministically known from the color name.
However, based on knowledge of the population's use of the words, a probability distribution as to the color intended can be found.
Here issues of illumination and perceived color based on context will be disregarded, to focus on the core problem of the color of a single patch.


Color understanding is a core subtask in natural language understanding.
The color language sub-domain displays many of the same features and difficulties as natural language as a whole.
Recent state of the art systems for image generation has demonstrated their capacity by generating from texts containing complex color descriptions such as ``the flower has petals that 
are bright pinkish purple with white stigma'' \parencite{reed2016generative, 2015arXiv151102793M}.
It is a challenging domain, due to ambiguity, multiple roles taken by the same words, many modifiers, and shades of meaning.
Understanding color is crucial to understanding language. 

Properly understanding color language requires a model capable of understanding linguistic constitutionality.
It must understand how modifiers such as ``dark'' modify basic colors; and must understand how other modifiers such as ``very'' would interact as a modifier to modifiers.
It also must understand the functioning of affixes such as ``-ish'' in ``greenish''.
This compositional understanding is needed both as a point of theory: a model without it is not interesting; and practically: the generalisation ability from a compositional model allows it to handle color descriptions not seen in training.
Due to the combinatorial nature of language a data-sparsity problem exists where there are very few example in any given corpus of a very large number of combinations of words -- a well known problem in n-gram language modelling for example \parencite{kneser1995improved,chen1996empirical,rosenfeld2000two}.
To handle this we take inspiration from a solution used in language modelling: the use of recurrent neural network \parencite{mikolov2010recurrent,mikolov2011RnnLM} to process each color description a compositional sequence of terms.


The mapping from color name to color could be considered a regression problem, solving to find a function that when input some text such as ``forest green'', outputs a numerical value in a color space such as HSV or RGB.
However, regression discards information about the distribution.
If the distributions in color space were mono-modal
\footnote{It should be understood that in this paper, when say \emph{mono-modal} or \emph{multimodal} it is meant in the sense of the number of peaks in the probability distribution; not in the sense of the number of modalities of the data -- e.g. multimodal audio-visual data. 
\textcite{mcmahan2015bayesian} call this convex, we prefer the term multimodal.}
and symmetric with consistently small variance, then considering the problem as regression with noise would be adequate.
However, the distributions in color space are generally non-systematic, wide variance, and are often multi-model.
Thus regression to a single value is inadequate.
 
Rather than attempting to predict a single point in color space, an estimate of the distribution in color space must be produced.
To do this we divide the color space into a number of even sized regions, i.e.  bins, and then discretize color observations from the training data into thethe probability distribution into these bins.
This transforms the problem of estimating a distribution in color-space,
into the very well understood problem of classification into categories -- where each category is a region of color-space.

A classifier will output probabilities for each of the possible categories its input could belong to, thus fulfilling the goal of learning a probability distribution.
Basic classification model consider each category as being distinct and unrelated.
In discretization the notion of continuousness is lost.
To counter this, we employ a novel blurring strategy, to train the model to learn that points near in color space, should be similar in likelihood.


Understanding color distributions has a clear use as a subsystem many systems.
For example, in a human-interfacing image-processing system, when asked to select the ``dark bluish green'' object, each object can be ranked based on how likely it's color is according to the distribution.
This way if extra information eliminates the most-likely object, the second most likely object can immediately be determined.
Further, as the probability of the color of the object being the color being described by the user input is known, a threshold can be given to report no object found, or to ask for additional confirmation.


The set of all possible color descriptions form a much large space of multiword descriptions than can reasonably be collected in detail.
Possible 

The core contribution of this work is a method for estimating probability distributions in color-space given a color names which is able to handle color descriptions that are never seen during training.
To handle distribution estimation we employ a novel discretization and blurring procedure.
To allow for the capacity to predict distributions for colors never seen in training we define a GRU-based neural network which learns the compositional relationship expressed by the sequence terms making up the color description.
We call this model the Color Distribution Estimation from Sequence of Terms (CDEST) model.




 
\section{Related Work}\label{sec:related-work}

\subsection{Color Naming}
Color naming is the reverse of the task investigated in this work.
The color naming task takes a point in color-space as in input, and outputs a probability distribution over possible names for that color.
There a several notable recent works on color-naming.
\textcite{meomcmahanstone:color} and \textcite{mcmahan2015bayesian} present a full description Bayesian approach, which outputs the probability of a whole description.
\textcite{2016arXiv160603821M} presents a per-word LSTM approach, which produces a conditional language model -- sequentially outputting a probability of each word in the description.
\textcite{DBLP:journals/corr/KawakamiDRS16} presents a per-character LSTM and Variational Autoencoder approach, which products a conditional character language model -- sequentially outputting a probability of each character in the description.
The work by Kawakami et al, also includes a method for generating colors.

\subsection{Color Generation}
Color generation closely related to the primary task considered here.
The process of going from the name of a color, to an actual color -- a single point in a color space.
\textcite{DBLP:journals/corr/KawakamiDRS16} presents a method using RNN, and LSTM, as well as baselines using unigram and bigrams, over characters, to predict a point in \emph{Lab} color space \cite{hunter1958photoelectric}.
Color generation is the single output version of our task of color distribution estimation.


Color generation system systems outputting a single color can approximate a probability distribution by using the distance in color space, from a observed color to the predicted color as a proxy for the probability of the observed color. However, this does not handle asymmetric, or multimodal distributions, nor does it take into account that the range of values reasonable for one color description, often significantly differs in width from that reasonable for another.

\subsection{Color Identification}

\textcite{DBLP:journals/corr/MonroeHGP17} presents a neural network solution to communication game, where a is presented with three color patches and ask to describe one of them, 
such that when the listener is presented with the same color patches in randomized order they can select the one the speaker was describing.
In this game, the color descriptions have context, for example the speaker can say ``the darker blue one, not the cyan''.
Both speaker and listener models are trained, using and LSTM based decoder and encoders respectively.
They present several variants of their models, including pragmatic models, and models that combine knowledge.
The base listener model is of particular relevance to the distribution modeling task.
The final time-step of the base listener LSTM produces a 100 dimensional representation of description provided.
From this, a Gaussian distributed score function, over the Fourier-transform based color space of \textcite{2016arXiv160603821M}.
By normalizing the scores of the three colors the listener is to choose from, the conditional probability of each can be found.
It should be noted that while this method does output a probability distribution, as a by-product of the task
it is Gaussian distributed for all inputs -- both symmetric and mono-modal, albeit in a high-dimensional non-linear color space.
This is arguably reasonable distribution for use in this color-game, where the speaker is expressly trying to avoid ambiguity,
and where color descriptions can feature reference to other colors in the context.
Without this contextual information in the color-naming, distributions over all colors are required,
and these distributions are not expected to be symmetric in any consistent color space.



\section{Color Distribution Estimation Framework}\label{sec:method}

\subsection{Conditional Independence Assumption}\label{sec:conditional-independence-assumption}
For HSV colors we make the assumption that given the name of the color, then the distribution of the H, S and V components are independent.
That is to say, it is assumed if the color name is known, then  knowing the value of one channel would provide any additional information as to the value  of the other channels.
This assumption is not made in related works such as \textcite{DBLP:journals/corr/MonroeHGP17,DBLP:journals/corr/KawakamiDRS16}.
These works do not attempt to fully model the distribution,
and instead make other assumptions suitable to their own related tasks -- such as assuming symmetrical distributions, identical up to translation.
When attempting to fully model the distributions, the assumption of conditional independence allows considerable saving in computational resources.
Approximating the 3D joint distribution as the product of three 1D distributions decreases the space complexity from $O(n^3)$ to $O(n)$.



Superficial checks were carried out the the accuracy of this assumption.
Spearman's correlation on the training data suggests that for over three quarters of all color names, there is only weak to zero maximum pairwise absolute correlation between the H,S, and V components (\mbox{Q3 = 0.187}).
However, this measure underestimates correlation for values that have circular relative value, such as hue.
This correlation measure was the lowest by a large margin  when compared amongst 16 color spaces; RGB, HSV, HSI, HSL, xyY, XYZ, CIELab, Luv, LCHab, LCHuv, DIN99, DIN99d, DIN99o, LMS, YIQ, and YCbCr; by a full 100\%.
The table is available in the supplementary materials.
\pdfcomment{TODO: Add the table to the supplementary matrials}
Given the limitations of the evaluation methodology, these results are suggestive, rather than solidly indicative of the degree of correctness of the conditional independence assumption.
For the investigation here, we consider the conditional independence assumption sufficient.

Note that the evaluation metrics chosen do not assume conditional independence.
Though the models, including the baseline model, do.
Better results may be obtained by outputting a 3D joint distribution -- which is implementation wise a trivial extension of the models, 
though practically the increased memory use renders it computationally infeasible on most hardware.



\subsection{Discretization and Blurring}
The core problem is estimating a continuous  probability distribution, conditional on the color name.
Estimating conditional discrete distribution is a significantly more studied application of neural networks
-- this is the basic function of any softmax classifier.
As such, to simplify the problem, we transform it to be a discrete distribution estimation task, by discretizing the colorspace.
This allows us to use the well established classification techniques,
however discretization intrinsically loses the notion of continuousness
The particular feature we will miss is captured in the neighbourhood definition of continuousness:
points in color-space within a neighbourhood, have probabilities within a neighbourhood.
This can be informally considered as the smoothness of the output.
The probability of two nearby points in color space, should be similar --not sharply jumping.
A discrete distribution can not truely be continuousness ,
however the output can be encouraged to be smoother as a form of prior knowledge addition.



The prior knowledge of the smoothness of adjacent outputs is added to the models by enhancing the training data.
Prior knowledge can be added to the model either by modifying the training objective penalize outputs with sharp differences in adjaence output bins,
or by ensuring the knowledge is present in the training data.
As directly penalizing outputs based on lack smoothness would require modifying the loss function in ways not feasible for simpler models (See the Baseline model),
we modify the training data by introducing a blur.

The naive method to discretize continous observations is to use one-hot encoding.
One-hot encoding the observations would be to partition the color space into evenly sized patchs, 
and convert each observation into a vector that is zero everwhere,
except in the bin corresponding to the patch in color space where the observation lies -- which is set to one.
This looses all notions of smoothness of adjacent bins.
To add the knowledge back, we distribute the probability mass from hot bin, to the adjacent bins, via our novel blurring method.

The blurred discretization method functions by repressenting each training observation as a histogram of a continous distribution, with expected value equal to the value of the observation, and varience determined by the blurring factor.
The majority of the probability mass is in the bin containing the training observation, with a decreasing amount in each adjacent bin.
This method naturally allows restrictions on the continous space being approximated to be encodes by using different distribution when finding the blurred repressentation.
As the saturation and value channels are bounded, we use a truncated Gaussian distribution.
As the hue channel is circular -- with red being both the smallest and largest value --  we use a wrap-around Gaussian distribution.
By adjusting the varience of these distributions we can adjust the priority of smoothness vs sharpness.

If the varience of the distributions is very small the output distributions become sharper with more spikes and discontinousalities,
when the varience is large the model is smoother but can't repressent sharp changes.
We conducted a hyper-parameter sweep on the development dataset and found the best results were for having the blurring level given by $\sigma = \frac{1}{2n}$ where $n$ is the number of bins used in discretization.
For a training point that would be at the center of a bin, this roughly corresponds to 68.3\% of the probably mass assigned to the central bin, 15.7\% assigned to the bin on each side, and the remaining 0.3\% distributed to the remaining bins.
However, in general points are not aligned to the center of bins, so they generate asymmetric training cases.
All results presented here are for this value of the blurring hyper parameter.
Further tuning of this parameter might enable better results -- particularly using different blurring levels for the difference channels, or tuning the blurring based on the number of observations for a particular color name.


\subsection{Baseline Model}\label{sec:baseline-model}
We define a new simplistic model to evaluate the capacity to model color distributions.
This model loosely resembles the histogram model used by \textcite{mcmahan2015bayesian} in their Baseline model to characterise their input space.




For comparison we define an additional model based more directly on the training data.
This is a simpler model with no machine learning component.
For this Baseline model, the estimated distribution for each color description is produced by averaging all of that colors discretized observations per output bin.
This features the same blurring, as in the CDEST model.
During our investigations we found that a model based only taking on the mean would return a predicted probability of zero for some of observations in the development dataset.
To handle this add-one smoothing is applied to each output distribution.
The Baseline model can be used to predict distributions for all color descriptions in the training set.
This is inferior in generalisability to the CDEST model, which can handle any combination of tokens from the training set.
Without the requirement to learn the how the compositional structure of the terms in the color name function, it is a much simpler modeling problem, as such we suggest it is a strong baseline for evaluational.

\subsection{CDEST Model}

\begin{figure}
	\resizebox{\columnwidth}{!}{\input{./figs/neuralnet}}
	
	\caption{\label{network}
		The CDEST model for predicting the color-space probability distributions of color.
		The section in the dotted-boxes is repeated for each time step.
	}
\end{figure}

We present a neural network based model, with sequential inputs which predicts the 3 separate output distributions shown in \Cref{network}. The general structure of this network is similar to \textcite{2016arXiv160603821M}, or indeed to most other word sequence learning models.
Each word first is transformed to an embedding representation.
This representation is randomly initialized and is trained with the rest of the network allowing a per word information to be efficiently learned.
The embedding is used as the input for a Gated Recurrent Unit (GRU)  \parencite{cho2014properties}.
The output of final time-step is feed to a Rectified Linear Unit (ReLU)  \parencite{dahl2013reludropout}.
Finally, the this used as the input the three distinct softmax output layers -- one for each of hue, saturation and value.
The distinguishing features of this model compared to other word sequence learning models, is the use of GRU, rather than Long Short Term Memory (LSTM), and the split into three output layers.


We choose GRU as the basis of our reused structure in the recurrent network.
GRU has fewer parameters to learn than the more established LSTM.
It has generally been found to preform similarly well to LSTM \parencite{chung2014empirical};
including on the color naming problem \parencite{2016arXiv160603821M}.
A component for processing per-term such as the GRU, is essential in allowing the model to learn the compositional function of each term,  and thus to learn to handle color descriptions from outside the training set.

The three output layers are used to predict the distributions for the three channels -- hue, saturation and value.
Seperating them like this requires a conditional independence assumption (see \Cref{sec:conditional-independence-assumption}).
The network was trained to minimize to sum of the three cross-entropy losses of these output layers.
The multiple output layers commonly occur joint learning and related transfer learning problems.
\pdfcomment{I was sure I had a good citation from something by Bengio for this techneque in transfer learning. But I can't seem to find it}
The layers prior to the output are shared, allowing common patterns to be learned.
This model is the primary contribution of this work.




\section{Experimental Setup}\label{sec:experimental-setup}
\subsection{Data Preparation}
We use the Monroe dataset \parencite{Monroe2010XKCDdataset}, as prepared by McMahan and Stone \parencite{mcmahan2015bayesian}.
This dataset is partitioned into test, development, and training sets; and has had some cleaning from the original data collected by Randell \textcite{Monroe2010XKCDdataset}.
It is also used by Will \textcite{2016arXiv160603821M} and \textcite{DBLP:journals/corr/KawakamiDRS16}.
\pdfcomment{I'm not sure how to make it clear that the two Monroes involved are not the same person.}
The color space is HSV, with all values between zero and one.
Each is paired with one of 829 color description, as provided by participants in the Color Survey.

The text descriptions are loosely tokenized into separate words and affixes.
Beyond simply breaking up a description ``greenish blue'' into words: ``greenish'', ``blue'', the suffixes ``-ish'' and ``-y' are also separated at their own tokens: ``green'', ``ish'', ``blue''.
This tokenization is achieved through a short list of word replacement rules.
Hyphens are also treated as their own tokens: ``blue-green'' becomes ``blue'', ``-'', ``green''.
The beginning and end of the color description is not demarcated with any form of marker token.
Using this tokenization, each description is split into between one and four terms.
This results in a total of 311 unique tokens used by the CDEST model.
The Baseline model does not function per term, so uses the original 829 descriptions directly.


\subsection{Extrapolation Sub-Dataset}
The key advantage of CDEST over the Baseline model is its ability to predict the distribution for never before seen descriptions of colors.
For example, based on the learned understanding of ``bright'', from examples like ``bright green'' and ``bright red'', and of ``salmon'', our system can suggest the distribution in color space of ``bright salmon'', even though that color never occurs in the training data.
To evaluate this generalisation capacity a new dataset is derived from Monroe dataset, which we will call the extrapolation sub-dataset.
This is defined by selecting the rarest 100 color descriptions from the dataset,
with the restriction that every token in a selected description must still have at least 8 uses in other descriptions.
The selected examples include multi-token descriptions such as: ``"bright yellow green'' and also some single tokens that occur more commonly as modifiers than as stand-alone descriptions: ``pale''.
The test and development datasets are restricted to contain only observations of these selected color descriptions.
Conversely, the extrapolation training set has no observations of these color descriptions.
This produces a dataset suitable for evaluating the capacity of our model to estimate the distributions for color descriptions not seen in training.
A similar approach for testing generalisation of compositional models by removing items from the from the training set that occur in the test set can be seen in \textcite{DBLP:journals/corr/AtzmonBKGC16,DBLP:journals/corr/Hassanien16}.

\subsection{CDEST Model Parameters}
For the CDEST model, regardless of output resolution the same network parameters are used.
All hidden layers have width 128, except the embedding layer with width 16.
These values were found on a coarse search of the hyper parameters using the development portion of the data set with the output resolution being 64 bins.
These parameters were also used for the 256 bin output resolution, to simplify comparison, though we suggest increasing the hidden layer size would give additional benefit for the higher output resolution case.
During the hyper-parameter search, it was noted that the accuracy continued to improve as hidden layer width was increased,
however significantly diminishing returns in terms of training time vs accuracy lead us to limit the hidden layer sizes.
Dropout \parencite{srivastava2014dropout} with a probability of 0.5 was used during training, on all hidden layers, except the embedding layer.


\subsection{Evaluation Metrics}
We propose two key measures of evaluation: Perplexity, and Mean Squared Error to peak.
The Perplexity allows us to evaluate how well our estimated distribution matches the distribution of the observations in the test set.
Perplexity is commonly used for evaluating language models, however here it is being used to evaluate the discretized distribution.
It can loosely be through of as to how well the model's distribution does compared to a uniform distribution -- which has a perplexity equal to the number of bins.


For $\tau$ the test-set made up of pairs consisting of a textual color name $t$, and color-space observation $(v_{H}, v_{S}, v_{V})$.
Given an observation as a color description $t$ and a point in color space $\tilde{v}$
We define $p(\tilde{v}\mid t)$ being the output of the evaluated model.
From this the perplexity is defined in the normal way:

\[
 PP(\tau) = 2^{-\left(
 	\displaystyle\frac{1}{|\tau|} 
 	\displaystyle\sum_{
	 		\forall(t,(\tilde{v})) \in \tau}
 	 \log_2 p(\tilde{v}\mid t)\right)}
\]


As the perplexity varies depending on the output resolution,
we also define a standardized perplexity $\frac{PP(\tau)}{n^3}$, where $n$ is the per channel output resolution of the model.
This standardised perplexity allows us to compare models of different output resolution.
It is equivalent to comparing the relative performance of the model to that of a uniform distribution over color-space of that resolution: $PP_{uniform}=n^3$.
Perplexity is a measure of how well the distribution estimated by the model, matches reality according to the observations in the test set.


\pdfcomment{TODO: Rewrite this to not assume conditional independance as part of evalation}
As a second measure, we use the mean squared error to peak (MSE).
This is useful in the mono-modal symmetric case, and allows our model to be compared to regression models.
To do this, the output bin with the maximum probability according to $p_c(v_c\mid t)$ is found the continuous space value at the center of the bin's range is selected.
The mean square error is found in the transitional way, averaging over the three channels.
That is to say, with the definitions as before:
\[
binpeak_c(t)=\argmax_{1\le i \le n}{p_c\left(\tfrac{i}{n} \mid t\right)}
\]
\[
peak_c(t)=\frac{binpeak(t)}{n} - \frac{1}{2n}
\]
\[
SE(t, (v_{H}, v_{S}, v_{V})) = \frac{1}{3}
	\sum_{\mathclap{\forall d\in{H,S,V}}} (peak_c(t) - v_d)^2
\]
\[
MSE(\tau) =\frac{1}{|\tau|}
	\sum_{\mathclap{\qquad\qquad\quad
		\forall(t,(v_{H}, v_{S}, v_{V})) \in \tau}}
	 SE(t,(v_{H}, v_{S}, v_{V}))
\]

The space that the error is measured in is the 0-1 scaled HSV space that is used in the source dataset.
This measurement of the error to peak is the error that would be obtained if a single color output was greedily chosen.


\subsection{Implementation}
The implementation of the models and all evaluations was made in the julia programming language \parencite{Julia},
using the bindings for TensorFlow \parencite{tensorflow2015-whitepaper}.
The full source code is included in the supplementary materials.

\section{Results and Discussion}\label{sec:results-and-discussion}

\subsection{Qualitative Comparison of the Distribution}\label{resultsdistributions}


\newcommand{\multimodalfig}[2]{
	\begin{figure}
		\includegraphics[width=\columnwidth]{multimodal/empiri256#1}
		
		\vspace{3mm}
		
		\includegraphics[width=\columnwidth]{multimodal/gru256#1}	
		\caption{\label{fig#1} Distribution estimate for \mbox{``#2''}}
	\end{figure}
}

\multimodalfig{indigo}{indigo}
\multimodalfig{greenish}{greenish}
\multimodalfig{purplishgrey}{purplish grey}



Shown in \Cref{figindigo,figgreenish,figpurplishgrey} are side by-side comparisons of the output of the CDEST and the Baseline models.
Overall, it can be seen the Baseline model is a lot sharper, with more spikes,
where as the CDEST model tends to be much smoother, even though both use the same blurring during discretization.
Smoothness is generally desirable, the increased smoothness particularly as seen in the saturation and value channels, is intuitively correct.
Though not always.

It can be seen that the CDEST model fails for some multimodal colors -- such as the hue ``greenish'' (\Cref{figgreenish}) where the concave section is filled in;
but succeeds for others such as ``purplish grey'' (\Cref{figpurplishgrey}).
We suggest the reason for this may be difficulties caused by the use of greenish both as a modifier: ``greenish blue'' and as a standalone description covering the edges of the green band.

The horizontal bands in the Baseline models are the result of the add-one smoothing process, they are larger for colors with fewer examples -- such as ``purplish grey''.
In the seminal work of \textcite{NPLM} one of the motivations for employing neural networks in natural language processing was to better handle cases that do not occur in the training data, by sharing information between terms.
While \textcite{NPLM} was looking at language modeling, where the key technique for handling unseen cases was back-off, the case equally applies here for distribution estimation, where unseen cases are handled with add-one smoothing.
The neural model of CDEST can, by knowledge sharing, better estimate the values for the unseen cases in color space.
 


\subsection{Distribution Estimation}
\pgfkeys{/pgf/number format/.cd, fixed relative,precision=4}
\pgfplotstableset{
	col sep=tab,
	header=has colnames,
	ignore chars={"},
	column type={l},
	every head row/.style={before row=\toprule,	after row=\midrule},
	columns={model,resolution,msetopeak,perp,perpstd},
	columns/model/.style={string type},
	columns/resolution/.style={column name=$n$},
	columns/perp/.style={column name=$PP$},
	%columns/perphue/.style={column name=$PP_{H}$},
	%columns/perpsat/.style={column name=$PP_{S}$},
	%columns/perpval/.style={column name=$PP_{V}$},
	columns/perpstd/.style={column name=$\frac{PP}{n^3}$},
	columns/msetopeak/.style={column name=$MSE$}
}

\begin{table*}
	\centering
	%\resizebox{\columnwidth}{!}{
	\pgfplotstabletypeset[
		every row 2 column perpstd/.style={
			postproc cell content/.style={@cell content/.add={$\bf}{$}}}
		]{results/full.tsv}
	%}
	\caption{\label{tblresfull} The results of evaluation on the full Monroe color dataset. Here $n$ is the output resolution of the model in each channel, $PP$ is the perplexity, and $MSE$ is the mean squared error to the peak of the output distribution.}
\end{table*}

The primary task here is the estimation the distribution in color-space for a given color description.
The results are shown in \Cref{tblresfull}.
It can be seen that all models perform similarly.
This confirms that the CDEST model is fitting correctly.
The CDEST model basing on its input sequence of color tokens,
reflects real use of the terms in the test set; 
equally well as the non-compositional Baseline, that counts the exact uses of whole descriptions.
Across all models, the perplexity for the hue channel is much smaller than for the saturation or value channels.
This suggests that in the data there is more consistency in the hue, associated with a color name, than with the 
This aligns with the notion that people describe color primarily with reference to the hue, rather than the shade.
It also aligns with the notion that how \emph{dark} for example ``dark blue'' is, is not a precise quantity.
%By a small margin the best performing model, in terms of standardised perplexity, was the GRU with resolution 256.
The CDEST model performs similarly to similarly to the baseline, when trained on a full set of color terms with all combinations of terms present in the training data.
The key advantage of the CDEST model is its ability to predict a distribution for an unseen combination of colors, this is evaluated using the extrapolation task.

\subsection{Extrapolation}

\begin{table*}
	\centering
	%\resizebox{\columnwidth}{!}{
	\pgfplotstabletypeset[
		every row 0 column 0/.style={
			postproc cell content/.style={@cell content/.add={\it}{}}},
		every row 3 column 0/.style={
			postproc cell content/.style={@cell content/.add={\it}{}}}
		]{results/extrapo.tsv}
	%}
	\caption{\label{tblresextrapo} The results of evaluation on the extrapolation sub-dataset. Here $n$ is the output resolution of the model, $PP$ is the perplexity, and $MSE$ is the mean squared error to the peak of the output distribution.}
\end{table*}


A core motivation of using the CDEST model, over the Baseline, is its ability to learn to combine tokens in a description in ways not seen in training.
The best the baseline model can do on extrapolation is a uniform distribution -- $\frac{PP}{n}=1.0$.
To evaluate how well the model does at predicting these distributions,
we compare a CDEST model trained on the extrapolation sub-dataset, to the models trained on the full dataset.
Both the non-extrapolating, and extrapolating models are evaluated on the same set of rare color descriptions,
but the non-extrapolating models are also shown these rare descriptions during training.
The extrapolating model has never been trained on these combinations of color terms,
and instead must use the knowledge of how those color terms influence the color distribution in the other cases.

The results for this evaluation are shown in \Cref{tblresextrapo}.
It can be seen that the extrapolation is successful, the results on the extrapolation sub-dataset are similar to the overall results for the whole dataset in \Cref{tblresfull}.
The non-extrapolating CDEST results are better than the extrapolation model results.
This is as expected since the non-extrapolating models have training data for the rare color descriptions that occur in the extrapolation test set.

The non-extrapolating CDEST also befitting from the same knowledge sharing that allows extrapolating CDEST model to function at all.
This can be seen from CDEST model out performing the Baseline model.
The baseline model can not benefit from the knowledge sharing based on term use for estimating the curve of the rare descriptions.
This is to the extend that in the high resolution case (256 bin),
the sparsity of training data is such that the extrapolating CDEST model out performs the non-extrapolating Baseline.




\section{Conclusion}\label{sec:conclusion}
We have presented a method for estimating the probably distribution of colors that may be ascribed an input name.
This methods uses a discretization process based on treating each training point as the center of a Gaussian, or wrap-around Gaussian distribution, and finding the probability distribution for discrete regions of the color-space.
The blurring in the discretized training points helps the model's softmax output to learn a reasonable continuous probability distribution, as approximated using a discrete distribution.
Working with probability distributions, rather than regression to a single color-space point on color, allows for better handling of colors with observed distributions that are asymmetric, wide variance or multimodal in the color-space -- most colors.

The model learns the compositional structure of a color name, which it is able to use to predict distributions for colors not seen given during training.
The input terms learn separate representations, which are together used to estimate the distribution.
For example: the color ``dirty brown'' does not occur in the training data, but there are many used of ``dirt'', the suffix ``y'' and ``brown'' in other combinations.
So the CDEST model can estimate a distribution.



\subsection{Future-work}
To enhance the capacity to model the different ways a word can be used in a color description we suggest a parsing step could be added prior to any modeling to tag each token with a linguistic role.
We suggest that this would improve the handling of cases such as ``greenish''.
The two uses of ``greenish'', as a color and as a modifier (e.g. ``greenish blue'') is currently both supported by the same embedding layer representations.
Leaving the output layers to determine the shape of the curve when it is used on its own.
By adding additional role labels by a parsing step these cases could more easily be distinguished and given separate representations.


The discretization process representing a continuous probability distribution as a discrete distribution is pragmatically effective, but unsatisfying.
We suggest there are avenues for advancement here by the extension of \textcite{magdon1998neural} to handle conditional distributions.


\clearpage

\bibliography{master}
\bibliographystyle{ijcnlp2017nourl}

\end{document}
