\documentclass[]{article}

%opening
\title{Learning Distributions of Meant Color}
\author{}
\date{}

\begin{document}

\maketitle

\begin{abstract}
When a speaker says the name of a color, the color they picture is not necessarily the same color the hearer imagines.
Color is a grounded semantic task, but that grounding is not a single value, but rather a range of possible values that could be meant.
To handle this case, we predict a estimated distribution of values, for any given description.
This work further generalises to predict distributions for multi-word descriptions never scene during training.



\end{abstract}

\section{Introduction}
When a person says \emph{tan} they may mean a number of colors: From the bronze of a tanned sunbather, to the brown of tanned leather. When they say \emph{green} they may mean anything from \emph{aquamarine} to \emph{forest green}; and \emph{forest green} itself may mean the shades of a rain-forest, or of a fir-wood, or of a particular branded crayon the speaker had as a child.
Here we will disregard issues of illumination and perceived color based on context, to focus on the core problem of the color of a single patch.

The mapping from color name to color could be considered a regression problem. Solving to find a function that when input some text such as ``forest green'', outputs a numerical value in a color space such as HSV or RGB.
However, regression discards information about the distribution.
If the distributions in color space were mono-modal
\footnote{It should be understood that in this paper, when say \emph{mono-modal} or \emph{multimodal} it is meant in the sense of the number of peaks in the probability distribution; not in the sense of the number of modalities of the data -- e.g. multimodal audio-visual data.}
 and symmetric with consistently small variance, then considering the problem as regression with noise would be adequate.
If the distribution were multi-model (e.g. a mixture model), or non-symmetric (e.g. a truncated distribution) or with varying and wide variances, then regression is loosing valuable information and is unable to produce a model that aligns well with reality.
At the other end from regression, is classification.

A classifier will output probabilities for each of the possible categories an input could belong to.
By dividing color space into thresholded bins, where each bin is its own category.
Then by classifying a color, one gets getting probabilities of it laying in each bin. The output of the classifier defines an empirical distribution in color-space.
Basic classification models consider each category as being distinct and unrelated.
However, we know that if a color name has a high probability of corresponding to a particular point in color space, then it should have a similar probability for other points in that neighbourhood -- this is a notion of continuousness.
A variety of approaches called ordinal regression or ordinal classification exist to handle this case, where there is a order to the categories.
However, there is no natural total ordering of colors.
In in Hue-Saturation-Value (HSV) color space; we think of the hue dimension as being cyclic: \emph{red}, \emph{orange}, \emph{yellow}, \emph{green}, \emph{blue}, \emph{purple}, and back to \emph{red}.
One solution would be to move to a color-space which is orderable in all dimensions: for example Red-Green-Blue (RGB).
However, some of these space do not well align to human perception; or exhibit strong correlation between the values on different dimensions.
Further, to handle non-independence of the dimensions of color space, it is natural to consider the categories as regions of a 2D or 3D space (Much like a 2D histogram) for bins of two or three color space dimensions at one.
Again, there is no ordering on such multiple dimensional points.
So classical ordinal classification methods have limited utility on the problem.

We instead look to helping a normal classifier learn the continuous relationship between adjacent bins by enhancing the training data.
Rather than representing each observation as a one-hot output with the hot cell being the bin it lays in, we represent it as a histogram of values for a continuous probability distribution with the expected value being the original value of the observation.
This can be seen as blurring a one-hot representation.
By adding this blur to all observations, it encourages the classifier to learn at lower level the tied value between the adjunct bins.
For representations of the HSV Saturation and Value dimensions a truncated Gaussian distribution is used for the representation, restricting the value to between 0 and 1.
For the Hue dimension wrapped normal distribution is used \footnote{In implementation, we initially approximated the wrapped normal distribution with a von Mises distribution with support between 0 and 1; however the CDF for this was computationally expensive; so we switched to a truncated Gaussian with the support between -1 and 2, which allowed for very fast implementation by aliasing the memory locations outside the true value's support of 0 to 1. The difference in value is negligible until variance becomes much large than we consider here.}




\section{Highly multimodal colors}
One of the core motivating factors of this work is to be able to handle the color names which have multiple distinct modes.
That is to say there are distinct peaks of the most likely region in color space.
This is not the solo motivation -- there is also the important factors of knowing the confidence of a particular sample of color-space, and understanding the other factors of the distribution, eg spread and more general shape.
As well as the linguistic interest in how the modifier words influence the colours, and the capacity to combine terms in ways never seen in the training data.
However, a core interest here is in colors with a multi-modal distribution; such colors can not be considered as targets for regression as they do not have a symmetric "noise" variation around their mode.






 
\section{Related Work}
\pdfcomment
\subsection{Ordinal Regression}
\subsection{Color Naming}
Color naming is the reverse of the task we investigate in this work.
The color naming task takes a point in color-space as in input, and outputs a probability distribution over possible names for that color.
There a three notable recent works on color-naming.
\cite{mcmahan2015bayesian} presents a full description Bayesian approach, which outputs the probability of a whole description.
\cite{2016arXiv160603821M} presents a per-word LSTM approach, which produces a conditional language model -- sequentially outputting a probability of each word in the description.
\cite{DBLP:journals/corr/KawakamiDRS16} presents a per-character LSTM and Variational Autoencoder approach, which products a conditional character language model -- sequentially outputting a probability of each character in the description.
This work by Kawakami et al, also includes a method for generating colours.

\subsection{Color Generation}
Color generation closely related to the primary task considered here.
The process of going from the name of a color, to an actual color -- a single point in a color space.
\cite{DBLP:journals/corr/KawakamiDRS16} presents a method using RNN, and LSTM, as well as baselines using unigram and bigrams, over characters, to predict a point in \emph{Lab} color space \cite{hunter1958photoelectric}.
Color generation is the single output version of our task of color distribution estimation.

The work we present here, goes beyond just color generation.
As given a color name, we generate probability distributions in color space,
these distributions can be sampled, or the peaks selected, to generate colours.
However, they have further use, as we have the whole distribution.
For example, as a subsystem in image processing, when asked to select the ``dark bluish green'' object, each object can be ranked based on how likely it's color is according to the distribution.
This way if extra information eliminates the most-likely object, the second most likely object can immediately be determined.
Further, as the probability of the color of the object being the color being described by the user input is known, a threshold can be given to report no object found, or to ask for additional confirmation.

Color generation system systems outputting a single color can approximate this by using the distance in color space, from a observed color to the predicted color as a proxy for the probability of the observed color. However, this does not handle asymmetric, or multimodal distributions, nor does it take into account that the range of values reasonable for one color description, often significantly differs in width from that reasonable for another.

\subsection{Image Generation}

Recent works on image generation produce very impressive images based on sentence inputs\cite{reed2016generative,2015arXiv151102793M}.
These end-to-end systems take in a sentence and synthesize a full image.
They have been demonstrated to be able to generate images taking into account the colors described.
Like color generation, this is a single output, or a sampling of outputs rather than distribution.
These works show that it is possible to encode complex information about color based on the wrods being used.


\section{Method}

\section{The Models}

\subsection{GRU Model}

\subsection{Baseline Empirical Model}
For comparison we define an additional model  


\section{Conditional Independence Assumption}
For HSV colors we make the assumption that given the name of the color, then the distribution of the H, S and V components are independent.
That is to say, we assume that if knowing the value of one component would not inform us as to the value of the other if we already know the name of the color.

Superficial checks were carried out the the accuracy of this assumption.
The Spearman's correlation on the training datas suggest that for over three quarters of all color names, there is only weak to zero pairwise correlation (Q3 < 0.20) between the H,S, and V components.
This was the lowest, when compared amongst 16 color spaces RGB, HSV, HSI, HSL, xyY, XYZ, Lab, Luv, LCHab, LCHuv, DIN99, DIN99d, DIN99o, LMS, YIQ, YCbCr.
However, Speakman correlation is underestimated for values that have circular relative value, such as hue.
For the investigation here, we consider the conditional independence assumption sufficient. Better results may be obtained by attempting to output a 3D joint distribution.

\section{Experimental Setup}
\subsection{Data Preparation}
We use the Monroe dataset \parencite{Monroe2010XKCDdataset}, as prepared by McMahan and Stone \parencite{mcmahan2015bayesian}.
This data has minor cleaning, and a standard division into test, development, and train subdatasets. It is also used by \textcite{2016arXiv160603821M} and \textcite{DBLP:journals/corr/KawakamiDRS16}.

The text descriptions are loosely tokenised into separate words and affixes.
Beyond simply breaking up a description ``greenish blue'' into words: ``greenish'', ``blue'', we further separate the suffix as it's own token: ``green'', ``ish'', ``blue''.
We separate the ``-ish'' and the ``-y'' suffixes.
This tokenization is achieved through a short list of word replacement rules.
We also separate `-` into a distinct token. ``blue-green'' becomes ``blue'', ``-'', ``green''.
We do not demarcate the beginning or end of the color description with any for of marker token.

The Monroe dataset has 829 unique color descriptions.
Each description has a varying number of observations, where each observation is a pairing of description and a point in HSV space.
Using the tokenization described above, we break each description into between one and four tokens.
This results in a total of 311 unique tokens.

\subsection{Extrapolation Sub-Dataset}
One of the key advantages of our proposed system is its ability to predict the distribution for never before scene descriptions of colours.
For example, based on the learned understanding of ``bright'', from examples like ``bright green'' and ``bright red'', and of ``salmon'', our system can suggest the distribution in color space of ``bright salmon'', even though that color never occurs in the training data.
To evaluate this, we derive a new dataset from Monroe dataset, which we will call the extrapolation sub-dataset.
To define this, we select the rarest 100 color descriptions,
with the restriction that every token in a selected description must still have at least 8 uses in other descriptions.
The selected examples include multi-token descriptions such as: ``"bright yellow green'' and also some single tokens that occur more commonly as modifiers that as stand alone descriptions: ``pale''.
We restrict the original test and development datasets to contain only observations of these selected colour descriptions.
Conversely, we remove all observations of these color descriptions from the training set.
This produces a dataset suitable for evaluating the capacity of our model to extrapolate the distributions for colour descriptions not seen in training.




\section{Results and Discussion}
\section{Conclusion}

\end{document}
