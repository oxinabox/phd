\documentclass[]{article}

%opening
\title{Learning Distributions of Meant Color}
\author{}
\date{}

\begin{document}

\maketitle

\begin{abstract}
When a speaker says the name of a color, the color they picture is not necessarily the same color the hearer imagines.
Color is a grounded semantic task, but that grounding is not a single value, but rather a range of possible values that could be meant.
To handle this case, we predict a estimated distribution of values, for any given description.
This work further generalises to predict distributions for multi-word descriptions never scene during training.



\end{abstract}

\section{Introduction}
When a person says \emph{tan} they may mean a number of colors: From the bronze of a tanned sunbather, to the brown of tanned leather. When they say \emph{green} they may mean anything from \emph{aquamarine} to \emph{forest green}; and \emph{forest green} itself may mean the shades of a rain-forest, or of a fir-wood, or of a particular branded crayon the speaker had as a child.
Here we will disregard issues of illumination and perceived color based on context, to focus on the core problem of the color of a single patch.

The mapping from color name to color could be considered a regression problem. Solving to find a function that when input some text such as ``forest green'', outputs a numerical value in a color space such as HSV or RGB.
However, regression discards information about the distribution.
If the distributions in color space were mono-modal and symmetric with consistently small variance, then considering the problem as regression with noise would be adequate.
If the distribution were multi-model (e.g. a mixture model), or non-symmetric (e.g. a truncated distribution) or with varying and wide variances, then regression is loosing valuable information and is unable to produce a model that aligns well with reality.
At the other end from regression, is classification.

A classifier will output probabilities for each of the possible categories an input could belong to.
By dividing color space into thresholded bins, where each bin is its own category.
Then by classifying a color, one gets getting probabilities of it laying in each bin. The output of the classifier defines an empirical distribution in color-space.
Basic classification models consider each category as being distinct and unrelated.
However, we know that if a color name has a high probability of corresponding to a particular point in color space, then it should have a similar probability for other points in that neighbourhood -- this is a notion of continuousness.
A variety of approaches called ordinal regression or ordinal classification exist to handle this case, where there is a order to the categories.
However, there is no natural total ordering of colors.
In in Hue-Saturation-Value (HSV) color space; we think of the hue dimension as being cyclic: \emph{red}, \emph{orange}, \emph{yellow}, \emph{green}, \emph{blue}, \emph{purple}, and back to \emph{red}.
One solution would be to move to a color-space which is orderable in all dimensions: for example Red-Green-Blue (RGB).
However, some of these space do not well align to human perception; or exhibit strong correlation between the values on different dimensions.
Further, to handle non-independence of the dimensions of color space, it is natural to consider the categories as regions of a 2D or 3D space (Much like a 2D histogram) for bins of two or three color space dimensions at one.
Again, there is no ordering on such multiple dimensional points.
So classical ordinal classification methods have limited utility on the problem.

We instead look to helping a normal classifier learn the continuous relationship between adjacent bins by enhancing the training data.
Rather than representing each observation as a one-hot output with the hot cell being the bin it lays in, we represent it as a histogram of values for a continuous probability distribution with the expected value being the original value of the observation.
This can be seen as blurring a one-hot representation.
By adding this blur to all observations, it encourages the classifier to learn at lower level the tied value between the adjunct bins.
For representations of the HSV Saturation and Value dimensions a truncated Gaussian distribution is used for the representation, restricting the value to between 0 and 1.
For the Hue dimension wrapped normal distribution is used \footnote{In implementation, we initially approximated the wrapped normal distribution with a von Mises distribution with support between 0 and 1; however the CDF for this was computationally expensive; so we switched to a truncated Gaussian with the support between -1 and 2, which allowed for very fast implementation by aliasing the memory locations outside the true value's support of 0 to 1. The difference in value is negligible until variance becomes much large than we consider here.}




\section{Highly multimodal colors}
One of the core motivating factors of this work is to be able to handle the color names which have multiple distinct modes.
That is to say there are distinct peaks of the most likely region in color space.
This is not the solo motivation -- there is also the important factors of knowing the confidence of a particular sample of color-space, and understanding the other factors of the distribution, eg spread and more general shape.
As well as the linguistic interest in how the modifier words influence the colours, and the capacity to combine terms in ways never seen in the training data.
However, a core interest here is in colors with a multi-modal distribution; such colors can not be considered as targets for regression as they do not have a symmetric "noise" variation around their mode.






 
\section{Related Work}
\subsection{Ordinal Regression}
\subsection{Color Naming}
\subsection{Color Generation}
\subsection{Image Generation}


\section{Method}
\section{Conditional Independence Assumption}
For HSV colors we make the assumption that given the name of the color, then the distribution of the H, S and V components are independent.
That is to say, we assume that if knowing the value of one component would not inform us as to the value of the other if we already know the name of the color.

Superficial checks were carried out the the accuracy of this assumption.
The Spearman's correlation on the training datas suggest that for over three quarters of all color names, there is only weak to zero pairwise correlation (Q3 < 0.20) between the H,S, and V components.
This was the lowest, when compared amongst 16 color spaces RGB, HSV, HSI, HSL, xyY, XYZ, Lab, Luv, LCHab, LCHuv, DIN99, DIN99d, DIN99o, LMS, YIQ, YCbCr.
However, Speakman correlation is underestimated for values that have circular relative value, such as hue.
For the investigation here, we consider the conditional independence assumption sufficient. Better results may be obtained by attempting to output a 3D joint distribution.

\section{Experimental Setup}
\subsection{Data Preparation}
We use the Monroe dataset \parencite{Monroe2010XKCDdataset}, as prepared by McMahan and Stone \parencite{mcmahan2015bayesian}.
This data has minor cleaning, and a standard division into test, development, and train subdatasets. It is also used by \textcite{2016arXiv160603821M} and \textcite{DBLP:journals/corr/KawakamiDRS16}.

The text descriptions are loosely tokenised into separate words and affixes.
Beyond simply breaking up a description ``greenish blue'' into words: ``greenish'', ``blue'', we further separate the suffix as it's own token: ``green'', ``ish'', ``blue''.
This tokenization is achieved through a short list of word replacement rules.
We also separate `-` into a distinct token. ``blue-green'' becomes ``blue'', ``-'', ``green''.
We do not demarcate the beginning or end of the color description with any for of marker token.



\section{Results and Discussion}
\section{Conclusion}

\end{document}
