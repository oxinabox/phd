\documentclass[]{article}

\usepackage{cleveref}

\newcommand{\tcite}{\cite}

%opening
\title{Reproducible Data Loading For\part{title} Repeatable Data Science}
\author{}

\begin{document}

\maketitle

\begin{abstract}
We present a framework DataDeps.jl for the reproducible handling of static datasets to allow for repeatable Data and Computational Sciences.



\end{abstract}

One would expect that data driven computer science research would be easily replicable.
Unlike experimental science where there are many avenues for human error, and for key information to be left out of the methods sections of papers, a computer program runs the same every time.
The vast majority of research in Data Science, and in the Computational Sciences depend on data.
Even in cases where the data is not directly required for the result, it is required for the demonstration that the methods work.

DataDeps.jl is a compact tool with a single job, following the unix philosophy of doing one job well.
It allows code to depend on data, and have that data automatically downloaded as required.
It is not complicated software.
It does not directly advance science, or help to solve classical well defined problems in any field.
What it does do is increase repeatability of any scientific code that uses data.
Decreasing the effort to get such a program working on a new system, while also decreasing the effort to write the code in the first place.



In this paper we will use the word acquisition of data to refer to the downloading of the dataset, extracting it from an archive and any post processing that need to be done to it before the experimental or analytical code is run.
We do not use it to refer to the original collection of data from a primary source, i.e. the creation of the dataset.



Automating the acquisition of the data is in-line with one of the key best practices for Scientific Computing identified by \textcite{10.1371/journal.pbio.1001745}.
This is to ``Let the computer do the work.''.
While it is possible, and indeed common, to include in the readme instructions on how to download the data, it is possible for the computer to do the work.
DataDeps.jl provides a mechanisms by which this is done.





\section{The need to repeatable acquire data}

\subsection{Reproducing researchers' don't have time to manually acquire date}
\tcite{VabdewakkeReproduceableResearch} distinguishes 6 degrees of reproducibility:
\begin{enumerate}
\addtocounter{enumi}{-1} % start at zero
\item The results cannot be reproduced by an independent researcher.
\item The results cannot seem to be reproduced by an independent researcher.
\item The results could be reproduced by an independent researcher, requiring extreme effort. 
\item The results can be reproduced by an independent researcher, requiring considerable effort.
\item The results can be easily reproduced by an independent researcher with at most 15 minutes of user effort, requiring some proprietary source packages (MATLAB, etc.).
\item The results can be easily reproduced by an independent researcher with at most 15 min of user effort, requiring only standard, freely available tools (C compiler, etc.).
\end{enumerate}
Note Vandewalle et. al. say 15 minutes of user effort, so excluding the time taken for the running of the simulations etc.
Vandewalle's 15 minutes still is pretty harsh, but it is not unreasonable.
Consider a busy reviewer who has spent several hours reviewing a paper.
They don't want to spend several more trying to get the code working.
But if it is just 15 minutes, its hard to justify not doing so.

Consider a graduate student new to the area and the tools involved.
Given their unfamiliarity, we can expect everything to take 4 times as long.
At 15 minutes, that is still just an hour.
But if it takes someone who is familiar with the field over an hour to get things running, then that is the best part of a day.


Let us consider how this time may be spent:
\begin{itemize}
	\item 1 minute on finding and downloading the code for the paper.
	\item 5 minutes on reading the read-me, and working out how to run the script,
	\item 5 minutes on finding, downloading the data, extracting it, putting it on the right location on disk. (Being generous and not including the download times for large datasets).
	\item 3 minutes are spent quickly glancing over the code to make sure it vaguely does what the paper says.
	\item 3 minutes are spent interpreting and checking the results
\end{itemize}
We are already several minutes over budget, and that is assuming nothing went wrong.
That we had the same assumptions as the author about ``standard, freely available tools'', and didn't have to install anything,
and that the read-me, the code, and the format of the results were quiet understandable.

What is happening during those 5 minutes of dealing with the data.
They are looking in the readme for where to download the data.
Going to that website, downloading it, then maybe transferring it from their workstation to the faster server.
They are trying to remember the flags to the \texttt{tar} command.
They are trying to determine if the code wants the path to the folder, to its parent folder, or to a file within it.


To use a phrase from Agile software engineering practices, Vanderalle's 15 minutes can not be achieved without \emph{pervasive automation}.

\subsection{The original researchers don't have time to reaquire data}
It often occurs that we want to rerun experiments in a new environment.
If the data was manually set-up onto a computer, then those steps must be repeated on every new machine.
If for example we are running our simulations on a cloud computing environment,
and unexpected (or even expected) maintenance occurs, and we want to run some move variations on our local workstation -- it won't be as fast as the original environment, but it can be made to run now.

Another example:
A paper was submitted, and the reviewer feedback comes back 3 months later.
They want a few more experiments run.
Unfortunately, in the intervening time, you've replaced your PC.
None of the data is where you left it.
So now to satisfy those reviewers you have to go and repeat the work of downloading the data,
and setting it up again.


\section{Assumptions About Data}
We make the following assumptions about data used in research.
They do not hold universally, but we target the cases where they do.
Which we suggest are the majority of cases.


\begin{enumerate}
	\item Data is publicly available.
	\item Data is static.
	\item Data is file-based
\end{enumerate}

\subsection{Data is publicly available}
Once researchers held their data secret and close, and to get it you had to contact them personally and directly.
That is not the case today.
Universities, publishers and conferences have open-data policies forcing (or at least strongly encouraging) data to be made public.
Papers should report results based on standard datasets other people have also used.
Writing a paper where the primary contribution is a new dataset for others to use is a real thing.


\subsection{Data is static}
A new version of a dataset is considered a new dataset.
This makes sense, since results reported on the old version won't precisely repeat on the new version.

For prototypes and models that are being developed along side the creation of a new dataset this won't always hold.
However, by the time the paper about the model and the dataset is submitted the dataset will now be static.

\subsection{Data is file-based}
Not all data is file based, this assumption is the one that is most-likely to fail.
For example data might be accessible only via an API call to some large privately held database.
Or the data might be too large to fit on a traditional file system.

In the case of data that is only available via API, many of the data management concerns do not apply (different concerns do instead).
In the case of data tool large to fit on a traditional file-system we simply do not handle it.

The majority of academic research however is not on such true ``Big Data''.
but rather on small data, or moderately large data.


\section{Issues Researcher's have with data}
\begin{enumerate}
	\item Where do I put it? \label{itm:where}
	\begin{itemize}
		\item Should it be on the local disk? but that is small
		\item Should it be on the network file-store? but that is slow
		\item If I move it, I'm going to have to reconfigure things
	\end{itemize}
	\item Is it ok to include it with my work? \item{itm:ownredistribute}
	\begin{itemize}
		\item I don't own copyright on this data.
		\item Am I allowed to redistribute this data?
		\item What if people getting the data from me as part of my experiments don't realize the original creator?
	\end{itemize}
	\item Should I hard-code the path, or make it an argument to my script? \label{itm:path}
	\begin{itemize}
		\item If I make it an argument to my script, then running my code is annoying as I have to remember where it is and enter that.
		\item If I hard code it, then I have to edit my script when I move it, and anyone running the script will have to know where to put it, or edit the script too.
	\end{itemize}
\end{enumerate}


\subsection{Where do I put it}
We resolve the issue of where to put data, by allowing the data to be put any a number of locations, and then checking if it is in any of them.
The default locations include locations with-in the users home directory, and several locations commonly used for local, and network file stores.
Further locations can be configured per machine.
Checking multiple locations is trivial in the time taken, thus it is worth doing to save the user effort of working out where the data is.

The actual decision as to where to put the data is made automatically by DataDeps.jl, using the first existent location in the list of locations it checks.
The user can then move the data to any other location later if that would be more convenient.
Allowing users to move data without concerns, about updating code removes a level of complexity from any data management problem.


\subsection{Is it ok to include it with my work?}
It is a common worry that by including a dataset may be violating the original creators legal or moral rights.
This worry is indeed not unjustified many datasets do not have clear licensing statements,
or even have licensing statements forbidding redistribution.
Use or Redistribution may require the display of a notice about the data's origin.
Even when it does not there is the further worry that someone acquiring the dataset may not realise the original author, which would be failing to give due credit.


DataDeps.jl solves this in two ways.
Firstly, it avoids the need to redistribute data.
DataDeps.jl fetches the data from the original source, just like an user following the readme would.
Secondly it allows (and indeed requires), that when the data dependency is declared it include a message which is then displayed to the user when the data is retrieved.
This message is intended to allow the display of such copyright information and terms of use -- the user is able to refused after the message is displayed, in which case the data is not downloaded.

\subsection{Should I hard-code the path, or make it an argument to my script?}
Write about using evaluation order on optional arguments


\section{Examples of need}

\subsection{WordNet.jl}
To quote the WordNet.jl package \footnote{\url{https://github.com/JuliaText/WordNet.jl#wordnet-data}} ``I don't include the data in this repository. Ignoring copyright concerns, I dislike big chunks of data in my .julia/vX.X directory. Something about it worries me. To use this library, you must download and install version 3.0 of WordNet. Download it here. Then, decompress it. The resulting directory is the one to use when constructing a Worknet.DB type.''.


The first point is that they do not include the data in the repository.
This is good practice, as git does not handle static data storage well.
However, as the repository is what is redistributed to users when they install the package,
including the data in the repository is the obvious way to make sure the users acquire it.


We see the author has has copyright concerns (but is putting them aside for the discussion), even though license to use and redistribute the data for any purposes is expressly granted\footnote{\url{https://wordnet.princeton.edu/wordnet/license/}}.

They are also concerned about having the data in the .julia/vX.X directory.
If the data was included in the repository that would be where it ends up.
The default location for that directory is in the users home directory.
This is often subject to size quotas, and is often a networked directory.
This is an understandable concern.

The paragraph concludes with instruction on how to acquire the data.
This instructions must be followed manually and require the user to understand how to extract the data.
They are not unambiguous.
The linked file is a gzipped tarball an an unfamiliar user might only extract the gzip part, and be left with a tarball.
However, a more familiar user would know one has to both unzip the archive then untar the tarball (and indeed would know the single command to do both as one step).
Further to this, even after extracting it is unclear as to if the folder to be used is \texttt{WNdb-3.0} (which may not exist depending on the method used to extract), or its sole contents: the directory \texttt{dict}.
It is in fact the former.
This manual labour, and opportunities for misunderstanding is what this software package aims to remove.
















\end{document}
