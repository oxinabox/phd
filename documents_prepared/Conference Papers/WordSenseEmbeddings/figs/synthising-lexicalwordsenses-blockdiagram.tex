\documentclass{standalone}

\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{graphs} 
%\usegdlibrary{layered}
\usetikzlibrary{graphs,graphdrawing}
\usegdlibrary{force, layered, trees}
\usetikzlibrary{decorations.pathmorphing}
\begin{document}

\begin{tikzpicture}[align=center, 	decoration={bent,aspect=.9, amplitude=-2},
	note/.style= {blue, 
						  font=\tiny\itshape
	},
	notepoint/.style= {->,
						  blue, 
						  dashed,
						  decorate, shorten <= -32pt
	},
]

%AlignText/"" [draw];

\graph[ layered layout, sibling distance=7mm, layer sep = 5mm]{
 %grow down, branch right sep]{ %
	
	Context/"Context \\{\small (sentence to be disambiguated)}";

%
	{Lemma, POS} -> Synsets -> Glosses -> Align[draw];
	Word -> "Unsupervised \\Word Sense Embeddings" -> Align;
	Align -> Lexically Aligned Word Sense Embeddings -> WSD[draw];	
	Context -> WSD -> "Word Sense\\{\small (The synset with the meaning of this word in this context)}";

{ [same layer] Lemma, POS, Word, Context};
};
\node[note, left = 0.5 of WSD] (WSDText) { Use language model to find \\ the probability of the context \\ given the input of each\\ lexically aligned word sense embedding\\ Return the one that gives highest probability of context};
 \node[note, left = 0.5 of Align] (AlignText) {Use language model to find \\ the probability of each Gloss \\ given each embeddings. \\ Use this to find a new embeddings, \\ as a weighted sum of each unsupervised embedding \\ proportional how likely the gloss was to occur \\ when it is used input to the language model probability};

\draw[notepoint] (AlignText) -> (Align);
\draw[notepoint] (WSDText) -> (WSD);

\end{tikzpicture}


\end{document}