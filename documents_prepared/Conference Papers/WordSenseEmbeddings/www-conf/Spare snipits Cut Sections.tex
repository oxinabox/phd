Many word embedding techniques train their word vectors, in a unsupervised fashion, using the task of training a language model \parencite{NPLM, collobert2008unified, mikolov2013efficient}.
For most applications, this language model is discarded, and only the trained word embeddings are kept and utilised.
This seems wasteful, as the language model itself contains useful information. In particular we


Here, the language models which when given a word sense embedding output the probability of the words that may occur near it are particularly useful. It has been shown that a simple application of Bayes' theorem can be used to move from the language model's context likelihood, to the posterior probability of a particular word sense \cite{tian2014probabilistic,AdaGrams}. We go beyond this to show that it can then be used to generate new sense embeddings, without further training, and show how they can be used for several benchmark applications.



It is very quick to refit trained senses to new labelled meanings -- but compratively slow to train a word sense embedding model from scratch. We propose that very high-quality unsupervised word senses could be trained over huge amounts of training data taking weeks or even months. Then in only a few hours, they could be refitted to match a new sense inventory.  This is a contrast to word sense embedding systems that incorporate the sense inventory information into their training \parencite{Chen2014, iacobacci2015sensembed} for which matching to a new inventory task requires at least a additional full pass over the training data.
Being able to be rapidly refitted to a new sense inventory would allow the model to be trained just once on a source language, then any number of currently published translators dictionaries could be directly employed, to allow the model to target suitable WSD for use in any machine translation system. The prototype system we present in this paper does not have the accuracy WSD for immediate utilisation in such a task; but we suggest that future work building upon this has potential for great utility in machine translation systems.