\documentclass{sig-alternate}


\usepackage[author={Lyndon White}]{pdfcomment}



\newcommand{\W}{\mathcal{W}}
\newcommand{\ci}{\perp\!\!\!\perp} % from Wikipedia

\begin{document}
\title{Aligning Word Sense Embeddings, Using a Single Example, with Applications in Word Sense Disambiguation}
\maketitle


There have been many recent works focused on inducing representations for different word senses based on context; particularly using neural network vector embeddings.
However these induced representations do not normally line up with the standard human created definitions of any given word sense.
It can certainly be argued that the induced senses may capture better senses, more comprehensively, or with better granularity than human lexicographers do.
However, without links from the induced senses to lexical sense, many applications are limited; and such systems which induce word senses can not be integrated with systems that utilize the wealth of existing lexical knowledge.

We thus propose a method for quickly aligning the indicted senses, to correspond with the sense in a single example, so as to allow the shift from an representation that makes sense only in the context of other representations of that form, to a representation corresponding to some gold standard label.


Our method generated new lexically vectors from the standard vectors as a weighted sum of the unsupervised vectors, with the weighting determined by how likely they (the unsupervised) are to be the correct (unsupervised) sense intended in a example sentence containing the gold standard lexical sentence.

Aligning senses this way also side steps two problems of induced word sense representations: Discarding un-useful representations -- that may have never been trained, and merging overly similar representations. Un-useful representations will never having a high probability for any real context where the target word occurs -- otherwise they would be useful, so will not contribute to the aligned vector. Overly similar representations will  predict similar contexts, and will contribute similar vectors shifts -- it will be roughly equivalent to as if a single vectors was twice as likely.


The method we propose is applicable for the synthesising of new lexically aligned word sense vectors from unsupervised induced word sense vectors, which come from a skip-gram language model such or similar.



We demonstrate this method on AdaGrams\cite{AdaGrams}, and on our own simpler greedy multiple word-sense embeddings. The method is generally applicable to any skip-gram-like language model that can take multiple vectors as input, and can output the probability of a word appearing in their context.

\section{Framework}


\subsection{Sense Language Model}
A traditional language model defined the probability of a word, given its context -- such as the words preceding it.
With skip-gram style language models, the context is abstracted into the vector input -- representing the information from all contexts of a word. This is generalised to have multiple different senses per word, for a sense language model.
It takes a representation of a word sense as an in input, and outputs the probability of any word occurring in the context of the that word sense. In the cases we consider that word sense representation is a vector, and the language model is something similar to a skip-gram. Do note the asymmetry, in the input and output space: while the input is a vector representing a word-sense, the output is the probability of just a word -- no difference is made between different senses occurring in the context. Given a particular word-sense the language model gives us the probability of the word occurring in the context of that word sense. Stated formally, for a word from the space of words $w\in \W$, and for a sense representation $s$, the language model gives $$P(w | s)$$.

\subsection{A General WSD method}
Using the language model, we define a general word-sense disambiguation method, that will be used for serval parts of our process.
Taking some collection of word sense representations, we aim to use the context to determine which is the most suitable for this use.
We will call the word we are trying to disambiguate the \emph{target word}.
Let $\mathbf{s}=(s_{1},...,s_{m})$, be the collection of possible word sense representations for target word, they may be induced senses, or lexical senses.
Let $\mathbf{c}=(w_{1},...,w_{n})$ be a sequence of word from around the target word -- that is to say it's context.
For example for the target word \emph{kid}, the context could be \mbox{$\mathbf{c}=(wow,\; the,\; wool,\; from,\; the,\; is,\; so,\; soft,\; and,\; fluffy)$}, where \emph{kid} is the centeral word taken from between \emph{the} and \emph{fluffy}.
Ideally our contexts would be symmetric with similar window size to that used for training the language model, though as shall be discussed below this is not always possible.
 
For any particular sense, $s_i$, we calculate the probability of the context with the language model: \[P(\mathbf{c}|s_{i})=\prod_{\forall w_{j}\in\mathbf{c}}P(w_{j}|s_{i})\]
Here, we make the assumption that given any sense representation $s_j$, the probability of each word in it's context is conditionally independent. Stated formally we assume $\forall a,b \in [1,n],\; a \ne b\; \wedge \forall s_i \in \mathbf{s},\:w_a \perp w_b \mid s_i$.
The correctness of this assumption depends on the quality of the representation -- the ideal sense representation would fully capture all information about the contexts it can appear in -- that making elements of those contexts not present any additional information, thus making $P(w_a \mid w_b,s_i)=P(w_a \mid s_i)$ i.e. conditionally independent.

From this, by applying Baye's Theorem, we can calculate the likelyhood function, given a prior for $P(s_i)$.

\[P(s_{i}|\mathbf{c}) = \dfrac{P(\mathbf{c}|s_{j})P(s_{j})}{\sum_{s_{j}\in\mathbf{s}}P(s_{j}|\mathbf{c})P(s_{j})}\].


\subsection Synthesising New Embeddings from a Single Example

Given a collection of induced embeddings 


\subsubsection Fallback for dictionary phrases (colocations)
Unless a specialised tokenizing method is used, a word embedding method will not learn embedding for colocations such as ``civil war'' or ``martial arts''. Normal tokenizers will split them at the word level, learning embeddings for ``civil'', and ``war'', and for ``martial'' and ``arts''. This issue is often considered minimal for word embeddings, as a approximate embedding can be constructed by summing embeddings for each word  in the phrase.

It has been constantly noted that for single-sense word embeddings the summing the vectors for each word in the phrase results in reasonable representation for the phrase  \cite{mikolovSkip} \pdfcomment{Doesn't Rui Wang have a paper than goes into some detail on this? I can't find it}. The intuition from this is that for multiple sense-word embeddings, there is a correct selection of senses for each word such that the sum of these senses will be a representation for the phrase.

Simply evaluating the each combination of word-sense sums for multiple word colocations is computationally expensive. For a $m$ length colocation, where each word has $n$ senses, this requires $m^n$ evaluations of probability of the context ($P(\mathbb{c}\mid u_i)$). When inducing unsupervised word embeddings it is not unreasonable to have $n=30$ sense, so for a 2 word phrase (eg ``civil war'') this is 900 evaluations.
Instead, we we just add the additional sense embeddings for each word to the total pool of sense embeddings to be combined ($\mathbf{u}$ above), requiring instead $m \times n$ evaluations. So for our 2 word example, this is 60 evaluations, instead of the 30 evaluations that is require for a single word entry. This also has other advantages.


In the case that the word-sense induced for one word 

For example, ``civil war''. The context that ``war'' in ``civil war'' occurs, is very similar to the context that ``war'' appears in general -- for both ``war'' and ``civil war'' we expect the context to include words like ``casualties'', ``miltia'' etc, thus we would not expect a our word sense induction method to produce a specific sense of ``war'' for this context.
Compare ``civil'': ``civil'' as in ``civil war'', has very different expected contexts to ``civil'' as in ``civil servant'' or ``civil behaviour''. In this case we would expect 3 different senses of ``civil'' to be induced -- one of which does contain context information for ``civil war'' -- information not captured by the single representation of ``war''.

The extreme version of this is if one or more words in a multiword expression have no embedding defined at all. In this case we fall back to only using senses from words which do have embeddings. An example of this would be ``Fulton County Court'', while ``County'' and ``Court'' are common words, that are certain to occur in any reasonable training corpus; ``Fulton'' is a rare proper noun -- if it occurs at all it likely does not occur enough to train the representation. We use the remaining words: ``County'' and ``Court'' to determine the meaning of the whole. This is not always going to be suitable, but it is the best that can be done.



\end{document}