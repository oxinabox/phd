\documentclass{sig-alternate}

\newcommand{\W}{\mathcal{W}}

\begin{document}
\title{Aligning Word Sense Embeddings, Using a Single Example, with Applications in Word Sense Disambiguation}
\maketitle


There have been many recent works focused on inducing representations for different word senses based on context; particularly using neural network vector embeddings.
However these induced representations do not normally line up with the standard human created definitions of any given word sense.
It can certainly be argued that the induced senses may capture better senses, more comprehensively, or with better granularity than human lexicographers do.
However, without links from the induced senses to lexical sense, many applications are limited; and such systems which induce word senses can not be integrated with systems that utilize the wealth of existing lexical knowledge.

We thus propose a method for quickly aligning the indicted senses, to correspond with the sense in a single example, so as to allow the shift from an representation that makes sense only in the context of other representations of that form, to a representation corresponding to some gold standard label.

The method we propose is applicable for the synthesising of new lexically aligned word sense vectors from unsupervised induced word sense vectors, which come from a language model such a skip-gram.



We demonstrate this method on AdaGrams\cite{AdaGrams}, and on our own simpler greedy multiple word-sense embeddings. The method is generally applicable to any skip-gram-like language model that can take multiple vectors as input, and can output the probability of a word appearing in their context.

\subsection{Sense Language Model}
A traditional language model defined the probability of a word, given its context -- such as the words preceding it.
With skip-gram style language models, the context is abstracted into the vector input -- repressenting the information from all contexts of a word. This is generalised to have multiple different senses per word, for a sense language model.
It takes a representation of a word sense as an in input, and outputs the probability of any word occurring in the context of the that word sense. In the cases we consider that word sense representation is a vector, and the language model is something similar to a skip-gram. Do note the asymmetry, in the input and output space: while the input is a vector representing a word-sense, the output is the probability of just a word -- no difference is made between different senses occurring in the context. Stated formally, for a word from the space of words $w\in \W$, and for a sense representation $s$











\end{document}