\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[default]{raleway}
\usepackage{microtype}
%\renewcommand\familydefault{qag}%{lmss}
%\linespread{1.3}
\usepackage[top=2cm,bottom=2cm]{geometry}

\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{calc}

\edef\restoreparindent{\parindent=\the\parindent\relax}
\usepackage{parskip}
\parskip=0.3\parskip
\restoreparindent
	
	
\usepackage[backend=bibtex,
style=alphabetic,
%bibencoding=ascii,
maxcitenames=99,
url=true,
hyperref=true
]{biblatex}
\bibliography{master}
	
\usepackage{hyperref}

\title{Proposal for a method for word sense embedding, induction and disambiguation, based on breaking non-elastic objects by applying radial forces}
\author{Lyndon White}
\begin{document}

\maketitle

\section{Introduction}

The base notion comes from Firth's principle: ``You shall know a word by the company it keeps.''. From which it is drawn that a words meaning is defined by its context when considered over very large corpora of text; covering all it's usages. This logically infers that you shall also recognise various word senses by their differing companies.
Skipgrams and other word embeddings learn through language modelling tasks capture Firths principle into a vector. 
However, due to the multiple senses of a polysemous words, there is expected to be contention in the ideal placement of this vector -- different word senses keep different companies.
I propose that this contention should be enough to ``break'' or split a word embedding.


\newcommand{\summaryfromcite}[1]{
	\subsubsection{\Citetitle{#1}{title} -- \citeauthor{#1} (\citeyear{#1})}
	\fullcite{#1}
	\smallskip \\
	\citefield{#1}{annotation}
}

\section{Word sense representation (WSR)}
Word sense representation (WSR) is the process by which a variety of repressentations, for different work senses is generated.
Here we use the term to refer to the supervised case -- that the word senses are labelled in what ever training data is used.
We are concerned with vector representations.
 
\summaryfromcite{iacobacci2015sensembed}

\section{Word sense induction (WSI)}
Word sense induction (WSI) is (for purposes of this discussion) the process of using unsupervised data to discover, (and implect in that task represent) word senses.
We can see this as similar to an unsupervised analogue to WSR.

\summaryfromcite{Chen2014}

\summaryfromcite{AdaGrams}

I have AdaGrams running. Running on default settings did not yield a variety of senses from my inspection. I have rerun it using the settings used in the paper now, much better.

\summaryfromcite{pantel2002WSI}


\section{Word sense disambiguation (WSD)}
Word sense disambiguation (WSD) is the process to assign a word sense, to an instance of a word. We use this term primarily in the context of WSD to a sense from a standard sense inventory. Those it is also used 

\summaryfromcite{veronis2004hyperlex}

\section{Sense alignment -- going from WSI to WSD}
Here we look at methods that allow induced word senses (from WSI), to be used on WSD tasks.
A WSD task, requires determining which of the standard word-senses a particular example of a word in its context belongs to.
These standard word senses, are from some dictionary (or Sense Inventory) created by lexicographers, such as WordNet, BabelNet.

\summaryfromcite{agirre2006}


\subsection{see also:}
\summaryfromcite{SemEval2007WSIandWSD}



\end{document}