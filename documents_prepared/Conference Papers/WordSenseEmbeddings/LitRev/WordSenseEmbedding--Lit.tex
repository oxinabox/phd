\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[default]{raleway}
\usepackage{microtype}
%\renewcommand\familydefault{qag}%{lmss}
%\linespread{1.3}
\usepackage[top=2cm,bottom=2cm]{geometry}

\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{calc}

\edef\restoreparindent{\parindent=\the\parindent\relax}
\usepackage{parskip}
\parskip=0.3\parskip
\restoreparindent
	
	
\usepackage[backend=bibtex,
style=alphabetic,
bibencoding=ascii,
maxcitenames=99,
url=true,
hyperref=true
]{biblatex}
\bibliography{master}
	
\usepackage{hyperref}

\title{Proposal for a method for word sense embedding, induction and disambiguation, based on breaking non-elastic objects by applying radial forces}
\author{Lyndon White}
\begin{document}

\maketitle

\section{Introduction}

The base notion comes from Firth's principle: ``You shall know a word by the company it keeps.''. From which it is drawn that a words meaning is defined by its context when considered over very large corpora of text; covering all it's usages. This logically infers that you shall also recognise various word senses by their differing companies.
Skipgrams and other word embeddings learn through language modelling tasks capture Firths principle into a vector. 
However, due to the multiple senses of a polysemous words, there is expected to be contention in the ideal placement of this vector -- different word senses keep different companies.
I propose that this contention should be enough to ``break'' or split a word embedding.


\newcommand{\summaryfromcite}[1]{
	\subsection{\citefield{#1}{title}}
	\fullcite{#1}
	\smallskip \\
	\citefield{#1}{annotation}
}

\section{Word sense representation (WSR)}
Word sense representation (WSR) is the process by which a variety of repressentations, for different work senses is generated.
Here we use the term to refer to the supervised case -- that the word senses are labelled in what ever training data is used.
We are concerned with vector representations.
 
\summaryfromcite{iacobacci2015sensembed}

\section{Word sense induction (WSI)}
Word sense induction (WSI) is (for purposes of this discussion) the process of using unsupervised data to discover, (and implect in that task represent) word senses.
We use this term as the unsupervised analogue to WSR.

\summaryfromcite{Chen2014}

\summaryfromcite{AdaGrams}

\section{Word sense disambiguation (WSD)}
Word sense disambiguation (WSD) is the process

\section{Sense alignment -- going from WSI to WSD}
Here we look at methods that allow induced word senses (from WSI), to be used on WSD tasks.
A WSD task, requires determining which of the ``standard'' word-senses a particular example of a word in its context belongs to.
These ``standard'' word senses, are from some dictionary (or Sense Inventory) created by lexicographers, such as WordNet, BabelNet.

\summaryfromcite{agirre2006}

see also:
\summaryfromcite{SemEval2007WSIandWSD}



\end{document}