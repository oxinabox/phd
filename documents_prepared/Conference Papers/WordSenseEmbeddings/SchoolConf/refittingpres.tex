\documentclass[dvipsnames]{beamer}
%\usepackage[dvipsnames]{xcolor}
\usepackage{verbatim}
\usepackage[subpreambles=false]{standalone}
\usepackage{microtype}
\usepackage{adjustbox}


\usepackage{tikz}

\input{../figs/block-diagram-bits.tex}

\input{brownbeamer}
\bibliography{master.bib}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\inputcolumn}[1]{%
	\begin{column}{0.5\textwidth}
		\begin{adjustbox}{max width=\columnwidth}
			\input{#1}
		\end{adjustbox}
	\end{column}%
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\W}{\mathcal{W}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\s}{\mathbf{s}}
\renewcommand{\l}{\mathbf{l}}
\renewcommand{\u}{\mathbf{u}}
\newcommand{\ci}{\perp\!\!\!\perp} % from Wikipedia
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\ubraceword}[2]{\underbrace{\mathtt{\alert{#1}}}_{#2}\:}
\newcommand{\obraceword}[2]{\overbrace{\mathtt{\alert{#1}}}^{#2}\:}

%%%%%%%%%%%%%%%%%%%%%


\author{\textbf{Lyndon White},\\ Roberto Togneri, Wei Liu, Mohammed Bennamoun}
\institute{School of Electical, Electronic and Computer Engineering\\The University of Western Australia}
\title{Finding Word Sense Embeddings of Known Meaning}

\subtitle{A method for refitting word sense embeddings,  using a single example, by application of Bayes' theorem to the language model}
\date{}
%\logo{\hfill\includegraphics[scale=0.12]{uwa}\hfill\hspace{1.5cm}\vspace{0.5cm}}
\begin{document}
\centering %Center everywhere
\frame{\maketitle}

\begin{frame}{Word embeddings represent each word as a single vector}

\end{frame}

\begin{frame}{Word sense embeddings represent each word as a multiple vectors}
	
\end{frame}

\begin{frame}{Many sense embeddings don't know what the senses mean}
	
\end{frame}

\begin{frame}{We will solve this by \emph{refitting} them to be for the sense we mean}
	\begin{columns}[T]
		\begin{column}{0.5\textwidth}
			\begin{itemize}
				\item<1-> Take in the \alert{word}, the \alert{context}, and the \alert{pretrained senses}
				\item<2-> Output a \alert{new sense embedding} that is for the meaning present in \alert{that sentence}
			\end{itemize} 
		\end{column}
		\inputcolumn{../figs/refitting.tex}

	\end{columns}
\end{frame}


\newcommand{\sentexample}{
	\[
	\overbrace{
		\obraceword{wow}{w_1}
		\obraceword{the} {w_2}
		\obraceword{wool} {w_3}
		\obraceword{from} {w_4}
		\obraceword{the} {w_5}
		\ubraceword{kid} {target\: word}
		\obraceword{is} {w_6}
		\obraceword{so}{w_7}
		\obraceword{soft}{w_8}
		\obraceword{and} {w_9}
		\obraceword{fluffy}{w_{10}}
	}^{\c}
	\]
}

\begin{frame}{Refitting uses a probability weighted sum}
	\vspace{-1em}
	\begin{columns}[T]
	\begin{column}{0.5\textwidth}

		\begin{equation*} \label{eq:synth}
			l(\u \mid \c ) = \sum_{\forall u_i \in \u} u_i P(u_i \mid \c)
		\end{equation*}
		

		Unsupervised Senses Vectors: $\u = \{u_1,...u_{n_u}\}$ 
		
		\vspace{1em}
		
		Example Sentence: $\c = \{w_1,...w_{n_c}\}$
		 
	\end{column}
	
	\inputcolumn{../figs/refitting.tex}

\end{columns}
\vspace{0em}
\sentexample
\end{frame}

\begin{frame}{The probabilities are found using Bayes' theorem}
	\vspace{-1em}
	\begin{columns}[T]
		\begin{column}{0.5\textwidth}
			
			\onslide<1->{Language model: $P(w_i \mid s_{i})$}
			\vspace{1em}
			\onslide<2->{
				Conditional Independence:
				\begin{equation*} \label{eq:contextprobtrue}
				P(\c \mid s_{i})=\prod_{\forall w_{j}\in\c}P(w_{j} \mid s_{i})
				\end{equation*}
			}
			\vspace{1em}
			\onslide<3->{
				Bayes Theorem:
				\[ \label{eq:generalwsd}
				P(s_{i} \mid \c) =
				\dfrac{P_S(\c \mid s_{i})P(s_{i})}
				{\sum_{s_{j}\in\s} P_S(s_{j} \mid \c)P(s_{j})}
				\] 
			}
			
		\end{column}
		\inputcolumn{../figs/refitting.tex}
	\end{columns}
	\vspace{0em}
	\sentexample
\end{frame}

\begin{frame}{Use for word similarity with context}
	
\end{frame}

\begin{frame}{Results on word similarity with context}
	
\end{frame}

\begin{frame}{Use for word sense disambiguation }
	
\end{frame}

\begin{frame}{Results for word sense disambiguation}
	
\end{frame}

\begin{frame}{What went wrong? The posterior distribution was too sharp}
	
\end{frame}

\begin{frame}{Geometric Smoothing -- Replace products of likelihoods, with the geometric mean of likelihoods}
	
\end{frame}


\begin{frame}{Improved results on word similarity with context}
	
\end{frame}


\begin{frame}{Improved results  for word sense disambiguation}
	
\end{frame}


\begin{frame}{A method for refitting sense embeddings}
	
\end{frame}

	
\end{document}
