\documentclass[dvipsnames]{beamer}
\usepackage{verbatim}

\usepackage{microtype}
\usepackage{adjustbox}
\usepackage{amsmath}

\usepackage[subpreambles=false]{standalone}
\usepackage{tikz}

\newlength\xunit


\input{brownbeamer}
\usepackage[color]{../figs/blockdiagrambits}
\tikzset{
	backgroundcolor/.style ={fill=chamois}	
}



\bibliography{master.bib}



\usepackage{booktabs, array} % Generates table from .csv
\usepackage{pgfplots, pgfplotstable}
\pgfplotsset{
	compat=1.12,
	/pgfplots/table/search path={.,..,../data}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\inputcolumn}[1]{%
	\begin{column}{0.5\textwidth}
		\begin{adjustbox}{max width=\columnwidth}
			\input{#1}
		\end{adjustbox}
	\end{column}%
}


\newcommand{\fitmath}[1]{
	\begin{adjustbox}{max width = \textwidth}	
		$\displaystyle
		#1
		$
	\end{adjustbox}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\W}{\mathcal{W}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\s}{\mathbf{s}}
\renewcommand{\l}{\mathbf{l}}
\renewcommand{\u}{\mathbf{u}}
\newcommand{\ci}{\perp\!\!\!\perp} % from Wikipedia
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\ubraceword}[2]{\underbrace{\mathtt{\alert{#1}}}_{#2}\:}
\newcommand{\obraceword}[2]{\overbrace{\mathtt{\alert{#1}}}^{#2}\:}

%%%%%%%%%%%%%%%%%%%%%


\author{\textbf{Lyndon White},\\ Roberto Togneri, Wei Liu, Mohammed Bennamoun}
\institute{School of Electical, Electronic and Computer Engineering\\The University of Western Australia}
\title{Finding Word Sense Embeddings of Known Meaning}

\subtitle{A method for refitting word sense embeddings,  using a single example, by application of Bayes' theorem to the language model}
\date{}
%\logo{\hfill\includegraphics[scale=0.12]{uwa}\hfill\hspace{1.5cm}\vspace{0.5cm}}
\begin{document}
\newcommand{\sentexample}{\fitmath{
		\overbrace{
			\obraceword{wow}{w_1}
			\obraceword{the} {w_2}
			\obraceword{wool} {w_3}
			\obraceword{from} {w_4}
			\obraceword{the} {w_5}
			\ubraceword{kid} {target\: word}
			\obraceword{is} {w_6}
			\obraceword{so}{w_7}
			\obraceword{soft}{w_8}
			\obraceword{and} {w_9}
			\obraceword{fluffy}{w_{10}}
		}^{\c}
	}}
	

%\centering %Center everywhere
\frame{\maketitle}




\begin{frame}{Words don't only have one meaning}
	
	\vspace{1em}
	\alert{Kid \emph{(Noun)}} \footfullcite{miller1995wordnet}
	\begin{enumerate}
		\item (a young person of either sex) "she writes books for children"; "they're just kids"; "`tiddler' is a British term for youngster"
		\item (English dramatist (1558-1594)) 
		\item (a human offspring (son or daughter) of any age) "they had three children"; "they were able to send their kids to college"
		\item (young goat) 
	\end{enumerate}
\end{frame}

\begin{frame}{Word embeddings represent each word as a single vector}
	\begin{block}<1->{SkipGram Language Model:}
		\begin{itemize}
			\item \alert{Input} a word $w_T$
			\item \alert{Output} probabilities of words appearing in its context \alert{$P(w_i \mid w_T)$}
		\end{itemize}
	\end{block}
	
	\begin{block}<2->{Word Embeddings Implementation}
		\begin{itemize}
			\item Represent each input word as a \alert{vector}
			\item Train a neural network to estimate  \alert{$P(w_i \mid w_T)$}
			\item Back-prop will find a good representation for your input vector -- i.e. word
		\end{itemize}
	\end{block}	
	\sentexample
\end{frame}



\begin{frame}{Word sense embeddings represent each word as a multiple vectors}
	\begin{itemize}
		\item Each word has multiple senses $\{u_1, u_2, ..., u_n\}$
		\item SkipGram Language Model becomes \begin{itemize}
			\item \alert{Input} a word sense $u_i$
			\item \alert{Output} probabilities of words appearing in its context \alert{$P(w_i \mid u_j$}
		\end{itemize}
	\end{itemize}
	\footfullcite{AdaGrams}
	\sentexample
\end{frame}

\begin{frame}{Many sense embeddings don't produce human recognisable senses}
	 \begin{itemize}
	 	\item<1-> Embeddings are learn by modelling what words occur near the sense
	 	\item<2-> No control over the meanings of the senses \begin{itemize}
	 		\item Cover overlapping definitions
	 		\item Find overly narrow meanings
	 		\item Capture rare jargon uses
	 	\end{itemize}
		\item<3-> Useful, but not interoperable with lexical knowledge bases.	
	 \end{itemize}
\end{frame}



\begin{frame}{We will solve this by \emph{refitting} them to be for the sense we mean}
	\begin{columns}[T]
		\begin{column}{0.5\textwidth}
			\begin{itemize}
				\item \alert{Refitting} constructs new sense embeddings out of the old.
				
				\item It uses the \alert{probabilities} of example sentence occuring.
				
				\item The new embedding aligns to the meaning in that sentence.
			\end{itemize}			
		\end{column}
		
		\inputcolumn{../figs/refitting.tex}
		
	\end{columns}
\end{frame}




\begin{frame}{Refitting uses a probability weighted sum}
	\vspace{-1em}
	\begin{columns}[T]
	\begin{column}{0.5\textwidth}
		
		\vfill 		
		
		New sense embedding:
		\begin{equation*} \label{eq:synth}
			l(\u \mid \c ) = \sum_{\forall u_i \in \u} u_i P(u_i \mid \c)
		\end{equation*}
		

%		Unsupervised Senses Vectors: $\u = \{u_1,...u_{n_u}\}$ 
%		
%		\vspace{1em}
%		
%		Example Sentence: $\c = \{w_1,...w_{n_c}\}$
		 \vfill 		
		 
		 
	\end{column}
	
	\inputcolumn{../figs/refitting.tex}

	\end{columns}
	\vspace{0em}
	\sentexample
\end{frame}

\begin{frame}{The probabilities are found using Bayes' theorem}
	\vspace{-1em}
	\begin{columns}[T]
		\begin{column}{0.5\textwidth}
			
			\onslide<1->{Language model: $P(w_i \mid s_{i})$}
			\vspace{1em}
			\onslide<2->{
				Conditional Independence:
				$\displaystyle%
				P(\c \mid s_{i})=\prod_{\forall w_{j}\in\c}P(w_{j} \mid s_{i})
				$
			}
			\vspace{1em}
			\onslide<3->{
				Bayes Theorem: 
				$\displaystyle%
				P(s_{i} \mid \c) =
				\dfrac{P(\c \mid s_{i})P(s_{i})}
				{\sum_{s_{j}\in\s} P(\c \mid s_{j})P(s_{j})}
				$
			}
		\end{column}

		\inputcolumn{../figs/refitting.tex}
	\end{columns}
	\vspace{0em}
	\sentexample
\end{frame}


\begin{frame}{The posterior distribution (over senses) is too sharp, so we smooth it}
	\vspace{-1em}
	\begin{columns}[T]
		\begin{column}{0.5\textwidth}
			\begin{block}<1->{Original:}
				\vspace{1em}
				\alert{Context Likelihood:}
				$\displaystyle%
				P(\c \mid s_{i})=\prod_{\forall w_{j}\in\c}P(w_{j} \mid s_{i})
				$
							
				\vspace{1em}
				\alert{Sense Likelihood:}
				$\displaystyle%
				P(s_{i} \mid \c) =
				\dfrac{P(\c \mid s_{i})P(s_{i})}
				{\sum_{s_{j}\in\s} P(\c \mid s_{j})P(s_{j})}
				$
						
			\end{block}
		\end{column}
		
		
		\begin{column}{0.5\textwidth}
			\structure{Smoothed:}
			
		\end{column}		
	\end{columns}
	\vspace{0em}
	\sentexample
\end{frame}



\newcommand{\simsentone}{%
	\fitmath{
		\overbrace{
			\obraceword{stone}{w_1}
			\obraceword{axes}{w_w}
			\obraceword{was}{w_3}
			\obraceword{a}{w_4}
			\obraceword{major}{w_5}
			\ubraceword{advance}{word\: 1}
			\obraceword{because}{w_6}
			\obraceword{it}{w_7}
			\obraceword{allowed}{w_8}
			\obraceword{forest}{w_9}
			\obraceword{clearance}{w_{10}}
		}^{\c}%
	}
}

\newcommand{\simsenttwo}{%
	\fitmath{
			\overbrace{
			\obraceword{forces}{w_1^\prime}
			\obraceword{could}{w_2^\prime}
			\obraceword{make}{w_3^\prime}
			\obraceword{no}{w_4^\prime}
			\obraceword{further}{w_5^\prime}
			\ubraceword{headway}{word\: 2}
			\obraceword{without}{w_6^\prime}
			\obraceword{resting}{w_7^\prime}
			\obraceword{and}{w_8^\prime}
			\obraceword{regrouping}{w_9^\prime}
			\obraceword{.}{w_{10}^\prime}
		}^{\c^\prime}%
	}}

\begin{frame}{Similarity with context, is the task of ranking how similar a word is, given its usage}
	
	\vfill
	\simsentone
	\vfill
	\simsenttwo
	\vfill

\end{frame}


\begin{frame}{Use for word similarity with context}
	\vspace{-1.5em}
	\begin{adjustbox}{max height=0.5\textheight}
		\input{../figs/refittedsim}
	\end{adjustbox}
	\vfill
	\fitmath{
	 \begin{aligned}
	 	\mathrm{RefittedSim}((\u,\c),(\u^{\prime},\c^{\prime})) 
	 	&= d(l(\u \mid \c), l(\u^\prime \mid \c^\prime)\\
	 	\mathrm{RefittedSim}((\u,\c),(\u^{\prime},\c^{\prime}))
	 	&= d\left(
	 	\sum_{u_{i}\in\u}u_{i}P(u_{i}\mid\c),\:
	 	\sum_{u_{j}^{\prime}\in\u^{\prime}}u_{i}P(u_{j}^{\prime}\mid\c^{\prime})\right)
	 \end{aligned}
	}
	\vfill

\end{frame}



\begin{frame}[verbatim]{Results on word similarity with context}
	\pgfplotstabletypeset[col sep=comma, header=has colnames, string type,
		columns/Smoothing/.style={column name={$\substack{\mathrm{Geometric}\\\mathrm{Smoothing}}$}},
		columns/Use Prior/.style={column name={$\substack{\mathrm{Use}\\\mathrm{Prior}}$}},
		%		columns/Use Prior/.style={column name={\small{Use Prior}}},
		columns/AvgSimC/.style={
			%column name={\small{AvgSimC}},
			numeric type,
			precision=1,
			fixed zerofill=true,
			preproc/expr={100*##1},
			column type=c
		},
		columns/RefittedSim/.style={
			%column name={\small{RefittedSim}},
			numeric type,
			precision=1,
			fixed zerofill=true,
			preproc/expr={100*##1},
			column type=c
		},
		every row 1 column RefittedSim/.style={
			postproc cell content/.style={
				@cell content/.add={$\bf}{$}
			}
		},
		every row 0 column AvgSimC/.style={
			postproc cell content/.style={
				@cell content/.add={$\bf}{$}
			}
		},
		every head row/.style={after row = {\toprule}}
		]{swsc-grid.csv}
		
\end{frame}

\begin{frame}{Use of refitted senses for word sense disambiguation }
	\begin{adjustbox}{max height=0.5\textheight}
		\xunit=2.5cm
		\input{../figs/wsd.tex}
	\end{adjustbox}
	\begin{equation*}
	\begin{aligned}\label{eq:lexicalwsd}
	l^\star (\l, \c_T) &= \argmax_{\forall l_i \in \l} P(l_i|\c_T) \\
	l^\star (\l, \c_T) &= \argmax_{\forall l_i \in \l} \frac{P(\c_T \mid l_i)P(l_i)}{\sum_{\forall l_j \in \l} P(\c_T \mid l_j)P(l_j)}
	\end{aligned}
	\end{equation*}

\end{frame}

\begin{frame}{Results for word sense disambiguation}
	
\end{frame}

\begin{frame}{What went wrong? The posterior distribution was too sharp}
	
\end{frame}

\begin{frame}{Geometric Smoothing -- Replace products of likelihoods, with the geometric mean of likelihoods}
	
\end{frame}


\begin{frame}{Improved results on word similarity with context}
	
\end{frame}


\begin{frame}{Improved results  for word sense disambiguation}
	
\end{frame}




\section{Appendix}

\begin{frame}{RefittedSim vs AvgSimC}
	\vspace{-1em}
	\vfill
	\begin{block}{RefittedSim}
		\fitmath{
			\mathrm{RefittedSim}((\u,\c),(\u^{\prime},\c^{\prime}))
			= d\left(
			\sum_{u_{i}\in\u}u_{i}P(u_{i}\mid\c),\:
			\sum_{u_{j}^{\prime}\in\u^{\prime}}u_{i}P(u_{j}^{\prime}\mid\c^{\prime})\right)
		}
		
		\vspace{0.2em}
		Time Complexity: 
		$O(n\left\Vert \c\right\Vert +n^{\prime}\left\Vert \c^{\prime}\right\Vert)$
	\end{block}
	\vfill
	
	\begin{block}{AvgSimC}
		\fitmath{
			\mathrm{AvgSimC}((\u,\c),(\u^{\prime},\c^{\prime})) 
			=  \frac{1}{n \times n^{\prime}}
			\sum_{u_{i}\in\u}
			\sum_{u_{j}^{\prime}\in\u^{\prime}}
			P(u_{i}\mid\c)\,P(u_{j}^{\prime}\mid\c^{\prime})\,d(u_{i},u_{j}^{\prime})
		}
		
		\vspace{0.2em}
		Time Complexity: $O(n\left\Vert \c\right\Vert +n^{\prime}\left\Vert \c^{\prime}\right\Vert +n\times n^{\prime})$
	\end{block}
\end{frame}
	
\end{document}
