\documentclass[dvipsnames]{beamer}
\usepackage{verbatim}

\usepackage{microtype}
\usepackage{adjustbox}
\usepackage{amsmath}

\usepackage[subpreambles=false]{standalone}
\usepackage{tikz}

\newlength\xunit


\input{brownbeamer}
\usepackage[color]{../figs/blockdiagrambits}
\tikzset{
	backgroundcolor/.style ={fill=chamois}	
}



\bibliography{master.bib}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\inputcolumn}[1]{%
	\begin{column}{0.5\textwidth}
		\begin{adjustbox}{max width=\columnwidth}
			\input{#1}
		\end{adjustbox}
	\end{column}%
}


\newcommand{\fitmath}[1]{
	\begin{adjustbox}{max width = \textwidth}	
		$\displaystyle
		#1
		$
	\end{adjustbox}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\W}{\mathcal{W}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\s}{\mathbf{s}}
\renewcommand{\l}{\mathbf{l}}
\renewcommand{\u}{\mathbf{u}}
\newcommand{\ci}{\perp\!\!\!\perp} % from Wikipedia
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\ubraceword}[2]{\underbrace{\mathtt{\alert{#1}}}_{#2}\:}
\newcommand{\obraceword}[2]{\overbrace{\mathtt{\alert{#1}}}^{#2}\:}

%%%%%%%%%%%%%%%%%%%%%


\author{\textbf{Lyndon White},\\ Roberto Togneri, Wei Liu, Mohammed Bennamoun}
\institute{School of Electical, Electronic and Computer Engineering\\The University of Western Australia}
\title{Finding Word Sense Embeddings of Known Meaning}

\subtitle{A method for refitting word sense embeddings,  using a single example, by application of Bayes' theorem to the language model}
\date{}
%\logo{\hfill\includegraphics[scale=0.12]{uwa}\hfill\hspace{1.5cm}\vspace{0.5cm}}
\begin{document}
\centering %Center everywhere
\frame{\maketitle}

\begin{frame}{Word embeddings represent each word as a single vector}

\end{frame}

\begin{frame}{Word sense embeddings represent each word as a multiple vectors}
	
\end{frame}

\begin{frame}{Many sense embeddings don't know what the senses mean}
	
\end{frame}

\begin{frame}{We will solve this by \emph{refitting} them to be for the sense we mean}
	\begin{columns}[T]
		\begin{column}{0.5\textwidth}
			\begin{itemize}
				\item<1-> Take in the \alert{word}, the \alert{context}, and the \alert{pretrained senses}
				\item<2-> Output a \alert{new sense embedding} that is for the meaning present in \alert{that sentence}
			\end{itemize} 
		\end{column}
		\inputcolumn{../figs/refitting.tex}
	\end{columns}
\end{frame}


\newcommand{\sentexample}{\fitmath{
	\overbrace{
		\obraceword{wow}{w_1}
		\obraceword{the} {w_2}
		\obraceword{wool} {w_3}
		\obraceword{from} {w_4}
		\obraceword{the} {w_5}
		\ubraceword{kid} {target\: word}
		\obraceword{is} {w_6}
		\obraceword{so}{w_7}
		\obraceword{soft}{w_8}
		\obraceword{and} {w_9}
		\obraceword{fluffy}{w_{10}}
	}^{\c}
}}

\begin{frame}{Refitting uses a probability weighted sum}
	\vspace{-1em}
	\begin{columns}[T]
	\begin{column}{0.5\textwidth}

		\begin{equation*} \label{eq:synth}
			l(\u \mid \c ) = \sum_{\forall u_i \in \u} u_i P(u_i \mid \c)
		\end{equation*}
		

		Unsupervised Senses Vectors: $\u = \{u_1,...u_{n_u}\}$ 
		
		\vspace{1em}
		
		Example Sentence: $\c = \{w_1,...w_{n_c}\}$
		 
	\end{column}
	
	\inputcolumn{../figs/refitting.tex}

\end{columns}
\vspace{0em}
\sentexample
\end{frame}

\begin{frame}{The probabilities are found using Bayes' theorem}
	\vspace{-1em}
	\begin{columns}[T]
		\begin{column}{0.5\textwidth}
			
			\onslide<1->{Language model: $P(w_i \mid s_{i})$}
			\vspace{1em}
			\onslide<2->{
				Conditional Independence:
				$\displaystyle%
				P(\c \mid s_{i})=\prod_{\forall w_{j}\in\c}P(w_{j} \mid s_{i})
				$
			}
			\vspace{1em}
			\onslide<3->{
				Bayes Theorem: 
				$\displaystyle%
				P(s_{i} \mid \c) =
				\dfrac{P(\c \mid s_{i})P(s_{i})}
				{\sum_{s_{j}\in\s} P(\c \mid s_{j})P(s_{j})}
				$
			}
		\end{column}
		\inputcolumn{../figs/refitting.tex}
	\end{columns}
	\vspace{0em}
	\sentexample
\end{frame}



\newcommand{\simsentone}{%
	\fitmath{
		\overbrace{
			\obraceword{stone}{w_1}
			\obraceword{axes}{w_w}
			\obraceword{was}{w_3}
			\obraceword{a}{w_4}
			\obraceword{major}{w_5}
			\ubraceword{advance}{word\: 1}
			\obraceword{because}{w_6}
			\obraceword{it}{w_7}
			\obraceword{allowed}{w_8}
			\obraceword{forest}{w_9}
			\obraceword{clearance}{w_{10}}
		}^{\c}%
	}
}

\newcommand{\simsenttwo}{%
	\fitmath{
			\overbrace{
			\obraceword{forces}{w_1^\prime}
			\obraceword{could}{w_2^\prime}
			\obraceword{make}{w_3^\prime}
			\obraceword{no}{w_4^\prime}
			\obraceword{further}{w_5^\prime}
			\ubraceword{headway}{word\: 2}
			\obraceword{without}{w_6^\prime}
			\obraceword{resting}{w_7^\prime}
			\obraceword{and}{w_8^\prime}
			\obraceword{regrouping}{w_9^\prime}
			\obraceword{.}{w_{10}^\prime}
		}^{\c^\prime}%
	}}

\begin{frame}{Similarity with context, is the task of ranking how similar a word is, given its usage}
	
	\vfill
	\simsentone
	\vfill
	\simsenttwo
	\vfill

\end{frame}


\begin{frame}{Use for word similarity with context}
	\vspace{-1.5em}
	\begin{adjustbox}{max height=0.5\textheight}
		\input{../figs/refittedsim}
	\end{adjustbox}
	\vfill
	\fitmath{
	 \begin{aligned}
	 	\mathrm{RefittedSim}((\u,\c),(\u^{\prime},\c^{\prime})) 
	 	&= d(l(\u \mid \c), l(\u^\prime \mid \c^\prime)\\
	 	\mathrm{RefittedSim}((\u,\c),(\u^{\prime},\c^{\prime}))
	 	&= d\left(
	 	\sum_{u_{i}\in\u}u_{i}P(u_{i}\mid\c),\:
	 	\sum_{u_{j}^{\prime}\in\u^{\prime}}u_{i}P(u_{j}^{\prime}\mid\c^{\prime})\right)
	 \end{aligned}
	}
	\vfill

\end{frame}

\begin{frame}{RefittedSim vs AvgSimC}
	\vspace{-1em}
	\vfill
	\begin{block}{RefittedSim}
		\fitmath{
			\mathrm{RefittedSim}((\u,\c),(\u^{\prime},\c^{\prime}))
			= d\left(
			\sum_{u_{i}\in\u}u_{i}P(u_{i}\mid\c),\:
			\sum_{u_{j}^{\prime}\in\u^{\prime}}u_{i}P(u_{j}^{\prime}\mid\c^{\prime})\right)
		}

		\vspace{0.2em}
		Time Complexity: 
		 $O(n\left\Vert \c\right\Vert +n^{\prime}\left\Vert \c^{\prime}\right\Vert)$
	\end{block}
	\vfill
	
	\begin{block}{AvgSimC}
		\fitmath{
			\mathrm{AvgSimC}((\u,\c),(\u^{\prime},\c^{\prime})) 
			=  \frac{1}{n \times n^{\prime}}
			\sum_{u_{i}\in\u}
			\sum_{u_{j}^{\prime}\in\u^{\prime}}
			P(u_{i}\mid\c)\,P(u_{j}^{\prime}\mid\c^{\prime})\,d(u_{i},u_{j}^{\prime})
		}
	
		\vspace{0.2em}
		Time Complexity: $O(n\left\Vert \c\right\Vert +n^{\prime}\left\Vert \c^{\prime}\right\Vert +n\times n^{\prime})$
	\end{block}
\end{frame}

\begin{frame}{Results on word similarity with context}
	
\end{frame}

\begin{frame}{Use of refitted senses for word sense disambiguation }
	\begin{adjustbox}{max height=0.5\textheight}
		\xunit=2.5cm
		\input{../figs/wsd.tex}
	\end{adjustbox}
	\begin{equation*}
	\begin{aligned}\label{eq:lexicalwsd}
	l^\star (\l, \c_T) &= \argmax_{\forall l_i \in \l} P(l_i|\c_T) \\
	l^\star (\l, \c_T) &= \argmax_{\forall l_i \in \l} \frac{P(\c_T \mid l_i)P(l_i)}{\sum_{\forall l_j \in \l} P(\c_T \mid l_j)P(l_j)}
	\end{aligned}
	\end{equation*}

\end{frame}

\begin{frame}{Results for word sense disambiguation}
	
\end{frame}

\begin{frame}{What went wrong? The posterior distribution was too sharp}
	
\end{frame}

\begin{frame}{Geometric Smoothing -- Replace products of likelihoods, with the geometric mean of likelihoods}
	
\end{frame}


\begin{frame}{Improved results on word similarity with context}
	
\end{frame}


\begin{frame}{Improved results  for word sense disambiguation}
	
\end{frame}


\begin{frame}{A method for refitting sense embeddings}
	
\end{frame}

	
\end{document}
