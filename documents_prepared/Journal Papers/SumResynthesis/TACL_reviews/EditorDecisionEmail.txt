

Dear Lyndon White:

Thank you for submitting paper 902, "A Two Step Process for Generating
Sentences from the Sums of their Embeddings", to TACL.  However, I regret to
inform you that the paper will not be accepted, with no possibility of
resubmission to TACL for a period of 1 year.

The detailed reviews are below. Reviewer C has also supplied a pdf annotated
with minor corrections, please find it attached. I hope you will find their
feedback useful in your future research.

I am sorry that I cannot give you better news, but we do appreciate your
submitting your work to TACL, and wish you the best in your future research.

Phil Blunsom
Oxford University
pblunsom@cs.ox.ac.uk
------------------------------------------------------
------------------------------------------------------
....THE REVIEWS....
------------------------------------------------------
------------------------------------------------------
Reviewer A:

APPROPRIATENESS: Does the paper fit in TACL? (Please answer this question in
light of the desire to broaden the scope of the research areas represented
in the ACL community.): 
	5. Certainly.

CLARITY: For the reasonably well-prepared reader, is it clear what was done
and why? Is the paper well-written and well-structured?: 
	3. Mostly understandable to me with some effort.

ORIGINALITY/INNOVATIVENESS: Does this paper break new ground in topic or
content? How exciting and innovative is the research?: 
	3. Respectable: A nice research contribution that represents a notable
extension of prior work.
Note that a paper could score high for originality even if the results
do not show a convincing benefit.

SOUNDNESS/CORRECTNESS: First, is the theoretical approach sound and
well-chosen?  Second, can one trust the claims of the paper -- for example,
are they supported by an appropriate proof or analysis?: 
	2. Troublesome. There are some ideas worth salvaging here, but the work
should really have been done differently.

MEANINGFUL COMPARISON: Does the author make clear where the work sits with
respect to existing literature? Are the references adequate? Are the
benefits of the theoretical approach well-supported?: 
	2. Only partial awareness and understanding of related work, or a flawed
comparison or deficient comparison with other work.

SUBSTANCE: Does this paper have enough substance (in terms of the amount of
work), or would it benefit from more ideas or analysis?: 
	4. Represents an appropriate amount of work for a publication in this
journal. (most submissions)

IMPACT OF IDEAS OR RESULTS: How significant is the work described? If the
ideas are novel, will they also be useful or inspirational? If the results
are sound, are they also important? Does the paper bring new insights into
the nature of the problem?: 
	2. Marginally interesting. May or may not be cited.

IMPACT OF PROMISED SOFTWARE: If the authors state (in anonymous fashion)
that their software will be available, what is the expected impact of the
software package?: 
	1. No usable software released.

IMPACT OF PROMISED DATASET(S): If the authors state (in anonymous fashion)
that datasets will be released, how valuable will they be to others?: 
	1. No usable datasets submitted.


TACL-WORTHY AS IS? In answering, think over all your scores above. If a
paper has some weaknesses, but you really got a lot out of it, feel free to
recommend it. If a paper is solid but you could live without it, let us know
that you're ambivalent.

Note: after you submit this review form, you'll need to answer a related but
different question via a pull-down menu: how long would it take for the
authors to revise the submission to be TACL-worthy?
: 
	2. Leaning against: I'd rather not see appear it in TACL.

Detailed Comments for the Authors: 
	There are two separate elements to this paper: one, a new technique for
solving the word-ordering-from-a-bag-of-words problem; and two, generating
from a sum-of-word-embeddings representation. The main contribution of the
paper is the new MIP formulation of the word ordering problem.

The authors refer to the previous word-ordering work of Horvat and Byrne,
2014, but there is a more extensive, and more recent, literature:

Discriminative Syntax-Based Word Ordering for Text Generation
Yue Zhang and Stephen Clark
Computational Linguistics, 41(3), pp.503-538, 2015

Yijia Liu, Yue Zhang, Wanxiang Che and Bing Qin. Transition-Based Syntactic
Linearization. In Proceedings of NAACL 2015, Denver, Colorado, USA, May. 

It would also be worth taking a look at this paper recently posted on arxiv,
which uses a simple LSTM plus beam search architecture to get impressive
results:

Word Ordering Without Syntax
Allen Schmaltz and Alexander M. Rush and Stuart M. Shieber

To improve the part of the paper focused on the word ordering problem, it
would be useful to compare with some of the existing work by using the same
datasets.

For the generation from sentence embeddings part, it would be good to
provide more motivation for this task. The authors suggest image captioning
as a possible application, but the technique relies heavily on the assumed
SOWE representation. It's not at all clear that the greedy search for words
would work at all with an image vector, even assuming that we have a joint
text-image space to start with.

Similar to the above comment, there are a few places in the paper where the
authors suggest that the generation method would open up many avenues for
new applications, based on "translation tasks", but don't give any examples.

There is a vast literature in NLP on natural language generation, dating
back decades. The authors should at least acknowledge this fact: a new
student to the field might get the impression from this paper that NLG was
invented only in the last few years.

The authors claim that their technique "can often exactly recreate the
original sentence", in comparison to other existing techniques which can't.
This claim sounds too strong, and needs further experimental comparisons to
be convincing.

More detailed comments, in the order of the paper:

The authors claim that this is the first work to "reproduce text from a
large corpus". It's not entirely clear what this claim means, but the papers
referenced above presumably do this?

"dual space theory" - sounds like this might be a technical term, but I don't think the authors are using it in that way.

No need to give an arbitrary example of a real-valued vector (!)

It's important to define the acronyms and specialist terms you use. I don't believe that SOWE is defined anywhere.

Rather than n-substitution, can you not use a beam search?

The restriction to a maximum of 18 words is unfortunate. Most of the sentences being used are short, and this makes the paper as a whole less convincing.

The point about duplicate sentences is also worrying. Why not just take these out, or use a corpus where duplicates are likely to be less of a problem?

Need to define the various evaluation metrics: Portion perfect, Portion feasible, Mean precision, Mean Jaccard

There are a number of places in the paper where I don't understand the point being made:

The last line of the abstract - what are the "further theoretical investigations"? Why does this paper prompt them?

The sentence starting "With appropriate generative models..."

typos:

by as a the

that differs this from

performance drip

appear adjunctly

REVIEWER CONFIDENCE: 
	5. Positive that my evaluation is correct. I read the paper very carefully
and am familiar with related work.

------------------------------------------------------

------------------------------------------------------
Reviewer B:

APPROPRIATENESS: Does the paper fit in TACL? (Please answer this question in
light of the desire to broaden the scope of the research areas represented
in the ACL community.): 
	4. Probably.

CLARITY: For the reasonably well-prepared reader, is it clear what was done
and why? Is the paper well-written and well-structured?: 
	4. Understandable by most readers.

ORIGINALITY/INNOVATIVENESS: Does this paper break new ground in topic or
content? How exciting and innovative is the research?: 
	1. Significant portions have actually been done before or done better.

SOUNDNESS/CORRECTNESS: First, is the theoretical approach sound and
well-chosen?  Second, can one trust the claims of the paper -- for example,
are they supported by an appropriate proof or analysis?: 
	2. Troublesome. There are some ideas worth salvaging here, but the work
should really have been done differently.

MEANINGFUL COMPARISON: Does the author make clear where the work sits with
respect to existing literature? Are the references adequate? Are the
benefits of the theoretical approach well-supported?: 
	1. Little awareness of related work, or insufficient justification of
benefits and discussion of limitations.

SUBSTANCE: Does this paper have enough substance (in terms of the amount of
work), or would it benefit from more ideas or analysis?: 
	1. Seems thin. Not enough ideas here for a full-length paper.

IMPACT OF IDEAS OR RESULTS: How significant is the work described? If the
ideas are novel, will they also be useful or inspirational? If the results
are sound, are they also important? Does the paper bring new insights into
the nature of the problem?: 
	1. Will have no impact on the field.

IMPACT OF PROMISED SOFTWARE: If the authors state (in anonymous fashion)
that their software will be available, what is the expected impact of the
software package?: 
	1. No usable software released.

IMPACT OF PROMISED DATASET(S): If the authors state (in anonymous fashion)
that datasets will be released, how valuable will they be to others?: 
	1. No usable datasets submitted.


TACL-WORTHY AS IS? In answering, think over all your scores above. If a
paper has some weaknesses, but you really got a lot out of it, feel free to
recommend it. If a paper is solid but you could live without it, let us know
that you're ambivalent.

Note: after you submit this review form, you'll need to answer a related but
different question via a pull-down menu: how long would it take for the
authors to revise the submission to be TACL-worthy?
: 
	1. Poor: I'd fight to have it rejected.

Detailed Comments for the Authors: 
	This paper proposes a two step process for generating a sentence from its
embedding, itself obtained from the sum of its constituent word embeddings.

The first step is an iterative process which effectively brute-forces
generation of a bag of words, the embeddings of which sum to a vector which
is close to the sentence embedding. Not only is this method dreadfully crude
(and no mention is made of its complexity, but it appears to be something of
the order O(nk) for k steps to convergence and n vocabulary words), but it
will not generalise to many methods of sentence embedding which capture the
non-linear relationships between word embeddings, such as encoders in
encoder-decoder models.

The second step uses mixed integer programming to learn the re-ordering
process which, from a bag of words, re-produces a valid sentence of English.

I apologise in advance to the authors, who I am sure have put in a
significant effort to produce this work, for the harshness of the review
which follows, but this paper is so dramatically disconnected from not just
the last 5 years of work on this topic, but arguably also the last few
decades of work in statistical NLP, that I cannot possibly endorse any
recommendation other than rejection. The authors have clearly never heard of
sequence to sequence (a.k.a. encoder-decoder models), and perhaps are not
aware of IBM model 1 either (or even the concept of language modelling) as
both their proposed methods and constant claims that other sequence models
have no quantitative evaluation attests. Some recent efforts in generative
sequence modelling, such as Bowman et al. (2015) are mentioned, but the
authors do not seem to have read the background literature to these papers.

The model proposed in this paper is extremely naive and entirely subsumed by
work sequence to sequence transduction with recurrent neural networks, which
it takes a hearty step back from in returning to back of words from models
which can capture the disambiguation and compositionality which nonlinear
combinations of word embeddings provide. It also ignores language modelling
(both neural variants and traditional n-gram variants) as a method for
re-ordering bags of words into sequences which have higher likelihood
according to models trained on large bodies of text, in favour of a strange
and ill-suited method for replicating the same functionality.

Finally, the entire task of learning sentence representations which permit
the reconstruction of the same sentence is trivial and uninteresting. Such
copying tasks are baselines for more complex transduction models such as
those seen in Grave et al. (2014) or Grefenstette et al. (2015), where LSTMs
are shown to be able to learn to copy sequences of arbitrary symbols within
bounded lengths, with the state between encoder and decoder serving as a
(wholly uninteresting) sentence representation, with accuracies
significantly higher than those reported in this paper (about 98% accuracy
for 64 words if memory serves).

In summary, the methods and models provided here are pedestrian and not
worthy of publication, the task does not yield any convincing semantically
rich representations or applications, and the complete lack of knowledge of
the literature dooms this paper to failure. I, again, apologise for my
frankness.

REVIEWER CONFIDENCE: 
	5. Positive that my evaluation is correct. I read the paper very carefully
and am familiar with related work.

------------------------------------------------------

------------------------------------------------------
Reviewer C:

APPROPRIATENESS: Does the paper fit in TACL? (Please answer this question in
light of the desire to broaden the scope of the research areas represented
in the ACL community.): 
	5. Certainly.

CLARITY: For the reasonably well-prepared reader, is it clear what was done
and why? Is the paper well-written and well-structured?: 
	4. Understandable by most readers.

ORIGINALITY/INNOVATIVENESS: Does this paper break new ground in topic or
content? How exciting and innovative is the research?: 
	3. Respectable: A nice research contribution that represents a notable
extension of prior work.
Note that a paper could score high for originality even if the results
do not show a convincing benefit.

SOUNDNESS/CORRECTNESS: First, is the theoretical approach sound and
well-chosen?  Second, can one trust the claims of the paper -- for example,
are they supported by an appropriate proof or analysis?: 
	4. Generally solid work, although there are some aspects of the approach I
am not sure about or the analysis could be stronger.

MEANINGFUL COMPARISON: Does the author make clear where the work sits with
respect to existing literature? Are the references adequate? Are the
benefits of the theoretical approach well-supported?: 
	5. Precise and complete comparison with related work.

SUBSTANCE: Does this paper have enough substance (in terms of the amount of
work), or would it benefit from more ideas or analysis?: 
	4. Represents an appropriate amount of work for a publication in this
journal. (most submissions)

IMPACT OF IDEAS OR RESULTS: How significant is the work described? If the
ideas are novel, will they also be useful or inspirational? If the results
are sound, are they also important? Does the paper bring new insights into
the nature of the problem?: 
	3. Interesting but not too influential. The work will be cited, but mainly
for comparison or as a source of minor contributions.

IMPACT OF PROMISED SOFTWARE: If the authors state (in anonymous fashion)
that their software will be available, what is the expected impact of the
software package?: 
	1. No usable software released.

IMPACT OF PROMISED DATASET(S): If the authors state (in anonymous fashion)
that datasets will be released, how valuable will they be to others?: 
	1. No usable datasets submitted.


TACL-WORTHY AS IS? In answering, think over all your scores above. If a
paper has some weaknesses, but you really got a lot out of it, feel free to
recommend it. If a paper is solid but you could live without it, let us know
that you're ambivalent.

Note: after you submit this review form, you'll need to answer a related but
different question via a pull-down menu: how long would it take for the
authors to revise the submission to be TACL-worthy?
: 
	4. Worthy: A good paper that is worthy of being published in TACL.

Detailed Comments for the Authors: 
	This paper studies the problem of re-constructing the input sentence from
its vector encoding. The authors propose a two-step approach to the problem,
first generating vectors for the words in the sentence, and then using
information from a language model to reconstruct word order.

The topic is very intriguing and relatively original and the proposed
method, while perhaps not too suprrising, should at the very least serve as
a strong baseline for future work in the area.

Still, I think a few issues should be addressed before the paper is
publishable.

Most importantly, the proposed method is not compared, quantitatively, to
any alternative approach. It would be good if the authors re-implemented the
Iyyer et al and Bowman et al methods (possibly simplified), to directly
compare to their approach (or, at least, the authors should explain why this
is not feasible). The authors should also provide a set of interesting
baselines, minimally including random ordering given the right word set, and
random ordering given the right word set when only short sentences are
considered.

The evaluation section should also be clarified. The authors do not define
the evaluation measures they use in the text. I found the notion of
"feasible", in particular, confusing, in that the qualitative results tables
suggest that the algorithm produced results even when the MIP solver found
no solution: how was this possible?

The authors state that their method will have interesting applications: they
should discuss what these might be.

Last but not least, the authors should make it clearer that the BOW+Language
Modeling approach can basically only serve as a baseline given that, as they
actually illustrate in Table 5, there will always be sentences that we will
utter despite the fact that the same words might occur in a more likely word
order.

Please see the attached PDF for minor comments, typos, etc.

REVIEWER CONFIDENCE: 
	4. Quite sure. I tried to check the important points carefully. It's
unlikely, though conceivable, that I missed something that should affect my
ratings.

------------------------------------------------------
------------------------------------------------------
------------------------------------------------------
....THE REVIEWS....
------------------------------------------------------
------------------------------------------------------
Reviewer A:

APPROPRIATENESS: Does the paper fit in TACL? (Please answer this question in
light of the desire to broaden the scope of the research areas represented
in the ACL community.): 
	5. Certainly.

CLARITY: For the reasonably well-prepared reader, is it clear what was done
and why? Is the paper well-written and well-structured?: 
	3. Mostly understandable to me with some effort.

ORIGINALITY/INNOVATIVENESS: Does this paper break new ground in topic or
content? How exciting and innovative is the research?: 
	3. Respectable: A nice research contribution that represents a notable
extension of prior work.
Note that a paper could score high for originality even if the results
do not show a convincing benefit.

SOUNDNESS/CORRECTNESS: First, is the theoretical approach sound and
well-chosen?  Second, can one trust the claims of the paper -- for example,
are they supported by an appropriate proof or analysis?: 
	2. Troublesome. There are some ideas worth salvaging here, but the work
should really have been done differently.

MEANINGFUL COMPARISON: Does the author make clear where the work sits with
respect to existing literature? Are the references adequate? Are the
benefits of the theoretical approach well-supported?: 
	2. Only partial awareness and understanding of related work, or a flawed
comparison or deficient comparison with other work.

SUBSTANCE: Does this paper have enough substance (in terms of the amount of
work), or would it benefit from more ideas or analysis?: 
	4. Represents an appropriate amount of work for a publication in this
journal. (most submissions)

IMPACT OF IDEAS OR RESULTS: How significant is the work described? If the
ideas are novel, will they also be useful or inspirational? If the results
are sound, are they also important? Does the paper bring new insights into
the nature of the problem?: 
	2. Marginally interesting. May or may not be cited.

IMPACT OF PROMISED SOFTWARE: If the authors state (in anonymous fashion)
that their software will be available, what is the expected impact of the
software package?: 
	1. No usable software released.

IMPACT OF PROMISED DATASET(S): If the authors state (in anonymous fashion)
that datasets will be released, how valuable will they be to others?: 
	1. No usable datasets submitted.


TACL-WORTHY AS IS? In answering, think over all your scores above. If a
paper has some weaknesses, but you really got a lot out of it, feel free to
recommend it. If a paper is solid but you could live without it, let us know
that you're ambivalent.

Note: after you submit this review form, you'll need to answer a related but
different question via a pull-down menu: how long would it take for the
authors to revise the submission to be TACL-worthy?
: 
	2. Leaning against: I'd rather not see appear it in TACL.

Detailed Comments for the Authors: 
	There are two separate elements to this paper: one, a new technique for
solving the word-ordering-from-a-bag-of-words problem; and two, generating
from a sum-of-word-embeddings representation. The main contribution of the
paper is the new MIP formulation of the word ordering problem.

The authors refer to the previous word-ordering work of Horvat and Byrne,
2014, but there is a more extensive, and more recent, literature:

Discriminative Syntax-Based Word Ordering for Text Generation
Yue Zhang and Stephen Clark
Computational Linguistics, 41(3), pp.503-538, 2015

Yijia Liu, Yue Zhang, Wanxiang Che and Bing Qin. Transition-Based Syntactic
Linearization. In Proceedings of NAACL 2015, Denver, Colorado, USA, May. 

It would also be worth taking a look at this paper recently posted on arxiv,
which uses a simple LSTM plus beam search architecture to get impressive
results:

Word Ordering Without Syntax
Allen Schmaltz and Alexander M. Rush and Stuart M. Shieber

To improve the part of the paper focused on the word ordering problem, it
would be useful to compare with some of the existing work by using the same
datasets.

For the generation from sentence embeddings part, it would be good to
provide more motivation for this task. The authors suggest image captioning
as a possible application, but the technique relies heavily on the assumed
SOWE representation. It's not at all clear that the greedy search for words
would work at all with an image vector, even assuming that we have a joint
text-image space to start with.

Similar to the above comment, there are a few places in the paper where the
authors suggest that the generation method would open up many avenues for
new applications, based on "translation tasks", but don't give any examples.

There is a vast literature in NLP on natural language generation, dating
back decades. The authors should at least acknowledge this fact: a new
student to the field might get the impression from this paper that NLG was
invented only in the last few years.

The authors claim that their technique "can often exactly recreate the
original sentence", in comparison to other existing techniques which can't.
This claim sounds too strong, and needs further experimental comparisons to
be convincing.

More detailed comments, in the order of the paper:

The authors claim that this is the first work to "reproduce text from a
large corpus". It's not entirely clear what this claim means, but the papers
referenced above presumably do this?

"dual space theory" - sounds like this might be a technical term, but I
don't think the authors are using it in that way.

No need to give an arbitrary example of a real-valued vector (!)

It's important to define the acronyms and specialist terms you use. I don't
believe that SOWE is defined anywhere.

Rather than n-substitution, can you not use a beam search?

The restriction to a maximum of 18 words is unfortunate. Most of the
sentences being used are short, and this makes the paper as a whole less
convincing.

The point about duplicate sentences is also worrying. Why not just take
these out, or use a corpus where duplicates are likely to be less of a
problem?

Need to define the various evaluation metrics: Portion perfect, Portion
feasible, Mean precision, Mean Jaccard

There are a number of places in the paper where I don't understand the point
being made:

The last line of the abstract - what are the "further theoretical
investigations"? Why does this paper prompt them?

The sentence starting "With appropriate generative models..."

typos:

by as a the

that differs this from

performance drip

appear adjunctly

REVIEWER CONFIDENCE: 
	5. Positive that my evaluation is correct. I read the paper very carefully
and am familiar with related work.

------------------------------------------------------

------------------------------------------------------
Reviewer B:

APPROPRIATENESS: Does the paper fit in TACL? (Please answer this question in
light of the desire to broaden the scope of the research areas represented
in the ACL community.): 
	4. Probably.

CLARITY: For the reasonably well-prepared reader, is it clear what was done
and why? Is the paper well-written and well-structured?: 
	4. Understandable by most readers.

ORIGINALITY/INNOVATIVENESS: Does this paper break new ground in topic or
content? How exciting and innovative is the research?: 
	1. Significant portions have actually been done before or done better.

SOUNDNESS/CORRECTNESS: First, is the theoretical approach sound and
well-chosen?  Second, can one trust the claims of the paper -- for example,
are they supported by an appropriate proof or analysis?: 
	2. Troublesome. There are some ideas worth salvaging here, but the work
should really have been done differently.

MEANINGFUL COMPARISON: Does the author make clear where the work sits with
respect to existing literature? Are the references adequate? Are the
benefits of the theoretical approach well-supported?: 
	1. Little awareness of related work, or insufficient justification of
benefits and discussion of limitations.

SUBSTANCE: Does this paper have enough substance (in terms of the amount of
work), or would it benefit from more ideas or analysis?: 
	1. Seems thin. Not enough ideas here for a full-length paper.

IMPACT OF IDEAS OR RESULTS: How significant is the work described? If the
ideas are novel, will they also be useful or inspirational? If the results
are sound, are they also important? Does the paper bring new insights into
the nature of the problem?: 
	1. Will have no impact on the field.

IMPACT OF PROMISED SOFTWARE: If the authors state (in anonymous fashion)
that their software will be available, what is the expected impact of the
software package?: 
	1. No usable software released.

IMPACT OF PROMISED DATASET(S): If the authors state (in anonymous fashion)
that datasets will be released, how valuable will they be to others?: 
	1. No usable datasets submitted.


TACL-WORTHY AS IS? In answering, think over all your scores above. If a
paper has some weaknesses, but you really got a lot out of it, feel free to
recommend it. If a paper is solid but you could live without it, let us know
that you're ambivalent.

Note: after you submit this review form, you'll need to answer a related but
different question via a pull-down menu: how long would it take for the
authors to revise the submission to be TACL-worthy?
: 
	1. Poor: I'd fight to have it rejected.

Detailed Comments for the Authors: 
	This paper proposes a two step process for generating a sentence from its
embedding, itself obtained from the sum of its constituant word embeddings.

The first step is an iterative process which effectively brute-forces
generation of a bag of words, the embeddings of which sum to a vector which
is close to the sentence embedding. Not only is this method dreadfully crude
(and no mention is made of its complexity, but it appears to be something of
the order O(nk) for k steps to convergence and n vocabulary words), but it
will not generalise to many methods of sentence embedding which capture the
non-linear relationships between word embeddings, such as encoders in
encoder-decoder models.

The second step uses mixed integer programming to learn the re-ordering
process which, from a bag of words, re-produces a valid sentence of English.

I apologise in advance to the authors, who I am sure have put in a
significant effort to produce this work, for the harshness of the review
which follows, but this paper is so dramatically disconnected from not just
the last 5 years of work on this topic, but arguably also the last few
decades of work in statistical NLP, that I cannot possibly endorse any
recommendation other than rejection. The authors have clearly never heard of
sequence to sequence (a.k.a. encoder-decoder models), and perhaps are not
aware of IBM model 1 either (or even the concept of language modelling) as
both their proposed methods and constant claims that other sequence models
have no quantitative evaluation attests. Some recent efforts in generative
sequence modelling, such as Bowman et al. (2015) are mentioned, but the
authors do not seem to have read the background literature to these papers.

The model proposed in this paper is extremely naive and entirely subsumed by
work sequence to sequence transduction with recurrent neural networks, which
it takes a hearty step back from in returning to back of words from models
which can capture the disambiguation and compositionality which nonlinear
combinations of word embeddings provide. It also ignores language modelling
(both neural variants and traditional n-gram variants) as a method for
re-ordering bags of words into sequences which have higher likelihood
according to models trained on large bodies of text, in favour of a strange
and ill-suited method for replicating the same functionality.

Finally, the entire task of learning sentence representations which permit
the reconstruction of the same sentence is trivial and uninteresting. Such
copying tasks are baselines for more complex transduction models such as
those seen in Grave et al. (2014) or Grefenstette et al. (2015), where LSTMs
are shown to be able to learn to copy sequences of arbitrary symbols within
bounded lengths, with the state between encoder and decoder serving as a
(wholly uninteresting) sentence representation, with accuracies
significantly higher than those reported in this paper (about 98% accuracy
for 64 words if memory serves).

In summary, the methods and models provided here are pedestrian and not
worthy of publication, the task does not yield any convincing semantically
rich representations or applications, and the complete lack of knowledge of
the literature dooms this paper to failure. I, again, apologise for my
frankness.

REVIEWER CONFIDENCE: 
	5. Positive that my evaluation is correct. I read the paper very carefully
and am familiar with related work.

------------------------------------------------------

------------------------------------------------------
Reviewer C:

APPROPRIATENESS: Does the paper fit in TACL? (Please answer this question in
light of the desire to broaden the scope of the research areas represented
in the ACL community.): 
	5. Certainly.

CLARITY: For the reasonably well-prepared reader, is it clear what was done
and why? Is the paper well-written and well-structured?: 
	4. Understandable by most readers.

ORIGINALITY/INNOVATIVENESS: Does this paper break new ground in topic or
content? How exciting and innovative is the research?: 
	3. Respectable: A nice research contribution that represents a notable
extension of prior work.
Note that a paper could score high for originality even if the results
do not show a convincing benefit.

SOUNDNESS/CORRECTNESS: First, is the theoretical approach sound and
well-chosen?  Second, can one trust the claims of the paper -- for example,
are they supported by an appropriate proof or analysis?: 
	4. Generally solid work, although there are some aspects of the approach I
am not sure about or the analysis could be stronger.

MEANINGFUL COMPARISON: Does the author make clear where the work sits with
respect to existing literature? Are the references adequate? Are the
benefits of the theoretical approach well-supported?: 
	5. Precise and complete comparison with related work.

SUBSTANCE: Does this paper have enough substance (in terms of the amount of
work), or would it benefit from more ideas or analysis?: 
	4. Represents an appropriate amount of work for a publication in this
journal. (most submissions)

IMPACT OF IDEAS OR RESULTS: How significant is the work described? If the
ideas are novel, will they also be useful or inspirational? If the results
are sound, are they also important? Does the paper bring new insights into
the nature of the problem?: 
	3. Interesting but not too influential. The work will be cited, but mainly
for comparison or as a source of minor contributions.

IMPACT OF PROMISED SOFTWARE: If the authors state (in anonymous fashion)
that their software will be available, what is the expected impact of the
software package?: 
	1. No usable software released.

IMPACT OF PROMISED DATASET(S): If the authors state (in anonymous fashion)
that datasets will be released, how valuable will they be to others?: 
	1. No usable datasets submitted.


TACL-WORTHY AS IS? In answering, think over all your scores above. If a
paper has some weaknesses, but you really got a lot out of it, feel free to
recommend it. If a paper is solid but you could live without it, let us know
that you're ambivalent.

Note: after you submit this review form, you'll need to answer a related but
different question via a pull-down menu: how long would it take for the
authors to revise the submission to be TACL-worthy?
: 
	4. Worthy: A good paper that is worthy of being published in TACL.

Detailed Comments for the Authors: 
	This paper studies the problem of re-constructing the input sentence from
its vector encoding. The authors propose a two-step approach to the problem,
first generating vectors for the words in the sentence, and then using
information from a language model to reconstruct word order.

The topic is very intriguing and relatively original and the proposed
method, while perhaps not too suprrising, should at the very least serve as
a strong baseline for future work in the area.

Still, I think a few issues should be addressed before the paper is
publishable.

Most importantly, the proposed method is not compared, quantitatively, to
any alternative approach. It would be good if the authors re-implemented the
Iyyer et al and Bowman et al methods (possibly simplified), to directly
compare to their approach (or, at least, the authors should explain why this
is not feasible). The authors should also provide a set of interesting
baselines, minimally including random ordering given the right word set, and
random ordering given the right word set when only short sentences are
considered.

The evaluation section should also be clarified. The authors do not define
the evaluation measures they use in the text. I found the notion of
"feasible", in particular, confusing, in that the qualitative results tables
suggest that the algorithm produced results even when the MIP solver found
no solution: how was this possible?

The authors state that their method will have interesting applications: they
should discuss what these might be.

Last but not least, the authors should make it clearer that the BOW+Language
Modeling approach can basically only serve as a baseline given that, as they
actually illustrate in Table 5, there will always be sentences that we will
utter despite the fact that the same words might occur in a more likely word
order.

Please see the attached PDF for minor comments, typos, etc.

REVIEWER CONFIDENCE: 
	4. Quite sure. I tried to check the important points carefully. It's
unlikely, though conceivable, that I missed something that should affect my
ratings.

------------------------------------------------------
------------------------------------------------------
------------------------------------------------------
....THE REVIEWS....
------------------------------------------------------
------------------------------------------------------
Reviewer A:

APPROPRIATENESS: Does the paper fit in TACL? (Please answer this question in
light of the desire to broaden the scope of the research areas represented
in the ACL community.): 
	5. Certainly.

CLARITY: For the reasonably well-prepared reader, is it clear what was done
and why? Is the paper well-written and well-structured?: 
	3. Mostly understandable to me with some effort.

ORIGINALITY/INNOVATIVENESS: Does this paper break new ground in topic or
content? How exciting and innovative is the research?: 
	3. Respectable: A nice research contribution that represents a notable
extension of prior work.
Note that a paper could score high for originality even if the results
do not show a convincing benefit.

SOUNDNESS/CORRECTNESS: First, is the theoretical approach sound and
well-chosen?  Second, can one trust the claims of the paper -- for example,
are they supported by an appropriate proof or analysis?: 
	2. Troublesome. There are some ideas worth salvaging here, but the work
should really have been done differently.

MEANINGFUL COMPARISON: Does the author make clear where the work sits with
respect to existing literature? Are the references adequate? Are the
benefits of the theoretical approach well-supported?: 
	2. Only partial awareness and understanding of related work, or a flawed
comparison or deficient comparison with other work.

SUBSTANCE: Does this paper have enough substance (in terms of the amount of
work), or would it benefit from more ideas or analysis?: 
	4. Represents an appropriate amount of work for a publication in this
journal. (most submissions)

IMPACT OF IDEAS OR RESULTS: How significant is the work described? If the
ideas are novel, will they also be useful or inspirational? If the results
are sound, are they also important? Does the paper bring new insights into
the nature of the problem?: 
	2. Marginally interesting. May or may not be cited.

IMPACT OF PROMISED SOFTWARE: If the authors state (in anonymous fashion)
that their software will be available, what is the expected impact of the
software package?: 
	1. No usable software released.

IMPACT OF PROMISED DATASET(S): If the authors state (in anonymous fashion)
that datasets will be released, how valuable will they be to others?: 
	1. No usable datasets submitted.


TACL-WORTHY AS IS? In answering, think over all your scores above. If a
paper has some weaknesses, but you really got a lot out of it, feel free to
recommend it. If a paper is solid but you could live without it, let us know
that you're ambivalent.

Note: after you submit this review form, you'll need to answer a related but
different question via a pull-down menu: how long would it take for the
authors to revise the submission to be TACL-worthy?
: 
	2. Leaning against: I'd rather not see appear it in TACL.

Detailed Comments for the Authors: 
	There are two separate elements to this paper: one, a new technique for
solving the word-ordering-from-a-bag-of-words problem; and two, generating
from a sum-of-word-embeddings representation. The main contribution of the
paper is the new MIP formulation of the word ordering problem.

The authors refer to the previous word-ordering work of Horvat and Byrne,
2014, but there is a more extensive, and more recent, literature:

Discriminative Syntax-Based Word Ordering for Text Generation
Yue Zhang and Stephen Clark
Computational Linguistics, 41(3), pp.503-538, 2015

Yijia Liu, Yue Zhang, Wanxiang Che and Bing Qin. Transition-Based Syntactic
Linearization. In Proceedings of NAACL 2015, Denver, Colorado, USA, May. 

It would also be worth taking a look at this paper recently posted on arxiv,
which uses a simple LSTM plus beam search architecture to get impressive
results:

Word Ordering Without Syntax
Allen Schmaltz and Alexander M. Rush and Stuart M. Shieber

To improve the part of the paper focused on the word ordering problem, it
would be useful to compare with some of the existing work by using the same
datasets.

For the generation from sentence embeddings part, it would be good to
provide more motivation for this task. The authors suggest image captioning
as a possible application, but the technique relies heavily on the assumed
SOWE representation. It's not at all clear that the greedy search for words
would work at all with an image vector, even assuming that we have a joint
text-image space to start with.

Similar to the above comment, there are a few places in the paper where the
authors suggest that the generation method would open up many avenues for
new applications, based on "translation tasks", but don't give any examples.

There is a vast literature in NLP on natural language generation, dating
back decades. The authors should at least acknowledge this fact: a new
student to the field might get the impression from this paper that NLG was
invented only in the last few years.

The authors claim that their technique "can often exactly recreate the
original sentence", in comparison to other existing techniques which can't.
This claim sounds too strong, and needs further experimental comparisons to
be convincing.

More detailed comments, in the order of the paper:

The authors claim that this is the first work to "reproduce text from a
large corpus". It's not entirely clear what this claim means, but the papers
referenced above presumably do this?

"dual space theory" - sounds like this might be a technical term, but I
don't think the authors are using it in that way.

No need to give an arbitrary example of a real-valued vector (!)

It's important to define the acronyms and specialist terms you use. I don't
believe that SOWE is defined anywhere.

Rather than n-substitution, can you not use a beam search?

The restriction to a maximum of 18 words is unfortunate. Most of the
sentences being used are short, and this makes the paper as a whole less
convincing.

The point about duplicate sentences is also worrying. Why not just take
these out, or use a corpus where duplicates are likely to be less of a
problem?

Need to define the various evaluation metrics: Portion perfect, Portion
feasible, Mean precision, Mean Jaccard

There are a number of places in the paper where I don't understand the point
being made:

The last line of the abstract - what are the "further theoretical
investigations"? Why does this paper prompt them?

The sentence starting "With appropriate generative models..."

typos:

by as a the

that differs this from

performance drip

appear adjunctly

REVIEWER CONFIDENCE: 
	5. Positive that my evaluation is correct. I read the paper very carefully
and am familiar with related work.

------------------------------------------------------

------------------------------------------------------
Reviewer B:

APPROPRIATENESS: Does the paper fit in TACL? (Please answer this question in
light of the desire to broaden the scope of the research areas represented
in the ACL community.): 
	4. Probably.

CLARITY: For the reasonably well-prepared reader, is it clear what was done
and why? Is the paper well-written and well-structured?: 
	4. Understandable by most readers.

ORIGINALITY/INNOVATIVENESS: Does this paper break new ground in topic or
content? How exciting and innovative is the research?: 
	1. Significant portions have actually been done before or done better.

SOUNDNESS/CORRECTNESS: First, is the theoretical approach sound and
well-chosen?  Second, can one trust the claims of the paper -- for example,
are they supported by an appropriate proof or analysis?: 
	2. Troublesome. There are some ideas worth salvaging here, but the work
should really have been done differently.

MEANINGFUL COMPARISON: Does the author make clear where the work sits with
respect to existing literature? Are the references adequate? Are the
benefits of the theoretical approach well-supported?: 
	1. Little awareness of related work, or insufficient justification of
benefits and discussion of limitations.

SUBSTANCE: Does this paper have enough substance (in terms of the amount of
work), or would it benefit from more ideas or analysis?: 
	1. Seems thin. Not enough ideas here for a full-length paper.

IMPACT OF IDEAS OR RESULTS: How significant is the work described? If the
ideas are novel, will they also be useful or inspirational? If the results
are sound, are they also important? Does the paper bring new insights into
the nature of the problem?: 
	1. Will have no impact on the field.

IMPACT OF PROMISED SOFTWARE: If the authors state (in anonymous fashion)
that their software will be available, what is the expected impact of the
software package?: 
	1. No usable software released.

IMPACT OF PROMISED DATASET(S): If the authors state (in anonymous fashion)
that datasets will be released, how valuable will they be to others?: 
	1. No usable datasets submitted.


TACL-WORTHY AS IS? In answering, think over all your scores above. If a
paper has some weaknesses, but you really got a lot out of it, feel free to
recommend it. If a paper is solid but you could live without it, let us know
that you're ambivalent.

Note: after you submit this review form, you'll need to answer a related but
different question via a pull-down menu: how long would it take for the
authors to revise the submission to be TACL-worthy?
: 
	1. Poor: I'd fight to have it rejected.

Detailed Comments for the Authors: 
	This paper proposes a two step process for generating a sentence from its
embedding, itself obtained from the sum of its constituant word embeddings.

The first step is an iterative process which effectively brute-forces
generation of a bag of words, the embeddings of which sum to a vector which
is close to the sentence embedding. Not only is this method dreadfully crude
(and no mention is made of its complexity, but it appears to be something of
the order O(nk) for k steps to convergence and n vocabulary words), but it
will not generalise to many methods of sentence embedding which capture the
non-linear relationships between word embeddings, such as encoders in
encoder-decoder models.

The second step uses mixed integer programming to learn the re-ordering
process which, from a bag of words, re-produces a valid sentence of English.

I apologise in advance to the authors, who I am sure have put in a
significant effort to produce this work, for the harshness of the review
which follows, but this paper is so dramatically disconnected from not just
the last 5 years of work on this topic, but arguably also the last few
decades of work in statistical NLP, that I cannot possibly endorse any
recommendation other than rejection. The authors have clearly never heard of
sequence to sequence (a.k.a. encoder-decoder models), and perhaps are not
aware of IBM model 1 either (or even the concept of language modelling) as
both their proposed methods and constant claims that other sequence models
have no quantitative evaluation attests. Some recent efforts in generative
sequence modelling, such as Bowman et al. (2015) are mentioned, but the
authors do not seem to have read the background literature to these papers.

The model proposed in this paper is extremely naive and entirely subsumed by
work sequence to sequence transduction with recurrent neural networks, which
it takes a hearty step back from in returning to back of words from models
which can capture the disambiguation and compositionality which nonlinear
combinations of word embeddings provide. It also ignores language modelling
(both neural variants and traditional n-gram variants) as a method for
re-ordering bags of words into sequences which have higher likelihood
according to models trained on large bodies of text, in favour of a strange
and ill-suited method for replicating the same functionality.

Finally, the entire task of learning sentence representations which permit
the reconstruction of the same sentence is trivial and uninteresting. Such
copying tasks are baselines for more complex transduction models such as
those seen in Grave et al. (2014) or Grefenstette et al. (2015), where LSTMs
are shown to be able to learn to copy sequences of arbitrary symbols within
bounded lengths, with the state between encoder and decoder serving as a
(wholly uninteresting) sentence representation, with accuracies
significantly higher than those reported in this paper (about 98% accuracy
for 64 words if memory serves).

In summary, the methods and models provided here are pedestrian and not
worthy of publication, the task does not yield any convincing semantically
rich representations or applications, and the complete lack of knowledge of
the literature dooms this paper to failure. I, again, apologise for my
frankness.

REVIEWER CONFIDENCE: 
	5. Positive that my evaluation is correct. I read the paper very carefully
and am familiar with related work.

------------------------------------------------------

------------------------------------------------------
Reviewer C:

APPROPRIATENESS: Does the paper fit in TACL? (Please answer this question in
light of the desire to broaden the scope of the research areas represented
in the ACL community.): 
	5. Certainly.

CLARITY: For the reasonably well-prepared reader, is it clear what was done
and why? Is the paper well-written and well-structured?: 
	4. Understandable by most readers.

ORIGINALITY/INNOVATIVENESS: Does this paper break new ground in topic or
content? How exciting and innovative is the research?: 
	3. Respectable: A nice research contribution that represents a notable
extension of prior work.
Note that a paper could score high for originality even if the results
do not show a convincing benefit.

SOUNDNESS/CORRECTNESS: First, is the theoretical approach sound and
well-chosen?  Second, can one trust the claims of the paper -- for example,
are they supported by an appropriate proof or analysis?: 
	4. Generally solid work, although there are some aspects of the approach I
am not sure about or the analysis could be stronger.

MEANINGFUL COMPARISON: Does the author make clear where the work sits with
respect to existing literature? Are the references adequate? Are the
benefits of the theoretical approach well-supported?: 
	5. Precise and complete comparison with related work.

SUBSTANCE: Does this paper have enough substance (in terms of the amount of
work), or would it benefit from more ideas or analysis?: 
	4. Represents an appropriate amount of work for a publication in this
journal. (most submissions)

IMPACT OF IDEAS OR RESULTS: How significant is the work described? If the
ideas are novel, will they also be useful or inspirational? If the results
are sound, are they also important? Does the paper bring new insights into
the nature of the problem?: 
	3. Interesting but not too influential. The work will be cited, but mainly
for comparison or as a source of minor contributions.

IMPACT OF PROMISED SOFTWARE: If the authors state (in anonymous fashion)
that their software will be available, what is the expected impact of the
software package?: 
	1. No usable software released.

IMPACT OF PROMISED DATASET(S): If the authors state (in anonymous fashion)
that datasets will be released, how valuable will they be to others?: 
	1. No usable datasets submitted.


TACL-WORTHY AS IS? In answering, think over all your scores above. If a
paper has some weaknesses, but you really got a lot out of it, feel free to
recommend it. If a paper is solid but you could live without it, let us know
that you're ambivalent.

Note: after you submit this review form, you'll need to answer a related but
different question via a pull-down menu: how long would it take for the
authors to revise the submission to be TACL-worthy?
: 
	4. Worthy: A good paper that is worthy of being published in TACL.

Detailed Comments for the Authors: 
	This paper studies the problem of re-constructing the input sentence from
its vector encoding. The authors propose a two-step approach to the problem,
first generating vectors for the words in the sentence, and then using
information from a language model to reconstruct word order.

The topic is very intriguing and relatively original and the proposed
method, while perhaps not too suprrising, should at the very least serve as
a strong baseline for future work in the area.

Still, I think a few issues should be addressed before the paper is
publishable.

Most importantly, the proposed method is not compared, quantitatively, to
any alternative approach. It would be good if the authors re-implemented the
Iyyer et al and Bowman et al methods (possibly simplified), to directly
compare to their approach (or, at least, the authors should explain why this
is not feasible). The authors should also provide a set of interesting
baselines, minimally including random ordering given the right word set, and
random ordering given the right word set when only short sentences are
considered.

The evaluation section should also be clarified. The authors do not define
the evaluation measures they use in the text. I found the notion of
"feasible", in particular, confusing, in that the qualitative results tables
suggest that the algorithm produced results even when the MIP solver found
no solution: how was this possible?

The authors state that their method will have interesting applications: they
should discuss what these might be.

Last but not least, the authors should make it clearer that the BOW+Language
Modeling approach can basically only serve as a baseline given that, as they
actually illustrate in Table 5, there will always be sentences that we will
utter despite the fact that the same words might occur in a more likely word
order.

Please see the attached PDF for minor comments, typos, etc.

REVIEWER CONFIDENCE: 
	4. Quite sure. I tried to check the important points carefully. It's
unlikely, though conceivable, that I missed something that should affect my
ratings.

------------------------------------------------------
________________________________________________________________________
Transactions of the Association for Computational Linguistics
https://www.transacl.org/ojs/index.php/tacl/

