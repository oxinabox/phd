\documentclass[11pt]{article}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}


\usepackage[author={Lyndon White}]{pdfcomment}

\usepackage{verbatim}

\usepackage[sharp]{easylist}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{pdflscape}

\usepackage{booktabs, array, pgfplotstable} % Generates table from .csv


%%%%%%%%%%MATH
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}

\numberwithin{equation}{section}
\numberwithin{figure}{section}
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
\theoremstyle{definition}
\newtheorem{defn}[thm]{\protect\definitionname}
\usepackage{csquotes}
\usepackage[english]{babel}
\providecommand{\definitionname}{Definition}
\providecommand{\theoremname}{Theorem}
%%%%%%

\usepackage{algorithm2e}

 
\usepackage[backend=bibtex,
 style=authoryear-icomp,
 bibencoding=ascii,
 maxcitenames=2,
 url=false
]{biblatex}
\bibliography{master}

\usepackage{cleveref}

%End Packages

%%%%%%%%%%%%%%%%%%%%%% Magic Float Layer Fix Settings http://www-rohan.sdsu.edu/~aty/bibliog/latex/floats.html
% Alter some LaTeX defaults for better treatment of figures:
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
\renewcommand{\topfraction}{0.9}	% max fraction of floats at top
\renewcommand{\bottomfraction}{0.8}	% max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}     % 2 may work better
\setcounter{dbltopnumber}{2}    % for 2-column pages
\renewcommand{\dbltopfraction}{0.9}	% fit big float above 2-col. text
\renewcommand{\textfraction}{0.07}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
\renewcommand{\floatpagefraction}{0.7}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\renewcommand{\dblfloatpagefraction}{0.7}	% require fuller float pages

%%%%%%%%%%%%%%%%%





\DeclareMathOperator*{\argmin}{argmin}

%%%%%%% Tables
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
\pgfplotstableset{
	columns/Model/.style={
		string replace={glove50}{50d GLoVE},
		string replace={glove100}{100d GLoVE},
		string replace={glove200}{200d GLoVE},
		string replace={glove300}{300d GLoVE},
		string type,
		column type=C{5.5em}		
	}	
} 

\pgfplotstableset{percent style/.style={%
		preproc/expr={100*##1},
		postproc cell content/.append code={\pgfkeysalso{@cell content/.add={}{\%}}
		}},
		%
		%
		every head row/.style={after row=\midrule},
	}

%%%%%%



%opening
\title{Generating Sentences from the Sums of their Embeddings using a Greedy Algorithm}
\author{Lyndon White}
\graphicspath{{./figs/}}
\setlength\itemsep{2mm}
\begin{document}

\maketitle

\begin{abstract}
Vector representations for sentences have a number of applications.
Many more applications open up, if the it is possible to move not only from natural language to vectors, but also from vectors back to natural language. We introduce such a method for moving from sum of word embedding representations back to the original sentences. This is done using a greedy algorithm to convert from vector to a bag of words. For demonstration purposes we show how the bag of words can be ordered using simple probabilistic language models to get back the sentence. To out knowledge this is the first work to demonstrate qualitatively the ability to reproduce text from a large corpus based on its sentence embeddings. 
As well as practical applications for sentence generation, the success of this method has theoretical implications on the degree of information maintained by the sum of embedding representation.

\end{abstract}

\section{Introduction} \label{intro}




The task being tackled here is the resynthesise of sentences from vector embeddings. In particular it is concerned with generation of sentences from vectors that are the the sum of the word embeddings. The generation task, going from vector to sentence, is quiet challenging.It has not received a lot of attention.

The motivations for this work remain the same as in the first work in the area: \textcite{Dinu2014CompositionalGeneration}. From a theoretical perspective given that a sentence encodes its meaning, and the vector encodes the same meaning, then it must be possible to translate between the natural language and the vector language. An implementation, such as this, which demonstrates the truth of the theory has its own value. Potential practical applications of this often range around such ``translation'' tasks.

Given suitable bidirectional methods for converting sentence vectors and sentences, the sentence vector space can be employed as a \emph{lingua franca} for translation between various forms of information. The most obvious of which is literal translation; however the use extends beyond. Over the past several years significant work has gone into representing images and sentences in a common vector space (REFS HERE). Most current work uses this to select a suitable caption to be selected from a list of candidates, but with vector to sentence generation a caption can be produced from just the image alone. Another similar use is the replacement of vector based extractive summarisation \textcite{KaagebExtractiveSummaristation,yogatamaextractive}, with abstractive summarisation.


The method proposed in this paper takes in a sum of word embeddings (SOWE) sentence vector, and outputs the sentence which it corresponds to. This is done in two steps, as shown in \Cref{block_diagram}. The first step is to determine which words are in the sentence -- this converts the SOWE embedding into a bag of words. The second step is to order that bag of words into a sentence -- a language modelling task.This paper will focus on the first part -- the word selection problem. A demonstration of the feasibly of solving the second part -- the ordering problem --  is shown in \Cref{ordering}.

\begin{figure}
	\centering 
	\includegraphics[width=\columnwidth]{block_diagram}
	\caption{The process for the regenerating sentences from SOWE-type sentence vectors.}
	\label{block_diagram}
\end{figure}

The rest of the paper is organized into the following sections. \Cref{relwork} introduces the area, discussing in general sentence models, and prior work on generation. \Cref{framework} explains the problem in detail and our algorithm for solving it. \Cref{evalsettings} described the settings used for evaluation. \Cref{results} presents the results on this evaluation. The paper concluded with \Cref{conclusion} and a discussion of future work on this problem.


\section{Background}\label{relwork}

\subsection{Embedding Models}
In general sentence embeddings models may be described as compositional, or non-compositional.
The compositional models come from classical linguistics -- taking the hierarchical  break-down of a sentence into clauses, phrases and words, compositional vector models produce vectors by moving up the tree combining (or composing) the vectors in a substructure to find a vector for the substructure. This composition continues until the a vector has been found for the root of the sentence tree. Many successful compositional models have been developed, of particular note have been the recursive neural network based methods of  \textcite{socher2010PhraseEmbedding}%,SocherEtAl2011:RAE,SocherEtAl2011:PoolRAE,Socher2011ParsingPhrases,SocherMVRNN,RvNTN,SocherEtAl2013:CVG,Socher2013TensorReasoning}. Compositional embedding models have strong basis from their linguistic underpinnings.

Non-compositional embeddings cover all other embedding techniques. Recently significant attention has come towards  \textcite{le2014distributed}, which is a direct augmentation of word embedding creation techniques, with an additional paragraph or sentence vector. Other non-compositional techniques include the classic bag of words (BOW), and the simple sum of word embeddings (SOWE). Non-compositional methods draw a significant criticism for disregarding structure; BOW and SOWE draw additional criticism for also disregarding word order. However these weaknesses, while correct, may be entirely offset by the improved discrimination in other factors -- allowed for some non-compositional methods to be comparable overall. 


Recently \textcite{White2015SentVecMeaning} found that classifying real word data sentences according to meaning SOWE was a very strong model. \textcite{RitterPosition} presents a similar task on spacial-positional meaning, which used carefully constructed artificial data, for which it was believed only compositional models could solve, here also SOWE was found performing comparably and again even out performing the compositional models. Thus simple methods such as SOWE are still worth consideration. SOWE is the basis of the work presented in this paper.

\subsection{Sentence Generation from Vector Embeddings}

To our knowledge only three prior works exist in the area of generation from vectors.

\textcite{Dinu2014CompositionalGeneration}  extends the models described by \textcite{zanzotto2010estimating, Guevara2010} for generation; those models are in turn which are in turn specific cases of the general compositional model described by Mitchell and Lapata\textcite{Mitchell2008}. The compositional models used in these works are pre-neural network based techniques used in embeddings today -- though the work Dinu and Baroni does apply to some of today's compositional models in general. While it theoretically generalised to whole sentences Dinu and Baroni's work is quantitatively assessed only for very short phrases.

%Dinu and Baroni \textcite{Dinu2014CompositionalGeneration} focuses on describing composition as a pair of linear transformations of word vectors, followed by a sum $$f_{comp}\::\:\mathbb{R}^d\times\mathbb{R}^d \to \mathbb{R}^d\::\: (\tilde{u}, \tilde{v})\mapsto W_1\tilde{u}+W_2\tilde{v}$$ where the linear transformations $W_1$,$W_2$ are dependent on the parts of speech.

The work of Iyyer et. al. \textcite{iyyer2014generating} is based on an extension of the unfolding recursive autoencoder\textcite{SocherEtAl2011:PoolRAE}. It can produce sentences from vectors, if given the structure to decompose the vector into. Theoretically the unfolding recursive auto-encoder could be used in the same way to generate sentences, however this has not been shown. Recursive neural networks are jointly trained for both composing the sentence's words into a vector, and for decomposing that vector into words. This method was demonstrated quantitatively on five examples. There are not currently any large scale quantitative results published for the use of Iyyer et. al.'s technique.

Both Dinu and Baroni \textcite{Dinu2014CompositionalGeneration}, and Iyyer et. al. \textcite{iyyer2014generating}, require an output structure to be provided for the decomposition. The new method proposed here, does not require, or indeed allow, structural information.

The third, and most recent work, of \textcite{Bowman2015SmoothGeneration} is an adaptation on the recurrent neural network language model of \textcite{mikolov2011RnnLM}, which produces and utilises a new type of non-compositional sentence embedding. While it is a generative model it does not seek to recreate a sentence purely from its vector input, but rather to product distributions on the words. These distributions can be evaluated greedily, which the authors used to give three quantitative examples resynthesises. They found the sentence embeddings created was storing largely syntactic and loss topical information. \pdfcomment{Longest example was 9 tokens}

All of the existing methods have produced only similar sentences, or short phrases -- itself very useful. Due to not completing a close to round trip back to input sentences, none have been demonstrated on a full corpus. That is not the case for the method described in the next section.



\begin{comment}
\subsection{Generative Multi-model Recursive Neural Networks}
An approach for the information domain translation problem which bipasses the \emph{lingua franca} stage of a sentence embedding was developed by \textcite{Mao2014GenerateImageDesc}. The method is based on creating a multi-modal recursive neural network, which takes the source document representation and the previous word output, and predicts the next word; note that unlike the reoccurant neural network this is not a compositional model. While Mao et. al. developed the method for generating captions for images it was extended by \textcite{TarasovGenerateAbstractSummary} for abstractive summarisation. This related technique has the same applications as the suggested translation to common vector space proposals for the use of vector to sentence generation.

\end{comment}

\section{General Framework}\label{framework}
As discussed in \Cref{intro}, and shown in \Cref{block_diagram}, the approach taken 

\subsection{The Selection Problem}

At the core of this problem is what we will call the Vector Selection Problem, to select which word vectors, with option of repetition, are sum to be closest to the target SOWE.
As there is a one to one correspondence between the vector word embeddings and their words, solving for the word embeddings used also solves for the words uses. This is reliant on the fact that in practice not two words are similar enough in usage in any reasonable training corpora to have the precise same embedding under current methods, thus the one to one correspondence.

\renewcommand{\c}{\tilde{c}}
\newcommand{\s}{\tilde{s}}
\newcommand{\x}{\tilde{x}}
\renewcommand{\t}{\tilde{t}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\V}{\mathcal{V}}

\begin{defn}{The Vector Selection Problem}
	The Vector Selection Problem is defined on $(\V, \s,\,d)$ for
	\begin{itemize}
		\item A finite vocabulary of vectors $\V$, $\V\subset{\R}^{n}$
		\item a target vector $ \s$, $ \s\in\R^{n}$
		\item any metric $d$ 
		
		by
		\[
		\argmin_{\left\{ \forall\c\in\N_{0}^{V}\right\} }\:d( \s,\,\sum_{j=1}^{j=V}\:\x_{j}c_{j})
		\]
		
		\medskip{}
				
		\item $V$ is size of the vocabulary $\V$. (\textasciitilde{}1300
		for ATIS2, \textasciitilde{}50,000 for Brown, \textasciitilde{}10,000
		for daily English) 
		\item $\x_{j}$ is the vector embedding for the jth word in the vocabulary
		$\x_{j}\in\V$ 
		
		\begin{itemize}
			\item We can express the embedding vocabulary $\V=\{\x_{j}\,|\,\forall j\in\N\:\wedge\;1\le j\le V\}\subset\R^{n}$
			\item If we treat $\V$ as a matrix with vectors $\x_{j}$
			for rows,(and treat length 1 vectors as scalars) we get the compact
			notation 
			\[
			\argmin_{\left\{ \forall\tilde{c}\in\N_{0}^{J}\right\} }\:d(\s,\,\sum_{j=1}^{j=V}\:\x_{j}c_{j})=\argmin_{\left\{ \forall\tilde{c}\in\N_{0}^{J}\right\} }\:d(\s,\,\V\,\c^{T})
			\]
			
		\end{itemize}
		\item $c_{j}$ is the count of how many times the jth word in the vocabulary
		occurs. $\c\in\N_{0}^{V}$, so $c_{j}\in\N_{0}$
		\item $n$ is the dimensional of the word vectors, $n=300$ in current trials. \end{itemize}
\end{defn}

\subsubsection{Hardness Result}
The vector selection problem is NP-Hard. It is possible to reduce from any given instance of a subset sum problem to a vector selection problem. The subset sum problem is NP-complete \textcite{karp1972reducibility}. Is defined: for some set $\mathcal{S}\subset\mathbb{Z}$, does there exist $\mathcal{L}\subseteq\mathcal{S}$ such that $0=\sum_{l_i}\in L l_i$.  A suitable metric and a set of vectors corresponding to the elements $\mathcal{S}$ can be defined by a bijection; such that solving the vector selection problem will give the vectors corresponding to the closest subset of $\mathcal{S}$ that has the smallest sum. When such a sum is zero than the subset sum result is true, and if not it is false. A fully detailed proof can be found on the first author's website.

\subsubsection{Selection Algorithm}
The algorithm proposed here to solve the selection problem is a greedy iterative processes that continues to convergence. In each iteration, first a greedy search (Greedy Addition) for a path to the targeted sum point $\s$ is done, followed by correction with a substitution based step (n/m--substitution). This process is repeated until no change is made to the path. The majority of the selection is done in the Greedy Addition step, the substitution is handles the fine tuning.

\subsubsection{Greedy Addition}
The greedy addition step is characterised by adding the best vector to the bag at each step (see the pseudo-code in \Cref{pseudocode:greedyaddition}). At each step, all the vectors in the bag are summed, and each vector in the vocabulary is added in turn to evaluate the new distance the new bag would have from the target, if any of the new bags are closer than the existing bag, then the best of them replaces the existing bag. This continues until there is no option to add any of the vectors without moving the sum away from the target. Greedy addition works surprisingly well on its own, but it is enhanced with a fine tuning step to decrease its greed.

\begin{algorithm}
	\SetAlgoLined
		\KwData{the metric $d$\\the target sum $\s$\\ the vocabulary of vectors $\V$\\The current best bag of vectors $bag_c$: initially $\emptyset$}
		\KwResult{The modified $bag_c$  which sum to be as close as greedy search can get to the target $\s$, under the metric $d$}
		\Begin{
		$\t \longleftarrow \sum\limits_{x_i\in bag_c} x_i$\;

		\While{true}{
			$\x^\ast \longleftarrow \argmin\limits_{x_j\in \V} d(\s, \t+\x_j) $\;
			
			\eIf{$d(\s, \t+\x^\ast)  < d(\s, \t)$}{
				$t \longleftarrow \t + \x^\ast$\;
				$bag_c \longleftarrow bag_c \cup \{\x^\ast\}$\;	
			}{
				\tcc{No further improving step found, so done}
				\Return{$bag_c$} 
			}
		}
		
	}
\caption{GreedyAddition. In practical implementation, the bag of vectors can be represented as list of indexes into columns of the embedding vocabulary matrix}
\label{pseudocode:greedyaddition}
\end{algorithm}


\subsubsection{n/m--Substitution}
We define a new substitution based method for fine tuning solutions called n/m--Substitution. It can be described as considering all subbags containing up to $n$ elements, consider replacing them with a new sub-bag of up to size $m$ from the vocabulary, including none at all, if that would result in getting closer to the target $\s$. As this involves $O(V^m)$ operations, with large vocabularies it is only possible to consider 1/1--Substitution, this itself works quiet well. This was done with the brown corpus, where $V\approxeq 40,000$. On other smaller trial corpora, where $V\approxeq 1,000$ 2/2--Substitution was used and found to give an improvement. In general it is possible to initially use with 1/1--Substitution, with the overall algorithm, and if the algorithm converges to a poor solution (i.e. nonzero distance to target in the evaluation trials), then the selection algorithm can be retired from the converged solution, using 2/2--substitution and so forth.

The reasoning behind performing n/m--substitution is to correct for greedy mistakes. Consider the 1 dimensional case where $\V={24,25,50}$ and $\s=98$, $d(x,y)=\left|x-y\right|$. Greedy addition would give  $bag_c=[50,25,24]$ for a distance of $1$, but a better solution (that is found with 1/1 substitution) is $bag_c=[50,24,24]$. This substitution method can be looked at as re-evaluating past decisions in light of the future decisions. In this way it lessens the greed of the addition step. 

\begin{comment}
\begin{algorithm}
	\SetAlgoLined
	\KwData{the metric $d$\\the target sum $\s$\\ the vocabulary of vectors $\V$\\The current best bag of vectors $bag_c$: initially $\emptyset$}
	\KwResult{The $bag_n$  which sum to be at least as close as the input $bag_c$ to the target $\s$, under the metric $d$}
	\Begin{
		
		$\t \longleftarrow \sum\limits_{x_i\in bag_c} x_i$\;
		$bag_n \longleftarrow bag_c$\;
		\ForAll{$\x_i \in bag_c$}{
			$\t^\ast \longleftarrow \t - \x_i$\;
			$bag_c^\ast \longleftarrow bag_c\setminus \{\x_i\}$\;
			
			$\x^\ast \longleftarrow \argmin\limits_{x_j\in \V\cup{\tilde{0}}} d(\s, \t^\ast+\x_j) $\;
			
			\If{$d(\s, \t^\ast+\x^\ast)  < d(\s, \t)$}{
				$t \longleftarrow \t^\ast + \x^\ast$\;
				$bag_n \longleftarrow bag_c^\ast \cup \{\x^\ast\}$\;	
			}
		}
	}
	\Return{$bag_n$}
\caption{1/1--Substitution. Note that $\tilde{0}$ is the zero vector, including it with the vocabulary $\V$ is equivalent to considering the option of simply removing the $\x_i$ without replacement.}
\end{algorithm}
\end{comment}


\subsection{The Ordering Problem} \label{ordering}
\begin{figure*}
	\centering 
	%\includegraphics[scale=0.5]{markov_reordering1}
	\includegraphics[scale=0.15]{markov_reordering2}
	\caption{The Markov graph of possible states in the search through trigrams for the most-likely ordering of the sentence 
%		``Show cheap flights''.}
		``Show The Cheapest Flight''.}
	\label{markov_diagram}
\end{figure*}

In this section a demonstration of the feasibly of solving for the correct word order, given the output of the previous step is shown. This is a stochastic language modelling problem -- to determine the most likely sequence of words, given the constraints that the words are exactly those given by the BOW output from the previous step. The method presented here simply to show its feasibility; the development of efficient technique for use in the ordering problem is left for future work.


For purposes of this demonstration a simple Kneser-Ney smoothed trigram model \parencite{kneser1995improved} was used. The language model allows us to evaluate the probability of any given sequence of words using a Markov model.
A directed acyclic graph of states can be produced for possible sequences of words as shown in \Cref{markov_diagram}. The graph is restricted, in that terminal nodes must be followed by the end state, and must also have no words from the input BOW unassigned. The task is then modelled as finding the most-likely path through the graph.
Beam search was used to search the graph. At each node in the graph only the beam width (in the results following the beam width was five) most likely transitions (using just the a signal step of the trigram) were considered. This search is not complete, nor optimal -- there may be a better ordering that requires a transition that is outside the beam width. The beam-width restriction allows the combinatorial problem to be completed in feasible time. For a BOW with $n$ words, and beam width $b$ (assuming $b<n$) the time complexity is $O(b^{n-b}b!)$, rather than $O(n!)$ for a full search of all possible orderings. Memorization is also used to avoid re-searching known branches; and branches cease to be  if their accumulated probability for the path becomes zero (potentially do to underflow on the 64-bit IEEE floating point numbers).

Even with these techniques to decrease the time taken, running the search over long sentences takes significant amount of time: on the order of 10 seconds for a sentence of length 17. For this reason the ordering step is only carried out for the BOWs corresponding to reference sentences of length 18 or less. 
 
 
Of course, for some sentences the BOW can not always be resolve to the correct ordering -- when some words can be swapped in position and still have a valid and likely sentence. For example "Show me flights from Paris to New York." is just as reasonable as  "Show me flights from New York to Paris". On the other hand often one ordering is far more likely and reasonable than the others, "The dog chased the cat." is much more often correct than "The cat chased the dog.", it is also preferable over "The cat the dog chased." though all three are grammatical. It is assumed here that, for most BOWs there is one ordering that is far far more likely to occur than the others, and thus is the correct ordering, the results in the next section suggest this assumption is correct -- at least for the brown corpus. \pdfcomment{It would be good to have an a citation for how often this occurs.}

\textcite{Mitchell2008} does bring up the more substantive example of  "It was not the sales manager who  hit  the bottle that day, but the office worker with the serious drinking problem." and "That day the office manager,   who was drinking, hit the problem sales worker with a bottle, but it was not serious.". It nevertheless can be argued that this example is synthetic and that more natural phrasing would have resulted in sentence with different wordings. Though certainly some sentenced where this is not true must exist, work with the assumption they are rare.
  



\section{Evaluation Setting} \label{evalsettings}

The evaluation was performed on the  Standard Sample of Present-Day American English -- better known as the Brown Corpus. While the Brown Corpus is the oldest of all corpora it remains a challenging task, due to its broad topical domain and large vocabulary. The sources include a wide variety of fictional and non-fictional works from 1961 \parencite{francis1979brown}. It contains almost 50,000 unique tokens \pdfcomment{(49,815 to be exact)}. Note we do not distinguish tokens (such as punctuation) from words. For word embeddings we make used of the GloVe representations\footnote{Kindly made available online at \url{http://nlp.stanford.edu/projects/glove/}} from \textcite{pennington2014glove}, pre-trained on 2014 Wikipedia and Gigaword 5. 

For simplicity of evaluation, sentence containing words not found in the vector vocabulary are excluded. Similarly, words which are not used in the corpus at least once are excluded from the vocabulary. In general it is of-course possible to simply to have trained a set of vocabulary of word vectors including those words (See also the \Cref{future} for another method to handle them.). 




\subsection{Selection}
\subsection{Ordering}
As the overall problem can be consider a task of translating from a vector to a natural language sentence, the BLEU  score \parencite{Papineni2002} can be used for assessing how well the output sentence matches the input reference.

\section{Results and Discussion} \label{results}

\begin{table}
	\pgfplotstabletypeset[col sep=comma,fixed zerofill, precision=3,column type=C{2.5em},
		columns/Mean Jaccard Index Overall/.style={
			column name={\footnotesize Mean Jaccard Index}
		},
		columns/Mean Jaccard Index on Shorter Sentences/.style={
			column name={\footnotesize Mean Jaccard Index}
		},
		columns/Exact Matches/.style={
			precision=1,
			percent style,
			column name={\footnotesize Exact Matches},
			column type=C{2.5em}|
			},
		%
		columns/Exact Matches on Shorter Sentences/.style={
			precision=1,
			percent style,
			column name={\footnotesize Exact Matches}},	
			%
		every head row/.style={
	    	before row={%
		   		& \multicolumn{2}{c|}{\smaller \textbf{Overall}} & \multicolumn{2}{c}{\smaller \textbf{Shorter Sentences} }\\
	    	},
	    	after row=\midrule,
		},
	]{data/selection_overall_len_scores.csv}
	\caption{ The performance of the word selection step. Shorter sentences are those with a true length of 18 or less. This is just over the median sentences length (17) in the Brown Corpus. Only these shorter sentences were considered for the ordering step.}
	%\label{table:overall}
\end{table}

\begin{figure}
	\pgfplotstableread[col sep=comma]{data/selection_len_scores.csv}{\sellenscores}
	\pgfplotstableread[col sep=comma]{data/selection_cum_len_scores.csv}{\sellenscorescum}
	\begin{tikzpicture}[scale=0.85]
	\begin{axis}[xlabel=Ground Truth Sentence Length, ylabel=Mean Jacard Index]
	\addplot table [y="glove50_jaccard_mean",x="ground_len"]{\sellenscores};
	\addplot table [y="glove100_jaccard_mean",x="ground_len"]{\sellenscores};
	\addplot table [y="glove200_jaccard_mean",x="ground_len"]{\sellenscores};
	\addplot table [y="glove300_jaccard_mean",x="ground_len"]{\sellenscores};
	\legend{50d GLoVE, 100d GLoVE,200d GLoVE,300d GLoVE}					
	\end{axis}
	\end{tikzpicture}
	\hfill
	\begin{tikzpicture}[scale=0.85]
	\begin{axis}[xlabel=Maximum Ground Truth Sentence Length, ylabel=Cumulative Mean Jacard Index]
	\addplot table [y="glove50_jaccard",x="max_ground_len"]{\sellenscorescum};
	\addplot table [y="glove100_jaccard",x="max_ground_len"]{\sellenscorescum};
	\addplot table [y="glove200_jaccard",x="max_ground_len"]{\sellenscorescum};
	\addplot table [y="glove300_jaccard",x="max_ground_len"]{\sellenscorescum};
	%\legend{50d GLoVE, 100d GLoVE,200d GLoVE,300d GLoVE}					
	\end{axis}
	\end{tikzpicture}
	
	\caption{\label{figure:exactlenscore}The mean jaccard index achieved during the word selection step, shown against the ground truth length of the sentence. On the left is the score at each length, and the right is the cumulative score averaged arcoss sentences of up to the indicated length.  Note that the results become noisy for longer sentences, as there are very few long sentences being averaged over -- 75\% of all sentences in the corpus are of length 25 or less. \pdfcomment{Shall I just show a histogram here?}}
\end{figure}

The overall results for the selection step are presented in \Cref{table:overall}. The results at a fixed ground truth length are shown in \Cref{figure:exactlenscore}. It can be noted that the performance improves the higher the embedding dimentionality, and degrades as the sentence length becomes longer. With high dimensional representations it continues to perform well for sentences of reasonable length for normal use. 


It was noted that unlike in the work of \textcite{iyyer2014generating}, this processes does not often generate paraphrases, but rather the original sentence. This is due, at least in part, to the method being able to successfully minimising the distance to the original down to zero. Thus resulting in the original bag of words.

Beyond these methods, the new method proposed in this paper is shown to be able to exactly reproduce sentences based on there embeddings.  Due to its exact and near exact resynthesis of whole sentences is is possible to assess it on a whole corpus, rather than on a short phrase corpus, or having to demonstrate it only on a few examples.  However the rigidity which allows for the exact reproduction is also likely associated with limitation, where failures are likely to be ungrammatical 

\begin{table*}[t]
	\centering
	\pgfplotstabletypeset[col sep=comma,ignore chars={"},fixed zerofill, precision=3,column type=C{5em},
	columns/Giveups/.style={precision=1, percent style},
	columns/Exact Ordered Matches/.style={precision=1, percent style},
	columns/Exact Ordered Matches Excluding Giveups/.style={precision=1, percent style},
	]{data/ordering_scores.csv}
	\caption{ The performance of the ordering step. Only evaluated on sentences of ground truth length 18 or less. A Giveup occurs when the beam search could not find an ordering with nonzero probability, and so no attempt was made to order the words beyond the order they were found in the selection step.}
	\label{table:ordering}
\end{table*}

It can be seen from \Cref{table:ordering} that a very significant portion of the failures come from the ordering step "Giving up". A give-up occurs if the beam search failed to find any ordering to which non-zero probability was assigned. This could be because the ordering was outside of the search width, or because no reasonable ordering was possible. The former can be solved with a wider beam search -- or switching to a complete search in general. The later can only be solved with a better language model, or more likely by making less errors in the word selection step. Both issues certainly occur. 

When the sentences which are given-up on are excluded, the ordering performance actually gets worse as the dimensionality increases. This is because the lower dimensional models output invalid word combinations in the selection step

%Need to include Perfect results -- if the selection step was done perfectly.


\section{Conclusion} \label{conclusion}
A method was presented for how to regenerate a bag of words, from the sum of a sentence's word embeddings. The word selection problem, of going from the sum of embeddings to the words, is NP-Hard. A greedy algorithm was found to perform well at the task, particularly for shorter sentences when high dimensional embeddings are used. It was also demonstrated that a simple probabilistic language model can be used to order the bag of words output to regenerate the original sentences.

\subsection{Future Work}\label{future}
One of the restrictions placed during the evaluation of the technique was that pretrained embeddings must be available for all words, which is reasonable as the embeddings can always be trained to include those words. Another technique we have been developing is to assign missing words a vector of zero. This results in them never appearing in the generated BOWS. However assuming they do appear in the language model training corpus, they can be inserted during the ordering step. If it is more likely to have that missing word, than to not. This technique showed promising results when used with the \texttt{word2vec} vectors \parencite{mikolovSkip}, which exclude common stop words like "to".
\printbibliography
	
\end{document}


