\documentclass[11pt]{article}
\usepackage{standalone}

\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}

\usepackage[final]{microtype}
\usepackage{flushend}

\usepackage[author={Lyndon White}]{pdfcomment}

\usepackage[boxed]{algorithm2e}


\usepackage{verbatim}
\usepackage{grffile}
%\usepackage[draft]{graphicx}

\usepackage{pgfplots}
\pgfplotsset{compat=1.12}
\usepackage{booktabs, array} % Generates table from .csv
\usepackage{pgfplotstable}

\usepackage{tikz}
\usetikzlibrary{positioning}


%%%%%%%%%%MATH
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}

\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
\theoremstyle{definition}
\newtheorem{defn}[thm]{\protect\definitionname}
\usepackage{csquotes}
\usepackage[english]{babel}
\providecommand{\definitionname}{Definition}
\providecommand{\theoremname}{Theorem}
%%%%%



%\hypersetup{draft} %Avoid hyperef causing issuse when links broken over pages by disabling hyperref

\usepackage[backend=bibtex,
 style=authoryear-icomp,
 bibencoding=ascii,
 maxcitenames=2,
 url=false,
 hyperref=false
]{biblatex}
\bibliography{master}

\usepackage{cleveref}


%End Packages

%%%%%%%%%%%%%%%%%%%%%% Magic Float Layer Fix Settings 
\renewcommand{\topfraction}{.85}
\renewcommand{\bottomfraction}{.7}
\renewcommand{\textfraction}{.15}
\renewcommand{\floatpagefraction}{.66}
\renewcommand{\dbltopfraction}{.66}
\renewcommand{\dblfloatpagefraction}{.66}
\setcounter{topnumber}{9}
\setcounter{bottomnumber}{9}
\setcounter{totalnumber}{20}
\setcounter{dbltopnumber}{9}

%%%%%%%%%%%%%%%%%



\DeclareMathOperator*{\argmin}{argmin}

%%%%%%% Tables
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
\pgfplotstableset{
	columns/Model/.style={
		string replace={glove50}{50d GLoVE},
		string replace={glove100}{100d GLoVE},
		string replace={glove200}{200d GLoVE},
		string replace={glove300}{300d GLoVE},
		string type,
		column type=C{5.5em}		
	}	
} 

\pgfplotstableset{percent style/.style={%
		preproc/expr={100*##1},
		postproc cell content/.append code={\pgfkeysalso{@cell content/.add={}{\%}}
		}},
		%
		%
		every head row/.style={after row=\midrule},
	}

%%%%%%



%opening
\title{Generating Sentences from the Sums of their Embeddings using a Greedy Algorithm}
\author{}
\graphicspath{{./figs/}}
\setlength\itemsep{2mm}
\begin{document}

\maketitle

\begin{abstract}
Converting a sentence to a meaningful vector representation has uses in many NLP tasks, however very few methods allow that representation to be restored to a human readable sentence. Being able to generate sentences from the vector representations is expected to open up many new applications. We introduce such a method for moving from sum of word embedding representations back to the original sentences. This is done using a greedy algorithm to convert the vector to a bag of words. We then show how the bag of words can be ordered using simple probabilistic language models to get back the sentence. To our knowledge this is the first work to demonstrate qualitatively the ability to reproduce text from a large corpus based on its sentence embeddings. 
As well as practical applications for sentence generation, the success of this method has theoretical implications on the degree of information maintained by the sum of embeddings representation.
\end{abstract}

\section{Introduction} \label{intro}


The task being tackled here is the \emph{resynthesis} of sentences from vector embeddings. In particular the generation of sentences from vectors based on the sum of their constituent words' embeddings. The generation task, going from vector to sentence, is quite challenging. It has not received a lot of attention.

The motivations for this work remain the same as in the first work in the area: \textcite{Dinu2014CompositionalGeneration}. From a theoretical perspective given that a sentence encodes its meaning, and the vector encodes the same meaning, then it must be possible to translate in both directions between the natural language and the vector representation. An implementation, such as the work reported in this paper, which demonstrates the truth of this dual space theory has its own value. There are also many potential practical applications of such an implementation, often ranging around certain types of ``translation'' tasks.

Given suitable bidirectional methods for converting between sentence vectors and sentences, the sentence vector space can be employed as a \emph{lingua franca} for translation between various forms of information. The most obvious of which is literal translation between different natural languages; however the use extends beyond this.

Several approaches have been developed for representing images and sentences in a common vector space. This is used to select a suitable caption a list of candidates \parencite{farhadi2010every,socherDTRNN}. The caption could instead be generated from the common vector space, through sentence resynthesis -- without any need for a list. There has also been significant recent work on this caption generation task directly using language generation, without the common space step, such as in \textcite{donahue2014long,Mao2014GenerateImageDesc}. The translation though common space approach suggested may provide a competing or supplementary technique for this task.

Another similar use is the replacement of vector based extractive summarisation \textcite{KaagebExtractiveSummaristation,yogatamaextractive}, with abstractive summarisation. The promising use of sentence generation for all these applications is to have a separate model trained to take the source information (e.g. a picture for image description, or a cluster of sentences for abstract summarisation) as its input and train it to output a sentence vector which can then be used to generate the sentence.

There are currently three existing methods for regeneration -- each tied to different machine learnt embedding representation. 
The current state of the art for full sentence generation are the works of \cite{iyyer2014generating} and \cite{Bowman2015SmoothGeneration}. 
Both these advance beyond the original work in the area of \cite{Dinu2014CompositionalGeneration} which is only theorised to extend beyond short phrases, both produce full sentences. These sentences are qualitatively shown to be loosely similar in meaning to the original sentences. Neither works has produced quantitative evaluation, making it hard to determine between them. Both are detailed further in the next.


The method proposed in this paper takes in a sum of word embeddings (SOWE) sentence vector, and outputs the sentence which it corresponds to. The input is a vector for example $\tilde{s}=[0.11, 0.57,-0.21,...,1.29]$, which approximates a SOWE vector, and outputs a sentence: for example "The boy was happy.". That input vector could come direct as the SOWE representation of a reference sentence (as is the case for the evaluation of our method presented in this paper). More practically it could come as the output of some other process; for example a machine learnt mapping from an image to the vector representation of its textual description. This vector representation is transformed through our process into a human readable sentence.

The sentence generation is done in two steps, as shown in \Cref{block_diagram}. The first step is to determine which words are in the sentence -- this converts the SOWE embedding into a bag of words. The second step is to order that bag of words into a sentence -- a language modelling task. This paper will focus on the first part -- the word selection problem. A demonstration of the feasibly of solving the second part -- the ordering problem --  is shown in \Cref{ordering}.

\begin{figure}
	\centering 
	\input{figs/block_diagram.tex}
	\caption{The process for the regenerating sentences from SOWE-type sentence vectors. \pdfcomment{Update diagram to refer to the input as the target vector}}
	\label{block_diagram}
\end{figure}

The rest of the paper is organized into the following sections. \Cref{relwork} introduces the area, discussing in general sentence models, and prior work on generation. \Cref{framework} explains the problem in detail and our algorithm for solving it. \Cref{evalsettings} described the settings used for evaluation. \Cref{results} presents the results on this evaluation. The paper concluded with \Cref{conclusion} and a discussion of future work on this problem.


\section{Related Works}\label{relwork}
\subsection{Embedding Models}



The compositional models come from classical linguistics -- based on the hierarchical  break-down of a sentence into clauses, phrases and words. Compositional vector models produce vectors by moving up the hierarchical tree combining (or ``composing'') the vectors for elements within a substructure to find the substructure's vector. This composition continues until the a vector has been found for the root of the sentence tree. The original work in this area is \textcite{Mitchell2008}. A variety of recursive neural network based methods were developed during the thesis of \textcite{socher2014recursive}. Compositional methods continue to improve, with newer methods as in \textcite{TACL15CompVector}, focusing on capturing with more efficiency and precision how words interact to control the meaning of their superstructures. 

%\textcite{socher2010PhraseEmbedding}%,SocherEtAl2011:RAE,SocherEtAl2011:PoolRAE,Socher2011ParsingPhrases,SocherMVRNN,RvNTN,SocherEtAl2013:CVG,Socher2013TensorReasoning}. Compositional embedding models have strong basis from their linguistic underpinnings.

Non-compositional embeddings allowing each word to more directly affect the overall sentence vector, rather than doing so through intermediate substructure vectors. This is a broad category covering all other embedding techniques. From the classic bag of words (BOW), and the simple sum of word embeddings (SOWE); to more complex approaches augmenting windowed language modelling neural networks with an addition sentence memory \parencite{le2014distributed}.  Non-compositional methods draw significant criticism for disregarding structure; BOW and SOWE draw additional criticism for entirely disregarding word order. However these weaknesses, while valid, may be offset by the improved discrimination allowed through words directly affecting the sentence vector -- without the potential lose in the repeated composing steps. This allows some non-compositional methods to be comparable overall to the more linguistically consistent compositional embeddings. 


Recently \textcite{White2015SentVecMeaning} found that when classifying real-world sentences into groups of semantically equivalent paraphrases, that using SOWE as the input resulted in very accurate classifications. In that work White et. al. partitioned the sentences into groups of paraphrases, then evaluated how well a linear SVM could classify unseen sentences into the class given by its meaning. They tested this using a variety of different sentence embeddings techniques as input to the classifier. They found that the classification accuracy when using SOWE as the input very similar to that of the best performing methods -- less that 0.6\% worse on the harder task. From this they concluded that the mapping from the space of sentence meaning to the vector space of the SOWE, resulted in sentences with the same meaning going to distinct areas of the vector space.

\textcite{RitterPosition} presents a similar task on spacial-positional meaning, which used carefully constructed artificial data, for which the meanings of the words interacted non-simply -- thus favouring the compositional models. In their evaluation the task was classification with a Naive Bayes classifier into one of five categories of different spatial relationships. Here also SOWE was found performing comparably and again even out performing the compositional models. The best of the SOWE models they evaluated, outperformed the next best model (compositional or otherwise) by over 5\%. These results suggest this simple method is still worth consideration. SOWE is the basis of the work presented in this paper.

\subsection{Sentence Generation from Vector Embeddings}

To our knowledge only three prior works exist in the area of sentence generation from vectors. The first two of which are based on compositional embeddings, while the most recent work at the time of this writing, is based on a non-compositional approach.

\newcommand{\p}{\tilde{p}_{1,2}}
\renewcommand{\u}{\tilde{u}}


\textcite{Dinu2014CompositionalGeneration}  extends the models described by \textcite{zanzotto2010estimating, Guevara2010} for generation. The composition is described as the sum of pair of linear transformations of the input vectors ($\u_1$ and $\u_2$) to get a output vector $\p$. Dinu and Baroni, investigate two sources for the input and target output vectors: word and short-phrase embeddings generated using the continuous bag of words method of \cite{mikolov2013efficient}, and the method used by \textcite{Guevara2010} using single value decomposition on the word and phrase co-occurrence matrix for the training data. The composition function is given by:  $$f_{comp}\::\:\mathbb{R}^d\times\mathbb{R}^d \to \mathbb{R}^d\::\: (\u_1, \u_2)\mapsto W_1\u_2+W_2\u_2$$
Likewise, decomposition can be described by: $$f_{decomp}\::\:\mathbb{R}^d\to \mathbb{R}^d\times\mathbb{R}^d \::\: \p \mapsto (W^\prime_1\p, W^\prime_2\p)$$.
During training linear transformation matrices $W_1$,$W_2$ and $W_1^\prime$,$W_2^\prime$ can be solved for using least squares regression.
While it theoretically generalises to whole sentences, by recursive application of the composition or decomposition functions, Dinu and Baroni's work is quantitatively assessed only on direct reconstruction only for decomposing Preposition-Noun and Adjective-Noun 2 word phrases. In the case where the  where the decomposition function was trained on vectors generated using the compositional function they were able to get perfect reconstruction on the word embedding based inputs.
% -- though as they note, this is not extremely difficult as it is fitting linear transformation to invert a linear transformation.

\renewcommand{\p}{\tilde{p}}

The work of \textcite{iyyer2014generating} is based on an extension of the unfolding recursive autoencoder \textcite{SocherEtAl2011:PoolRAE}. Recursive neural networks are jointly trained for both composing the sentence's words into a vector, and for decomposing that vector into words. This composition and decomposition is done by reusing a composition neural network at each vertex the  dependency tree structure with different weight matrices for each dependency relation. The total network is trained based on its accuracy to reproduce its input word vectors, through back-propagation. Through just using the top decomposing half of the network, from the sentence embedding up, it can be used to regenerate the words. This method was demonstrated quantitatively on five examples, shown in \Cref{results}; they show the generated sentences of \textcite{iyyer2014generating} to be loosely semantically similar to the originals.

%$$g_{comp}\:: (u_i)^N_{i=1} \mapsto \sigma(\sum_{i=1}^N W_i\u_i+\tilde{b})$$
%$$g_{decomp} \p \mapsto  (\sigma(W^\prime_i\p+\tilde{b}^\prime+W_{sib}\u^\prime_j)^N_{i=1}$$
%$\u^\prime_j$ is $g_{decomp}(p)_j$ if $j$ is the left most sibling of $i$ with the same dependancy relation to the parent as $i$ if one exists, or the zero vector otherwith
%Here $W_i$ and $W^\prime_i$ vary not by order, but by the dependency tree part of speech of the $i$th element.
%$\tilde{b}$ and $\tilde{b}^\prime$ are the biases.
%$\sigma$ is a nonlinear activation function, such as the sigmoid or the hyperbolic tangent functions. It is this nonlinearity that prevents the values being solved by any kind of least squares.

%Theoretically, even without the extensions the unfolding recursive auto-encoder could be used in the same way to generate sentences, however this has not been shown.


\textcite{Bowman2015SmoothGeneration} is an adaptation on the recurrent neural network language model of \textcite{mikolov2011RnnLM}, to produces and utilise a new type of non-compositional sentence embedding. Bowman et. al. devised a modification of the variational autoencoder  \parencite{kingma2013auto} with natural language inputs and outputs, to learn the sentence representations. These input and output stages are performed using long short-term memory recurrent neural networks \parencite{hochreiter1997long}. They demonstrate a number of uses of this technique, only one of which is sentence generation, in the sense of this paper.
While it is a generative model it does not seek to recreate a sentence purely from its vector input, but rather to produce a series of probability distributions on the words in the sentence. These distributions can be evaluated greedily, which the authors used to give three quantitative examples resynthesis (also shown in \Cref{results}). They found the sentence embeddings created were storing largely syntactic and loose topical information. 


Both \textcite{Dinu2014CompositionalGeneration}, and  \textcite{iyyer2014generating}, require an output structure to be provided for the decomposition. Like the method of  \textcite{Bowman2015SmoothGeneration}, the new method proposed in this paper does not require this structural information.

None of the existing methods have demonstrated recreation of the full sentence input close enough to allow for quantitative evaluation on a full corpus. They tend to output lose paraphrases, or roughly similar sentences -- itself a slightly distinct and useful task.  That is not the case for our method described in the next section, which can often exactly recreate the original sentence from its vector representation.

\section{General Framework}\label{framework}
As discussed in \Cref{intro}, and shown in \Cref{block_diagram}, the approach taken to generate the sentences from the vectors comes in two steps. First selecting the words used -- this is done deterministically, based on a search of the embedding space. Followed by ordering them, here demonstrated using a classic stochastic language modelling approach. The separating of the process into two steps is unlike any of the existing methods for sentence generation from vectors. The two subproblems which result from this split resemble more classical computer science problems. The selection problem, of choosing the best word vectors to sum to the target is a knapsack-like problem. The word-ordering problem is a search for the most-likely path through a Markov graph of n-gram probabilities. It may be noted, that both these sub-problems are NP-hard.

\subsection{The Vector Selection Problem}

At the core of this problem is what we will call the Vector Selection Problem, to select which word vectors from the vector vocabulary, with potential repetition, will sum to be closest to the target SOWE.
As there is a one to one correspondence between the vector word embeddings and their words, solving for the word embeddings used provides a unique solution to this. This relies on no two words having exactly the same embeddings -- which is true for all current word embedding techniques.

\renewcommand{\c}{\tilde{c}}
\newcommand{\s}{\tilde{s}}
\newcommand{\x}{\tilde{x}}
\renewcommand{\t}{\tilde{t}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\V}{\mathcal{V}}

\begin{defn}{The Vector Selection Problem}
	is defined on $(\V, \s,\,d)$ for a finite vocabulary of vectors $\V$, $\V\subset{\R}^{n}$, a target sentence vector $ \s$, $ \s\in\R^{n}$ and any distance metric $d$ by
		\[
		\argmin_{\left\{ \forall\c\in\N_{0}^{V}\right\} }\:d( \s,\,\sum_{\x_j\in\V}\:\x_{j}c_{j})
		\]
						
		$\x_{j}$ is the vector embedding for the jth word in the vocabulary
		$\x_{j}\in\V$ and $c_j$ is the jth element of the count vector $\c$ being optimised -- it is the count of how many times the $x_j$ occurs in approximation to the sum being assessed; and correspondingly it is the count of how many times the jth word from the vocabulary occurs in the bag of words.
		The selection problem is thus finding the right words with the right multiplicity, such that the sum of their vectors is as close to the input target vector, $\s$, as possible.
\end{defn}

\subsubsection{NP-Hard Proof}
The vector selection problem is NP-Hard. It is possible to reduce from any given instance of a \emph{subset sum problem} to a vector selection problem. The \emph{subset sum problem} is NP-complete \parencite{karp1972reducibility}. It is defined: for some set of integers ($\mathcal{S}\subset\mathbb{Z}$), does there exist a subset ($\mathcal{L}\subseteq\mathcal{S}$) which sums to zero ($0=\sum_{l_i\in \mathcal{L}} l_i$).  A suitable metric, target vector and  vocabulary of vectors corresponding to the elements $\mathcal{S}$ can be defined by a bijection; such that solving the vector selection problem will give the subset vectors corresponding a subset of $\mathcal{S}$ with the smallest sum; which if zero indicates that the subset sum does exists, and if nonzero indicates that no such subset ($\mathcal{L}$) exists. A fully detailed proof of the reduction from subset sum to the vector selection problem can be found on the first author's website.\footnote{[[URL removed for blinding.]]}

\subsubsection{Selection Algorithm}
The algorithm proposed here to solve the selection problem is a greedy iterative processes that continues to convergence. In each iteration, first a greedy search (Greedy Addition) for a path to the targeted sum point $\s$ is done, followed by correction with a substitution based step (n-substitution). This process is repeated until no change is made to the path. The majority of the selection is done in the Greedy Addition step, while the substitution is handles fine tuning.

\paragraph{Greedy Addition}
The greedy addition step is characterised by adding the best vector to the bag at each step (see the pseudo-code in \Cref{pseudocode:greedyaddition}). At each step, all the vectors in the bag are summed, and each vector in the vocabulary is added in turn to evaluate the new distance the new bag would have from the target, if any of the new bags are closer than the existing bag, then the best of them replaces the existing bag. This continues until there is no option to add any of the vectors without moving the sum away from the target. Greedy addition works surprisingly well on its own, but it is enhanced with a fine tuning step to decrease its greediness.

\begin{algorithm}
	\SetAlgoLined
		\KwData{the metric $d$\\the target sum $\s$\\ the vocabulary of vectors $\V$\\The current best bag of vectors $bag_c$: initially $\emptyset$}
		\KwResult{The modified $bag_c$  which sum to be as close as greedy search can get to the target $\s$, under the metric $d$}
		\Begin{
		$\t \longleftarrow \sum\limits_{x_i\in bag_c} x_i$\;

		\While{true}{
			$\x^\ast \longleftarrow \argmin\limits_{x_j\in \V} d(\s, \t+\x_j) $\;
			
			\eIf{$d(\s, \t+\x^\ast)  < d(\s, \t)$}{
				$t \longleftarrow \t + \x^\ast$\;
				$bag_c \longleftarrow bag_c \cup \{\x^\ast\}$\;	
			}{
				\tcc{No further improving step found}
				\Return{$bag_c$} 
			}
		}
		
	}
\caption{GreedyAddition. In practical implementation, the bag of vectors can be represented as list of indexes into columns of the embedding vocabulary matrix}
\label{pseudocode:greedyaddition}
\end{algorithm}


\subsubsection{n-Substitution}
We define a new substitution based method for fine tuning solutions called n-substitution. It can be described as considering all subbags containing up to $n$ elements, consider replacing them with a new sub-bag of up that size from the vocabulary, including none at all, if that would result in getting closer to the target $\s$. 

The reasoning behind performing the n-substitution is to correct for greedy mistakes. Consider the 1 dimensional case where $\V={24,25,50}$ and $\s=98$, $d(x,y)=\left|x-y\right|$. Greedy addition would give  $bag_c=[50,25,24]$ for a norm-distance of $1$, but a perfect solution  is $bag_c=[50,24,24]$ which is found using  substitution. This substitution method can be looked at as re-evaluating past decisions in light of the future decisions. In this way it lessens the greed of the addition step. 

The n-subsitution step has time complexity $O(\binom{C}{n}V^n)$ operations -- for $C=\sum \c$ i.e. current cardinality of $bag_c$. With large vocabularies it is only practical to consider 1-substitution. With the Brown Corpus, where $V\approxeq 40,000$, was found that 1-substitution provides a significant improvement over greedy addition alone. On a smaller trial corpora, where $V\approxeq 1,000$, 2-substitution was used and found to give further improvement. In general it is possible to initially use 1-substitution, with the overall algorithm, and if the algorithm converges to a poor solution (i.e. nonzero distance to target in the evaluation trials), then the selection algorithm can be retired from the converged solution, using 2-substitution and so forth. As $n$ increases the greed decreases; at the limit the overall algorithm is not greedy at all, but is rather an exhaustive search.


\begin{comment}
\begin{algorithm}
	\SetAlgoLined
	\KwData{the metric $d$\\the target sum $\s$\\ the vocabulary of vectors $\V$\\The current best bag of vectors $bag_c$: initially $\emptyset$}
	\KwResult{The $bag_n$  which sum to be at least as close as the input $bag_c$ to the target $\s$, under the metric $d$}
	\Begin{
		
		$\t \longleftarrow \sum\limits_{x_i\in bag_c} x_i$\;
		$bag_n \longleftarrow bag_c$\;
		\ForAll{$\x_i \in bag_c$}{
			$\t^\ast \longleftarrow \t - \x_i$\;
			$bag_c^\ast \longleftarrow bag_c\setminus \{\x_i\}$\;
			
			$\x^\ast \longleftarrow \argmin\limits_{x_j\in \V\cup{\tilde{0}}} d(\s, \t^\ast+\x_j) $\;
			
			\If{$d(\s, \t^\ast+\x^\ast)  < d(\s, \t)$}{
				$t \longleftarrow \t^\ast + \x^\ast$\;
				$bag_n \longleftarrow bag_c^\ast \cup \{\x^\ast\}$\;	
			}
		}
	}
	\Return{$bag_n$}
\caption{1/1-substitution. Note that $\tilde{0}$ is the zero vector, including it with the vocabulary $\V$ is equivalent to considering the option of simply removing the $\x_i$ without replacement.}
\end{algorithm}
\end{comment}


\subsection{The Ordering Problem} \label{ordering}

\pdfcomment{Insert citation here for Ordering being NP-Hard. There are esisting papers on this that i need to nail down as this problem occurs in machine transalation.}
\pdfcomment{after taking negative logs of the probilities, this is a shortest path problem, which has fast soluations that are O(V\^2), for V the number of vertices.	However V, is itelf huge relative the the number of words W, V=Sum_{i=0}^W (W-i)! -- I think -- worse than expodential time.
}

After the bag of words has been generated by the previous step, it must be ordered. For example “are how , today hello ? you”, is to be ordered into the sentence: “hello , how are you today ?”. This problem can not always be solved to a single correct solution. \textcite{Mitchell2008}  gives the example of "It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem." which has the same word content (though not punctuation) as "That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious.". However, while a unique ordering can not be guaranteed, finding the most likely word ordering is possible.

Our method for word ordering is to formulate the problem as a linear mixed order programming problem (MIP). This lets us take advantage of the efficient existing solvers for this problem. As in the work of \textcite{Horvat2014} we expressed the problem in terms of a generalised asymmetrical travelling salesman problem (TSP). Beyond this, we went on to express the problem further into a MIP form. This meant the use with a a MIP solver, rather than a TSP solver. This direct expression allowed us to vary the problem, with the potential for small optimisations.

Normally, for a problem with known start and end vertices to be considered as a travelling salesman problem, the vertices are connected with a zero cost link. As there are exactly one Start and End pseudo-word states we simply special case them in the MIP definition as not requiring incoming or outgoing connections, respectively. We also made direct use of the knowledge that the Start and End states must to always occur in every tour, to simplify the sub-tour elimination. 

When the travelling salesman problem is solved as a MIP,  some potential solutions contain non-connected subtours. These are eliminated by adding new "lazy" constraints are added, each time such a solution is suggested. For the normal TSP, subtours are eliminated with the constraint that every subtour must have an additional outgoing connection to one of the other subtours. This is more complex in case of the generalised asymmetrical TSP: for each subtour there must an outgoing connection from the nodes of the neighbours covered by it, to one of the nodes in the union of the neighbourhoods covered by the other subtours. Finding these unions takes a non-negligible amount of time; but only one of them is needed to eliminate this subtour. When multiple subtours are common adding multiple constraints cause more to be eliminated; however we've found that subtours are rare in the word ordering problems, and multiple subtours even more so -- as such, pre-emptively adding constraints to eliminate them is not efficient computationally. We can quickly find the core tour, by following the path from the Start to the End pseudo-word states. Then we add only the requirement that there is an outgoing connection from the nodes in neighbourhoods covered by the core tour, to one or more of the nodes outside that set. Preliminary testing found this to give a marginal speed-up.



\begin{comment}
We first formulate the problem into a Markov graph of states (each correspond to potential subsequences of two words), with edges weighted according to the log of the trigram possibility of the transitions between them 

The probability of any sequence of words can be found using a trigram language model. We can express all possible orderings as a graph between Markov states of the last two words, connected to allowable following words by edged weighted by the negative log of the trigram probability. 
 with edges weighted with the logThis language model can be used to 


The problem for ordering is assign an map between 

The problem can be expressed in terms of a Markov  chain
In this section a demonstration of the feasibly of solving for the correct word order, given the output of the previous step is shown. This is a stochastic language modelling problem -- to determine the most likely sequence of words, given the constraints that the words are exactly those given by the BOW output from the previous step. The method presented here is simply to show its feasibility; the development of efficient technique for use in the ordering problem is left for future work.\pdfcomment{LW: I think this sentence/paragraph can be cut down a lot. RT: I agree, If this is a standard approach (used in ASR?) is there a need to describe the background of the beam seach using tri-gram models. YOu should just state what you implemented (with reference of where the beam search using trigrams is used), provide the parameter details in Section 4 and perhaps move the computational analysis (of all compoenents) into a separate section.}

For purposes of this demonstration a simple Kneser-Ney smoothed trigram model \parencite{kneser1995improved} was used. The language model allows us to evaluate the probability of any given sequence of words using a Markov model.
A directed acyclic graph of states can be produced for possible sequences of words as shown in \Cref{markov_diagram}. The graph is restricted, in that terminal nodes must be followed by the end state, and must also have no words from the input BOW unassigned. The task is then modelled as finding the most-likely path through the graph.
Beam search was used to search the graph. At each node in the graph only the beam width most likely transitions, using just the a signal step of the trigram as the heuristic, were considered. This search is not complete, nor optimal -- there may be a better ordering that requires a transition that is outside the beam width. The beam-width restriction allows the combinatorial problem to be completed in feasible time. For a BOW with $n$ words, and beam width $b$ (assuming $b<n$) the time complexity is $O(b^{n-b}b!)$, rather than $O(n!)$ for a full search of all possible orderings. Memorization is also used to avoid re-searching known branches; and branches cease to be  if their accumulated probability for the path becomes zero -- potentially due to underflow on the 64-bit IEEE floating point numbers.

Even with these techniques to decrease the time taken, running the search over long sentences takes significant amount of time: on the order of 10 seconds for a sentence with 18 words, and increasing rapidly. For this reason the ordering step is only carried out for the BOWs corresponding to reference sentences with no more than 18 words.
 
  
Of course, for some sentences the BOW can not always be resolved to the correct ordering -- when some words can be swapped in position and still have a valid and likely sentence. For example ``Show me flights from London to New York.'' is just as reasonable as  ``Show me flights from New York to London.''. On the other hand often one ordering is far more likely and reasonable than the others, "The dog chased the cat." is much more often correct than "The cat chased the dog.", it is also preferable over "The cat the dog chased." though all three are grammatically correct. It is assumed here that, for most bags of words there is one ordering that is far far more likely to occur than the others, and thus is the correct ordering. The results for the Oracle BOW in the next section suggest this assumption is largely correct -- at least for the Brown Corpus.

%\pdfcomment{The next paragraph would benefit from being merged into the former. Or perhaps being made a footnote? Or deleted}
%\textcite{Mitchell2008} gives  example of  "It was not the sales manager who  hit  the bottle that day, but the office worker with the serious drinking problem." and "That day the office manager,   who was drinking, hit the problem sales worker with a bottle, but it was not serious.". It  can be argued that this example is synthetic and that more natural phrasing would have resulted in sentences with different wordings. A second issue is the differing numbers if commas in the texts. Though certainly some sentenced where this is not true must exist, work with the assumption they are rare.
\end{comment}



\section{Experimental Setup and Evaluations} \label{evalsettings}
\pdfcomment{I perhaps this section can be merged into the previous and the following sections.}

\subsection{Word Embeddings}
GloVe representations of words are used in our evaluations \parencite{pennington2014glove}. There are many varieties of word embeddings which function with our algorithm. GloVe was chosen simply become of the availability of a large pre-trained vocabulary of vectors. The representations used for evalation were pretrained on pre-trained on 2014 Wikipedia and Gigaword 5\footnote{Kindly made available online at \url{http://nlp.stanford.edu/projects/glove/}}.  Other vector representations are presumed to function similarly.

\subsection{Corpus}
The evaluation was performed on the  Standard Sample of Present-Day American English -- better known as the Brown Corpus \parencite{francis1979brown}. While the Brown Corpus is one of the oldest of all corpora it remains a challenging task, due to its broad topical domain and relatively large vocabulary. The sources include a wide variety of fictional and non-fictional works from 1961 \parencite{francis1979brown}. It contains almost 50,000 unique tokens \pdfcomment{(49,815 to be exact)}. Note we do not distinguish tokens (such as punctuation) from words.


For simplicity of evaluation, sentences containing words not found in the vector vocabulary are excluded.  In general it is of-course possible to simply to have trained a set of vocabulary of word vectors including those words (See also \Cref{future} for another method to handle them.). Similarly, words which are not used in the corpus are excluded from the vector vocabulary. 

\subsection{Vector Selection}
The euclidean metric was used to measure how close potential solutions were to the target vector. The choice of distance metric controls how close each vector is considered to the partial sum during the greedy selection. The commonly used cosign similarity, or the linked angular distance, have an issue of non-zero distances between distinct points -- making them not true distance metrics. For example the SOWE of \emph{So he said `` ``So'' said he.''.} has a zero distance under those measures to \emph{"He said so.''}. Though, that example is ungrammatical, and pathological. True metrics such a the euclidean metric used do not have this problem. Further investigation may find other better distance metrics for this step.

Selection results are shown for the whole corpus -- the selection algorithm is deterministic, so no training subset or cross-validation is required. The mean Jaccard index is used to assess the similarity of the output BOWs to the reference BOWs. The portion of exact matches, without respect for order, is also presented.

\subsection{Overall}
For the overall problem, including both the selection, and the ordering step, is assessed using methods commonly used for assessing translations. As the overall problem can be consider a task of translating from a vector to a natural language sentence, the BLEU  score \parencite{Papineni2002} is a suitable measure. The BLEU score is a assessing how well the output sentence matches the input reference, allowing for partials scoring for partial matches

The overall process does require training the language model. For this 10-fold cross-validation was used, creating and validating separate training and evaluation sets. During the beam search, a beam-width of 5 was used.

The Julia programming language \parencite{Julia}, was used to create the implementation of the method, and the evaluation scripts for the results presented in the next section. These this implementation  and the scripts are available on-line.\footnote{[[URL Blinded for Review]]}

\section{Results and Discussion} \label{results}

\begin{table}
	\pgfplotstabletypeset[col sep=comma,fixed zerofill, precision=3,column type=C{2.5em},
		columns/Mean Jaccard Index Overall/.style={
			column name={\footnotesize Mean Jaccard Index}
		},
		columns/Mean Jaccard Index on Shorter Sentences/.style={
			column name={\footnotesize Mean Jaccard Index}
		},
		columns/Exact Matches/.style={
			precision=1,
			percent style,
			column name={\footnotesize Exact Matches},
			column type=C{2.5em}|
			},
		%
		columns/Exact Matches on Shorter Sentences/.style={
			precision=1,
			percent style,
			column name={\footnotesize Exact Matches}},	
			%
		every head row/.style={
	    	before row={%
		   		& \multicolumn{2}{c|}{\footnotesize \textbf{Overall}} & \multicolumn{2}{c}{\footnotesize \textbf{Shorter Sentences} }\\},
	    	after row=\midrule
		}]{data/selection_overall_len_scores.csv}
	\caption{ The performance of the word selection step. Shorter sentences are those with a reference length of 18 or less. This is just over the median sentences length (17) in the corpus. Only these shorter sentences were considered for the ordering step.}
	\label{table:overall}
\end{table}


\begin{figure}
	\pgfplotstableread[col sep=comma]{data/selection_len_scores.csv}{\sellenscores}
	\pgfplotstableread[col sep=comma]{data/selection_cum_len_scores.csv}{\sellenscorescum}
	\begin{tikzpicture}[scale=0.85]
	\begin{axis}[xlabel=Ground Truth Sentence Length, ylabel=Mean Jacard Index]
%	\addplot table [y="glove50_jaccard_mean",x="ground_len"]{\sellenscores};
%	\addplot table [y="glove100_jaccard_mean",x="ground_len"]{\sellenscores};
%	\addplot table [y="glove200_jaccard_mean",x="ground_len"]{\sellenscores};
%	\addplot table [y="glove300_jaccard_mean",x="ground_len"]{\sellenscores};
	\legend{50d GLoVE, 100d GLoVE,200d GLoVE,300d GLoVE}					
	\end{axis}
	\end{tikzpicture}
	\hfill
	\begin{tikzpicture}[scale=0.85]
	\begin{axis}[xlabel=Maximum Ground Truth Sentence Length, ylabel=Cumulative Mean Jacard Index]
%	\addplot table [y="glove50_jaccard",x="max_ground_len"]{\sellenscorescum};
%	\addplot table [y="glove100_jaccard",x="max_ground_len"]{\sellenscorescum};
%	\addplot table [y="glove200_jaccard",x="max_ground_len"]{\sellenscorescum};
%	\addplot table [y="glove300_jaccard",x="max_ground_len"]{\sellenscorescum};
	%\legend{50d GLoVE, 100d GLoVE,200d GLoVE,300d GLoVE}					
	\end{axis}
	\end{tikzpicture}
	
	\caption{The mean Jaccard index achieved during the word selection step, shown against the ground truth length of the sentence. On the left is the score at each length, and the right is the cumulative score averaged across sentences of up to the indicated length.  Note that the results become noisy for longer sentences, as there are very few long sentences being averaged over -- 75\% of all sentences in the corpus are of length 25 or less. \pdfcomment{LW: Shall I just show a histogram here? RT: Depends on what you want to show. If the sentence distribution is important then you should, but you have indirectly provided this from the current information (in the caption and text) so perhaps not necessary.}}
	\label{figure:exactlenscore}
\end{figure}
\begin{table*}[t!]
	\centering
	\pgfplotstabletypeset[col sep=comma,ignore chars={"},fixed zerofill, precision=3,column type=C{4em},
	columns/Giveups/.style={precision=1, percent style},
	columns/Exact Ordered Matches/.style={precision=1, percent style},
	columns/Exact Ordered Matches Excluding Giveups/.style={precision=1, percent style},
	]{data/ordering_scores.csv}
	\caption{ The overall round-trip performance, including the ordering step with various embedding dimensionalities. Only evaluated on sentences of ground truth length 18 or less. A Giveup occurs when the beam search could not find an ordering with nonzero probability, and so no attempt was made to order the words beyond the order they were found in the selection step. The right hand columns exclude these Giveup cases where ordering failed -- they show only the accuracy when the ordering step suggested it produced a valid ordering at all; the left hand columns are more significant as to the overall performance of the method. The Oracle BOW bypasses the selection step, by providing to the ordering step randomly shuffled reference sentences. It is presented for comparison of the best possible performance using the ordering method.}
	\label{table:ordering}
\end{table*}

\begin{table*}[t!]
	\centering
	INSERT TABLE OF EXAMPLES HERE
	
	\caption{Examples of  }
	\label{table:examples}
\end{table*}


The overall results for the selection step are presented in \Cref{table:overall}. The results plotted against the ground truth sentence length are shown in \Cref{figure:exactlenscore}. It can be noted that the performance improves the higher the embedding dimensionality, and degrades as the sentence length becomes longer. With high dimensional representations it continues to perform well for sentences of reasonable length.


The method is shown to be able to often exactly reproduce sentences based on their embeddings.  Due to its exact and near exact resynthesis of whole sentences it is possible to assess it on a whole corpus, rather than having to demonstrate it only on a few examples. For comparison, results on the examples from \textcite{Bowman2015SmoothGeneration,iyyer2014generating} are shown in \Cref{table:examples}.
It can be noted that  our process does not often generate paraphrases, but rather the original sentence. This is due, at least in part, to the method being able to successful minimise the distance to the original down to zero. Thus resulting in the original bag of words -- rather than a similar sentence.


However the rigidity which allows for the exact reproduction is also seems likely to also cause a limitation, where failures are likely to be ungrammatical e.g. missing conjunctions -- which would cause finding a reasonable ordering to be impossible -- without further processing to repair. This would cause the any search of the ordering graph to return no results.


The results on the overall process are shown in \Cref{table:ordering}. These results show the performance of ordering the output of the previous selection step, at the various different embedding dimensionality. Shown also is a Oracle BOW, where the selection step is bypassed, to establish an upper bound on the performance. For the Oracle result, the bag of words input comes just from shuffling the words in the reference sentence.

The high score of the Oracle BOW, confirms the hypothesis that for most sentences the word order can be inferred from the word content alone -- but certainly not all. This presents an upper bound on the performance of any method attempting to regenerate sentences from embeddings which do not preserve word order. 

It can be that a very significant portion of the failures come from the ordering step ``Giving up''. A give-up occurs if the beam search failed to find any ordering to which non-zero probability was assigned. This could be because the ordering was outside of the search width, or because no reasonable ordering was possible. The former can be solved with a wider beam search -- or switching to a complete search in general. The later can only be solved by making less errors in the word selection step. Both issues certainly occur. 

When the sentences which are given-up on are excluded, the ordering performance actually gets worse as the dimensionality increases. This is because the lower dimensional models output invalid word combinations in the selection step for more difficult sentences -- those which are more likely to be ordered incorrectly even if the selection is done perfectly.




\section{Conclusion} \label{conclusion}
A method was presented for how to regenerate a bag of words, from the sum of a sentence's word embeddings. The word selection problem, of going from the sum of embeddings to the words, is NP-Hard. A greedy algorithm was found to perform well at the task, particularly for shorter sentences when high dimensional embeddings are used. It was also demonstrated that a simple probabilistic language model can be used to order the bag of words output to regenerate the original sentences.

Resynthesis degraded as sentence length increased, but remained strong with higher dimensional models up to reasonable length. The fairly basic beam-search based ordering technique was only evaluated on sentences with up to 18 words (inclusive), due to computational time limitations. On these it performed quite well with the best model achieving perfect recreation 80.7\% of cases. Performances, both accuracy and running time, is expected to worsen were longer sentences considered. With that said, 18 words is sufficient for many uses.

From a theoretical basis the resolvability of the selection problem shows that adding up the word vectors does preserve the information on which words were used; particularly for higher dimensional embeddings. This shows clearly that collisions do not occur (at least with frequency) such that two unrelated sentences do not end up with the same SOWE representation. 

\subsubsection{Future Work}\label{future}
One of the restrictions placed during the evaluation of the technique was that pretrained embeddings must be available for all words, which is reasonable as the embeddings can always be trained to include those words. Another technique we have been developing is to assign missing words a vector of zero. This results in them never appearing in the generated BOWs. However assuming they do appear in the language model training corpus, they can be inserted during the ordering step. If it is more likely to have that missing word, than to not. This technique showed promising results when used with the \texttt{word2vec} vectors \parencite{mikolovSkip}, which exclude common stop words like ``to''. This technique also has the potential to allow for mistakes made in the selection step, for example do to noise in the process outputting the vectors, to be corrected in during the ordering step.




\printbibliography
	
\end{document}


