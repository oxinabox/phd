\documentclass[]{scrartcl}
\usepackage[sharp]{easylist}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{pdflscape}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}

\usepackage{algorithm2e}


\usepackage[author={Lyndon White}]{pdfcomment}

\makeatletter
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
\theoremstyle{definition}
\newtheorem{defn}[thm]{\protect\definitionname}
\makeatother

\usepackage[english]{babel}
\providecommand{\definitionname}{Definition}
\providecommand{\theoremname}{Theorem}


 
\usepackage[backend=bibtex,
 style=authoryear-icomp,
 maxcitenames=2
]{biblatex}
\bibliography{../../../Resources/master_bibliography/master.bib }
\usepackage{cleveref}

%End Packages


\DeclareMathOperator*{\argmin}{argmin}


%opening
\title{Resynthesising Sentences from the Sums of their Embeddings using a Greedy Algorithm}
\author{Lyndon White}
\graphicspath{{./figs/}}
\setlength\itemsep{2mm}
\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}


\begin{figure}
	\centering 
	\includegraphics{block_diagram}
	\caption{The process for the regenerating sentences from SOWE-type sentence vectors.}
	\label{block_diagram}
\end{figure}
The task being tackled here is the resynthesise of sentences from vector embeddings.
In particular it is concerned with the sum of word embeddings type vectors.


The generation task, going from vector to sentence, is quiet challenging.
It has not received a lot of attention.



In general sentence embeddings may be described as compositional, or non-compositional.
The compositional models come from classical linguistics -- taking the hierarchical  break-down of a sentence into clauses, phrases and words, compositional vector models produce vectors by moving up the tree combining (or composing) the vectors in a substructure to find a vector for the substructure. This composition continues until the a vector has been found for the root of the sentence tree. Many successful compositional models have been developed, of particular note have been the recursive neural network based methods of  \cite{socher2010PhraseEmbedding}%,SocherEtAl2011:RAE,SocherEtAl2011:PoolRAE,Socher2011ParsingPhrases,SocherMVRNN,RvNTN,SocherEtAl2013:CVG,Socher2013TensorReasoning}. Compositional embedding models have strong basis from their linguistic underpinnings.

Non-compositional embeddings cover all other embedding techniques. Recently significant attention has come to Le and Mikolov's work \cite{le2014distributed}, which is a direct augmentation of word embedding creation techniques, with an additional paragraph or sentence vector. Other non-compositional techniques include the classic bag of words (BOW), and the simple sum of word embeddings (SOWE). Non-compositional methods draw a significant criticism for disregarding structure; BOW and SOWE draw additional criticism for also disregarding word order. However these weaknesses, while correct, may be entirely offset by the improved discrimination in other factors -- allowed for some non-compositional methods to be comparable overall. 


Recently \cite{White2015SentVecMeaning} found that classifying real word data sentences according to meaning SOWE was a very strong model. Work of Ritter et. al. presents a similar task on spacial-positional meaning, which used carefully constructed artificial data, for which it was believed only compositional models could solve, here also SOWE was found performing comparably and again even out performing the compositional models. Thus simple methods such as SOWE are still worth consideration. SOWE is the basic of the work presented in this paper.


To our knowledge only two prior works exist in this area, both concerned with compositional vector embeddings.

\cite{Dinu2014CompositionalGeneration}  extends the models described by \cite{zanzotto2010estimating, Guevara2010} for generation; those models are in turn which are in turn specific cases of the general compositional model described by Mitchell and Lapata\cite{Mitchell2008}. The compositional models used in these works are pre-neural network based techniques used in embeddings today -- though the work Dinu and Baroni does apply to some of today's compositional models in general. While it theoretically generalised to whole sentences Dinu and Baroni's work is quantitatively assessed for very short phrases.

%Dinu and Baroni \cite{Dinu2014CompositionalGeneration} focuses on describing composition as a pair of linear transformations of word vectors, followed by a sum $$f_{comp}\::\:\mathbb{R}^d\times\mathbb{R}^d \to \mathbb{R}^d\::\: (\tilde{u}, \tilde{v})\mapsto W_1\tilde{u}+W_2\tilde{v}$$ where the linear transformations $W_1$,$W_2$ are dependent on the parts of speech.

The work of Iyyer et. al. \cite{iyyer2014generating} is based on an extension of the unfolding recursive autoencoder\cite{SocherEtAl2011:PoolRAE}. It can produce sentences from vectors, if given the structure to decompose the vector into. Theoretically the unfolding recursive auto-encoder could be used in the same way to generate sentences, however that has never been shown. Recursive neural networks are jointly trained for both composing the sentence's words into a vector, and for decomposing that vector into words. This method was demonstrated quantitatively on five examples. There are not currently any large scale quantitative results published for the use of Iyyer et. al.'s technique.

Both Dinu and Baroni \cite{Dinu2014CompositionalGeneration}, and Iyyer et. al. \cite{iyyer2014generating}, require an output structure to be provided for the decomposition.
The new method proposed here, does not require, or indeed allow, structural information.



The method proposed in this paper The method takes in a SOWE-type sentence vector, and outputs the sentence which it corresponds to.
This is done in two steps, as shown in \cref{block_diagram}.
The first step is to determine which words are in the sentence -- this converts the SOWE embedding into a bag of words.
The second step is to order that bag of words into a sentence -- a language modelling task.
This paper will focus on the first part -- the word selection problem. 
Though a demonstration of the feasibly of solving the second part -- the ordering problem --  is shown in \cref{ordering}.

\section {The Selection Problem}


At the core of this problem is what we will call the Vector Selection Problem, to select which word vectors, with option of repetition, are sum to be closest to the target SOWE.
As there is a bijection between word embeddings and their words, solving for the word embeddings used also solves for the words used -- note that practice not two words are similar enough in usage in any reasonable training corpora to have the precise same embedding under current methods

\subsection{Formal Definition: }
\renewcommand{\c}{\tilde{c}}
\newcommand{\s}{\tilde{s}}
\newcommand{\x}{\tilde{x}}
\renewcommand{\t}{\tilde{t}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\V}{\mathcal{V}}

\begin{defn} {The Vector Selection Problem}
	

	The Vector Selection Problem is defined on $(\V, \s,\,d)$ for
	\begin{itemize}
		\item A finite vocabulary of vectors $\V$, $\V\subset{\R}^{n}$
		\item a target vector $ \s$, $ \s\in\R^{n}$
		\item any metric $d$ 
		
		by
		\[
		\argmin_{\left\{ \forall\tilde{c}\in\N_{0}^{V}\right\} }\:d( \s,\,\sum_{j=1}^{j=V}\:\x_{j}c_{j})
		\]
		
		\medskip{}
				
		\item $V$ is size of the vocabulary $\V$. (\textasciitilde{}1300
		for ATIS2, \textasciitilde{}50,000 for Brown, \textasciitilde{}10,000
		for daily English) 
		\item $\x_{j}$ is the vector embedding for the jth word in the vocabulary
		$\x_{j}\in\V$ 
		
		\begin{itemize}
			\item We can express the embedding vocabulary $\V=\{\x_{j}\,|\,\forall j\in\N\:\wedge\;1\le j\le V\}\subset\R^{n}$
			\item If we treat $\V$ as a matrix with vectors $\x_{j}$
			for rows,(and treat length 1 vectors as scalars) we get the compact
			notation 
			\[
			\argmin_{\left\{ \forall\tilde{c}\in\N_{0}^{J}\right\} }\:d(\s,\,\sum_{j=1}^{j=V}\:\x_{j}c_{j})=\argmin_{\left\{ \forall\tilde{c}\in\N_{0}^{J}\right\} }\:d(\s,\,\V\,\c^{T})
			\]
			
		\end{itemize}
		\item $c_{j}$ is the count of how many times the jth word in the vocabulary
		occurs. $\c\in\N_{0}^{V}$, so $c_{j}\in\N_{0}$
		\item $n$ is the dimensional of the word vectors, $n=300$ in current trials. \end{itemize}
\end{defn}

\subsection{Hardness Result}
The subset sum problem is NP-complete\cite{karp1972reducibility}. Is defined: for some set $\mathcal{S}\subset\mathbb{Z}$, does there exist $\mathcal{L}\subseteq\mathcal{S}$ such that $0=\sum_{l_i}\in L l_i$.  It is possible to reduce from any given instance of a subset sum problem to a vector selection problem. A suitable metric and a set of vectors corresponding to the elements $\mathcal{S}$ can be defined by a bijection; such that solving the vector selection problem will give the vectors corresponding to the closest subset of $\mathcal{S}$ that has the smallest sum. Thus the sum is zero than the subset sum result is true, and if not it is false. A fully detailed proof can be found in the supplementary materials. As the method details that given a solution to the vector selection problem, a subset sum solution could be created taking only linear time additional work, thus it is shown that the vector selection problem is NP-hard.


\subsection{Selection Algorithm}
The algorithm proposed here to solve the selection problem is a greedy search for a path to the targeted sum point $\s$, followed by a correction step; this process is repeated until no change is made to the path. 

\subsubsection{Greedy Addition}
	\begin{function}[H]
		\SetAlgoLined
			\KwData{the metric $d$\\the target sum $\s$\\ the vocabulary of vectors $\V$\\The current best bag of vectors $bag_c$: initially $\emptyset$}
			\KwResult{The modified $bag_c$  which sum to be as close as greedy search can get to the target $\s$, under the metric $d$}
			\Begin{
			$\t \longleftarrow \sum\limits_{x_i\in bag_c} x_i$\;
	
			\Repeat{$\neg improved$}{
				$\x^\ast \longleftarrow \argmin\limits_{x_j\in \V} d(\s, \t+\x_j) $\;
				
				\eIf{$d(\s, \t+\x^\ast)  < d(\s, \t)$}{
					$t \longleftarrow \t + \x^\ast$\;
					$bag_c \longleftarrow bag_c \cup \{\x^\ast\}$\;	
					$improved \longleftarrow true$\;
				}{
					$improved \longleftarrow false$\;
				}
			}
			\Return{$bag_c$}
		}
	\caption{GreedyAddition(). In practical implementation, the bag of vectors is represented as list of indexes into columns of the embedding vocabulary matrix}
	\end{function}

\subsubsection{Step Substitution}
	\begin{function}[H]
		\SetAlgoLined
		\KwData{the metric $d$\\the target sum $\s$\\ the vocabulary of vectors $\V$\\The current best bag of vectors $bag_c$: initially $\emptyset$}
		\KwResult{The modified $bag_c$  which sum to be as close as greedy search can get to the target $\s$, under the metric $d$}
		\Begin{
			$\t \longleftarrow \sum\limits_{x_i\in bag_c} x_i$\;
			
			\Repeat{$\neg improved$}{
				$\x^\ast \longleftarrow \argmin\limits_{x_j\in \V} d(\s, \t+\x_j) $\;
				
				\eIf{$d(\s, \t+\x^\ast)  < d(\s, \t)$}{
					$t \longleftarrow \t + \x^\ast$\;
					$bag_c \longleftarrow bag_c \cup \{\x^\ast\}$\;	
					$improved \longleftarrow true$\;
				}{
				$improved \longleftarrow false$\;
			}
		}
		\Return{$bag_c$}
	}
	\caption{(1/1)--Sub(). In practical implementation, the bag of vectors is represented as list of indexes into columns of the embedding vocabulary matrix}
\end{function}



\section{The Ordering Problem} \label{ordering}
\begin{figure}
	\centering 
	\includegraphics[scale=0.25]{markov_reordering2}
	\caption{The Markov graph of possible states in the search through trigrams for the most-likely ordering of the sentence ``Show The Cheapest Flight''.}
	\label{markov_diagram}
\end{figure}

In this section a demonstration of the feasibly of solving for the correct word order, given the output of the previous step is shown. This is a stochastic language modelling problem -- to determine the most likely sequence of words, given the constraints that the words are exactly those given by the BOW output from the previous step. The method presented here simply to show its feasibility; the development of efficient technique for use in the ordering problem is left for future work.


For purposes of this demonstration a simple Kneser-Ney smoothed trigram model \parencite{kneser1995improved} was used. The language model allows us to evaluate the probability of any given sequence of words using a Markov model.
A directed acyclic graph of states can be produced for possible sequences of words as shown in \cref{markov_diagram}. The graph is restricted, in that terminal nodes must be followed by the end state, and must also have no words from the input BOW unassigned. The task is then modelled as finding the most-likely path through the graph.
Beam search was used to search the graph. At each node in the graph only the beam width (in the results following the beam width was five) most likely transitions (using just the a signal step of the trigram) were considered. This search is not complete, nor optimal -- there may be a better ordering that requires a transition that is outside the beam width. The beam-width restriction allows the combinatorial problem to be completed in feasible time. For a BOW with $n$ words, and beam width $b$ (assuming $b<n$) the time complexity is $O(b^{n-b}b!)$, rather than $O(n!)$ for a full search of all possible orderings. Memorization is also used to avoid re-searching known branches; and branches cease to be  if their accumulated probability for the path becomes zero (potentially do to underflow on the 64-bit IEEE floating point numbers).

Even with these techniques to decrease the time taken, running the search over long sentences takes significant amount of time: on the order of 10 seconds for a sentence of length 17. For this reason the ordering step is only carried out for the BOWs corresponding to reference sentences of length 18 or less. 
 
 
Of course, for some sentences the BOW can not always be resolve to the correct ordering -- when some words can be swapped in position and still have a valid and likely sentence. For example "Show me flights from Paris to New York." is just as reasonable as  "Show me flights from New York to Paris". On the other hand often one ordering is far more likely and reasonable than the others, "The dog chased the cat." is much more often correct than "The cat chased the dog.", it is also preferable over "The cat the dog chased." though all three are grammatical. \pdfcomment{It would be good to have an a citation for how often this occurs.}
  


As the overall problem can be consider a task of translating from a vector to a natural language sentence, the BLEU  score \parencite{Papineni2002} can be used for assessing how well the output sentence matches the input reference.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Where do SOWE type embeddings come from?}
The most basic place to get a SOWE embedding is by adding up the vectors for the words in a sentence.
However that is obviously fairly uninteresting -- if the words are already known there is no need to resynthesises them. There may be some uses in this case for say compression.

Other sources are more interesting:

\begin{easylist}[itemize]
	# Averaging Sentences
	# Sampling Sentences from near to the SOWE
	# Transformations from Known Sentence SOWE
	## For example there is a known (rough) offset vector between Country names and their Capital City names
	## By adding this vector to sentences, we can transform references to countries into references to their cities
\end{easylist}


\section{Search for Bag of Words used in the Sentence}

\includepdf{./figs/algorithm_flowchart.pdf}

\begin{easylist}[itemize]
	# Method for finding the words that add up to the sentence vector -- or get as near to it as possible. See diagram
	#Addition Step:
	## First Add words to our word set, adding the one which gets us closest first
	## Til we can't find any words that we can add without moving away
	# Substitution step:
	## Consider deleting, or subsituting each word in the
	## This counters some of the greed in the addition step
	## But not always, sometimes two (or more) words might be wrong, but changing either of them alone does not move closer to target.
	# Repeat addition and substitution steps until converges
	## Can do better by also considering swapping two words
	### But this has time completity of $O(w^2 v^2)$ where $w$ is current word set size, and $v$ is vocab size.
	### And it gets word for higher order swaps
	### Still Double swaps may be worth doing. But it took about 30 seconds to solve one sentence that needed a double swap. and that is with a vocab of just 1300 words.
\end{easylist}

\subsection{Results}
\begin{easylist}[itemize]
	# Performance is near to perfect on ATIS2 and the Doctors Letters as far as getting back words, and also on ordering them (next section)
	
	# On a 10\% sample of the Brown corpus (4200 sentences),
	## Overall 68.8\% of sentences recovered all words.
	## Of the 2252 sentences from that sample with less than 20 words, 97.5\% recover all words. 
	## Of sentences with 20 or more words only 35.5\% recover all words, however these sentences have a greater problem, in that the ordering taking too long (see next section).
\end{easylist}







\subsubsection{Time Complexity}
The time complexity of the addition step is $O(t \cdot V\tau n)$
Where $t$ is the maximum number of terms (words) in a sentence that adds up to $S$.

Solving the above for $t$ is not feasible. However in practice, $t$ is equal to the length of the longest sentence allowed (Say 55 words for ATIS). Also, due to the degregation in performance of the greedy algorithm, and the time taken in the resconstruction step it has to be limitted anyway. $t$ can be treated as a small constant.

The time complexity of the swap step is $$O(tVn)$$ also.
The complexity of the double swap step is $$t^2 V^2 n^2$$

In all these complexities, the Vocabulary size $V$ dominates. and so other terms can be largely ignored.

\section{Search for Best Ordering}
\begin{easylist}[itemize]
	# Use classical Language Modelling to find probability of any string of words.
	# Using Kneser-Ney smoothed trigrams, modelled on 90\% of corpus
	## Save 10\% for test
	# Trigram has form $P(W_i\!=\!w_i \mid W_{i-1}\!=\!w_{i-1}, W_{i-2}\!=\!_{i-2})$
	## Which we will write as $P(w_i \mid w_{i-1}, w_{i-2})$
	## Have pseudo-words for start and end-states. Added before trigram language model is created.
	### Thus can always start from known states probability of 1.
	### And can use the information on words likely to start and end sentences
	# Language model is a Markov model
	## State is the last two words
	### Note this means $P(a,b|b,c)=P(a|b,c)$ as old first word in state always becomes new second word in state
	## Markov Assumption: the next state is dependent only on the current state
	##  $P(W_i \mid W_{i-1}, W_{i-2}, W_{i-3}, ...) = P(W_i \mid W_{i-1}, W_{i-2})$
	# Evaluate Markov Chains to find probability of any given ordering
	## For example:
	\begin{equation}
	\begin{split}
	P(d,c,b,a) &= P(d|c,b,a) P(c,b,a) \\
			   &= P(d|c,b) P(c,b,a) \\
			   &= P(d|c,b) P(c,b|b,a) P(b,a) \\
			   &=P(d|c,b) P(c|b,a) P(b,a)
	\end{split}
	\end{equation}
	
	# Finding all paths
	## We need to find the most likely path through the Markov Directed Acyclic Graph
	## This means finding probability of all possible paths
	## When considering multiple paths, many of the sub-probabilities will be reused. Thus Dynamic programming can/must/is be used.
	## Can save some time by removing suitable ``impossible" paths early
	### Currently defining this by any single trigram in the path having $P=0$ for what a double-precision floating point value considers zero.
	
	# Even with these efficiency's this method takes too long for more than 20 word sentences. 
	## This doesn't seem particularly problematic:
	### The finding words is more likely to fail for sentences that long
	### There are published results shown only for shorter sentences. Eg Socher's Parsing with RNNs was only shown for sentences of length up to 14
	### We can probably work out a way to solve this. More aggressive pruning. Graph partitioning. Something.

	
\end{easylist}

\subsection{Results}
It seems reasonable to evaluate the performance with BLEU score as if we were translating from numbers back to the original text. 
There 
\begin{easylist}[itemize]
	# Performance figures are for the subsets of length less than 20. 
	# It is on the reconstructed bags of words so can not out perform previous step
	
	# On ATIS2: Perfect 85\%,  BLEU >95\%
	## ATIS2 has a lot of sentences which are ambiguous as to order eg "Show me flights from Denvar to San Francisco"
	### Most language is not like this -- not this bad at least.
	
	# Doctors letters: Perfect 98\%, BLEU>99\%
	
	# Brown corpus still running.
\end{easylist}
 

\subsection{Freewords}
\begin{easylist}[itemize]
# A complication is the possibility of adding "freewords".
# These are words that can be added during the ordering step, even if they were not in the original.
# if they increase the probability of a sentence ordering, beyond one that does not have them.
# Two reasons why they are useful to the think about
## Some words are missing from the Word Embedding Vocabulary
### Eg the word \emph{to} is missing from the Google News pretrained skip-grams. 
### it is one of the most common words in ATIS2. The results reported above for ATIS2 have it inserted as a freeword
## Not all possitions in SOWE space are grammatical, some freewords can be added to form grammatical sentences 
### at the cost of potentially distorting meaning
### But if the right set of free words were chosen eg \emph{and}; this would not be an issue.
\end{easylist}


 
\newcommand{\mdiagram}[4][0.9]{%
\begin{landscape}
	\includepdf[landscape, scale=#1, offset=100 0,pagecommand={
		\thispagestyle{empty}
		\subsection{#3}
		\begin{itemize}
			\setlength{\itemsep}{-3pt}
			#4%
		\end{itemize}
	}]{./figs/#2.pdf}
\end{landscape}
}

\mdiagram[0.5]{markov_reordering1}{Show Cheap Flights}{
	\item cheap show flights $P=0.0$
	\item cheap flights show $P=0.0$
	\item show cheap flights $P=0.0$
	\item show flights cheap $P=0.0$
	\item flights cheap show $P=0.0$
	\item flights show cheap $P=0.0$
	\item Fails as this is an very poor sentence
	}
\mdiagram{markov_reordering2}{Show The Cheapest Flight}{
	\item show the cheapest flight $P=0.9994768960040612$
	\item show flight the cheapest $P=0.0005186917031990766$
	\item show cheapest flight the $P=4.412292739699182\times 10^{-6}$
	\item show cheapest the flight $P=0.0$
	\item flight show cheapest the $P=0.0$
	\item flight the show cheapest $P=0.0$
	\item ...
	}
\mdiagram{markov_reordering3}{Show All The Cheapest Flights}{
	\item show all the cheapest flights $P=0.9970072382584245$
	\item all flights show the cheapest $P=0.002394874466709871$
	\item all the flights show cheapest $P=0.0005588257639594186$
	\item flights show all the cheapest $P=3.902260703056939\times 10^{-5}$
	\item cheapest flights show all the $P=3.89038754905346\times 10^{-8}$
	\item all cheapest flights the show $P=0.0$
	\item ...
	}
\mdiagram{markov_reordering4}{Show Me All The Cheapest Flights}{
	\item show me all the cheapest flights $P=0.9976611556180363$
	\item all flights show me the cheapest $P=0.002287767676692731$
	\item flights show me all the cheapest $P=5.099115447705249\times 10^{-5}$
	\item cheapest flights show me all the $P=5.0836007018696596\times 10^{-8}$
	\item all the cheapest flights show me $P=3.4714786951415333\times 10^{-8}$
	\item show flights all cheapest the me $P=0.0$
	\item ...
	}
	
\printbibliography
	
\end{document}

