#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Subsection
Introduction: ordering a bag of words
\end_layout

\begin_layout Standard
The problem of solving for the order of an Bag of Word, 
\end_layout

\begin_layout Standard
is not intrinsically solvable.
\end_layout

\begin_layout Standard
Never the less, some attempts can be made.
\end_layout

\begin_layout Standard
In a bag of words all order information is lost.
\end_layout

\begin_layout Standard
The same (multi-)set of words can be ordered in different,
\end_layout

\begin_layout Standard
gramatically legal ways -- sometimes, but not always resulting in different
 meaning.
\end_layout

\begin_layout Standard
With that said, often there are only a few ways, order the words into a
 natural sentence.
\end_layout

\begin_layout Standard
Humans are very good at intuiting the correct order for a jumbled sentence
 -- this is a popular execise for people learning english as a second language.
\end_layout

\begin_layout Standard
The task of working out a likely order for a bag of words is related to
 the task of working out the correct order during machine translation.
\end_layout

\begin_layout Standard
Different languages have different basic orders.
\end_layout

\begin_layout Standard
For example English uses Subject Verb Object, where as Japanese uses Subject
 Object Verb.
 The difference between ordering a bag of words and reordering during translatio
n, is the facility to have a direct map.
 That is, whenever a Subject Verb Object sentence is encountered in the
 English original, it is known that the translated words in the sentence
 must be ordered into Subject Object Verb, in Japanese.
\end_layout

\begin_layout Standard
In the Ordering of the Bag of Words, this is not known, as there is no order
 to start with -- that information is already long lost.
\end_layout

\begin_layout Standard
This paper tackles the ordering problem using only tri-gram statistics.
 The approach generalises obviously to all n-gram models; but for clarity
 we only reference trigram sentence here.
\end_layout

\begin_layout Subsection
Trigram Language Models 
\end_layout

\begin_layout Standard
A trigram gives the conditional probability of the next work given the previous
 two:
\end_layout

\begin_layout Standard
that is 
\begin_inset Formula $P(W_{n}|W_{n-2},W_{n-1})$
\end_inset

.
 The proability of a particular sequence is for example 
\begin_inset Formula $P(W_{n}="Turkey"|W_{n-2}="Deep",W_{n-1}="Fried")$
\end_inset

, ie 
\begin_inset Quotes eld
\end_inset

What is the probability of the next word being 
\begin_inset Quotes eld
\end_inset

Turkey
\begin_inset Quotes erd
\end_inset

 gien the last two were 
\begin_inset Quotes eld
\end_inset

Deep Fried
\begin_inset Quotes erd
\end_inset

.
 For consiseness we will use positional semantics writing this as 
\begin_inset Formula $P("Turkey"|"Deep","Fried")$
\end_inset

, and more generally will follow the convention of lowercase for an instantated
 variable, eg 
\begin_inset Formula $P(W_{n}=w_{3}|W{}_{n-2}=w_{1},W_{n-1}=w_{2})=P(w_{3}|w_{1},w_{2})$
\end_inset

.
\end_layout

\begin_layout Standard
Following the baysian chain rule, and assuming the Markov property, 
\end_layout

\begin_layout Standard
we can use these trigrams to calculate the probaility of any sequence of
 words:
\end_layout

\begin_layout Standard
the (unnormalised) probability of a sequence of words: 
\begin_inset Formula $\mathcal{W}=[w_{1},w_{2},w_{3},w_{4}]$
\end_inset


\end_layout

\begin_layout Standard
is 
\begin_inset Formula $P(\mathcal{W})=P(w_{1},w_{2})P(w_{3}|w_{1},w_{2})P(w_{4}|w_{2},w_{3})$
\end_inset


\end_layout

\begin_layout Standard
This allows the ordering, but fails to taking advantage of what we could
 know about the likelyhood of words occuring at the start and end of the
 sentence.
\end_layout

\begin_layout Standard
We can augment our language model with the addition of pseudowords:
\end_layout

\begin_layout Standard
\begin_inset Formula $w_{S1}=\mathrm{“START1”},$
\end_inset


\begin_inset Formula $w_{S2}=\mathrm{“START2"},$
\end_inset

 and 
\begin_inset Formula $w_{E1}=\mathrm{“END1”},$
\end_inset


\begin_inset Formula $w_{E2}=\mathrm{\text{“}END2\text{”}}$
\end_inset


\end_layout

\begin_layout Standard
Which always occur at the start and end of sentences respectively.
\end_layout

\begin_layout Standard
Now we know 
\begin_inset Formula $P(w_{S1},w_{S2})=1$
\end_inset

 and 
\begin_inset Formula $P(w_{E1},w_{E2})=1$
\end_inset


\end_layout

\begin_layout Standard
We also know that 
\begin_inset Formula $w_{E2}$
\end_inset

 only and always occurs after 
\begin_inset Formula $w_{E1}$
\end_inset

 and so 
\end_layout

\begin_layout Standard
\begin_inset Formula $\forall w_{i}$
\end_inset

 
\begin_inset Formula $P(w_{E2}|w_{i},w_{Ei})=1$
\end_inset

.
\end_layout

\begin_layout Standard
And can write 
\begin_inset Formula $P(\mathcal{W})=P(w_{S1},w_{S2})P(w_{1}|w_{S1},w_{S2})P(w_{2}|w_{S2},w_{1})P(w_{3}|w_{1},w_{2})P(w_{4}|w_{2},w_{3})P(w_{E1}|w_{3},w_{4})P(w_{E2}|w_{4},w_{E1})$
\end_inset


\end_layout

\begin_layout Standard
Which given the afformantion constant probailities of 1
\end_layout

\begin_layout Standard
simplifies to 
\begin_inset Formula $P(\mathcal{W})=P(w_{1}|w_{S1},w_{S2})P(w_{2}|w_{S2},w_{1})P(w_{3}|w_{1},w_{2})P(w_{4}|w_{2},w_{3})P(w_{E1}|w_{3},w_{4})$
\end_inset


\end_layout

\begin_layout Standard
This is the basic tool for estimating the proability of any ordering.
\end_layout

\begin_layout Standard
We will assume that our trigram model for purtposes of this discussion is
 complete and can tell an accurate proability for any tripple of words.
 This is not true in general, but with the use of smoothing and back off
 techneques can be treated at true.
\end_layout

\begin_layout Subsection
On solving the word order using the language model
\end_layout

\begin_layout Standard
The language models give us a tool to describe the probability of any arbiary
 sequence of words.
\end_layout

\begin_layout Standard
So this suggests a naive way of finding the most likely ordering for the
 bag:
\end_layout

\begin_layout Standard
Enumerate all possible orderings and calculate the probability of each.
 Then choose order with the highest scoring probability.
\end_layout

\begin_layout Standard
However this is not compuationally tractable: For a bag containing 
\begin_inset Formula $N$
\end_inset

 words, there are 
\begin_inset Formula $N!$
\end_inset

 possible orderings.
\end_layout

\begin_layout Standard
Thus fiunding the best ordering naively would take 
\begin_inset Formula $O(N\cdot N!)$
\end_inset

time.
\end_layout

\begin_layout Standard
Thus the neive method must be ruled out.
 A far less naive method is to consider the problem a a graph search.
\end_layout

\begin_layout Section
The World Tour Problem
\end_layout

\begin_layout Standard
Consider first the traveling sales man: The Saleman must visit every city
 (say in a country) before returning home, and is looking to find the shortest
 route -- it terms of what order to visit the cities in.
 More general tham this is Travelling Purchaser: the purchaser has a list
 of items to aquire, add there are many shops each of which sell some of
 the items, and there is a distence between the shops, he wants to find
 which shops to visit in which order before returning home.
 This traveling sales man is a specific type of traveling purchaser, where
 there is only one shop that sells each item, and each shop only sells that
 one item -- e.g.
 the town has 1 baker who sells only bread, 1 butcher who sells only meat,
 one green groccer who sells only greens, and no general stores.
\end_layout

\begin_layout Standard
A generalisation of this (but still more specific than the Travelling purchaser
 problem) is a town with many bakers (who only sell bread), many butchers
 (who sell only meat) etc, but still no general stores.
 This problem is very simiklar to what we will call the world tour problem
\end_layout

\begin_layout Standard
The world tour problem: The travellers goal is to visit every country in
 the world, but only once; he doesn't care which airport in each country
 he visits but he must never return to the same airport again.
 Not all airports have connecting flights to all other airports (though
 we can model those as flights with infinite cost), and the price of travel
 is not always linked to the distance.
 The traveller wants to know which airports to visit minimise his cost on
 this tour.
 This problem (or slight variations) has been called: set TSP, generalized
 TSP, group TSP, One-of-a-Set TSP, Multiple Choice TSP, and Covering Salesman
 Problem.
\end_layout

\begin_layout Standard
A slight further generalistation is what we could call the World Touring
 Journey problem: The traveller knows the start and end cities, which are
 not the same, and wishes to visit every country along the way.
 This problem is the same as the word ordering probem.
\end_layout

\begin_layout Subsection
The Bag of Words ordering problem as a World Touring Journey problem
\end_layout

\begin_layout Standard
Each city is a pair of words -- the current state, the word just added and
 the word before that.
 This is writen as 
\begin_inset Formula $(w_{n-1},w_{n})$
\end_inset

, Where 
\begin_inset Formula $w_{i}$
\end_inset

 are the words to be orderd with arbiratily ascibed subscripts.
\end_layout

\begin_layout Standard
The countries are given by the sets of all cities that have a given word
 in the second position.
 So the country for word 
\begin_inset Formula $w_{j}$
\end_inset

 is given by 
\begin_inset Formula $S_{i}=\{(w_{i},w_{j})\,\mid\,w_{i}\ne w_{j}\,\wedge w_{i}\in\{START1,START2,END1\}\cup Bag\,of\,words\}$
\end_inset

.
\end_layout

\begin_layout Standard
The Start city is the 
\begin_inset Formula $(START1,START2)$
\end_inset

, the End city is the state 
\begin_inset Formula $(END1,END2)$
\end_inset

.
\end_layout

\begin_layout Standard
Each city is given by a 
\begin_inset Formula $(w_{n-2},w_{n-1})$
\end_inset

 and is allowed to transition only to cities 
\begin_inset Formula $(w_{n-1},w_{n})$
\end_inset

.
 Such transitions has cost 
\begin_inset Formula $C((w_{n-2},w_{n-1}),(w_{n-1},w_{n}))=-\log\left(P(w_{n}|w_{n-2},w_{n-1})\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The total cost, is given by the sum
\end_layout

\begin_layout Section
Case Study on Gurbodi vs GLTK for Word Ordering
\end_layout

\begin_layout Standard
The problem of how to order an unordered bag of words, occurs as part of
 a larger work on computer sentence generation.
 A bag of words is a collection of words without order, for example 
\begin_inset Quotes eld
\end_inset

today , ? hello are you how
\begin_inset Quotes erd
\end_inset

, is to be ordered into the sentence: 
\begin_inset Quotes eld
\end_inset

hello , how are you today ?
\begin_inset Quotes erd
\end_inset

.
 This problem can not always be solved to a correct solution -- many sentences
 have different legitimate orders.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Mitchell2008"

\end_inset

 gives the example of "It was not the sales manager who hit the bottle that
 day, but the office worker with the serious drinking problem." which has
 the same word content as "That day the office manager, who was drinking,
 hit the problem sales worker with a bottle, but it was not serious.".
 However , word ordering is often possible.
\end_layout

\begin_layout Standard
The problem of determining the likelyhood of any given sequence of words
 is the subject of language modeling.
 A language model is used to determine how likely one ordering is, compaired
 to another, based on the prevelence of subpatterns observed in a training
 corpus.
 For example, we know from past experience that the word 
\begin_inset Quotes eld
\end_inset

Baked
\begin_inset Quotes erd
\end_inset

 is more likely to be followed by 
\begin_inset Quotes eld
\end_inset

Potato
\begin_inset Quotes erd
\end_inset

 than it is to be followed by 
\begin_inset Quotes eld
\end_inset

Steak
\begin_inset Quotes erd
\end_inset

.
 A particular family of language models, based around this kind of fact
 are the tri-gram language models (and more generally the n-gram language
 models).
\end_layout

\begin_layout Standard
A trigram language model estimates the probability of a sequency of words
 as the product of conditional probabilties of each triple of words occuring.
 This makes use of a simplifying assumption, called a Markov assumption,
 that the probability of the current state occuring is only dependant on
 the previous state.
 In this case the state is the previous two words.
 This Markov assumption allows us to use the Baysian chain-rule to find
 the probility of any sequence, as a product of contitional proabilities.
 For example: 
\begin_inset Formula $P("Hello\,my\,\,old\,friend")=P(Hello\,,\,my)\cdot P(old\,|\,Hello,\,my)\cdot P(friend\,|\,my\,old)$
\end_inset

.
 An extension to the language is however required to allow us to make uses
 of knowledge about the begining and endings of sentences.
\end_layout

\begin_layout Standard
Sentences are often argumented with marker pseudowords to indicated the
 begining and end -- which helps us to use knowledge about what words often
 occur at the beginning or end respectively.
 For example 
\begin_inset Formula $P("Hello\,my\,friend")=P(Hello|START1,START2)\cdot P(my|START2,Hello)\cdot P(friend|Hello,my)\cdot P(END1|my,friend)\cdot P(END2|friend,END1)$
\end_inset

.
 Though this method the probability of any sequence can be estimated.
\end_layout

\begin_layout Standard
A naive method of finding the most likely order for a bag of words would
 thus be to enumerate all possible order, and evaluate them with the trigram
 language model.
 Then the most likely could be found.
 This works well for short sentences, with just a few words, but its not
 feasible for longer sentences.
 For example, there are there are over 6 quadrillan different possible orders
 for a sentence of length 18 -- which is a typical length sentence according
 to 
\begin_inset CommandInset citation
LatexCommand cite
key "BrownAnalysis"

\end_inset

.
 So the naive method will not do -- a more sensible way of search the space
 of possible orderings is required.
\end_layout

\begin_layout Standard
The word ordering problem can be expressed as, what we will call, the world
 touring journey problem -- a variation on the well known set travelling
 sales man problem.
 The world touring journey problem is as follows: On the way from my start
 city to my destination city, I wish to visit every country in the world,
 but I don't care which airports in the country I visit; though I only want
 to visit each country onces.
 The goal of the world touring journey problem is to select an ordering
 of airports that will allow the tour to be completed on the way, at the
 lowest total cost.
 Not all airports go to all other airports however.
 In the word ordering problem, trigram states replace airports, and uses
 of a word replace countries.
\end_layout

\begin_layout Standard
To model the word ordering problem as a world touring journey, the following
 notation is used:
\end_layout

\begin_layout Itemize
Each city is a pair of words -- a Markov state, consisting of word and its
 predecessor word.
 This is writen as 
\begin_inset Formula $(w_{n-1},w_{n})$
\end_inset

, Where 
\begin_inset Formula $w_{i}$
\end_inset

 are the words to be orderd with arbiratily ascibed subscripts.
\end_layout

\begin_layout Itemize
The Start city is the 
\begin_inset Formula $(START1,START2)$
\end_inset

, the End city is the state 
\begin_inset Formula $(END1,END2)$
\end_inset

.
\end_layout

\begin_layout Itemize
The countries are given by the sets of all cities that have a given word
 in the second position.
 So the country (
\begin_inset Formula $S_{j}$
\end_inset

)for word 
\begin_inset Formula $w_{j}$
\end_inset

 is given by 
\begin_inset Formula $S(w_{j})=\{(w_{i},w_{j})\,\mid\,w_{i}\ne w_{j}\,\wedge w_{i}\in\mathcal{W}\}$
\end_inset

.
 where 
\begin_inset Formula $\mathcal{W}$
\end_inset

 is the bag of words to be ordered, plus the start and end pseudowords.
\end_layout

\begin_layout Itemize
Each city is given by a 
\begin_inset Formula $(w_{n-2},w_{n-1})$
\end_inset

 and is allowed to transition only to cities 
\begin_inset Formula $(w_{n-1},w_{n})$
\end_inset

.
 Such transitions has cost 
\begin_inset Formula $C[(w_{n-2},w_{n-1}),(w_{n-1},w_{n})]=-\log\left(P(w_{n}|w_{n-2},w_{n-1})\right)$
\end_inset

.
 The expression in terms of negitive log likelyhood makes this problem linear
 costed, rather than having a product of probilitys, it is just a sum of
 there negitive log likelyhood.
\end_layout

\begin_layout Itemize
The table of transitions is given by 
\begin_inset Formula $X$
\end_inset

, where 
\begin_inset Formula $X[(w_{a},w_{b}),\,(w_{c},w_{d})]\begin{cases}
1 & if\,transition\,from\,city\,(w_{a},w_{b}),to\,city\,(w_{c},w_{d})\,occurs\\
0 & otherwise
\end{cases}$
\end_inset


\end_layout

\begin_layout Itemize
This table of tranistions is a binary variable to be optimised
\end_layout

\begin_layout Itemize
The total cost, is the sum of all costs of transitions taken.
 It is given by 
\begin_inset Formula $C_{total}(X)=\sum_{w_{a},w_{b},w_{c},w_{d}}X[(w_{a},w_{b}),\,(w_{c},w_{d})]\cdot C[(w_{a},w_{b}),\,(w_{c},w_{d})]$
\end_inset


\end_layout

\begin_layout Itemize
The probability of a particular journey (ie of a partular ordering) is thus
 given by 
\begin_inset Formula $P(X)=e^{-C_{total}(X)}$
\end_inset


\end_layout

\begin_layout Itemize
The requirements of the problem place various constraints on to X
\end_layout

\begin_deeper
\begin_layout Itemize
Markov State consitency if 
\begin_inset Formula $w_{b}\ne w_{c}$
\end_inset

 then
\begin_inset Formula $X[(w_{a},w_{b}),\,(w_{c},w_{d})]=0$
\end_inset

 
\end_layout

\begin_layout Itemize
Leave every city entered 
\begin_inset Formula $\sum_{(w_{a},w_{b})\in\mathcal{W}^{2}}X[(w_{a},w_{b}),\,(w_{i},w_{j})]=\sum_{(w_{c},w_{d})\in\mathcal{W}^{2}}X[(w_{i},w_{j}),\,(w_{c},w_{d})]$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
With the eception of the start city and end city, ie 
\begin_inset Formula $(w_{i},w_{j})\ne(START1,START2)$
\end_inset

 and 
\begin_inset Formula $(w_{i},w_{j})\ne(END1,END2)$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Itemize
Visit every country: 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "master"
options "plainnat"

\end_inset


\begin_inset space ~
\end_inset


\end_layout

\end_body
\end_document
