#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
On the Relationshop between Mixture Models and Neural Networks (v3)
\end_layout

\begin_layout Author
Lyndon White
\end_layout

\begin_layout Standard
This write-up is just to express the idea in concrete form.
\end_layout

\begin_layout Standard
It has a sketch of the primary result proof.
\end_layout

\begin_layout Standard
And the secondary results.
\end_layout

\begin_layout Standard
What it would really benifit from is naming the spaces concerned.
\end_layout

\begin_layout Standard
E.g.
 unnormalised CDF space in the primar result, 
\end_layout

\begin_layout Standard
and the integral space of the L2 function space in the secondary UAT result.
\end_layout

\begin_layout Standard
This is not a suitable draft for a journal paper.
\end_layout

\begin_layout Standard
but it is the ideas written down.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Primary Result
\end_layout

\begin_layout Standard
There are two roughly equivelent statements of the primary result.
\end_layout

\begin_layout Itemize
Nonnegativly weighted neural networks (NNWNN) are (at least) equivalent
 to the integral of unnormalized (Gaussian-like) mixture models, and this
 means it can approximate a class of functions that can be described as
 the set of unnormalised continuous cumulative probability density functions
 (CDFs) with compact supports.
\end_layout

\begin_layout Itemize
The derivative of a NNWNN , when normalized, can approximate any continuous
 probability density functions with compact supports, because it is a (Gaussian-
like) mixture model.
\end_layout

\begin_layout Standard
The former is I think the more interesting statement, particularly for a
 paper base on the theoretical side of this work, rather than the practical
 
\end_layout

\begin_layout Standard
This gives a Universal Approximation Theorem-like result for neural networks
 with non-negative weights, describing a (lower-bound) on the class of functions
 they can approximate.
\end_layout

\begin_layout Subsection
Secondary Results
\end_layout

\begin_layout Itemize
A new proof of the well known universal approximation theorem
\end_layout

\begin_layout Itemize
A novel non-parametric estimator for probability distributions
\end_layout

\begin_layout Standard
Also the side-result of a trivial method for the creation of NNWNNs
\end_layout

\begin_layout Subsection
Significance of primary result
\end_layout

\begin_layout Itemize
The primary result concerns linking two models.
\end_layout

\begin_deeper
\begin_layout Itemize
The well-known mixture models â€“ in particular models that are almost Gaussian
 mixture models (GMMs), 
\end_layout

\begin_layout Itemize
and the non-negatively weighted neural network (NNWNN).
\end_layout

\end_deeper
\begin_layout Itemize
Linking two models is great
\end_layout

\begin_deeper
\begin_layout Itemize
e.g.
 there are three separate papers linking Word2Vec to various forms of Matrix
 Factorization.
\end_layout

\begin_layout Itemize
So I think linking models is a fashionable thing to do.
\end_layout

\begin_layout Itemize
Allows knowledge about one to be applied to the other maybe.
\end_layout

\end_deeper
\begin_layout Itemize
The practical significance of the result depends on the significance of
 the later model -- the NNWNN
\end_layout

\begin_deeper
\begin_layout Itemize
I've (briefly) looked at some of the existing works in 
\begin_inset CommandInset citation
LatexCommand cite
key "lemme2010efficient"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "2014understandblenetworksnonnegweights"

\end_inset

.
\end_layout

\begin_layout Itemize
Between them they have ~3 dozen citations, some of which are other methods
 relating to the use of non-negatively weighted networks.
\end_layout

\begin_layout Itemize
From this, perhaps such NNWNNs are not currently hugely significant.
\end_layout

\end_deeper
\begin_layout Itemize
Understandable networks is a significant area
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset CommandInset citation
LatexCommand cite
key "2014understandblenetworksnonnegweights"

\end_inset

 motivates their work on non-negative neural networks as they are much more
 understandable to humans than normal networks.
\end_layout

\begin_layout Itemize
There is a push to make understandable neural networks
\end_layout

\end_deeper
\begin_layout Itemize
There are also links to non-negative matrix factorization, which is a significan
t area this is discussed in 
\begin_inset CommandInset citation
LatexCommand cite
key "lemme2010efficient"

\end_inset

.
\end_layout

\begin_layout Itemize
Finally there seems to be a sparse networks connection
\end_layout

\begin_deeper
\begin_layout Itemize
While both 
\begin_inset CommandInset citation
LatexCommand cite
key "lemme2010efficient"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "2014understandblenetworksnonnegweights"

\end_inset

 employ additional techniques to encourage sparsity
\end_layout

\begin_layout Itemize
my own observations when testing these networks (without extra sparsity
 techniques) has been that they are sparse, particularly in the higher layers.
\end_layout

\begin_layout Itemize
There are many many works on sparse neural networks
\end_layout

\end_deeper
\begin_layout Section
Notation:
\end_layout

\begin_layout Itemize
\begin_inset Formula $\nabla K=\left(\dfrac{\partial K}{\partial x_{1}},...,\dfrac{\partial K}{\partial x_{dim(x)}}\right)$
\end_inset

, 
\end_layout

\begin_deeper
\begin_layout Itemize
This is the element-wise derivatives with respect to the elements of 
\begin_inset Formula $x$
\end_inset

.
 All other partial derivatives are ignored
\end_layout

\begin_layout Itemize
\begin_inset Formula $\nabla K$
\end_inset

 may be a matrix described column-wise, or a vector; depending on 
\begin_inset Formula $K$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
The Hansard (element-wise) product notated with 
\begin_inset Formula $\odot$
\end_inset

 (odot)
\end_layout

\begin_layout Section
Primary Result: NNWNNs are able to approximate all functions that can be
 approximated by an un-normalised CDFs.
\end_layout

\begin_layout Itemize
We begin our consideration with a UAT style neural network.
 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $N(x;W,V,b)=V\varphi(Wx+b)=\left[\begin{array}{c}
{\displaystyle \sum_{j=1}^{j=dim(V,2)}}V_{1,j}\varphi(b_{j}+\sum_{\forall i}\left(W_{j,i}x_{j,i}\right))\\
\vdots\\
{\displaystyle \sum_{j=1}^{j=dim(V,2)}}V_{dim(V,1),j}\varphi(b_{j}+\sum_{\forall i}\left(W_{j,i}x_{j,i}\right))
\end{array}\right]$
\end_inset

 
\end_layout

\begin_layout Itemize
for 
\begin_inset Formula $x$
\end_inset

 the input vector, and for 
\begin_inset Formula $V$
\end_inset

 and 
\begin_inset Formula $W$
\end_inset

 non-negative weight matrices, and 
\begin_inset Formula $b$
\end_inset

 a bias vector, that may be negative
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $\varphi$
\end_inset

 a squashing function (bounded, monotonic, continuous)
\end_layout

\begin_deeper
\begin_layout Itemize
Do I need an additional constraint requiring that it is non-negative (i.e.
 ruling out 
\begin_inset Formula $\tanh$
\end_inset

).
 I don't think I actually do, no.
\end_layout

\begin_layout Itemize
Note that this does rule out RELU, and all leaky activation functions.
 And ELU and a few others that the UAT doesn't apply to either.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Consider its derivative WRT its input 
\begin_inset Formula $x$
\end_inset

,
\end_layout

\begin_deeper
\begin_layout Itemize
for 
\begin_inset Formula $g=\nabla\varphi$
\end_inset

, which is applied element-wise
\end_layout

\begin_layout Itemize
\begin_inset Formula $\nabla N=\left(V\odot W^{T}\right)g(Wx+b)=\left[\begin{array}{c}
{\displaystyle \sum_{j=1}^{j=dim(V,2)}}\left(V_{1,j}W_{j,1}\right)g(b_{j}+\sum_{\forall i}\left(W_{j,i}x_{j,i}\right))\\
\vdots\\
{\displaystyle \sum_{j=1}^{j=dim(V,2)}}\left(V_{dim(V,1),j}W_{j,dim(V,1)}\right)g(b_{j}+\sum_{\forall i}\left(W_{j,i}x_{j,i}\right))
\end{array}\right]$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Itemize
Now appreciate the value of 
\begin_inset Formula $g(x)$
\end_inset

: 
\end_layout

\begin_deeper
\begin_layout Itemize
for 
\begin_inset Formula $\varphi=\sigma$
\end_inset

, then 
\begin_inset Formula $g(x)=\sigma(-x)\sigma(x)=\frac{1}{(1+\exp(-z)(1+\exp(-z))}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Graphics
	filename sigmoid-der-plot.png
	width 5cm

\end_inset


\end_layout

\begin_layout Itemize
This is a bell-curve
\end_layout

\end_deeper
\begin_layout Itemize
For 
\begin_inset Formula $\varphi=RELU6=\begin{cases}
0 & x\le0\\
x & 0<x<6\\
6 & 6\le x
\end{cases}$
\end_inset

 then 
\begin_inset Formula $g(x)=\begin{cases}
0 & x\le0\\
1 & 0<x<6\\
0 & 6\le x
\end{cases}$
\end_inset

 IE a square function 
\end_layout

\begin_layout Itemize
As 
\begin_inset Formula $\varphi$
\end_inset

 is bounded, continuous, and monotonic, we have:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\lim_{z\to-\infty}\varphi(z)=c_{min}$
\end_inset

 and 
\begin_inset Formula $\lim_{z\to\infty}\varphi(z)=c_{max}$
\end_inset

 for some constants 
\begin_inset Formula $c_{min}$
\end_inset

 and 
\begin_inset Formula $c_{max}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula ${\displaystyle \lim_{x\to-\infty}g(x)}=0$
\end_inset

, and 
\begin_inset Formula ${\displaystyle \lim_{x\to\infty}g(x)}=0$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Intuitively notice that we now have an unnormalised mixture model of sorts
\end_layout

\begin_deeper
\begin_layout Itemize
The mixture components are these 
\begin_inset Formula $g(b_{j}+\sum_{\forall i}\left(W_{j,i}x_{j,i}\right))$
\end_inset

 curves
\end_layout

\begin_layout Itemize
They are weighted with 
\begin_inset Formula $V_{i,j}W_{j,i}$
\end_inset

 weights, 
\end_layout

\begin_layout Itemize
and while the value of 
\begin_inset Formula $W$
\end_inset

 is tied to the shape of those curves 
\begin_inset Formula $V$
\end_inset

 is not.
\end_layout

\begin_layout Itemize
As we have free choice on the value of 
\begin_inset Formula $V_{i,j}$
\end_inset

 so long as it is non-negative we can chose it such that 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $V_{i,j}=\dfrac{A_{i,j}}{W_{j,i}}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 for some non-negative 
\begin_inset Formula $A_{i,j}$
\end_inset


\end_layout

\begin_layout Itemize
Which gives us 
\begin_inset Formula $V_{i,j}W_{j,i}g(b_{j}+\sum_{\forall i}\left(W_{j,i}x_{j,i}\right))=A_{i,j}g(b_{j}+\sum_{\forall i}\left(W_{j,i}x_{j,i}\right))$
\end_inset

, which is a sensible (non-negatively) weighted mixture component.
\end_layout

\end_deeper
\begin_layout Itemize
Intuitively we have the idea that this kind of Mixture Models can approximate
 any PDF with compact support (and unnormalised ones, can approximate any
 unnormalised PDF with compact support)
\end_layout

\begin_deeper
\begin_layout Itemize
In particular by 
\begin_inset Quotes eld
\end_inset

kind of
\begin_inset Quotes erd
\end_inset

 we mean ones where the mixture components are finite area and bounded above
 and below at zero.
\end_layout

\begin_layout Itemize
\begin_inset CommandInset citation
LatexCommand cite
key "10.2307/2237880"

\end_inset

 (Equations 1.11-1.13) showed that for components with a lesser set of constraints
, were if one had one per training point, an unbiases estimator for PDFs.
\end_layout

\begin_layout Itemize
For GMMs 
\begin_inset CommandInset citation
LatexCommand cite
key "1100034"

\end_inset

draw together a collection of prior works (refs 11-16), to conclude that
 GMMs can approximate any PDF
\end_layout

\begin_deeper
\begin_layout Itemize
And further that as adding more mixture components are added, the estimation
 from the GMM improves (Uniform convergence)
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
the generalised result of theorem 1.1 
\begin_inset CommandInset citation
LatexCommand cite
key "bacharoglou2010approximation"

\end_inset

 applies if 
\begin_inset Formula $g(x)$
\end_inset

 is an approximate idenity, if normalized to unit area.
\end_layout

\begin_deeper
\begin_layout Itemize
In this context the relevant defintion of approximate identity is that for
 a convolution.
 i.e.
 a summability kernel
\end_layout

\begin_layout Itemize
Defn: we call 
\begin_inset Formula $\phi_{n}(t)$
\end_inset

 an summability kernel iff for 
\begin_inset Formula $\phi_{n}(t)$
\end_inset

 a sequence of continuous real-valued functions with the properties
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\int_{-\infty}^{\infty}\phi_{n}(t)dt=1$
\end_inset

 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
forall 
\begin_inset Formula $n>0$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\int_{-\infty}^{\infty}|\phi_{n}(t)|\,dt\le M$
\end_inset

 for some 
\begin_inset Formula $M$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula ${\displaystyle \lim_{n\to\infty}}\int_{|t|>\delta}|\phi_{n}(t)|dt=0$
\end_inset

 forall 
\begin_inset Formula $n>0$
\end_inset

 and forall 
\begin_inset Formula $\delta>0$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
The important property of all summability kernals is that 
\begin_inset Formula $\lim_{n\to\infty}\phi_{n}\ast f\to f$
\end_inset

 converging uniformly
\end_layout

\begin_deeper
\begin_layout Itemize
Here 
\begin_inset Formula $\ast$
\end_inset

 is the convolution operator
\end_layout

\begin_layout Itemize
More formally 
\begin_inset Formula $\forall f\in\mathcal{C}(\mathbb{R})$
\end_inset

 that 
\begin_inset Formula $\forall\epsilon\in\mathbb{R}\exists N\in\mathbb{R}\forall x\in\mathbb{R}$
\end_inset

 such that 
\begin_inset Formula $n>\mathbb{N}$
\end_inset

 we have 
\begin_inset Formula $\left|h_{n}\ast f(x)-f(x)\right|<\epsilon$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
We define a sequence 
\begin_inset Formula $g_{n}(x)=\dfrac{g(nx)}{\int_{-\infty}^{\infty}g(nz)dz}=\dfrac{g(nx)}{c_{max}-c_{min}}$
\end_inset

, for 
\begin_inset Formula $c_{max}$
\end_inset

 and 
\begin_inset Formula $c_{min}$
\end_inset

 the bounds of the activation function 
\begin_inset Formula $\varphi$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Note that this is a value that the terms in 
\begin_inset Formula $\nabla N$
\end_inset

 could take after normalization (or before even) if there were a unlimitted
 number of neurons.
\end_layout

\begin_deeper
\begin_layout Itemize
In paractice they may or may not take this value, but it is a value they
 are allowed to take, which is what matters for this proof of the capacity
 to approximate.
\end_layout

\begin_layout Itemize
Further more if we sorted the actual values of those terms by their 
\begin_inset Formula $W_{ij}$
\end_inset

 component (smallest to highest) then extraplated some monotonic for the
 weights after the actual values ended, we would end up with a similar proof.
 with 
\begin_inset Formula $g_{n}(x)=g(W_{nthSmallestIndex}x)$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
The first property 
\begin_inset Formula $\int_{-\infty}^{\infty}g_{n}(t)dt=1$
\end_inset

 holds as as 
\begin_inset Formula $\intop_{-\infty}^{\infty}\dfrac{g(nt)}{\int_{-\infty}^{\infty}g(nz)dz}dt=\dfrac{\intop_{-\infty}^{\infty}g(nt)dt}{\int_{-\infty}^{\infty}g(nz)dz}=1$
\end_inset


\end_layout

\begin_layout Itemize
The second holds as 
\begin_inset Formula $g_{n}(t)\ge0$
\end_inset

 as 
\begin_inset Formula $g=\nabla\varphi$
\end_inset

 and the activation function 
\begin_inset Formula $\varphi$
\end_inset

 is monotonic, thus its derivative is non-negative.
 Thus 
\begin_inset Formula $|g_{n}(t)|=g_{n}(t)$
\end_inset

, thus 
\begin_inset Formula $\int_{-\infty}^{\infty}|g_{n}(t)|dt=1$
\end_inset


\end_layout

\begin_layout Itemize
The final requirement is measuring the area of the tails: 
\begin_inset Formula ${\displaystyle \lim_{n\to\infty}}\int_{|t|>\delta}|g_{n}(t)|dt=0$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\int_{|t|>\delta}|g_{n}(t)|dt=\int_{\delta}^{\infty}|g_{n}(t)|dt+\int_{-\infty}^{-\delta}|g_{n}(t)|dt$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $=\dfrac{1}{c_{max}-c_{min}}\left(\int_{\delta}^{\infty}g(nt)dt+\int_{-\infty}^{-\delta}g(nt)dt\right)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $=\dfrac{1}{c_{max}-c_{min}}\left(\left(\left(\lim_{z\to\infty}\varphi(nz)\right)-\varphi(n\delta)\right)+\left(\varphi(-n\delta)\right)-\left(\lim_{z\to-\infty}\varphi(nz)\right)\right)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $=\dfrac{1}{c_{max}-c_{min}}\left(\left(c_{max}-\varphi(n\delta)\right)+\left(\varphi(-n\delta)\right)-c_{min}\right)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $=\dfrac{c_{max}-c_{min}+\left(\varphi(-n\delta)-\varphi(n\delta)\right)}{c_{max}-c_{min}}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $=1+\dfrac{\left(\varphi(-n\delta)-\varphi(n\delta)\right)}{c_{max}-c_{min}}$
\end_inset


\end_layout

\begin_layout Itemize
And so taking the limit:
\end_layout

\begin_layout Itemize
\begin_inset Formula ${\displaystyle \lim_{n\to\infty}}1+\dfrac{\left(\varphi(-n\delta)-\varphi(n\delta)\right)}{c_{max}-c_{min}}=1+\dfrac{\left(c_{min}-c_{max}\right)}{c_{max}-c_{min}}=1-1=0$
\end_inset


\end_layout

\begin_layout Itemize
Which Was What Was Wanted
\end_layout

\end_deeper
\begin_layout Itemize
Thus for all continuous squashing activation functions(bounded & monotonic)
 
\begin_inset Formula $g_{n}$
\end_inset

 is a summability kernel, i.e.
 a approximate identity.
\end_layout

\begin_layout Itemize
For noncontinous squashing functions we don't have a rule that says they
 make summability kernels with convolution giving the uniform convergence
 property
\end_layout

\begin_deeper
\begin_layout Itemize
But I have found that at least in the case of RELU1 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:RELU1"

\end_inset

 with convolution we have uniform convergence to bounded functions, and
 (at least) point-wise convergence to unbounded functions.
\end_layout

\begin_layout Itemize
This is enough for the proof I believe, since all continuous PDFs are bounded.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
This result would give us convergence 
\begin_inset Formula $L^{1}$
\end_inset

 and 
\begin_inset Formula $L^{\infty}$
\end_inset

 to any PDF with compact support.
 (and thus to any unnormalised PDF if we didn't normalize the components)
\end_layout

\begin_layout Itemize
I am going to rewrite the proof of theorem 1.1 
\begin_inset CommandInset citation
LatexCommand cite
key "bacharoglou2010approximation"

\end_inset

 to use the generalised result, as it is written for gaussians only.
 (the generalisation is just a side comment in that paper)
\end_layout

\begin_layout Itemize
Finally, since we have been working with the derivative of the network,
 we apply the fundamental theorem of integration.
 This changes
\end_layout

\begin_deeper
\begin_layout Itemize
The derivatives of a NNWNN can estimate any unnormalised PDF with compact
 support.
 To
\end_layout

\begin_layout Itemize
a NNWNN can estimate any unnormalised CDF with compact support.
 
\end_layout

\end_deeper
\begin_layout Itemize
This is probably a pretty wide range of functions, and might have a name.
\end_layout

\begin_layout Itemize
the generalised result of theorem 1.1 
\begin_inset CommandInset citation
LatexCommand cite
key "bacharoglou2010approximation"

\end_inset

 
\end_layout

\begin_layout Section
Secondary Result 1: A new proof of the well known universal approximation
 theorem
\end_layout

\begin_layout Standard
The universal approximation theorem was proven in the early 90s.
 (citations here).
\end_layout

\begin_layout Standard
We can derive a new proof of it which is kind of cute, at least for approximatin
g all L2 functions 
\end_layout

\begin_layout Itemize
We start with a 2 NNWNN networks.
 which we will call 
\begin_inset Formula $N_{1}$
\end_inset

 and 
\begin_inset Formula $N_{2}$
\end_inset


\end_layout

\begin_layout Itemize
Both 
\begin_inset Formula $\nabla N_{1}$
\end_inset

 and 
\begin_inset Formula $\nabla N_{2}$
\end_inset

 are able to approximate an unnormalised GMM
\end_layout

\begin_layout Itemize
We define a new neural network 
\begin_inset Formula $M$
\end_inset

, implicitly by 
\begin_inset Formula $\nabla M=\nabla N_{1}-\nabla N_{2}$
\end_inset

 
\end_layout

\begin_layout Itemize
\begin_inset Formula $\nabla M$
\end_inset

 can be any linear combination of Gaussian
\end_layout

\begin_layout Itemize
According to 
\begin_inset CommandInset citation
LatexCommand cite
key "calcaterra2008linear"

\end_inset

, a linear combination of Gaussian exists to approximate every 
\begin_inset Formula $L^{2}$
\end_inset

 function
\end_layout

\begin_layout Itemize
We take the integral of 
\begin_inset Formula $\nabla M$
\end_inset

 and we end up with some other space of functions, which is probably very
 general (I'm thinking it is the space of all continuous functions)
\end_layout

\begin_layout Section
Secondary Result 2: A novel non-parametric estimator for probability distributio
ns
\end_layout

\begin_layout Standard
We can use the results that 
\begin_inset Quotes eld
\end_inset

The derivatives of a NNWNN can estimate any unnormalised PDF with compact
 support.
\begin_inset Quotes erd
\end_inset

,
\end_layout

\begin_layout Standard
to construct an estimator for any PDF with compact support.
\end_layout

\begin_layout Standard
WE just need to normalize them.
\end_layout

\begin_layout Standard
This is the derivative of the work of 
\begin_inset CommandInset citation
LatexCommand cite
key "likas2001probability"

\end_inset

.
\end_layout

\begin_layout Standard
We remove the need to preform numerical integration at training time (slow,
 and according to Likas' does not scale to higher dimensions).
\end_layout

\begin_layout Standard
In the current stage of this work, it also does not need the pretraining
 method Likas used to fit it to a Parzen window model before proper training
 starts,
\end_layout

\begin_layout Standard
though it does benefit from having the network pretrained (I used the term
 
\begin_inset Quotes eld
\end_inset

conditioned
\begin_inset Quotes erd
\end_inset

 in my code) to make the boundaries of the support set to be marginally
 distant from each other.
\end_layout

\begin_layout Standard
It also seems to benefit from an exponential final layer on the network
 before taking the derivative, rather than an affine/linear final layer.
\end_layout

\begin_layout Standard
We know that use the PDF is given by 
\begin_inset Formula 
\[
f_{h}(x,p)=\dfrac{h(x,p)}{\int_{S}h(z,p)dz}
\]

\end_inset

In the case of Likas case 
\begin_inset Formula $h=N(x,p)$
\end_inset

 a neural network with weight and bias parameters elements of 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $S$
\end_inset

 is a compact support, or an approximation to that.
\end_layout

\begin_layout Standard
We say 
\begin_inset Formula $h=\frac{\partial N(x,p)}{\partial x}$
\end_inset

,
\end_layout

\begin_layout Standard
Giving us 
\begin_inset Formula 
\[
f_{dn}(x,p)=\dfrac{h(x,p)}{\int_{S}h(z,p)}=\dfrac{\frac{\partial N(x,p)}{\partial x}}{N(max(S),p)-N(min(S),p)}
\]

\end_inset


\end_layout

\begin_layout Standard
The denominator is of-course more complex for non-1D values of S.
\end_layout

\begin_layout Standard
The loss function given is the negative log-likelihood of the set of training
 samples X 
\begin_inset Formula 
\[
L(p)=-\sum_{\forall x\in X}ln(h(x,p))+|X|ln(\int_{S}h(z,p)dx)
\]

\end_inset


\end_layout

\begin_layout Standard
Which for our variant becomes:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L(p)=-\sum_{\forall x\in X}log(\frac{\partial N(x,p)}{\partial x})+|X|(ln(N(max(S),p)-N(min(S),p))dx
\]

\end_inset


\end_layout

\begin_layout Standard
Even with resorting to the sketch of the proof of the capacity of the NNWNN,
 given above,
\end_layout

\begin_layout Standard
we know that for 
\begin_inset Formula $N$
\end_inset

 being a NNWNN, that at very least 
\begin_inset Formula $f_{dn}(x,p)$
\end_inset

 is a valid PDF, as
\end_layout

\begin_layout Itemize
It is non-negative, as the only negative components in the derivative of
 the neural network are the product of the weights, which in a NNWNN are
 of-course non-negative
\end_layout

\begin_layout Itemize
and it has unit area, due to the normalization in the denominator
\end_layout

\begin_layout Standard
An example of the use of this can be found at:
\begin_inset CommandInset href
LatexCommand href
name "https://github.com/oxinabox/DensityEstimationML.jl/blob/7ff88e5510038e2aa1563616108694a171808487/proto/Max_of_logprob_of_derivative.ipynb"
target "https://github.com/oxinabox/DensityEstimationML.jl/blob/7ff88e5510038e2aa1563616108694a171808487/proto/Max_of_logprob_of_derivative.ipynb"

\end_inset


\end_layout

\begin_layout Standard
I personally don't feel like it is fitting very well, it is very GMM-like
 in its curve fitting.
\end_layout

\begin_layout Standard
I also think it might be biased in some sense (not nessicarily the statistical
 sense) to be a-symtrical, the Arcsine curve is constantly higher in the
 left peak than the right peak
\end_layout

\begin_layout Standard
It is however getting as-good or better Likelihood results than Likas was
 reporting.
\end_layout

\begin_layout Standard
Though this may be a poor measure, vs instead using say likelihood of a
 hold-out set, or using a measure of curve fit to the true PDF.
\end_layout

\begin_layout Section
Side-result: A Trivial Method for the Creation of NNWNN
\end_layout

\begin_layout Standard
To ensure a neural network has only non-negative weights several approaches
 have been used.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "lemme2010efficient"

\end_inset

 encourages non-negative weights via a regularization style component added
 to the loss â€“ adding a penalty to the network for having negative weights,
 thus encouraging it to gradient descend away from them.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "2014understandblenetworksnonnegweights"

\end_inset

 makes use of a constrained variation of gradient decent; adding a non-negativit
y constraint.
\end_layout

\begin_layout Standard
I propose a much simpler solution:
\end_layout

\begin_layout Standard
Consider a neural network of the standard UAT form: 
\begin_inset Formula $N(x;W,V,b)=V\varphi(Wx+b)$
\end_inset

.
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $\varphi$
\end_inset

 a squashing function (Bounded, monotonic, continuous), with 
\begin_inset Formula $V$
\end_inset

 and 
\begin_inset Formula $W$
\end_inset

 non-negative.
\end_layout

\begin_layout Standard
We use an alternative representation for the network that ensures that the
 weights are non-negative.
\end_layout

\begin_layout Standard
\begin_inset Formula $N(x;W,V,b)=\left(\hat{V}\right)^{2}\varphi(\left(\hat{W}\right)^{2}x+b)$
\end_inset


\end_layout

\begin_layout Standard
we simply element-wise square the weights matrices.
\end_layout

\begin_layout Standard
It is readily apparent that for any element-wise non-negative matrix
\begin_inset Formula $W\in\mathbb{R}_{+}^{n\times m}$
\end_inset

, there exists a matrix 
\begin_inset Formula $\hat{W}\in\mathbb{R}^{n\times m}$
\end_inset

 such that each element is the square of its corresponding element i.e.
 
\begin_inset Formula $W_{i,j}=\left(\hat{W}_{i,j}\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
There are in-fact two possible elements for each 
\begin_inset Formula $\hat{W}_{i,j}=\pm\sqrt{W_{i,j}}$
\end_inset

 (except at 
\begin_inset Formula $W_{i,i}=0$
\end_inset

).
\end_layout

\begin_layout Standard
Since these elements exists, there is no (additional) loss of representational
 capacity of 
\begin_inset Formula $N$
\end_inset

 from using this change.
 
\end_layout

\begin_layout Standard
There may be additional complications in practical training of such a network,
 due to adding an additional nonlinear term to the 
\begin_inset Formula $\frac{\partial N}{\partial W}$
\end_inset

 used in gradient descent, however this was already a nonlinear optimization
 procedure and modern optimization algorithms (e.g.
 ADAM) seem to handle it without issue.
\end_layout

\begin_layout Standard
This method seems much simpler than that used in 
\begin_inset CommandInset citation
LatexCommand cite
key "lemme2010efficient"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "2014understandblenetworksnonnegweights"

\end_inset

.
\end_layout

\begin_layout Section
Conclusion & Possible Publication Venue
\end_layout

\begin_layout Standard
Presented here is a connection of the link between a restricted form of
 neural net: the NNWNN and Mixture Models.
\end_layout

\begin_layout Standard
I think it is pretty interesting.
\end_layout

\begin_layout Standard
There are probably other interesting conclusions that can be drawn from
 it.
\end_layout

\begin_layout Standard
This perhaps may be already a known thing?
\end_layout

\begin_layout Standard
The CDF of a Gaussian after-all is well known to be sigmoidal function.
\end_layout

\begin_layout Standard
But NNWNNs seem like a fairly new thing.
\end_layout

\begin_layout Standard
I still have a fair bit more reading to do around them,
\end_layout

\begin_layout Standard
but I think this is a good contribution to this area.
\end_layout

\begin_layout Subsection
Possible venue: ICLR?
\end_layout

\begin_layout Standard
ICLR 2018, I think is the next conference this would be good at, 
\end_layout

\begin_layout Standard
and it has a very special rebuttal and review format.
\end_layout

\begin_layout Standard
But deadline is relatively soonâ€“ 27th of October.
\end_layout

\begin_layout Standard
It might be worth putting out part of this work there, just to get some
 feedback if the area is good.
\end_layout

\begin_layout Standard
I can't seem to find an impact factor for ICLR, but it is Bengio & LeCunn's
 conference so top notch ML conference.
\end_layout

\begin_layout Standard
But it has a weird weird process.
\end_layout

\begin_layout Subsubsection
Similar works submitted to ICLR:
\end_layout

\begin_layout Itemize
\begin_inset CommandInset href
LatexCommand href
name "https://openreview.net/forum?id=SyZprb5xg"
target "https://openreview.net/forum?id=SyZprb5xg"

\end_inset

 â€“ accepted to workshop track
\end_layout

\begin_layout Section
Bonus Math
\end_layout

\begin_layout Standard
\begin_inset Formula $ $
\end_inset


\end_layout

\begin_layout Subsection
Convolution with an pseudo-summability kernal Based on RELU1
\begin_inset CommandInset label
LatexCommand label
name "subsec:RELU1"

\end_inset


\end_layout

\begin_layout Standard
consider 
\begin_inset Formula $\varphi_{n}(x)=RELU1(nx)=\begin{cases}
0 & nx\le0\\
nx & 0<nx\le1\\
1 & 1<nx
\end{cases}=\begin{cases}
0 & x\le0\\
nx & 0<x\le\frac{1}{n}\\
1 & \frac{1}{n}<x
\end{cases}$
\end_inset


\end_layout

\begin_layout Standard
Consider 
\begin_inset Formula $g_{n}(x)=\dfrac{d(\varphi(x))}{dx}=\begin{cases}
n & 0<x\le\frac{1}{n}\\
0 & otherwise
\end{cases}$
\end_inset


\end_layout

\begin_layout Standard
Not a true summability kernal as has a jump discontinuality each end.
\end_layout

\begin_layout Standard
Integrates to 1: 
\begin_inset Formula $\int_{-\infty}^{\infty}g_{n}(x)\,dx=\int_{0}^{\frac{1}{n}}ndx=\left[nx\right]_{0}^{\frac{1}{n}}=1-0=1$
\end_inset


\end_layout

\begin_layout Standard
Always non negative so 
\begin_inset Formula $\int_{-\infty}^{\infty}|g_{n}(x)|\,dx=\int_{-\infty}^{\infty}g_{n}(x)\,dx=1$
\end_inset


\end_layout

\begin_layout Standard
the final requirement
\end_layout

\begin_layout Standard
\begin_inset Formula $\lim_{n\to\infty}\int_{|x|\ge\delta}|g_{n}(x)|\,dx=\lim_{n\to\infty}\int_{|x|\le\delta}g_{n}(x)\,dx$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $=\lim_{n\to\infty}\int_{\delta}^{\infty}g_{n}(x)dx+\int{}_{-\infty}^{-\delta}g_{n}(x)dx$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $=\lim_{n\to\infty}\int_{\delta}^{\infty}g_{n}(x)dx+0$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $=\lim_{n\to\infty}\int_{\delta}^{\infty}\begin{cases}
n & 0<x\le\frac{1}{n}\\
0 & otherwise
\end{cases}dx$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $=\lim_{n\to\infty}\left[\begin{cases}
nx & 0<x\le\frac{1}{n}\\
0 & otherwise
\end{cases}\right]_{\delta}^{\infty}dx$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $=\lim_{n\to\infty}0-\begin{cases}
n\delta & 0<\delta\le\frac{1}{n}\\
0 & otherwise
\end{cases}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $=\lim_{n\to\infty}0$
\end_inset


\end_layout

\begin_layout Subsubsection
Check trivial example
\end_layout

\begin_layout Standard
Consider 
\begin_inset Formula $f(x)=x$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\left|g_{n}\ast f(t)-f(t)\right|=\left|\int_{0}^{\frac{1}{n}}(t-\tau)g(\tau)d\tau-t\right|$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula $=\left|t\int_{0}^{\frac{1}{n}}g(\tau)d\tau-\int_{0}^{\frac{1}{n}}(\tau)g(\tau)d\tau-t\right|$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $=\left|t-t-\int_{0}^{\frac{1}{n}}(\tau)g(\tau)d\tau\right|$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $=\left|\left[\tau\int g_{n}(\tau)d\tau-\int\int g_{n}(\tau)d\tau d\tau\right]_{0}^{\frac{1}{n}}\right|$
\end_inset

 Integraiuon by parts
\end_layout

\begin_layout Standard
\begin_inset Formula $=\left|\left[\tau\int nd\tau-\int\int nd\tau d\tau\right]_{0}^{\frac{1}{n}}\right|$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $=\left|\left[\tau\tau n-\int\tau nd\tau\right]_{0}^{\frac{1}{n}}\right|$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $=\left|\left[\tau^{2}n-\frac{\tau^{2}n}{2}\right]_{0}^{\frac{1}{n}}\right|$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $=\left|\frac{1}{n}-\frac{1}{2n}\right|$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $=\frac{1}{2n}$
\end_inset


\end_layout

\begin_layout Standard
So it works for this case
\end_layout

\begin_layout Subsubsection
Prove uniform convergance
\end_layout

\begin_layout Standard
\begin_inset Formula $\left|g_{n}\ast f(t)-f(t)\right|=\left|\int_{0}^{\frac{1}{n}}f(t-\tau)g(\tau)d\tau-f(t)\right|$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $=\left|\left[f(t-\tau)\int g(\tau)d\tau-\int\dfrac{d(f(t-\tau)}{d\tau}\int g(\tau)d\tau d\tau\right]_{0}^{\frac{1}{n}}-f(t)\right|$
\end_inset

 apply integration by parts
\end_layout

\begin_layout Standard
\begin_inset Formula $=\left|\left[f(t-\tau)n\tau-\int\dfrac{d(f(t-\tau)}{d\tau}n\tau d\tau\right]_{0}^{\frac{1}{n}}-f(t)\right|$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\le\left|f(t-\frac{1}{n})-f(t)-\left[\int\dfrac{d(f(t-\tau)}{d\tau}n\tau d\tau\right]_{0}^{\frac{1}{n}}\right|$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\le\left|f(t-\frac{1}{n})-f(t)\right|+\left|\left[\int\dfrac{d(f(t-\tau)}{d\tau}n\tau d\tau\right]_{0}^{\frac{1}{n}}\right|$
\end_inset

 Triangle inequality
\end_layout

\begin_layout Standard
\begin_inset Formula $\le\left|f(t-\frac{1}{n})-f(t)\right|+\left|\left[\int\dfrac{d(f(t-\tau)}{d\tau}d\tau\right]_{0}^{\frac{1}{n}}\right|\left|\left[\int n\tau d\tau\right]_{0}^{\frac{1}{n}}\right|$
\end_inset

Holder's inequality
\end_layout

\begin_layout Standard
\begin_inset Formula $\le\left|f(t-\frac{1}{n})-f(t)\right|+\left|\left[\int\dfrac{d(f(t-\tau)}{d\tau}d\tau\right]_{0}^{\frac{1}{n}}\right|\left|\left[\dfrac{n\tau^{2}}{2}\right]_{0}^{\frac{1}{n}}\right|$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\le\left|f(t-\frac{1}{n})-f(t)\right|+\left|\left[f(t-\tau)\right]_{0}^{\frac{1}{n}}\right|\left|\dfrac{1}{2n}\right|$
\end_inset

Fundermental theorem of calculus (Abuse?)
\end_layout

\begin_layout Standard
\begin_inset Formula $\le\left|f(t-\frac{1}{n})-f(t)\right|+\left|\dfrac{f(t-\frac{1}{n})}{2n}\right|$
\end_inset


\end_layout

\begin_layout Standard
As 
\begin_inset Formula $f$
\end_inset

 is continuous the first term is unformly converent
\end_layout

\begin_layout Standard
It has some maximum gradient 
\end_layout

\begin_layout Standard
With the second: If the range of 
\begin_inset Formula $f(t)$
\end_inset

 is bounded then 
\begin_inset Formula $f(t-\frac{1}{n})<\sup f$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\left|g_{n}\ast f(t)-f(t)\right|\le\left|f(t-\frac{1}{n})-f(t)\right|+\left|\dfrac{\sup f}{2n}\right|$
\end_inset


\end_layout

\begin_layout Standard
Which lets us say that it is uniformly convergent
\end_layout

\begin_layout Standard
If not then we still have pointwise convergence.
\end_layout

\begin_layout Subsection
Convolution with a general pseudo-summability kernal 
\end_layout

\begin_layout Standard
for some pseudosummability kernal 
\begin_inset Formula $g_{n}$
\end_inset

, which is meets the 3 requirements of summability kernals, but is noncontinous,
 with a jump discontinuality to zero at 
\begin_inset Formula $\alpha_{1}(n)$
\end_inset

 and 
\begin_inset Formula $\alpha_{2}(n)$
\end_inset

 
\end_layout

\begin_layout Standard
This is based heavily on proof of theorem 3 from 
\begin_inset CommandInset href
LatexCommand href
name "https://bondmatt.files.wordpress.com/2009/09/weierstrass2-01.pdf"
target "https://bondmatt.files.wordpress.com/2009/09/weierstrass2-01.pdf"

\end_inset

, though that uses a different set of definitions.
\end_layout

\begin_layout Standard
\begin_inset Formula $\left|f\ast g_{n}(x)-f(x)\right|=\left|\int_{-\infty}^{\infty}f(x-t)g_{n}(t)dt-f(x)\right|$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $=\left|\int_{-\infty}^{\infty}f(x-t)g_{n}(t)dt-f(x)\int_{-\infty}^{\infty}g_{n}(t)dt\right|$
\end_inset

 as 
\begin_inset Formula $\int_{-\infty}^{\infty}g_{n}(t)dt=1$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $=\left|\int_{-\infty}^{\infty}f(x-t)g_{n}(t)-f(x)g_{n}(t)dt\right|$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $=\left|\int_{-\infty}^{\infty}\left(f(x-t)-f(x)\right)g_{n}(t)dt\right|$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\le\int_{-\infty}^{\infty}\left|f(x-t)-f(x)\right|\left|g_{n}(t)\right|dt$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\le\int_{-\delta}^{\delta}\left|f(x-t)-f(x)\right|\left|g_{n}(t)\right|dt+\int_{\left|t\right|>\delta}\left|f(x-t)-f(x)\right|\left|g_{n}(t)\right|dt$
\end_inset

 this holds for 
\begin_inset Formula $\forall\delta\in\mathbb{R}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\le\int_{-\delta}^{\delta}\left|f(x-t)-f(x)\right|dt\int_{-\delta}^{\delta}\left|g_{n}(t)\right|dt+\int_{\left|t\right|>\delta}\left|f(x-t)-f(x)\right|dt\int_{|t|>\delta}\left|g_{n}(t)\right|dt$
\end_inset

 by Holder's inequality
\end_layout

\begin_layout Standard
Choose 
\begin_inset Formula $\delta>max(\left|\alpha_{1}(n)\right|,\left|\alpha_{2}(n)\right|)$
\end_inset

 thus 
\begin_inset Formula $\int_{|t|>\delta}\left|g_{n}(t)\right|dt=0$
\end_inset

 and 
\begin_inset Formula $\int_{-\delta}^{\delta}\left|g_{n}(t)\right|dt=M$
\end_inset

 (second property)
\end_layout

\begin_layout Standard
\begin_inset Formula $\le M\int_{-\delta}^{\delta}\left|f(x-t)-f(x)\right|dt$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula $\le M\int_{-\delta}^{\delta}\left|f(x-t)\right|dt-M\left|f(x)\right|$
\end_inset


\end_layout

\begin_layout Standard
.Not Finished
\end_layout

\begin_layout Standard
.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "master"
options "apalike"

\end_inset


\end_layout

\end_body
\end_document
