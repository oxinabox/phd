#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
On the Relationshop between Mixture Models and Neural Networks
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Primary Result
\end_layout

\begin_layout Standard
There are two roughly equivelent statements of the primary result.
\end_layout

\begin_layout Itemize
Nonnegativly weighted neural networks (NNWNN) are (at least) equivalent
 to the integral of unnormalized (guassian-like) mixture models, and this
 means it can approximate a class of functions that can be described as
 the set of unnormalised continuous cumulative probability density functions
 (CDFs) with compact supports.
\end_layout

\begin_layout Itemize
The dervivitive of a NNWNN , when normalized, can approximate any continuous
 probability density functions with compact supports, because it is a (guassian-
like) mixture model.
\end_layout

\begin_layout Standard
The former is I think the more interesting statement, particularly for a
 paper base on the theoretical side of this work, rather than the practical
 
\end_layout

\begin_layout Standard
This gives a Universal Approximation Theorem-like result for neural networks
 with non-negative weights, describing a class of functions they can approximate.
\end_layout

\begin_layout Subsection
Secondary Results
\end_layout

\begin_layout Itemize
A new proof of the well known universal approximation theorem
\end_layout

\begin_layout Itemize
A novel nonparametric estimator for probability distributions
\end_layout

\begin_layout Standard
Also the side-result of a trivial method for the creation of non-negatively
 weighted neural networks.
\end_layout

\begin_layout Subsection
Significance of primary result
\end_layout

\begin_layout Itemize
The primary result concerns linking two models.
\end_layout

\begin_deeper
\begin_layout Itemize
The well-known mixture models – in particular models that are almost gaussian
 mixture models (GMMs), 
\end_layout

\begin_layout Itemize
and the non-negatively weighted neural network (NNWN).
\end_layout

\end_deeper
\begin_layout Itemize
Linking two models is great
\end_layout

\begin_deeper
\begin_layout Itemize
e.g.
 there are three seperate papers linking Word2Vec to various forms of Matrix
 Factorisation.
\end_layout

\begin_layout Itemize
So I think linking models is a fashionable thing to do.
\end_layout

\begin_layout Itemize
Allows knowledge about one to be applied to the other maybe.
\end_layout

\end_deeper
\begin_layout Itemize
The practical significance of the result depends on the significance of
 the later model -- the NNWNN
\end_layout

\begin_deeper
\begin_layout Itemize
I've (briefly) looked at some of the existing works in 
\begin_inset CommandInset citation
LatexCommand cite
key "lemme2010efficient"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "2014understandblenetworksnonnegweights"

\end_inset

.
\end_layout

\begin_layout Itemize
Between them they have ~3 dozen citations, some of which are other methods
 relating to the use of non-negatively weighted networks.
\end_layout

\begin_layout Itemize
From this, perhaps such NNWNNs are not currently hugely signifiant.
\end_layout

\end_deeper
\begin_layout Itemize
Understandable networks is a significant area
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset CommandInset citation
LatexCommand cite
key "2014understandblenetworksnonnegweights"

\end_inset

 motivates their work on non-negativ neural netoworks as they are much more
 understandable to humans than normal networks.
\end_layout

\begin_layout Itemize
There is a push to make understandable neural networks
\end_layout

\end_deeper
\begin_layout Itemize
There are also links to non-negative matrix factorisation, which is a significan
t area this is discussed in 
\begin_inset CommandInset citation
LatexCommand cite
key "lemme2010efficient"

\end_inset

.
\end_layout

\begin_layout Itemize
Finally there seems to be a sparse networks connection
\end_layout

\begin_deeper
\begin_layout Itemize
While both 
\begin_inset CommandInset citation
LatexCommand cite
key "lemme2010efficient"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "2014understandblenetworksnonnegweights"

\end_inset

 employ additional techniques to encourage sparsity
\end_layout

\begin_layout Itemize
my own observations when testing these networks (without extra sparsity
 techniques) has been that they are sparse, particularly in the higher layers.
\end_layout

\begin_layout Itemize
There are many many works on sparse neural networks
\end_layout

\end_deeper
\begin_layout Section
Primary Result: NNWNNs are able to approximate all functions that can be
 approximated by an unnormalised CDFs.
\end_layout

\begin_layout Itemize
We begin our consideration with a UAT style neural network.
 : 
\begin_inset Formula $N(x;W,V,b)=V\varphi(Wx+b)$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Itemize
for 
\begin_inset Formula $x$
\end_inset

 the input vector, and for 
\begin_inset Formula $V$
\end_inset

 and 
\begin_inset Formula $W$
\end_inset

 non-negative weight matricies, and 
\begin_inset Formula $b$
\end_inset

 a bias vector, that may be negative
\end_layout

\begin_layout Itemize
For 
\begin_inset Formula $\varphi$
\end_inset

 a squashing function (Bounded, monotonic, continuous), and with the additional
 constraint that it is a nonnegative function (eg 
\begin_inset Formula $\sigma$
\end_inset

 not 
\begin_inset Formula $\tanh$
\end_inset

)
\end_layout

\end_deeper
\begin_layout Section
Side-result: A Trivial Method for the Creation of NNWNN
\end_layout

\begin_layout Standard
To ensure a neural network has only non-negative weights several approaches
 have been used.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "lemme2010efficient"

\end_inset

 encourages non-negative weights via a regularization style component added
 to the loss – adding a penalty to the network for having negative weights,
 thus encouraging it to gradient descend away from them.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "2014understandblenetworksnonnegweights"

\end_inset

 makes use of a constrained variation of gradient decent; adding a non-negativit
y constraint.
\end_layout

\begin_layout Standard
I propose a much simpler solution:
\end_layout

\begin_layout Standard
Consider a neural network of the standard UAT form: 
\begin_inset Formula $N(x;W,V,b)=V\varphi(Wx+b)$
\end_inset

.
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $\varphi$
\end_inset

 a squashing function (Bounded, monotonic, continuous), with 
\begin_inset Formula $V$
\end_inset

 and 
\begin_inset Formula $W$
\end_inset

 non-negative.
\end_layout

\begin_layout Standard
We use an alternative representation for the network that ensures that the
 weights are non-negative.
\end_layout

\begin_layout Standard
\begin_inset Formula $N(x;W,V,b)=\left(\hat{V}\right)^{2}\varphi(\left(\hat{W}\right)^{2}x+b)$
\end_inset


\end_layout

\begin_layout Standard
we simply element-wise square the weights matrices.
\end_layout

\begin_layout Standard
It is readily apparent that for any element-wise non-negative matrix
\begin_inset Formula $W\in\mathbb{R}_{+}^{n\times m}$
\end_inset

, there exists a matrix 
\begin_inset Formula $\hat{W}\in\mathbb{R}^{n\times m}$
\end_inset

 such that each element is the square of its corresponding element i.e.
 
\begin_inset Formula $W_{i,j}=\left(\hat{W}_{i,j}\right)^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
There are in-fact two possible elements for each 
\begin_inset Formula $\hat{W}_{i,j}=\pm\sqrt{W_{i,j}}$
\end_inset

 (except at 
\begin_inset Formula $W_{i,i}=0$
\end_inset

).
\end_layout

\begin_layout Standard
Since these elements exists, there is no (additional) loss of representational
 capacity of 
\begin_inset Formula $N$
\end_inset

 from using this change.
 
\end_layout

\begin_layout Standard
There may be additional complications in practical training of such a network,
 due to adding an additional nonlinear term to the 
\begin_inset Formula $\frac{\partial N}{\partial W}$
\end_inset

 used in gradient descent, however this was already a nonlinear optimization
 procedure and modern optimization algorithms (eg ADAM) seem to handle it
 without issue.
\end_layout

\begin_layout Standard
This method seems much simpler than that used in 
\begin_inset CommandInset citation
LatexCommand cite
key "lemme2010efficient"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "2014understandblenetworksnonnegweights"

\end_inset

.
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "master"
options "apalike"

\end_inset


\end_layout

\end_body
\end_document
