#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.5cm
\topmargin 2.5cm
\rightmargin 1.5cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Explaining Derivative of Likas' Method
\end_layout

\begin_layout Subsection
Defining a PDF
\end_layout

\begin_layout Standard
The requirements for a PDF are:
\end_layout

\begin_layout Enumerate
Continuous 
\end_layout

\begin_layout Enumerate
Non-negative
\end_layout

\begin_layout Enumerate
Integrates over its domain to 1
\end_layout

\begin_layout Standard
The equivalent requirements on a CDF are:
\end_layout

\begin_layout Enumerate
Differentiable
\end_layout

\begin_layout Enumerate
Monotonically increasing.
 (A monotonic function always must have a non-negative gradient, which is
 equivalent to saying the the pdf must be non-negative)
\end_layout

\begin_layout Enumerate
Limits at low end of domain of 0, and high end of 1.
\end_layout

\begin_layout Subsection
Expressing a PDF
\end_layout

\begin_layout Standard
We begin with the notion that any PDF, can be written as 
\begin_inset Formula 
\[
f(x)=\frac{h(x)}{\int_{S}h(z)dz}
\]

\end_inset


\end_layout

\begin_layout Standard
for some suitably chosen continuous non-negative function 
\begin_inset Formula $h(x)$
\end_inset

.
\end_layout

\begin_layout Standard
This meets the requirements of a PDF:
\end_layout

\begin_layout Enumerate
Continuous: 
\begin_inset Formula $h(x)$
\end_inset

 itself is continuous.
 Dividing by its integral is dividing by a constant, ergo Ergo 
\begin_inset Formula $f(x)$
\end_inset

is positive and continuous.
\end_layout

\begin_layout Enumerate
non-negative 
\begin_inset Formula $h(x)$
\end_inset

 itself is non-negative.
 The integral of a non-negative function is always a positive constant.
 Ergo 
\begin_inset Formula $f(x)$
\end_inset

is positive.
\end_layout

\begin_layout Enumerate
Integrates to 1.
 
\begin_inset Formula $\int_{S}f(x)dx=\int_{S}\frac{h(x)}{\int_{S}h(z)dz}dx=\dfrac{1}{\int_{S}h(z)dz}\times\int_{S}h(x)dx=1$
\end_inset

 (can move denominator to outside as it is constant wrt 
\begin_inset Formula $x$
\end_inset

)
\end_layout

\begin_layout Standard
The obvious and direct training method is the minimisation of the negative
 loglikelyhood.
\end_layout

\begin_layout Standard
that is for some training set 
\begin_inset Formula $X$
\end_inset

,
\end_layout

\begin_layout Standard
minimizing :
\end_layout

\begin_layout Standard
\begin_inset Formula $L=\sum_{x\in X}-\log\frac{h(x)}{\int_{S}h(z)dz}$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula $L=|X|\log(\int_{S}h(z)dz)-\sum_{x\in X}\log h(x)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $L=|X|\log(H(max(S)-H(min(S))h-\sum_{x\in X}\log h(x)$
\end_inset


\end_layout

\begin_layout Subsection
Likas 2001 method
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "likas2001probability"

\end_inset

 
\end_layout

\begin_layout Standard
takes the approach of using 
\begin_inset Formula $h(x)=N(x,\tilde{p})$
\end_inset

, for 
\begin_inset Formula $N(x,\tilde{p})$
\end_inset

 being the output from 
\begin_inset Formula $x$
\end_inset

 for some neural network with weight and bias parameters 
\begin_inset Formula $\tilde{p}$
\end_inset

.
\end_layout

\begin_layout Standard
The PDF is given by 
\begin_inset Formula $f(x)=\frac{h(x)}{\int_{S}h(z)dz}=\frac{N(x,\tilde{p})}{\int_{S}N(z,\tilde{p})dz}$
\end_inset


\end_layout

\begin_layout Standard
With the network being given by:
\end_layout

\begin_layout Standard
\begin_inset Formula $z_{0}=x$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $z_{i}=\varphi(W_{i}z_{i-1}+b_{i})$
\end_inset

 for 
\begin_inset Formula $1\le i<n$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $z_{n}=\exp(W_{n}z_{n-1})$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $z_{n}=N(x,\tilde{p})\approx h(x)$
\end_inset


\end_layout

\begin_layout Standard
This meets the requirements for 
\begin_inset Formula $h$
\end_inset

:
\end_layout

\begin_layout Enumerate
Continuous neural networks are always continuous
\end_layout

\begin_layout Enumerate
Non-negative: from the final layer: 
\begin_inset Formula $\forall z\in\mathbb{R}$
\end_inset

 
\begin_inset Formula $e^{z}>0$
\end_inset


\end_layout

\begin_layout Enumerate
Able to represent any function subject to the above constraints: yes, the
 hidden layer 
\begin_inset Formula $z_{n-1}$
\end_inset

 meets the requirements for the Universal Approximation Theorem (UAT), thus
 
\begin_inset Formula $z_{n-1}$
\end_inset

 is an arbitrary continuous function appropriator on any compact subset
 of 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

.
 This means for any 
\begin_inset Formula $h(x)$
\end_inset

, it can be made such that 
\begin_inset Formula $z_{n}(x)\approx h(x)$
\end_inset

, when 
\begin_inset Formula $z_{n-1}(x)\approx log(h(x))$
\end_inset

, which by the UAT can be done.
\end_layout

\begin_layout Standard
The issue Likas raises with there method is that a neural network does not
 have an analytically derived form for its integral wrt its input.
\end_layout

\begin_layout Standard
(This is not strictly true: it is relatively trivial to show that a neural
 network with 1 hidden layer, and a linear output has a closed form anti-derivat
ive.
 But once you add another nonlinear layer, e.g.
 the exponential output layer, or a second hidden-layer things become really
 tricky, and in particular I think it is actually provable via Liouville's
 theorem in Differential Algebra, but that math is a bit beyond me right
 now.)
\end_layout

\begin_layout Standard
Thus Likas resorts to numerical integration.
\end_layout

\begin_layout Standard
Numerical integration is relatively slow -- Gaussian quadrature with the
 Golub-Welsch algorithm is 
\begin_inset Formula $O(n^{2})$
\end_inset

 where 
\begin_inset Formula $n$
\end_inset

 is the number of points in the integration grid.
\end_layout

\begin_layout Standard
This must be repeated every training iteration when the weights are adjusted.
\end_layout

\begin_layout Subsection
My solution.
 Version 1.
\end_layout

\begin_layout Standard
Consider instead if 
\begin_inset Formula $h(x)=\dfrac{\partial N(x,\tilde{p})}{\partial x}$
\end_inset


\end_layout

\begin_layout Standard
While networks are not integrable analytically, they are differentiable
 analytically see e.g.
 
\begin_inset CommandInset citation
LatexCommand cite
key "lagaris1998artificial"

\end_inset

.
\end_layout

\begin_layout Standard
Then the PDF is to be given by 
\begin_inset Formula $f(x)=\frac{h(x)}{\int_{S}h(z)dz}=\frac{\frac{\partial N(x,\tilde{p})}{\partial x}}{N(max(S),\tilde{p})-N(min(S),\tilde{p})}$
\end_inset


\end_layout

\begin_layout Standard
Does it meet the requirements on 
\begin_inset Formula $h$
\end_inset

?
\end_layout

\begin_layout Enumerate
Continuous: yes neural networks are differentiable everywhere.
\end_layout

\begin_layout Enumerate
Non-negative: 
\series bold
No, not necessarily
\series default
, in brief 
\begin_inset Formula $\dfrac{\partial N(x,\tilde{p})}{\partial x}=\left(\prod_{i=1}^{i=n-1}W_{i}\right)g(x)$
\end_inset

 for some function 
\begin_inset Formula $g(x)$
\end_inset

 that I don't particularly care to define (it is something moderately long
 involving 
\begin_inset Formula $\dfrac{d\varphi(x)}{d(x)}$
\end_inset

 , but that 
\series bold
is
\series default
 non-negative (it is something like 
\begin_inset Formula $g(x)=N(x,\tilde{p})\prod_{i=1}^{i=n-1}\dfrac{1-\sigma(W_{i}x+b)}{\sigma(W_{i}x+b)}$
\end_inset

).
 
\end_layout

\begin_layout Enumerate
Able to represent any function subject to the above constraints: Yes.
 via the UAT again.
 Consider that if a network is able to represent any continuous function,
 then for any function 
\begin_inset Formula $g(x)$
\end_inset

 a network can approximate a function 
\begin_inset Formula $G(x)$
\end_inset

 that has an appropriate derivative 
\begin_inset Formula $G(x)\approx N(x,p)$
\end_inset

 such that 
\begin_inset Formula $\dfrac{\partial N(x,\tilde{p})}{\partial x}\approx\dfrac{\partial G(x)}{\partial x}=g$
\end_inset

.
 (I think something mathematical can be said about of the image of the derivativ
e of the set of all continuous functions is dense in the space of continuous
 functions?).
\end_layout

\begin_layout Standard
So the failure is in point two.
 The network derivative is not always non-negative.
\end_layout

\begin_layout Standard
However, we can train for that using say an error term, similar to the monotonic
 constraint used in 
\begin_inset CommandInset citation
LatexCommand cite
key "1998NNpdfDiffCdf"

\end_inset

.
\end_layout

\begin_layout Standard
While when constrain 2 is being met, via training the function 
\end_layout

\begin_layout Standard
This however is fiddly.
\end_layout

\begin_layout Standard
Made more fiddly by directly using the negative log-likelihood as the loss
 function
\end_layout

\begin_layout Standard
because if the function if the derivative becomes negative, then the log
 stops being defined.
\end_layout

\begin_layout Standard
The loss function is:
\end_layout

\begin_layout Standard
\begin_inset Formula $L=|X|\log(H(max(S)-H(min(S))h-\sum_{x\in X}\log h(x)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $L=|X|\log(N(max(S),\tilde{p})-N(min(S),\tilde{p})-\sum_{x\in X}\log\dfrac{\partial N(x,\tilde{p})}{\partial x}$
\end_inset


\end_layout

\begin_layout Subsection
My Solution.
 Version 2.
\end_layout

\begin_layout Standard
We note that the source of the negative values in 
\begin_inset Formula $\dfrac{\partial N(x,\tilde{p})}{\partial x}$
\end_inset

 is the 
\begin_inset Formula $\left(\prod_{i=1}^{i=n-1}W_{i}\right)$
\end_inset

 terms.
\end_layout

\begin_layout Standard
If our network was instead:
\end_layout

\begin_layout Standard
\begin_inset Formula $z_{0}=x$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $z_{i}=\varphi(r\left(W_{i}\right)z_{i-1}+b_{i})$
\end_inset

 for 
\begin_inset Formula $1\le i<n$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $z_{n}=\exp(r\left(W_{n}\right)z_{n-1})$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $z_{n}\approx h(x)$
\end_inset


\end_layout

\begin_layout Standard
Then 
\begin_inset Formula $\dfrac{\partial N(x,\tilde{p})}{\partial x}=\left(\prod_{i=1}^{i=n-1}r\left(W_{i}\right)\right)g(x)$
\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $r:\mathbb{R}\to\left\{ x\in\mathbb{R}\mid x\ge0\right\} $
\end_inset

is a function that always returns a positive value, and is applied element-wise
 to matrices.
 
\end_layout

\begin_layout Standard
I've tried 
\begin_inset Formula $r(w)=w^{2}$
\end_inset

, and this seems to work (I can match/exceed the performance of Likas 2001
 on their first 2 example problems, though I have not tried with the very
 small sized network they used.
 And I've not yet written the generalization to n-dimensions, or to conditional
 PDFs).
\end_layout

\begin_layout Standard
But 
\begin_inset Formula $r$
\end_inset

 could be one of a number of functions, though I expect there is some advantage
 to them being differentiable everywhere (also there is no gradient vanishing/ex
ploding effect to worry about here.) 
\end_layout

\begin_layout Standard
This solves requirement 2 -- non-negative.
\end_layout

\begin_layout Standard
But the question is, has it broken requirement 3?
\end_layout

\begin_layout Subsubsection
Is a network without negative weights valid as a function appropriator?
\end_layout

\begin_layout Paragraph
Prior works
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "2014understandblenetworksnonnegweights"

\end_inset

 considers the case of a classifier with nonnegative weights.
\end_layout

\begin_layout Standard
In the appendix proves that the classifier still can (in theory) have the
 shattering property (which I understand to be the property that with the
 right set of weights can assign arbitrary class to each input.) 
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "2014understandblenetworksnonnegweights"

\end_inset

 achieve the non-negativity via different means than just squaring it, they
 do some form of constrained optimization.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "lemme2010efficient"

\end_inset

 uses non-negative weights in an autoencoder.
 Here they encourage the weights to be non-negative via an asymeterical
 regularization penalty added to the loss.
\end_layout

\begin_layout Subsection
Math
\end_layout

\begin_layout Standard
The form of 
\begin_inset Formula $\frac{\partial N(x,\tilde{p})}{\partial x}$
\end_inset

 a network with exponential final layer, and sigmoid hidden layer and 3
 hidden units is:
\end_layout

\begin_layout Standard
(Without the squaring of the 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $V$
\end_inset

)
\end_layout

\begin_layout Standard
\begin_inset Formula $\frac{\partial N(x,\tilde{p})}{\partial x}=\mathrm{exp}((\frac{V_{11}}{(1+\mathrm{exp}(((b_{1}+(x*W_{11})))))}+\frac{V_{12}}{(1+\mathrm{exp}(((b_{2}+(x*W_{21})))))}+\frac{V_{13}}{(1+\mathrm{exp}(((b_{3}+(x*W_{31})))))}))(\frac{(\mathrm{exp}(((b_{1}+(x*W_{11}))))*W_{11}*V_{11})}{(1+\mathrm{exp}(((b_{1}+(x*W_{11})))))^{2}}+\frac{(\mathrm{exp}(((b_{2}+(x*W_{21}))))*W_{21}*V_{12})}{(1+\mathrm{exp}(((b_{2}+(x*W_{21})))))^{2}}+\frac{(\mathrm{exp}(((b_{3}+(x*W_{31}))))*W_{31}*V_{13})}{(1+\mathrm{exp}(((b_{3}+(x*W_{31})))))^{2}}))$
\end_inset


\end_layout

\begin_layout Standard
i.e.
 
\end_layout

\begin_layout Standard
for 
\begin_inset Formula $N(x,\tilde{p})=\exp(V\sigma(Wx+b)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\frac{\partial N(x,\tilde{p})}{\partial x}=\mathrm{exp}\left(V\sigma(-Wx-b)\right)(\sigma(Wx+b)\sigma(-Wx-b)\odot WV)$
\end_inset

 -- aligning element terms might be wrong here.
 very fiddly
\end_layout

\begin_layout Standard
Without the final expodentiation one would have 
\end_layout

\begin_layout Standard
\begin_inset Formula $\frac{\partial N(x,\tilde{p})}{\partial x}=(\sigma(Wx+b)\sigma(-Wx-b)\odot WV)=\dfrac{exp(Wx+b)}{\left(1+exp(Wx+b)\right)^{2}}\odot WV$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\frac{\partial N(x,\tilde{p})}{\partial x}\le WV\odot exp(Wx-b)$
\end_inset


\end_layout

\begin_layout Standard
I think it is possible to go from this to a proof similar to that used in
 
\begin_inset CommandInset citation
LatexCommand cite
key "10.2307/2237880"

\end_inset

.
\end_layout

\begin_layout Standard
Which would end up showing the derivative of a neural network is some transform
 of an unbiased estimator for a proxy of its train data.
\end_layout

\begin_layout Subsection
Pretraining
\end_layout

\begin_layout Standard
Likas 2001 pretrained the network by first training some other non-parametric
 probability estimator 
\begin_inset Formula $\hat{p}$
\end_inset

 to estimate the distribution (in particular they us pazen windows), then
 training 
\begin_inset Formula $N(y_{i}\tilde{,p})$
\end_inset

 to approximate equal 
\begin_inset Formula $\hat{p}(y_{i})$
\end_inset

 for 
\begin_inset Formula $y_{i}$
\end_inset

 from their set of grid points, using a sum of squared errors loss.
\end_layout

\begin_layout Standard
Note that this is not quiet the same as training their PDF to imitate 
\begin_inset Formula $\hat{p}$
\end_inset

, as 
\begin_inset Formula $N(y_{i}\tilde{,p})$
\end_inset

 is just their numerator part (
\begin_inset Formula $h(x)$
\end_inset

).
\end_layout

\begin_layout Standard
I have not implemented this step.
 It seems rather time consuming as a process, and kinda cheating.
\end_layout

\begin_layout Standard
The motivation for this is they want to ensure the network learns to be
 zero for outside the support.
\end_layout

\begin_layout Standard
I have a simpler method for achieving this goal, which I have applied in
 both version 1 and version 2 of my modification.
\end_layout

\begin_layout Standard
I train (also using sum squared errors loss), 
\begin_inset Formula $N(min(S),\tilde{p})=1$
\end_inset

 and 
\begin_inset Formula $N(max(S),\tilde{p})=2$
\end_inset

.
 though the exact numbers don't matter.
\end_layout

\begin_layout Standard
A key effect is to spread out the values of the networks output, that the
 support covers.
\end_layout

\begin_layout Standard
With just random small Gaussian initialization 
\begin_inset Formula $N(min(S),\tilde{p})\approx N(max(S),\tilde{p})$
\end_inset

, which causes the denominator of 
\begin_inset Formula $f(x)=\frac{h(x)}{\int_{S}h(z)dz}=\frac{\frac{\partial N(x,\tilde{p})}{\partial x}}{N(max(S),\tilde{p})-N(min(S),\tilde{p})}$
\end_inset

 to be very small.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "master"
options "IEEEtran"

\end_inset


\end_layout

\end_body
\end_document
