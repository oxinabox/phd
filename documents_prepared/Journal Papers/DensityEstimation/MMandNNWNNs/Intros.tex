\documentclass[]{article}

\begin{document}

\section{Intro Version A: Domain Prior}
Incorporating domain expert  knowledge into neural networks is an important task.
Regardless of accuracy, no blackbox machine learning system will every be deployed if its predictions disagree with the domain expert.
Further to this, incorporating such knowledge can increase resistance to noise in the training data, and reduce over-fitting.
Most domain expert knowledge is in the form of uninformative priors.
A common class of which is is a monotonic relationship between the input and the output.
If all other variables are held the same, then one expects that:
Older cars will be cheaper.
The more someone smokes, the more likely they are to develop lung cancer.
The next generation of computers will be faster/cheaper than the current one.
Even if the data disagrees, it may be a modelling decision to consider this as a flaw in the data collection, or as noise around a more central truth that the model is to reflect. 

These monotonic relationships also occur as a mathematical requirement of of some functions one would like to estimate with a neural network.
A cumulative density function (CDF) must be monotonically increasing by definition.
If one is using a neural network, to estimate a CDF then it will not be valid if it is not monotonically increasing.
On the theoretical basis, even a poorly trained non-parametric estimator should be able to get that basic fact right.
Monotonic relationships are common and it is important to be able to enforce them in models.


Conversely, if a neural network model were to be describe only as a monotonic relationship between hidden layers (latent) features, which are in turn described monotonically


The core of this paper, is the mathematical proof that non-negatively weighted neural nets, can approximate any monotonic and bounded function.

That NNWNN has such a property, may seem obvious.
With some thought, one can conclude that the upper bound on the class of functions a NNWNN can approximate is monotonic.
In that it is a sum of monotonic functions, and without the hidden weights being allowed to be negative it can never be reflected horizontally, and without negative output weights, it can not be reflected vertically thus it must remain in the same direction (See XYZ for full detail), and thus at best NNWNN can approximate monotonic functions.
And from the below one might think that since they are a subset of normal neural networks which can approximate any continuous function, then thus they must be able to approximate any monotonic function.
However, that direct line of reasoning does not hold water -- this is a form of the falacy of drawing an affirmative conclusion from a negative premiss.
It may, for example, be that NNWNN could only represent a small subset of all monotonic functions.
We prove this not to be the case in REF YZX.
Thus confirming the intuition that NNWNN are universal approximators for bounded monotonic functions. 
 

\end{document}
