\documentclass{article} % For LaTeX2e
\usepackage{iclr2018_conference,times}

\usepackage[author={Lyndon White}]{pdfcomment}

\usepackage{amsmath, amsthm, amssymb, mathtools}

% Referencing

\usepackage{natbib}
\newcommand{\textcite}{\cite}
\newcommand{\parencite}{\citep}

\usepackage{hyperref}
\usepackage{url}
\usepackage{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Math


\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{defn}{Definition}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{rem}{Remark}

\newcommand{\R}{\mathbb{R}}
\newcommand{\dlim}{\displaystyle\lim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%&




\title{On Nonnegatively Weighted Neural Networks and Mixture Models}



\begin{document}
\maketitle

\begin{abstract}

\end{abstract}


\section{Introduction}

\section{Our Results}

\begin{thm}
	Nonnegativly weighted neural networks (NNWNN) are (at least) equivalent to the integral of unnormalized (Gaussian-like) mixture models, and this means it can approximate a class of functions that can be described as the set of unnormalised continuous cumulative probability density functions (CDFs) with compact supports.
\end{thm}

\section{Related Work}

\section{Preliminaries}

\begin{defn}{Squashing Function}
A squashing function is a function $\varphi\:\R \to \R$, that is monotonic, and is bounded above and below.
Many traditional normal neural network activation functions, including the logistic function (sigmoid), $\tanh$, RELU6, are squashing functions.
Other more recent activation functions are not, e.g. RELU, ELU, SELU, and varieties of "leaky" activation functions.
\end{defn}

\begin{defn}{Summability Kernel}
A summability kernel, or approximate identity, is a sequence of continuous real valued functions $\phi_n$ for $n>0$, with the following properties:
\begin{enumerate}
	\item $\forall n>0$, $\int_{-\infty}^{\infty}\phi_{n}(t)\,dt=1$
	\item $\forall n>0\, \exists M \in \R$ such that $\int_{-\infty}^{\infty}|\phi_{n}(t)|\,dt\le M$
	\item $\forall \delta > 0$, ${\displaystyle \lim_{n\to\infty}}\int_{|t|>\delta}|\phi_{n}(t)|\,dt = 0$
\end{enumerate}
\end{defn}

\begin{lem}\label{lem:squashderivissum}
	The derivative of any continuous squashing function gives rise to a summability kernel.
	For any squashing function $\varphi(z)$,
	we define a sequence of functions from scaling its derivative 
    $g \coloneqq \frac{\partial\varphi(z)}{\partial z}$, as follows:
	\begin{equation}
	g_n(z) \coloneqq \frac{g(k_n z)}{k_n\left( c_{max} - c_{min} \right)} 
	\end{equation}
	 where $c_{min}=\lim_{z \to -\infty} \varphi{z}$ and $c_{max}=\lim_{z \to \infty} \varphi{z}$,
	 and $k_n$ is the $n$th element of some increasing sequence of real numbers.
	This sequence is always a summability kernel, for any continuous squashing function.
	
	\pdfcomment{Issue: not all squashing functions I care about are continuous, eg RELU6, but I know this works anyway, see notes.
		See https://math.stackexchange.com/questions/2448729}

\end{lem}
\begin{proof}
For the first property:
\begin{align}
	\int_{-\infty}^{\infty} g_n(z) \, dz &=  \int_{-\infty}^{\infty} \frac{g(nz)}{k_n\left( c_{max} - c_{min} \right)} \, dz \\
	&= \frac{1}{c_{max} - c_{min}} \int_{-\infty}^{\infty} \frac{g(k_n z)}{n} \, dz \\
	&= \frac{\dlim_{t \to \infty} \varphi(k_n z) - \dlim_{t \to -\infty} \varphi(k_n z)}{c_{max} - c_{min}} && \text{(Fundermental Thm of Calculus)}\\
	&= \frac{c_{max} - c_{min}}{c_{max} - c_{min}}  && \text{(as $n>0$)}\\
	&= 1	
\end{align}
	
For the second property: as  $\varphi$ is monotonic, its derivative $g(z)$ is non-negative for all $z$. Thus for $|g_n(z)|=g_n(z)$ and the first property proof applies.

For the third property:
\begin{align}
	\lim_{n\to\infty} \int_{z>\delta} |g_n(z)|\, dz 
	 &= \lim_{n\to\infty} \int_{z>\delta} \frac{g(k_n z)}{n\left( c_{max} - c_{min} \right)} \\
	 &= \lim_{n\to\infty} \frac{1}{c_{max} - c_{min}} 
		 \left( \int_{-\infty}^{-\delta} \frac{g(k_n z)}{n} \, dz 
		 + \int_\delta^\infty \frac{g(k_n z)}{n} \, dz \right) \\
	 &= \lim_{n\to\infty} \frac{
	 	\left(\varphi(-k_n\delta)-\dlim_{t \to -\infty} \varphi(k_n z)\right)
	 	 + \left( \varphi(k_n\delta) - \dlim_{t \to \infty} \varphi(k_n z)\right)}%
	 	 {c_{max} - c_{min}} \\
	 &= \lim_{n\to\infty} \frac{
	 	\left(\varphi(-k_n\delta)-c_{min} \right)
	 	+ \left( \varphi(k_n\delta) - c_{max} \right)}%
	    {c_{max} - c_{min}} \\
	 &= \frac{
	 	\left(c_{min}-c_{min} \right)
	 	+ \left(c_{max} - c_{max} \right)}%
		{c_{max} - c_{min}} \\
	 &= 0
\end{align}

\end{proof}

\begin{lem}\label{lem:approxwithsummability}
For every continuous non-negative function with compact support $f$,
for all points in its domain $x$
then for any $\epsilon >0$ there exists an element $\phi_n$ from from a summability kernel $\lbrace \phi_i  \rbrace_{i>0}$, and a sequence of translation terms $c_1,\ldots,c_M$ and positive weights $b_1,\ldots,b_M$ with $\sum_{i=1}^{i=M} b_i = 1$ such that:


\begin{align}
\left\Vert f(x)-\sum_{i=1}^{i=M}b_{i}\phi_{n}(x-c_i)\right\Vert _{1}<\epsilon &&
 \text{and} &&
\left\Vert f(x)-\sum_{i=1}^{i=M}b_{i}\phi_{n}(x-c_i)\right\Vert _{\infty}<\epsilon
\end{align}

\end{lem}

This proof is based on a similar proof for Gaussian density functions which appears as Lemma 2.2 of \textcite{bacharoglou2010approximation}.
It relies on the uniform convergence of the convolution of a summability kernel and a continuous function to the continuous function, and the uniform convergence of the Riemann sum to an integral.
Another notionally similar (if significantly more dense) proof also appears as that for Theorem 3.1 of \textcite{NESTORIDIS20111783}.


\begin{proof}
	$\forall p \in \lbrace 1,\infty \rbrace \forall x  \forall \epsilon_1 \exists N_1>0 \forall n_1>N_1$ we have
	\begin{align}
	\left\| f \ast \phi_{n_1}(x) - f(x) \right\|_p &< \epsilon_1 && \text{approximate identity property} \\
	\left\| \int_{-\infty}^\infty f(t)\phi_{n_1}(x-t) \, dt - f(x)\right\|_p &< \epsilon_1 && \text{definition of convolution} \\
	\left\| \int_{t \in supp(f)]} f(t)\phi_{n_1}(x-t) \, dt - f(x)\right\|_p &< \epsilon_1 && \text{compact support of $f$}
	\end{align}
	
	By the use of a Riemann sum, over slices $V_1,\ldots, V_M$ covering the support of $f$, each containing one of the points $t_1,...,t_m$ respectively,
	we have for $|V_i|$ being the area of slice $V_i$ then
	$\forall \epsilon_2 \exists N_2>0 \forall n_2>N_2$ it is such that 
	\begin{align}
		\left\| f \ast \phi_{n_1}(x) - \sum^{n_2}_{i=1} f(t_i)|V_i|\phi_{n_1}(x-t_i) \right\|_p &< \epsilon_2 && \text{Riemann sum} 
	\end{align}
	
	Combining these and using $N > N_1 + N_2$ $n>N$ then by the triangle inequality we have:
	\begin{align}
	\left\| f(x) - \sum^{n}_{i=1} f(t_i)|V_i|\phi_n(x-t_i) \right\|_p &<  &&  \\
	\left\| f \ast \phi_n(x) - \sum^{n}_{i=1} f(t_i)|V_i|\phi_n(x-t_i) \right\|_p 
	+  \left\| f \ast \phi_n(x) - f(x) \right\|_p &< \epsilon_1 + \epsilon_2 
	\end{align}
	
	As $|V_i|$ as a volumes is greater than zero, and $f(t_i) \ge 0$ as $f$ is a non-negative function, we thus re-express the above using $b_i=f(t_i)|V_i|$.
	Also setting $c_i=t_i$, $\epsilon=\epsilon_1+\epsilon_2$.
	
	Thus 
	\begin{align}
	\left\| f(x) - \sum^{n}_{i=1} b_i \phi_n(x-c_i) \right\|_p &<  \epsilon
	\end{align}

\end{proof}


\section{Proof of Theorem 1}

We begin by considering some NNWNN with a single hidden layer, and an affine output layer.
For $\varphi$ being a squashing function, with weight matrixes $W$ and $V$ and bias vectors $b$ and $c$.
As it is a NNWNN $W_{i,j} \ge 0$ and $V_{i,j} \ge 0$.

For the case of a single output, single input network, with $n_h$ neurons in the hidden layer the network is described by
\begin{equation}
N(x;W,V,b,c) = \sum_{j=1}^{j=n_h} V_{1,j} \varphi (b_j+W_{j,1}x_1) + c_1
\end{equation}

The for the case with $n_i$ inputs and $n_o$ outputs this becomes:

\begin{equation}
N(x;W,V,b,c)=V\varphi(Wx+b)=\left[\begin{array}{c}
{\displaystyle \sum_{j=1}^{j=n_{h}}}V_{1,j}\varphi\left(b_{j}+\sum_{i=1}^{i=n_{i}}\varphi\left(W_{j,i}x_{i}\right)\right) + c_1\\
\vdots\\
{\displaystyle \sum_{j=1}^{j=n_{h}}}V_{n_{o},j}\varphi\left(b_{j}+\sum_{i=1}^{i=n_{i}}\varphi\left(W_{j,i}x_{i}\right)\right) +c_{n_o}
\end{array}\right]
\end{equation}

For the remained of this proof we will work with the single input single output case.
The generalisation to multi-input multi-output is straight forward, if verbose (to simply for the reader considering this generalisation, we will leave the subscripts for singleton dimensions in).

We consider the network's derivative with respect to its input $x$.

\begin{equation}
\frac{\partial N(x;W,V,b,c)}{\partial x_1} = \sum_{j=1}^{j=n_h} V_{1,j}W_{j,1} \frac{\partial(\varphi) (b_j+W_{j,1}x_1)}{\partial x_1}
\end{equation}

We can re-express the value of $V_{1,j}$ as $V_{1,j}=\frac{\alpha_{1,j}}{W_{j,1}^2(c_{max}-c_{min})}$,
for $c_{min}$ and $c_{max}$ the lower and upper bounds of the squashing function ($\varphi$) respectively.
Note that $\alpha_{1,j}$ is non-negative, as a consequence of $V_{1,j}$ and $W_{j,1}$ being non-negative.

We also re-express $b_j$ as $b_j=\frac{\beta_{j,1}}{W_{j,1}}$  using $g = \frac{\partial\varphi(z)}{\partial z}$.


\begin{align}
\frac{\partial N(x;W,V,b,c)}{\partial x_1} 
&= \sum_{j=1}^{j=n_h} \frac{\alpha_{1,j}}{W_{j,1}^2(c_{max}-c_{min})} \, W_{j,1} g(\frac{W_{j,1}\beta_{j,1}}{W_{j,1}} + W_{j,1}x_1)\\
\frac{\partial N(x;W,V,b,c)}{\partial x_1} 
&= \sum_{j=1}^{j=n_h} \alpha_{1,j} \frac{ g(W_{j,1}\left( x_1 + \beta_{j,1} \right))}{W_{j,1}(c_{max}-c_{min})} \\
\end{align}

We note that this is now in the form required for \Cref{lem:squashderivissum},
to apply.
So we use the summability kernel thus described $g_n(x) = \frac{k_n g(k_n x)}{k_n(c_{max}-c_{min})}$.
for arbitrary choice of $1 \le J \le n_h$ such that $W_{J,1} \ne 0$
define any arbitrary increasing sequence $k$ such that $k_n=W_{J,1}$,
not that we have not placed any restriction on $k_n=W_{J,1}$ beyond that it is positive -- it can be arbitrarily large.
We assign  $a_j=0$ for all $j \ne J$.


\begin{align}
\frac{\partial N(x;W,V,b)}{\partial x_1} 
&= \sum_{j=1}^{j=n_h} \alpha_{1,j} g_n\left( x_1 + \beta_{j,1} \right))
\end{align}

We can apply \Cref{lem:approxwithsummability}, and thus conclude:
for all non-negative continuous functions with compact support $f$,
for all points in its domain $x$
then for all $\epsilon > 0$, for all $p\in \lbrace 1, \infty \rbrace$
there exist a set of non-negative weight matrices $W$, $V$ (whose dimensions are determined by the hidden layer size $n_h$) and a bias vector $b$ and $c$
such that 
\begin{align}
\left\|\frac{\partial N(x;W,V,b, c)}{\partial x_1} - f(x) \right\|_p < \epsilon
\end{align}

We can thus say that $\frac{\partial N(x;W,V,b,c)}{\partial x_1}$ is dense in the space of non-negative continuous functions with compact support.

From this by application of the fundermental theorem of calculus,
we have
\begin{align}
\left\| \frac{\partial N(x;W,V,b, c)}{\partial x_1} - f(x) \right\|_p &< \epsilon \\
\left\| \int_{\inf supp(f)}^y \frac{\partial N(x;W,V,b, c)}{\partial x_1} - f(x) \, dx \right\|_p &< \left( y-\inf supp(f) \right)  \epsilon \\
\left\| N(y;W,V,b, c) - N(\inf supp(f);W,V,b, c) - \int_{\inf supp(f)}^y f(x) \, dx \right\|_p &< \left( y-\inf supp(f) \right)  \epsilon\\
\end{align}

\pdfcomment{Is this be right?}

We chose a value for $c$ as $c^\star=-N(\inf supp(f);W,V,b, 0)$, thus $N(\inf supp(f);W,V,b, c^\star)=0$.
We rewrite using $\varepsilon = \left( y-\inf supp(f) \right)  \epsilon$
\begin{align}
\left\| N(y;W,V,b, c^\star) - F(y) \right\|_p &< \varepsilon
\end{align}

Consider what $F$ represents.
As it is the integral of a non-negative continuous function, it is a monotonic differentiable function.
As outside the support of $f$ its gradient is zero, it is thus bounded below, and above.

We thus conclude with

For all monotonic differentiable functions bounded above and below $F$,
for all points in its domain $x$
then for all $\varepsilon > 0$, for all $p\in \lbrace 1, \infty \rbrace$
there exist a set of non-negative weight matrices $W$, $V$ (whose dimensions are determined by the hidden layer size $n_h$) and a bias vector $b$ and $c$

\begin{align}
\left\| N(x;W,V,b, c) - F(x) \right\|_p &< \varepsilon
\end{align}





\section{Conclusion and Future Work}

\bibliography{master}
\bibliographystyle{iclr2018_conference}

\end{document}
