\documentclass[11pt,a4paper]{article}
%\usepackage[author={Lyndon}]{pdfcomment}

\usepackage{booktabs}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage[space]{grffile}


\begin{document}
\clearpage


\def\maincolors{%
	brown-orange,
	orange-brown,	
	yellow-orange,
	orange-yellow,
	brownish green,
	greenish brown,
	bluish grey,
	greyish blue,
	pink-purple,
	purple-pink,
	green,
	greenish,
	purple,	
	purplish,
	brown,
	brownish,
	black,
	white,
	grey%
}

\def\oovcolors{
	%	brownish 1Green,
	%	bluish gray,
	1Brown,
	1Green,
	1Purple,
	gray,
	1Gray%
}


\tikzset{lastline/.style={opacity=1}};
\newcommand{\distfigs}[2]{
	\begin{tikzpicture}%
	\foreach[count=\mdlii from 0] \mdlname in #1 {%
		\node at (3.1*\mdlii cm, -1cm) {\sffamily \mdlname};%
		\foreach[count=\colii from 0] \colorname in #2 {%
			\xdef\topy{1.1*\colii}
			\node at (3.1*\mdlii cm, \topy cm) %
			{\includegraphics[height=0.95cm]{%
					figs/demo/dist/\mdlname/\colorname}%
			};%

		}%
		\xdef\topy{\topy+1}
		\node at (3.1*\mdlii cm, -1cm) {\sffamily \mdlname};%
		\node at (3.1*\mdlii cm, \topy) {\sffamily \mdlname};%
	}%
	%

	\draw[dashed] (1.555, -1.1) -- (1.555, \topy);%
	\draw[lastline] (3*1.555,-1.1) -- (3*1.555, \topy);%
	\draw[dashed,lastline] (5*1.555,-1.1) -- (5*1.555, \topy);%
	%
	\end{tikzpicture}%
}%

\subsection{Quantitative Results}

\newcommand{\pointfigs}[3]{
	\begin{tikzpicture}%
	\foreach[count=\mdlii from 0] \mdlname in #1 {%
		
		\node(fig\mdlname) at (3.1*\mdlii cm, 0) %
		{\includegraphics[height=#3 cm]{%
				figs/demo/point/\mdlname/#2}%
		};%
		\node[below = -0.7 of fig\mdlname] {\sffamily \mdlname};%
		\node[above = -0.7 of fig\mdlname] {\sffamily \mdlname};%
	}%
	\end{tikzpicture}%
}%


\begin{figure}
	\distfigs{{Direct, Direct-smoothed, SOWE, SOWE-smoothed}}{\maincolors}
	\caption{Some examples of the output distribution estimations from the models trained on the full dataset} \label{fig:distout1}
\end{figure}

\begin{figure}
	\distfigs{{CNN, CNN-smoothed, RNN, RNN-smoothed}}{\maincolors}
	\caption{Some examples of the output distribution estimations from the models trained on the full dataset} \label{fig:distout2}
\end{figure}

\begin{figure}
	\pointfigs{{Direct, SOWE, CNN, RNN}}{maincolors}{18}
	\caption{Some examples of the output distribution estimations from the models trained on the full dataset} \label{fig:pointout}
\end{figure}

\Cref{fig:distout1} and \Cref{fig:distout2} show some examples of distribution estimates for the models trained on the full data sets.
\Cref{fig:pointout} shows similar examples for point estimates.
It can be seen the the model's outputs using term based estimations are generally similar to the direct outputs, as is intended.
This aligns with the strong results found in that quantitative evaluations in \Cref{sec:quantitative-results}.
The models are correctly fitting to estimate the colors.

When considering the pairs of outputs that differ only in word order, such as \natlang{purple-pink} and \natlang{pink-purple} the models differ in behaviour.
The Direct results show that the ground truth is that such color name pairs are subtly different but very similar.
The SOWE models is unable to take into account word order at all, and so produces identical output for both.
The CNN models produce very similar outputs but not strictly identical -- spotting the different requires very close observation.
This is in-line with the different filter sizes allowing it to to use n-gram features, and it finding the unigram features most useful.
The RNN produces the most strikingly different results.
It seems the first term dominates the final output: \natlang{purple-pink} is more purple, and  \natlang{pink-purple} is more pink.
We can see that the first time is not solely responsible for the final output however, as \natlang{purple-pink}, \natlang{purple} and \natlang{purplish} (tokenized as \natlang{purple}, \natlang{ish}) are all different.
It is surprising that the RNN is dominated by the first term and not the latter terms\footnote{So much so that we double checked out implementation to be sure it wasn't processing the inputs backwards.} this shows the GRU is functioning to remember the earlier inputs.
This is happening too strongly however, as it causes the colors to differ to much.


The effect of the models on the smoothness when estimating distributions is interesting.
As expected, applying the KDE based smoothing to the training data produces a smoother output.
Further: the all the outputs of the neural network models are smoother than the correspond direct models.
This is true both when trained on smoothed and on unsmoothed training data.
The models are learning that adjacent bins should have similar output values, as this is a common feature of all the training data no matter the color.
An effect of this is that while the models capture the highly asymmetrical shapes of most distributions well,
they do not do well at capturing small dips.
Larger multi-modes as seen in the achromatic colors like \natlang{white}, \natlang{grey}, \natlang{black}, \natlang{white}, are captured; but smaller dips such as the hue of \natlang{greenish} being mostlikely to be either side of the green spectrum are largely filled in.
In general, it seems clear that the smoothing of the training data is not required for the neural network based models.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\subsection{Completely Unseen Color Estimation From Embedding Only}\label{sec:embeddingonly}
\Cref{fig:oovdist} and \Cref{fig:oovpoint} show quantitative examples of estimated colors for color-names that do not occur in the training data (or testing) at all.
This is even more extreme than the extrapolation tasks considered in \Cref{tbl:distextrapo} and \Cref{tbl:pointextrapo} where the terms appeared in training but not the combination of terms.
In these examples the terms are never occurred in the training data, but out models exploit the fact that they work by transforming the embedding space to use the word embeddings to predict the colors for them anyway.
There is no equivalent for this in the direct models.
While \natlang{Grey} and \natlang{gray} never occur in the training data; \natlang{grey} does, and it is near by in embedding space; similar is true for the other colors that vary by capitalization.
We only present a few examples of single term colors here, and no quantitative investigation, as this is merely a matter of interest.

It is particularly interesting to note that that all the models both for point estimation and distribution estimation make similar estimations for each color.
They do well on the same colors and make similar mistakes on the colors they do poorly at.
The saturation of \natlang{Gray} is estimated too high, making it appear too blue/purple, this is also true of \natlang{grey} though to a much lesser extent.
\natlang{Purple} and \natlang{Green} produce generally reasonable estimates.
The hue for \natlang{Brown} is estimated as having too much variance, allowing the color to swing into the red or yellowish-green parts of the spectrum.
In general the overall quality of each model seems to be in line with that found in the quantitative results for the full tests.







\begin{figure}
	\distfigs{{SOWE, SOWE-smoothed,CNN, CNN-smoothed}}{\oovcolors}
	\tikzset{lastline/.style={opacity=0}}
	\distfigs{{RNN, RNN-smoothed}}{\oovcolors}
	\tikzset{lastline/.style={opacity=1}}
	
	\caption{Some example distribution estimations for colors names which are completely outside the training data. The terms: \natlang{Brown}, \natlang{gray}, \natlang{Gray}, \natlang{Green}, and \natlang{Purple}, do not occur in any of the color data; however \natlang{brown}, \natlang{grey} \natlang{green}, and \natlang{purple} do occur.} \label{fig:oovdist}
\end{figure}

\begin{figure}
\pointfigs{{SOWE, CNN, RNN}}{oovcolors}{5}
\caption{Some example point estimates for colors names which are completely outside the training data. The terms: \natlang{Brown}, \natlang{gray}, \natlang{Gray}, \natlang{Green}, and \natlang{Purple}, do not occur in any of the color data; however \natlang{brown}, \natlang{grey} \natlang{green}, and \natlang{purple} do occur.} \label{fig:oovpoint}
\end{figure}

\end{document}