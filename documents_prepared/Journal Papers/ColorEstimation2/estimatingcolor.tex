\documentclass[11pt,a4paper]{article}
\usepackage[author={Lyndon}]{pdfcomment}

\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}

\usepackage{url}


\usepackage{tikz}
\usetikzlibrary{positioning, fit,  shapes.geometric}
\tikzset{
	node distance=1cm and 1.5cm,
	every text node part/.style= {
		align=center
	},
	word/.style= {
		blue,
		font=\itshape,
	},
	layer/.style= {rectangle, black,
		draw
	}
}


\usepackage{graphicx}
\graphicspath{{./figs/}, {./}}


\usepackage[subpreambles=false]{standalone}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}




\usepackage{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{url}
\usepackage{natbib}


\newcommand{\parencite}{\citep}
\newcommand{\textcite}{\citet}


%%%%%%%%%%%%%


\newcommand{\natlang}[1]{\texttt{#1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%opening
\title{Estimating Intended Color from its Name}
\author{Lyndon White, %
	Roberto Togneri, %
	Wei Liu, %
	\and Mohammed Bennamoun%
	\\ 
	\url{lyndon.white@research.uwa.edu.au}, %
	\url{roberto.togneri@uwa.edu.au},\\
	\url{wei.liu@uwa.edu.au}, %
	\and \url{mohammed.bennamoun@uwa.edu.au}%
	\\
	The University of Western Australia.
	35 Stirling Highway, Crawley, Western Australia
}



\begin{document}

\maketitle

\begin{abstract}
When a speaker says the name of a color, the color that they picture is not necessarily the same as the listener imagines.
Color is a grounded semantic task, but that grounding is not a mapping of a single word or phrase to a single point in color-space.
Proper understanding of color language requires the capacity to map a sequence of words to a probability distribution in color-space.
A distribution is required as there is no clear agreement between people as to what a particular color describes -- different people have a different idea of what it means to be ``very dark orange''.

Learning how each word in a color name contributes to the color described,
allows for knowledge sharing between uses of the words in different color names.
This knowledge sharing significantly improves predicative capacity for color names with sparse training data.
The extreme case of this challenge in data sparsity is for color names without any direct training data.
Our model is able to predict reasonable distributions for these cases, as evaluated on a held-out dataset consisting only of such terms.
\end{abstract}

\section{Introduction}\label{sec:intro}

Color understanding is an important subtask in natural language understanding.
It is a challenging domain, due to ambiguity, multiple roles taken by the same words, the many modifiers, and shades of meaning.
Due to its difficulty, texts containing color descriptions such as \texttt{the flower has petals that are bright pinkish purple with white stigma} are used as demonstrations for state of the art image generation systems \parencite{reed2016generative, 2015arXiv151102793M}.
The core focus of the work we present here is addressing these linguistic phenomena around the descriptions of the color, in a single patch, as represented in a color-space such as HSV \parencite{smith1978color}.
Issues of illumination and perceived color based on visual context are considered out of the scope.






Consider that the word \texttt{tan} may mean one of many colors for different people in different circumstances: ranging from the bronze of a tanned sunbather, to the brown of tanned leather;
\texttt{green} may mean anything from \texttt{aquamarine} to \texttt{forest green};
and even \texttt{forest green} may mean the rich shades of a rain-forest, or the near grey of the Australian bush.
Thus the color intended cannot be uniquely inferred from the color name.
Without further context, it does nevertheless remain possible to estimate likelihoods of which colors are be intended based on the population's use of the words.
The primary aim of this work is to map a sequence of color description words to a probability distribution over a color-space.
This is required for a proper understanding of color language.
We also consider the more basic point estimation of colors, though it's value is questionable.


Proper understanding requires considering \emph{the color intended} as a random variable.
In other words, a color name should map to a distribution, not just a single point or region.
For a given color name, any number of points in the color-space could be intended, with some being more or less likely than others.
Or equivalently, up to interpretation, it may intend a region but the likelihood of what points are covered is variable and uncertain.
This distribution is often multimodal and has high and asymmetrical variance, which further renders regression to a single point unsuitable.
We do produce results point estimate results for interest in \Cref{sec:results-point-est}, however for any form of precise work the use of such systems is limited
A single point estimate, does not capture the nature of the problem adequately.
The mean of a multimodal distribution (one with two peaks) will lie in the valley between the -- a less likely color.
Similarly it will be off to the side of the mode, in an asymmetrical distribution
Thus whi 


We estimate a probability distribution over the color-space.
To qualify our estimate of the distribution we discretize the space into a large number of patches, and produce an output much like a histogram.
This allows us to take advantage of the well-known softmax based methods for estimating a probability mass distribution using a neural network.


Estimating color probabilities has a clear use as a subsystem in many systems.
For example, in a human-interfacing system, when asked to select the \texttt{dark bluish green} object, each object can be ranked based on how likely its color is according to the distribution.
This way if extra information eliminates the most-likely object, the second most likely object can immediately be determined.
Further, if the probability of the color of the object being described by the user input is known, a threshold can be set to report that no object is found, or to ask for additional information.
More generally, the distribution based on the color name alone can be used as a prior probability and combined with additional context information to yield better predictions.







\section{Method}
\subsection{Tokenization}
For all the term based methods, we perform tokenization.
Tokenization 

\subsubsection{HSV color-space}
We use the HSV color-space \parencite{smith1978color}.
through-out this work.
We use the format which the data is originally provided in.
In this format hue, saturation and value all range between zero and one.
Unlike many other colors spaces (CIELab, Luv etc.) the gamut is square.
This regular space means that errors on all channels can be considered equally, and unlike other .
Though when working with the hue, all measures need to take into account the wrap-around effect.
When ever we refer to mean squared error, mean or mode on the HSV space in this paper, we are referring to the angularly corrected forms given in \Cref{sec:angularly-correct}.


Unlike the RGB color space, the HSV channels do correspond to how humans perceive colors.
Though it is not a perceptionally uniform color space across hue (unlike CIELab), which does suggest using mean squared error for the point estimates is not optimal.
However, given the other issues outlined above with point estimates we do not judge this a major concern.
There are no such issues for our 

One of it's important advantages over other color spaces is that it best meets the assumption that for a given color name each channel is statistically independent.
\subsubsection{The conditional independence assumption}\label{sec:conditional-independence-assumption}

\subsection{Angularly Correct Calculations on HSV}\label{sec:angularly-correct}




\section{Distribution Estimation}

\subsection{Discretization}
For distribution estimation, our models are trained to output histograms.
By making use of the conditional independence assumption \Cref{sec:conditional-independence-assumption},
we output one histogram per channel.


\subsection{Kernel-Density Based Smoothing}\label{sec:kernel-density-based-smoothing}


We make use of the \textcite{silverman1982algorithm}








\section{Experimental Setup}

\subsection{Implementation}
The implementation of the CDEST and baseline models was in the Julia programming language \parencite{Julia}.
The full implementation is included in the supplementary materials.
can be downloaded from the GitHub repository.\footnote{Implementation source is at \url{https://github.com/oxinabox/ColoringNames.jl}}
It makes heavy use of the MLDataUtils.jl\footnote{MLDataUtils.jl is available from \url{https://github.com/JuliaML/MLDataUtils.jl}} and TensorFlow.jl,\footnote{TensorFlow.jl is available from \url{https://github.com/malmaud/TensorFlow.jl}} packages.
the latter of which we enhanced significantly to allow for this work to be carried out.


\subsection{Common Network Features}
Dropout\parencite{srivastava2014dropout}  is used on all layers, other than the embedding layer, with threshold of 0.5 during training.
The network is optimized using Adam
\cite{kingma2014adam}, using a learning rate of 0.001.
Early stopping is checked every 10 epochs using the development dataset.
Distribution estimation methods are trained using full batch (where each observation is a distribution) for every epoch.
Point Estimation trains using randomized mini-batches of size $2^16$ observations (which are each color-space triples).
All hidden-layers, except as otherwise precluded (in side the convolution, and in the penultimate layer of the point estimation networks) have the same width 300, as does the the embedding layer.


\subsubsection{Embeddings}
All our neural network based solutions incorporate an embedding layer.
This embedding layer maps from tokenized words to vectors.
We make use of 300d pretrained FastText embeddings \textcite{bojanowski2016enriching}\footnote{Available from \url{https://fasttext.cc/docs/en/english-vectors.html}}.

The embeddings are not trained during the task, but are kept fixed.
The layers above allow for arbitrary non-linear transform of the result.



\subsection{Input Modules}


\subsubsection{Recurrent Neural Network(RNN)}
\begin{figure}
	\begin{tikzpicture}
		\node (hiddenoutput)[layer] at (0,0) {ReLU};
		\node (output)[dotted, layer, above=1 of hiddenoutput] {Output Module};
		\draw[->] (hiddenoutput) to (output);
		
		\node (GRU1)[layer, below = of hiddenoutput]{GRU};
		
		\foreach[count=\i from 1] \j in {2,...,5}
		{
			\node (GRU\j)[layer, left = of GRU\i]{GRU};
			\draw[->] (GRU\j) to (GRU\i);
		}
		
		\foreach[count=\i from 1] \word in {$\langle EOS \rangle$, green, ish, blue, light}
		{
			\node (emb\i)[layer, below = of GRU\i]  {Embedding};
			\node (word\i)[word, below = of emb\i]{\word};
			\draw[->] (word\i) to  (emb\i);
			\draw[->] (emb\i) to (GRU\i);
			\node[draw,dashed,fit= (emb\i)  (word\i)  (GRU\i)] {};
		}
		
		
		\draw[->] (GRU1) to (hiddenoutput);
	\end{tikzpicture}

	\caption{The RNN Input module for the example input \natlang{light greenish blue}. Each dashed box represents 1 time-step.}
\end{figure}


A Recurrent Neural Network is a common choice for this kind of task,
due to the variable length of the input.
The general structure of this network, shown in \Cref{fig:rnnmod} is similar to \textcite{2016arXiv160603821M}, or indeed to most other word sequence learning models.
Each word is first transformed to an embedding representation.
This representation is trained with the rest of the network allowing per word information to be efficiently learned.
The embedding is used as the input for a Gated Recurrent Unit (GRU)  \parencite{cho2014properties}.
The output of the last time-step is fed to a Rectified Linear Unit (ReLU)  \parencite{dahl2013reludropout}.


\subsubsection{Sum of Word Embeddings (SOWE)}
Using a simple sum of word embeddings as a layer in a neural network is less typical than an RNN structure.
Though it is well established as a useful representation, and has been used an input to other classifiers such as support vector machines. \pdfcomment{Cite some papers, e.g. mine where this is done.}
Any number of word embeddings can be added to the sum.
However, it has no representation of the order.
The structure we used is shown in \Cref{fig:sowemod}

\begin{figure}
	\begin{tikzpicture}

	\node (GRU1)[]{};
	
	\foreach[count=\i from 1] \j in {2,...,5}
	{
		\node (GRU\j)[left = 2 of GRU\i]{};
	}
	
	\node (hstack)[layer, above= of GRU3, xshift=1cm]{stack into grid};
	
	
	\foreach[count=\i from 1] \word in {green, ish, blue, light}
	{
		\node (emb\i)[layer, below = of GRU\i]  {Embedding};
		\node (word\i)[word, below = of emb\i]{\word};
		\draw[->] (word\i) to  (emb\i);
		\draw[->] (emb\i) to (hstack);
	}
	
	\node (conv)[layer, above= 0.5 of hstack]{2D Convolution};
	\node (innerconv)[layer, above= 0.5 of conv]{ReLU};
	\node (pool)[layer, above= 0.5 of innerconv]{MaxPooling};
	\node (hiddenoutput)[layer, above= 0.5 of pool] {ReLU};
	\node (output)[dotted, layer, above=1 of hiddenoutput] {Output Module};
	\draw[->] (hstack) to (conv);
	\draw[->] (conv) to (innerconv);
	\draw[->] (innerconv) to (pool);
	\draw[->] (pool) to (hiddenoutput);
	\draw[->] (hiddenoutput) to (output);
	
%	\draw[->] (hstack) to (hiddenoutput);
%	\draw[->] (hiddenoutput) to (output);
	
	\end{tikzpicture}
	
	\caption{The SOWE input module for the example input \natlang{light greenish blue}}
	\label{fig:sowemod}
\end{figure}


\subsubsection{Convolutional Neural Network(CNN)}

We apply a convulutional neural network to the task by applying 2D convolution over the stacked word embeddings.
\Cref{fig:cnnmod}
We use 64 filters of size between 1 and the length of the longest padded embedding (5).



\begin{figure}
	\begin{tikzpicture}
	
	\node (GRU1)[]{};
	
	\foreach[count=\i from 1] \j in {2,...,5}
	{
		\node (GRU\j)[left = 2 of GRU\i]{};
	}
	
	\node (sum)[layer, above= of GRU3, xshift=1cm]{$\sum$};
	
	\foreach[count=\i from 1] \word in {green, ish, blue, light}
	{
		\node (emb\i)[layer, below = of GRU\i]  {Embedding};
		\node (word\i)[word, below = of emb\i]{\word};
		\draw[->] (word\i) to  (emb\i);
		\draw[->] (emb\i) to (sum);
	}
	
	\node (hiddenoutput)[layer, above=of sum] {ReLU};
	\node (output)[dotted, layer, above=1 of hiddenoutput] {Output Module};
	\draw[->] (sum) to (hiddenoutput);
	\draw[->] (hiddenoutput) to (output);
	
	\end{tikzpicture}
	
	\caption{The SOWE input module for the example input \natlang{light greenish blue}}
	\label{fig:sowemod}
\end{figure}



\subsubsection{Non-term based Baseline}
To baseline the performance of our models we propose 
\paragraph{Histogram Baseline (Distribution Estimation, only)}
\paragraph{Mean Squared Error Baseline (Point Estimation, only)}

\subsection{Datasets}


\subsubsection{Full Training and Testing set}
\subsubsection{Extrapolation Training and Testing Set}
The primary goal in constructing using the term based models is to be able to make predictions for never before seen descriptions of colors.
For example, based on the learned understanding of \texttt{salmon} and of \texttt{bright}, from examples like \texttt{bright green} and \texttt{bright red}, we wish for the systems to make predictions about \texttt{bright salmon}, even though that description never occurs in the training data.
%
To evaluate this generalisation capacity, we define an extrapolation sub-dataset for both testing and training.
This is defined by selecting the rarest 100 color descriptions from the full dataset,
with the restriction that every token in a selected description must still have at least 8 uses in other descriptions in the training set.
The selected examples include multi-token descriptions such as: \texttt{bright yellow green} and also single tokens that occur more commonly as modifiers than as stand-alone descriptions such as \texttt{pale}.

The extrapolation training set is made up of the data from the full training set, excluding those  corresponding the the rare descriptions.
Similar is done for the development set, so as no direct knowledge of the combined terms can leak during early-stopping.
Conversely, the extrapolation test set is made up of only the observations from the full test set that do use those rare descriptions.


By training on the extrapolation training set and testing on the extrapolation test set, we can assess the capacity of the models to make predictions for color descriptions not seen in training.
A similar approach was used in \textcite{DBLP:journals/corr/AtzmonBKGC16}.
We contrast this to the same models when trained on the full training set, but tested on the extrapolation test set, to see how much accuracy was lost.


\subsection{Order Testset}
It is known that the order of words in a color description to some extent matters.
\natlang{greenish brown} and \natlang{brownish green} are distinct, if similar, colors.
To assess the models on there ability to make predictions when order matters we construct the order testset.
This is a subset of the full test set containing only descriptions with terms that occur in multiple different orders.
There are 76 such descriptions in the full dataset.
Each of which has exactly one alternate ordering.
This is unsurprising as while color descriptions may have more than 2 terms, normally one of the terms is a joining token such as \natlang{ish} or \natlang{-}.



\subsection{Output Modules}
\subsubsection{Distribution Estimation}

For all the distribution estimation systems we investigate here, 
we consider training both on the binned-data, and on the smoothed data (as described in \Cref{sec:kernel-density-based-smoothing}).
Making use of the conditional independence assumption (see \Cref{sec:conditional-independence-assumption}), we output the three discretized distributions.
This is done using 3 softmax output layers.

The output module for distribution estimation 

Contrasting to estimating continuous conditional distributions, 
estimating a discrete conditional distributions is a significantly more studied application of neural networks
-- this is the basic function of any softmax classifier.
To simplify the problem, we therefore transform it to be a discrete distribution estimation task, by discretizing the color-space.
Discretization to a resolution of 64 and 256 bins per channel is considered.


For the case of the machine learning models, the output is produced using softmax layers.


\begin{figure}
	\newcommand{\picwidth}{60pt}
	\begin{tikzpicture}
	
	\node (input)[layer, dotted]{Input Module};
	
	\node(outHue)[layer, above left = of input] {Softmax};
	\node(outSaturation)[layer, above = of input] {Softmax};
	\node(outValue)[layer, above right = of input] {Softmax};
	
	\foreach \p in {Hue, Saturation, Value} 
	{
		\draw[->] (input) to (out\p);
		
		\node(plot\p)[above = of out\p, text width=\picwidth]{
			\includegraphics[width=\picwidth]{netdia/\p}
			\\
			\p
		};
		\draw[->] (out\p) to (plot\p);
	}
	
	\end{tikzpicture}
	
	\caption{The Distribution Output Module \label{fig:distoutmod}}
\end{figure}
	

\subsubsection{Point Estimation}


\begin{figure}
	\newcommand{\picwidth}{60pt}
	\begin{tikzpicture}
	
	\node (input)[layer, dotted]{Input Module};
	\node (hiddenout)[layer, above= 1 of input]{ReLU, with output width 4};
	\draw[->](input) to (hiddenout);
	
	\node(outHue)[xshift=-1cm, above = 5 of hiddenout] {$y_{hue}$};
	\node(outSaturation)[above = 5 of hiddenout] {$y_{sat}$};
	\node(outValue)[xshift=1cm, above = 5 of hiddenout] {$y_{val}$};
	
	\node(Z)[above=0.1cm of hiddenout.north]{};
	\node(Z2)[left=0cm of Z]{$z_2$};
	\node(Z1)[left=0.3cm of Z2]{$z_1$};
	\node(Z3)[right=0cm of Z]{$z_3$};
	\node(Z4)[right=0.3cm of Z3]{$z_4$};
	
	
	\node(s3)[layer, above=1 of Z3, xshift=0.5cm]{$\sigma$};
	\node(s4)[layer, above=1 of Z4, xshift=1cm]{$\sigma$};
	\draw[->](Z3) to (s3) to (outSaturation);
	\draw[->](Z4) to (s4) to (outValue);
	
	
	\node(AngHue)[layer, below = 1 of outHue, xshift=-0.7cm]{$ \mathrm{atan2}^\ast \left(y_{shue}, y_{chue} \right) $ };
	\draw[->](AngHue) to (outHue.south);
	
	\node(s1)[layer, above=1 of Z1, xshift=-1cm]{$\tanh$};
	\node(s2)[layer, above=1 of Z2, xshift=-0.5cm]{$\tanh$};
	\draw[->](Z1) to (s1);
	\draw[->](s1) to node[left]{$y_{shue}$} (AngHue);
	\draw[->](Z2) to (s2);
	\draw[->](s2) to node[right]{$y_{chue}$}  (AngHue);
	
	
	
	
	
	\end{tikzpicture}
	
	\caption{The Point Estimate Output Module.
		Here $ \mathrm{atan2}^\ast$ is the quadrant preserving arctangent, outputting as a regularized angle (as per in all evaluations)
		 \label{fig:distoutmod}}
\end{figure}






\clearpage
\bibliography{master}

\clearpage
\appendix
\input{supp.tex}

\end{document}
