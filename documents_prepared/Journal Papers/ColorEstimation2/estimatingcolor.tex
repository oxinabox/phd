\documentclass[11pt,a4paper]{article}
\usepackage[author={Lyndon}]{pdfcomment}

\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}

\usepackage{url}


\usepackage{tikz}
\usetikzlibrary{positioning, fit,  shapes.geometric}
\usepackage{graphicx}

\graphicspath{{./figs/}, {./}}


\usepackage[subpreambles=false]{standalone}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


\usepackage{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{url}
\usepackage{natbib}


\newcommand{\parencite}{\citep}
\newcommand{\textcite}{\citet}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\natlang}[1]{\texttt{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%opening
\title{Estimating Intended Color from its name}
\author{Lyndon White, %
	Roberto Togneri, %
	Wei Liu, %
	\and Mohammed Bennamoun%
	\\ 
	\url{lyndon.white@research.uwa.edu.au}, %
	\url{roberto.togneri@uwa.edu.au},\\
	\url{wei.liu@uwa.edu.au}, %
	\and \url{mohammed.bennamoun@uwa.edu.au}%
	\\
	The University of Western Australia.
	35 Stirling Highway, Crawley, Western Australia
}



\begin{document}

\maketitle

\begin{abstract}
When a speaker says the name of a color, the color that they picture is not necessarily the same as the listener imagines.
Color is a grounded semantic task, but that grounding is not a mapping of a single word (or phrase) to a single point in color-space.
Proper understanding of color language requires the capacity to map a sequence of words to a probability distribution in color-space.
A distribution is required as there is no clear agreement between people as to what a particular color describes -- different people have a different idea of what it means to be ``very dark orange''.

Learning how each word in a color name contributes to the color described,
allows for knowledge sharing between uses of the words in different color names.
This knowledge sharing significantly improves predicative capacity for color names with sparse training data.
The extreme case of this challenge in data sparsity is for color names without any direct training data.
Our model is able to predict reasonable distributions for these cases, as evaluated on a held-out dataset consisting only of such terms.
\end{abstract}




\section{Method}
\subsection{Tokenization}
For all the term based methods, we perform tokenization.
During tokenization a color name is split into terms, with consistent spelling.
For example, \natlang{bluish kahki} would become the sequence of 3 tokens: [\natlang{blue}, \natlang{ish}, \natlang{khaki}].



 

\subsection{HSV color-space}

\section{Distribution Estimation}
\subsubsection{The conditional independence assumption}\label{sec:conditional-independence-assumption}
\subsection{Discretization}
For distribution estimation, our models are trained to output histograms.
By making use of the conditional independence assumption \Cref{sec:conditional-independence-assumption},
we output one histogram per channel.

We perform an uniform weight attribution of points to bins as described by \tcite{jones1984remark}.


\subsection{Kernel-Density Based Smoothing}\label{sec:kernel-density-based-smoothing}


We make use of the \textcite{silverman1982algorithm}


\subsection{Mean Squared Error on HSV}



\section{Experimental Setup}

\subsection{Input Modules}

\subsubsection{Sum of Word Embeddings (SOWE)}
\subsubsection{Convolutional Neural Network(CNN)}
\subsubsection{Recurrent Neural Network(RNN)}

We make use of a Gated Recurrent Unit (GRU), 
which we found to marginally out-perform the basic RNN in preliminary testing.
The small improvement is unsurprising, as the color names have at most 4 terms,
so longer short term memory is not required.

The sequence of word embeddings for each-term was input to the model.
The stream was terminated with an End of Streak (\natlang{<EOS>}) pseudotoken,
as represented using a zero-vector.

\subsubsection{Non-term based Direct Methods}
To establish an rough upper-limit on the modeling results
we evaluate simple direct methods.
These direct methods do not process term-by-term, they do not work with the language at all.
They simply map from the exact input text (no tokenization) to the precalculated histogram or mean of the training data for that exact text.

They represent an approximate upper bound, as they fully exploit all information in the training data for each input.
There is no attempting to determine how each term affects the result.
We say approximate upper-bound, as it is not strictly impossible that the ML methods may happen to model the test data better than can be directly determined by the training data.


\paragraph{Direct Histogram (Distribution Estimation, only)}
\paragraph{Direct Mean Point (Point Estimation, only)}





\subsection{Output Modules}
\subsubsection{Distribution Estimation}

For all the distribution estimation systems we investigate here, 
we consider training both on the direct bin-data,
and on the smoothed data (as described in )

\subsubsection{Point Estimation}









\clearpage
\bibliography{master}

\clearpage
\appendix
\input{supp.tex}

\end{document}
