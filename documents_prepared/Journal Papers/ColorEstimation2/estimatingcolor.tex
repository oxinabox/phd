\documentclass[11pt,a4paper]{article}
\usepackage[author={Lyndon}]{pdfcomment}

\usepackage{booktabs}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.14}

\DeclareMathVersion{tabularbold}
\SetSymbolFont{operators}{tabularbold}{OT1}{cmr}{b}{n}



\usepackage{url}


\usepackage{tikz}
\usetikzlibrary{positioning, fit,  shapes.geometric}
\tikzset{
	node distance=1cm and 1.5cm,
	every text node part/.style= {
		align=center
	},
	word/.style= {
		blue,
		font=\itshape,
	},
	layer/.style= {
		rectangle, 
		black,
		draw
	}
}

% Center all floats
\makeatletter
\g@addto@macro\@floatboxreset{\centering}
\makeatother

\usepackage{graphicx}
\graphicspath{{./figs/}, {./}}
\usepackage[space]{grffile}


\usepackage[subpreambles=false]{standalone}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}




\usepackage{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{url}
\usepackage{natbib}


\newcommand{\parencite}{\citep}
\newcommand{\textcite}{\citet}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\natlang}[1]{\texttt{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%opening
\title{Estimating the Intended Color from the Terms that make up it's Name}
\author{Lyndon White, %
	Roberto Togneri, %
	Wei Liu, %
	\and Mohammed Bennamoun%
	\\ 
	\url{lyndon.white@research.uwa.edu.au}, %
	\url{roberto.togneri@uwa.edu.au},\\
	\url{wei.liu@uwa.edu.au}, %
	\and \url{mohammed.bennamoun@uwa.edu.au}%
	\\
	The University of Western Australia.
	35 Stirling Highway, Crawley, Western Australia
}



\begin{document}

\maketitle

\begin{abstract}
When a speaker says the name of a color, the color that they picture is not necessarily the same as the listener imagines.

Further we argue that color understanding is a grounded semantic task, but that grounding is not a trivial mapping of a single word or phrase to a single point in color-space as there are many different interpretations of what  color is mean by a particular color name.
Proper understanding of color language requires the capacity to map a sequence of words to a probability distribution in color-space.
As such we focus our investigation on distribution estimation from color name.
A distribution is required as there is no clear agreement between people as to what a particular color describes -- different people have a different idea of what it means to be ``very dark orange''.
To access if our conclusions about our methods hold more generally, we also assess them on the task of point estimation of color names.
We find the results are consistent, though we argue this task is less useful.

Color names are often made up of multiple words,
as a task in natural language understanding we investigate in depth the capacity of neural networks based on sums of word embeddings (SOWE), recurrence (RNN) and convolution (CNN),  on their ability to make estimates of colors from sequences of terms.
We contrast their performance to direct non-term based methods which form a rough upper-bound by directly using the training data.
Surprisingly, the sum of word embeddings generally performs the best, coming close to this upper bound on almost all evaluations.
It is trailed slightly by the CNN, and most substantially by the RNN.
\end{abstract}

\section{Introduction}\label{sec:intro}

Color understanding is an important subtask in natural language understanding.
It is a challenging domain, due to ambiguity, multiple roles taken by the same words, the many modifiers, and shades of meaning.
In many ways it is a grounded microcosm of natural language understanding.
Due to its difficulty, texts containing color descriptions such as \texttt{the flower has petals that are bright pinkish purple with white stigma} are used as demonstrations for state of the art image generation systems \parencite{reed2016generative, 2015arXiv151102793M}.
The core focus of the work we present here is addressing these linguistic phenomena around the short-phrase descriptions of the color, in a single patch, as represented in a color-space such as HSV \parencite{smith1978color}.
Issues of illumination and perceived color based on context are considered out of the scope.
We evaluated a variety of neural-network based systems, for their capacity to be used for this natural language understanding task.
We believe our results results are suggestive to the capacity of these systems for other similar tasks involving short token sequences.
\pdfcomment{Rui has a paper for neural represenstation of phrases. Maybe I should link into that}


Consider that the word \texttt{tan} may mean one of many colors for different people in different circumstances: ranging from the bronze of a tanned sunbather, to the brown of tanned leather;
\texttt{green} may mean anything from \texttt{aquamarine} to \texttt{forest green};
and even \texttt{forest green} may mean the rich shades of a rain-forest, or the near grey of the Australian bush.
Thus the color intended cannot be uniquely inferred from the color name.
Without further context, it does nevertheless remain possible to estimate likelihoods of which colors are be intended based on the population's use of the words.
The primary aim of this work is to map a sequence of color description words to a probability distribution over a color-space.
This is required for a proper understanding of color language.
We also consider the more basic point estimation of colors, though we question its value.


Proper understanding requires considering \emph{the color intended} as a random variable.
In other words, a color name should map to a distribution, not just a single point or region.
For a given color name, any number of points in the color-space could be intended, with some being more or less likely than others.
Or equivalently, up to interpretation, it may intend a region but the likelihood of what points are covered is variable and uncertain.
This distribution is often multimodal and has high and asymmetrical variance, which further renders regression to a single point unsuitable.
We do produce results point estimate results for interest in \Cref{sec:results-point-est}, however for any form of precise work the use of such systems is limited
A single point estimate, does not capture the nature of the problem adequately.
The mean of a multimodal distribution (one with two peaks) will lie in the valley between the -- a less likely color.
Similarly it will be off to the side of the mode, in an asymmetrical distribution.
The other problem is that is that a point estimate does not capture the sensitivity.
In an asymmetrical distribution, having  point slight off-center in one direction may result in very different probability,
this more generally holds for a narrow variance distribution.
Conversely for a very wide variance distribution (for example one approaching the uniform distribution) the point estimate value may matter very little with all points providing similar probabilities.
Color distributions are always one or more of multimodal or asymmetrical, and feature widely different variances for different names.
As such, we feel producing a point estimate has limited value.
However we do consider the task, for comparison, and it remains an interesting challenge for assessing the systems under evaluation.
While by definition the mean color point minimizes the squared error, it may not meaningful actually be a reasonable point estimate.
Better is to estimate the distribution and allow the consumer of the estimate to determine how to reduce that to a point estimate.


When we estimate a probability distribution over the color-space.
To qualify our estimate of the distribution we discretize the space to produce into a histogram.
This allows us to take advantage of the well-known softmax based methods for estimating a probability mass distribution using a neural network.

An interesting consideration when considering this discretization is the smoothness estimate.
The true space is continuous, even if we are discretizing it at resolution as high as the original color displays used to collect the data.
Being continuous means that a small change in point the color-space should correspond to a small change in how likely that point is.
Or more informally: histograms should look smooth, and not spiky.
We investigate using a KDE based method for smoothing the training data, and further we conclude that the neural network based models are learning this smoothness with or without the smoothing of the training data.


Estimating color probabilities has a clear use as a subsystem in many systems.
For example, in a human-interfacing system, when asked to select the \texttt{dark bluish green} object, each object can be ranked based on how likely its color is according to the distribution.
This way if extra information eliminates the most-likely object, the second most likely object can immediately be determined.
Further, if the probability of the color of the object being described by the user input is known, a threshold can be set to report that no object is found, or to ask for additional information.
More generally, the distribution based on the color name alone can be used as a prior probability and combined with additional context information to yield better predictions.

\subsection{Contributions}
Our contributions in this work are (in order of significance)
\begin{itemize}
	\item Investigation of term-based neural network color estimation models based on:
	\begin{itemize}
		\item processing inputs per term using  Sum of Word Embeddings (SOWE), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).
		\item producing outputs as distributions models using softmax based histograms, and as point estimates using a novel HSV neural network output layer.
		\item we conclude that while all these models learn useful outputs, the SOWE and the CNN far out perform the RNN.
	\end{itemize}
	\item Investigation into the effect of smoothing the distribution training histograms, using a kernel density based (KDE) method; featuring wrapped around effects on the hue channel. We find the interesting conclusion that while smoothing is essential for direct methods of estimation, it has little effect on the machine learning based methods that we investigate.
\end{itemize}
The investigations include investigating the estimation capacity of the models:
\begin{itemize}
	\item generally on all color names (full task)
	\item for color names when the order of the words matters (order task)
	\item for color names which never occur in the training data exactly, but for which all terms occur in the training data (extrapolation task
	\item for color names with terms that do not occur in the training data at all, but for which we know word embeddings for (embedding only task).
\end{itemize}
The last investigation is not carried out in depth, but is presented merely as a point of interest.
We believe that due to the nature of color understanding as a microcosm of natural language understanding, the results of our investigations have some implications for the capacity of the models for their general use in short phrase understanding.


\section{Related Work}\label{sec:related-work}
The understanding of color names has long been a concern of psycholinguistics and anthropology  \parencite{berlin1969basic,heider1972universals,HEIDER1972337,mylonas2015use}.
It is thus no surprise that there should be a corresponds field of research in natural language processing.

The earliest works revolve around explicit color dictionaries.
This includes the ISCC-NBS color system \parencite{kelly1955iscc} of 26 words, including modifiers, that are composed according to a context free grammar such that phrases are mapped to single points in the color-space;
and the simpler, non-compositional, 11 basic colors of \textcite{berlin1969basic}.
Works including \textcite{Berk:1982:HFS:358589.358606,conway1992experimental,ele1994computational, mojsilovic2005computational, menegaz2007discrete,van2009learning} which propose methods for the automatic mapping of colors to and from these small manually defined sets of colors.
We note that \textcite{menegaz2007discrete,van2009learning} both propose systems that discretize the color-space, though to a much courser level than we consider in this work.


More recent works, including the work presented here, function with much larger number of colors, larger vocabularies, and larger pools of respondents.
In particular making uses of the large Munroe dataset \textcite{Munroe2010XKCDdataset}, as we do here.
This allows a data driven approach towards the modelling.

\textcite{mcmahan2015bayesian} and \textcite{meomcmahanstone:color} present color naming methods, mapping from colors to to their names, the reverse of our task.
These works are based on defining fuzzy rectangular distributions in the color-space to cover the distribution estimated from the data, which are used in a Bayesian system to non-compositionally determine the color name.
%
%\pdfcomment{During the similar time-period another online color naming survey was conducted.
%\textcite{mylonas2010online,mylonas2012colour} collected a total of 50,000 observations from 2500 respondents in 7 languages.
%In this work we use the larger, more publically available, Munroe dataset.}
%
\textcite{2016arXiv160603821M} maps a point in the color-space, to a sequence of probability estimates over color terms.
They extends beyond, all prior color naming systems to produce a compositional color namer based on the Munroe dataset.
Their method uses a recurrent neural network (RNN), which takes as input a color-space point, and the previous output word, and gives a probability of the next word to be output -- this is a conditional language model.
In this work we tackle the inverse problem to the creation of a conditional language model.
Our distribution estimation models map from a sequence of terms, to distribution in color space.
Similarly, our point estimation models map from sequence of terms to single point in color-space.



\textcite{DBLP:journals/corr/KawakamiDRS16} propose another compositional color naming model.
They use a per-character RNN and a variational autoencoder approach.
It is in principle very similar to \textcite{2016arXiv160603821M}, but functioning on a character, rather than a word level.
The work by Kawakami et al. also includes a method for generating colors.
However they only consider the generation of point estimated, rather than distributions.
The primary focus of our work is on generating distributions.
The datasets used by Kawakami et al. contain only very small numbers of observations for each color name (often just one).
These datasets are thus not suitable for modelling the distribution in color space as interpreted by the population.
Further, given the very small number of examples they are not well suited for use with word-based modelling: the character based modelling employed by Kawakami et al. is much more suitable.
As such we do not attempt comparison to their work.


\textcite{DBLP:journals/corr/MonroeHGP17} presents a neural network solution to a communication game, where a speaker is presented with three colors and asked to describe one of them, and the listener is to work out which is being described.
Speaker and listener models are trained, using LSTM-based decoders and encoders respectively.
The final time-step of their model produces a 100 dimensional representation of the description provided.
From this, a Gaussian distributed score function is calculated, over a high dimensional color-space from \textcite{2016arXiv160603821M}, which is then used to score each of the three options.
While this method does work with a probability distribution, as a step in its goal,
this distribution is always both symmetric and unimodal -- albeit in a high-dimensional color-space.

The generation of color from text has not received a signficant ammount of attention in prior work.
In particular the generation of probability distributions in color space, to our knowledge has not been considered at all.
Conversely, there has been several works on the reverse problem: the generation of a textual name for a color from color space point.
The work presented here closed that gap.




\section{Method}



\subsection{HSV color-space}
We use the HSV color-space \parencite{smith1978color}.
through-out this work.
We use the format which the data is originally provided in.
In this format hue, saturation and value all range between zero and one.
Note that in this dataset hue is measured in \emph{turns}, rather than the more traditional degrees, or radians.
Having hue be measured between zero and one (like the other channels) makes the modelling task more consistent as were the hue to range between $0$ and $2\pi$ (radians) or between $0$ and $360$ (degrees) it would be over-weighted compared to the other channels.
This regular space means that errors on all channels can be considered equally.
Unlike many other colors spaces (CIELab, Luv etc.) the gamut is square and all values of all channels corresponds to realizable colors.


When working with the hue, all measures need to take into account the wrap-around effect.
When ever we refer to mean squared error, mean or mode on the HSV space in this paper, we are referring to the angularly corrected forms given in \Cref{sec:angularly-correct}.


Unlike the RGB color space, the HSV channels do correspond to how humans perceive colors.
However, it is not designed to be a perceptionally uniform color space across hue (unlike CIELab), which does suggest using mean squared error for the point estimates is not optimal.
However, given the other issues outlined above with point estimates we do not judge this a major concern.
There are no such issues for our distribution estimations.

One of it's important advantages over other color spaces is that it best meets the assumption that for a given color name each channel is statistically independent.

\subsection{Angularly Correct Calculations on HSV}\label{sec:angularly-correct}
When performing calculations with the HSV color space it is important to take into account that hue is an angle.
As we are working with the color space regularized to range between zero and one for all channels,
this means a hue of one and a hue of zero are equivalent (as we measure in turns, in radian's this would be $0$ and $2\pi$).

The square error of two hue values is thus calculated as
\begin{equation}
SE(h_1, h_2) = \min \big( \left(h_1 - h_2 \right)^2, \, \left(h_1 - h_2 -1 \right)^2  \big)
\end{equation}
to take into account the error could be calculated clockwise or counter clockwise.
(Note that the $-1$ term related to using units of turns, were we using radians it would be $-2\pi$)


The mean of a set of hues ($\lbrace h_1,\,\ldots,\,h_N \rbrace$) is calculated as 
\begin{equation}
\bar h = \mathrm{atan2} \Bigg(%
	\frac{1}{N} \sum_{i=1}^{i=N} \sin (h_i), \,  %
	\frac{1}{N} \sum_{i=1}^{i=N} \cos (h_i)%
\Bigg)%
\end{equation}
(Note again: as we measure angle in turns we use the turn trigonometric functions in implementation, though this mean is the same expression or any units).




\section{Distribution Estimation}
\subsection{Conditional Independence Assumption}\label{sec:conditional-independence-assumption}
We make the assumption that given the name of the color, the distribution of the H, S and V channels are independent.
That is to say, it is assumed if the color name is known, then  knowing the value of one channel would not provide any additional information as to the value of the other two channels.
The same assumption is made, though not remarked upon, in \textcite{meomcmahanstone:color} and \textcite{mcmahan2015bayesian}.
This assumption of conditional independence allows considerable saving in computational resources.
Approximating the 3D joint distribution as the product of three 1D distributions decreases the space complexity from $O(n^3)$ to $O(n)$ in the discretized step that follows.

Superficial checks were carried out on the accuracy of this assumption.
Spearman's correlation on the training data suggests that for over three quarters of all color names, there is only weak correlation between the channels (\mbox{Q3 = 0.187}).
However, this measure underestimates correlation for values that have circular relative value, such as hue.
HSV had the lowest correlation by a large margin of the 16 color-spaces evaluated.
Full details, including the table of correlations, are available in supplementary materials.
These results are suggestive, rather than solidly indicative, on the degree of correctness of the conditional independence assumption.
We consider the assumption sufficient for this investigation.

\subsection{Discretization}\label{sec:discretization}
For distribution estimation, our models are trained to output histograms.
This is a discretized representation of the continuous distribution.
Following standard practice interpolation-based methods can be used to handle it as a continuous distribution.
By making use of the conditional independence assumption (see \Cref{sec:conditional-independence-assumption}), we output one 256-bin histogram per channel.
We note as  24-bit color (as was used in the survey that collected the dataset) can have all information captured by a 256 bin discretization  per channel.
24 bit color allows for a total of $2^{24}$ colors to be represented, and even onehot encoding for each of the 256 bin discretization channels allows for the same.
As such there is no meaningful loss of information when using histograms over a truely continuous estimation method, such as a Gaussian mixture model.
Although such models may have other advantages (such as the a priori information added by specifying the distribution), we do not investigating them here, instead considering the simplest non-parametric estimation model (the histogram), which has the simple implementation in a neural network using a softmax output layer.


Discretizing the data in this way is is a useful solution used in several other machine learning systems.
\textcite{oord2016pixel, DBLP:journals/corr/OordDZSVGKSK16} apply a similar discretization step and found their method to outperforming the more complex continuous distribution outputs.

For training purposes we thus need to convert all the observations into histograms.
One set of histograms is produced per color description present in the dataset.
We perform an uniform weight attribution of points to bins as described by \textcite{jones1984remark}. This method of tabulation is in-short to define the bins by there midpoints, and for a point observed between the centre of two bins then probability mass is allocated to each in proportion to how close the point is to the centre of each.

\subsection{Kernel-Density Estimation Based Smoothing}\label{sec:kernel-density-based-smoothing}
As the distribution is not truly discrete, we investigate adding a Kernel-Density Estimation (KDE) based smoothing step.
KDE is a common approach in non-parametric estimation.

This is expected as a blurred discrete distribution captures some of the notions of continuity that a truly continuous output distribution would intrinsically feature.


We use the Fast Fourier Transform (FFT) based KDE method of the \textcite{silverman1982algorithm}.
We use a Gaussian kernel, and select the bandwidth per color description based on leave-one-out cross validation on the training data.
A known issue with the FFT based KDE method is that it has a wrap-around near the boundaries where mass that would be assigned outside the boundaries is instead assigned to the bin on the other side.
For the value and saturation channels we follow the standard solution of initially defining additional bins outside the true boundaries, then discard those bins and rescale the probability to one.
For the hue channel this wrap-around effect is exactly as desired.



\subsection{Perplexity in Color-Space (Evaluation Metric)}
Perplexity is a measure of how well the distribution, estimated by the model, matches reality according to the observations in the test set.
Perplexity is commonly used for evaluating language models. However here it is being used to evaluate the discretized distribution estimate.
It can loosely be thought of as to how well the model's distribution does in terms of the size of an equivalent uniform distribution.
Note that this metrics does not assume conditional independence of the color channels.

Here $\tau$ is the test-set made up of pairs consisting of a color name $t$, and color-space point $\tilde{x}$;
and  $p(\tilde{x}\mid t)$  the output of the evaluated model.
Perplexity is defined:
\begin{equation}
PP(\tau) = \exp_2{\left(
	\frac{-1}{|\tau|} 
	\sum_{
		\forall(t,(\tilde{x})) \in \tau}
	\log_2 p(\tilde{x}\mid t)\right)}
\end{equation}

As the perplexity for a high-resolution discretized model will inherently be very large and difficult to read,
we define the standardized perplexity: $\frac{PP(\tau)}{n_{res}}$,
where $n_{res}$ is the total number of bins in the discretization scheme.
For all the results we present here $n_{res} = 256^3$.
This standardized perplexity gives the easily interpretable values \emph{usually} between zero and one.
It is equivalent to comparing the relative performance of the model to that of a uniform distribution of the same total resolution.
$\frac{PP(\tau)}{n_{res}}=1$ means the result is equal what we would have to if we had distributed the probability mass uniformly into all bins in a 3D histogram.
$\frac{PP(\tau)}{n_{res}}=0.5$ means the result twice as good as if we were to simply use an uniform distribution: it is equivalent to saying that the correct bin is selected as often as it would be had an uniform distribution with half as many bins (ie larger bins with twice the area) were used.
The standardised perplexity is also invariant under different output resolutions.
Though for brevity we only  present results with 256 bins per channel, our preliminary results for using other resolutions are similar under standardized perplexity.



\section{Experimental Setup}

\subsection{Implementation}
The implementation of the all models was in the Julia programming language \parencite{Julia}.
The full implementation is included in the supplementary materials.
can be downloaded from the GitHub repository.\footnote{Implementation source is at \url{https://github.com/oxinabox/ColoringNames.jl}}
The machine learning components make heavy use of the MLDataUtils.jl\footnote{MLDataUtils.jl is available from \url{https://github.com/JuliaML/MLDataUtils.jl}} and TensorFlow.jl,\footnote{TensorFlow.jl is available from \url{https://github.com/malmaud/TensorFlow.jl}} packages.
The latter of which we enhanced significantly to allow for this work to be carried out.
The discretization and blurring process is done using KernalDensityEstimation.jl.%
\footnote{KernalDensityEstimation.jl  is available from \url{https://github.com/JuliaStats/KernelDensity.jl}}


\subsection{Datasets}

\subsubsection{Full Training and Testing set}
We make use of the  Munroe dataset as prepared by \textcite{mcmahan2015bayesian} from the results of the XKCD color survey.
The XKCD color survey \parencite{Munroe2010XKCDdataset}, collected over 3.4 million observations from over 222,500 respondents.
McMahan and Stone take a subset from Munroe's full survey, by restricting it to the responses from native English speakers, 
and removing very rare color names with less than 100 uses.
This gives a total of 2,176,417 observations and 829 color names. 
They also define a standard test, development and train split.


\subsubsection{Extrapolation Training and Testing Set} \label{sec:extrapodata}
The primary goal in constructing using the term based models is to be able to make predictions for never before seen descriptions of colors.
For example, based on the learned understanding of \texttt{salmon} and of \texttt{bright}, from examples like \texttt{bright green} and \texttt{bright red}, we wish for the systems to make predictions about \texttt{bright salmon}, even though that description never occurs in the training data.
%
To evaluate this generalisation capacity, we define an extrapolation sub-dataset for both testing and training.
This is defined by selecting the rarest 100 color descriptions from the full dataset,
with the restriction that every token in a selected description must still have at least 8 uses in other descriptions in the training set.
The selected examples include multi-token descriptions such as: \texttt{bright yellow green} and also single tokens that occur more commonly as modifiers than as stand-alone descriptions such as \texttt{pale}.

The extrapolation training set is made up of the data from the full training set, excluding those  corresponding to the rare descriptions.
Similar is done for the development set, so as no direct knowledge of the combined terms can leak during early-stopping.
Conversely, the extrapolation test set is made up of only the observations from the full test set that do use those rare descriptions.


By training on the extrapolation training set and testing on the extrapolation test set, we can assess the capacity of the models to make predictions for color descriptions not seen in training (extrapolating result).
A similar approach was used in \textcite{DBLP:journals/corr/AtzmonBKGC16}.
We contrast this to the same models when trained on the full training set, but tested on the extrapolation test set, to see how much accuracy was lost (non-extrapolating result).


\subsubsection{Order Testing set}
It is known that the order of words in a color description to some extent matters.
\natlang{greenish brown} and \natlang{brownish green} are distinct, if similar, colors.
To assess the models on there ability to make predictions when order matters we construct the order testset.
This is a subset of the full test set containing only descriptions with terms that occur in multiple different orders.
There are 76 such descriptions in the full dataset.
Each of which has exactly one alternate ordering.
This is unsurprising as while color descriptions may have more than 2 terms, normally one of the terms is a joining token such as \natlang{ish} or \natlang{-}.
We only constructed a order testing set: not a corresponding training set, as this is an evaluation using the model trained on the full training data.



\subsection{Common Network Features}
Dropout\parencite{srivastava2014dropout}  is used on all layers, other than the embedding layer, with threshold of 0.5 during training.
The network is optimized using Adam
\cite{kingma2014adam}, using a learning rate of 0.001.
Early stopping is checked every 10 epochs using the development dataset.
Distribution estimation methods are trained using full batch (where each observation is a distribution) for every epoch.
Point Estimation trains using randomized mini-batches of size $2^16$ observations (which are each color-space triples).
All hidden-layers, except as otherwise precluded (in side the convolution, and in the penultimate layer of the point estimation networks) have the same width 300, as does the embedding layer.


\subsubsection{Embeddings}\label{sec:embeddings} 
All our neural network based solutions incorporate an embedding layer.
This embedding layer maps from tokenized words to vectors.
We make use of 300d pretrained FastText embeddings \textcite{bojanowski2016enriching}\footnote{Available from \url{https://fasttext.cc/docs/en/english-vectors.html}}.

The embeddings are not trained during the task, but are kept fixed.
As per the universal approximation theorem \parencite{leshno1993uat, SONODA2017uat} the layers above allow for arbitrary non-linear continuous transformation.
By fixing the embeddings, and learning this transformation,
we can produce estimates of colors from their embedding alone -- without any training data at all.
This is shown in \Cref{sec:embeddingonly}.

\subsubsection{Tokenization}
For all the term based methods, we perform tokenization.
During tokenization a color name is split into terms, with consistent spelling.
For example, \natlang{bluish kahki} would become the sequence of 3 tokens: [\natlang{blue}, \natlang{ish}, \natlang{khaki}].
Other than spelling, the tokenization results in the splitting of affixes and combine tokens.
Combining tokens and related affixes affect how multiple colors can be combined.
The full list of tokenization rules can be found in the accompanying source code.
Some further examples showing how combining tokens and affixes are used and tokenized:
\begin{itemize}
	\item \natlang{blue purple} $\mapsto$ [\natlang{blue}, \natlang{purple}].
	\item \natlang{blue-purple} $\mapsto$ [\natlang{blue}, \natlang{-}, \natlang{purple}].
	\item \natlang{bluish purple} $\mapsto$ [\natlang{blue}, \natlang{ish}, \natlang{purple}]
	\item \natlang{bluy purple} $\mapsto$ [\natlang{blue}, \natlang{y}, \natlang{purple}]
	\item \natlang{blurple} $\mapsto$ [\natlang{blue-purple}]
\end{itemize}
The final example of \natlang{blurple} is a special-case.
It is the only portmanteau in the dataset, and we do not have a clear way to tokenize it into a series of terms which occur in our pretrained embedding's vocabulary (see \Cref{sec:embeddings}).
The portmanteau \natlang{blurple} is not in common use in any training set used for creating word embeddings, so no pretrained embedding is available.
As such we handle it by treating it as the single token \natlang{blue-purple} for purposes of finding an embedding.
There are many similar hyphenated tokens in the pretrained embeddings vocabulary, however (with that exception) we do not use them as it reduced the sequential modelling task to the point of being uninteresting.


\subsection{Input Modules} \Cref{sec:inputmod}


\subsubsection{Recurrent Neural Network(RNN)}
\begin{figure}
	\begin{tikzpicture}
		\node (hiddenoutput)[layer] at (0,0) {ReLU};
		\node (output)[dotted, layer, above=1 of hiddenoutput] {Output Module};
		\draw[->] (hiddenoutput) to (output);
		
		\node (GRU1)[layer, below = of hiddenoutput]{GRU};
		
		\foreach[count=\i from 1] \j in {2,...,5}
		{
			\node (GRU\j)[layer, left = of GRU\i]{GRU};
			\draw[->] (GRU\j) to (GRU\i);
		}
		
		\foreach[count=\i from 1] \word in {$\langle EOS \rangle$, green, ish, blue, light}
		{
			\node (emb\i)[layer, below = of GRU\i]  {Embedding};
			\node (word\i)[word, below = of emb\i]{\word};
			\draw[->] (word\i) to  (emb\i);
			\draw[->] (emb\i) to (GRU\i);
			\node[draw,dashed,fit= (emb\i)  (word\i)  (GRU\i)] {};
		}
		
		
		\draw[->] (GRU1) to (hiddenoutput);
	\end{tikzpicture}

	\caption{The RNN Input module for the example input \natlang{light greenish blue}. Each dashed box represents 1 time-step. \label{fig:rnnmod}}
\end{figure}


A Recurrent Neural Network is a common choice for this kind of task,
due to the variable length of the input.
The general structure of this network, shown in \Cref{fig:rnnmod} is similar to \textcite{2016arXiv160603821M}, or indeed to most other word sequence learning models.
Each word is first transformed to an embedding representation.
This representation is trained with the rest of the network allowing per word information to be efficiently learned.
The embedding is used as the input for a Gated Recurrent Unit (GRU) 
The stream was terminated with an End of Stream (\natlang{<EOS>}) pseudo-token,
as represented using a zero-vector.
The output of the last time-step is fed to a Rectified Linear Unit (ReLU).

We make use of a GRU \parencite{cho2014properties},
which we found to marginally out-perform the basic RNN in preliminary testing.
The small improvement is unsurprising, as the color names have at most 5 terms,
so longer short term memory is not required.


\subsubsection{Sum of Word Embeddings (SOWE)}
Using a simple sum of word embeddings as a layer in a neural network is less typical than an RNN structure.
Though it is well established as a useful representation, and has been used an input to other classifiers such as support vector machines. \pdfcomment{Cite some papers, e.g. mine where this is done.}
Any number of word embeddings can be added to the sum.
However, it has no representation of the order.
The structure we used is shown in \Cref{fig:sowemod}.

\begin{figure}
	\begin{tikzpicture}
	
	\node (GRU1)[]{};
	
	\foreach[count=\i from 1] \j in {2,...,5}
	{
		\node (GRU\j)[left = 2 of GRU\i]{};
	}
	
	\node (sum)[layer, above= of GRU3, xshift=1cm]{$\sum$};
	
	\foreach[count=\i from 1] \word in {green, ish, blue, light}
	{
		\node (emb\i)[layer, below = of GRU\i]  {Embedding};
		\node (word\i)[word, below = of emb\i]{\word};
		\draw[->] (word\i) to  (emb\i);
		\draw[->] (emb\i) to (sum);
	}
	
	\node (hiddenoutput)[layer, above=of sum] {ReLU};
	\node (output)[dotted, layer, above=1 of hiddenoutput] {Output Module};
	\draw[->] (sum) to (hiddenoutput);
	\draw[->] (hiddenoutput) to (output);
	
	\end{tikzpicture}
	
	\caption{The SOWE input module for the example input \natlang{light greenish blue}}
	\label{fig:sowemod}
\end{figure}



\subsubsection{Convolutional Neural Network(CNN)}


We apply a convolutional neural network to the task by applying 2D convolution over the stacked word embeddings.
\Cref{fig:cnnmod}
We use 64 filters of size between 1 and the length of the longest padded embedding (5).

\begin{figure}
	\begin{tikzpicture}
	
	\node (GRU1)[]{};
	
	\foreach[count=\i from 1] \j in {2,...,5}
	{
		\node (GRU\j)[left = 2 of GRU\i]{};
	}
	
	\node (hstack)[layer, above= of GRU3, xshift=1cm]{stack into grid};
	
	
	\foreach[count=\i from 1] \word in {green, ish, blue, light}
	{
		\node (emb\i)[layer, below = of GRU\i]  {Embedding};
		\node (word\i)[word, below = of emb\i]{\word};
		\draw[->] (word\i) to  (emb\i);
		\draw[->] (emb\i) to (hstack);
	}
	
	\node (conv)[layer, above= 0.5 of hstack]{2D Convolution};
	\node (innerconv)[layer, above= 0.5 of conv]{ReLU};
	\node (pool)[layer, above= 0.5 of innerconv]{MaxPooling};
	\node (hiddenoutput)[layer, above= 0.5 of pool] {ReLU};
	\node (output)[dotted, layer, above=1 of hiddenoutput] {Output Module};
	\draw[->] (hstack) to (conv);
	\draw[->] (conv) to (innerconv);
	\draw[->] (innerconv) to (pool);
	\draw[->] (pool) to (hiddenoutput);
	\draw[->] (hiddenoutput) to (output);
	
	%	\draw[->] (hstack) to (hiddenoutput);
	%	\draw[->] (hiddenoutput) to (output);
	
	\end{tikzpicture}
	
	\caption{The CNN input module for the example input \natlang{light greenish blue}}
	\label{fig:cnnmod}
\end{figure}




\subsection{Non-term based Direct Methods}
To establish an rough upper-limit on the modeling results
we evaluate simple direct methods.
These direct methods do not process term-by-term, they do not work with the language at all.
They simply map from the exact input text (no tokenization) to the precalculated histogram or mean of the training data for the exact color name.
It is direct in the sense that it bypassing the compositional language understanding part of the process.
It is as if the input module (\Cref{sec:inputmod}) perfectly resolved the sequence of terms into a single item.


They represent an approximate upper bound, as they fully exploit all information in the training data for each input.
There is no attempting to determine how each term affects the result.
We say approximate upper-bound, as it is not strictly impossible that the ML methods may happen to model the test data better than can be directly determined by training data as processed per exact color name.
A term based model could out perform them, however this would require learning how the terms in the color name combine in a way that exceeds the information present in the training data.
It is this capacity of learning how the terms combine that allow for the models to extrapolate the outputs for combinations of terms that never occur in the training data (\Cref{sec:extrapodata}).





\subsubsection{Direct Histogram (Distribution Estimation, only)}\label{sec:direct-histogram}
The Direct Histogram method simply returns the histogram as found for training using the method given by \Cref{sec:discretization}.
With smoothing, it returns the smoothed histogram as per \Cref{sec:kernel-density-based-smoothing}.
However, with two variation over what is used for the creation of the training data.
Firstly, they histograms are calculated using 64 bit IEEE Floating point rather than 32 bit (as the remainder of the models use), this means that when smoothing is applied the estimated nonzero area is wider.
Secondly, any empty bins in final histogram are replaced with ones containing an arbitrary small value.
We (arbitrarily) use the 64 bit machine epsilon$\approx 2^{-16}$ as this minimum value.
While this technically this change would cause the bins to not sum to a total of one, the effect is negligible and is well below the error from using 32 bit math in the other models (which also cause those models not to sum to one).
The motivation for both these changes was to ensure the direct models would not return a probability estimate of 0 for any input.
Without this the perplexity of any model that incorrectly reported a probability of zero would be undefined (IEEE \texttt{+Inf}).
We found that capping the minimum value to the machine epsilon the gives much better results than add-one smoothing, (also known as  Laplace or Lidstone smoothing) on the development dataset.
While this minimum cap penalizes results that would be zero more than add-one smoothing would, it does not reassign probability mass away from results that would be nonzero.
We consider giving the direct method this advantage is in-line with determining a rough upper bound on the result.

\subsubsection{Direct Mean Point (Point Estimation, only)}
In a similar method approach, we also a method that directy produces a point estimate from a color name.
We define this by taking the mean of all the training observations for a given exact color name  (The mean is taken in the angularly correct way \Cref{sec:angularly-correct}).
Taking the mean of all the observations gives optimal solution to minimizing the mean squared error on the training data set.
As with our direct distribution estimation method, this bypasses the term based language understanding, and directly exploits the training data.
It thus represents an approximate upper bound on the performance of the term based models.




\subsection{Output Modules}
\subsubsection{Distribution Estimation}

For all the distribution estimation systems we investigate here, 
we consider training both on the binned-data, and on the smoothed data (as described in \Cref{sec:kernel-density-based-smoothing}).
Making use of the conditional independence assumption (see \Cref{sec:conditional-independence-assumption}), we output the three discretized distributions.
This is done using 3 softmax output layers.

The output module for distribution estimation 

Contrasting to estimating continuous conditional distributions, 
estimating a discrete conditional distributions is a significantly more studied application of neural networks
-- this is the basic function of any softmax classifier.
To simplify the problem, we therefore transform it to be a discrete distribution estimation task, by discretizing the color-space.
Discretization to a resolution of 64 and 256 bins per channel is considered.


For the case of the machine learning models, the output is produced using softmax layers.


\begin{figure}
	\newcommand{\picwidth}{60pt}
	\begin{tikzpicture}
	
	\node (input)[layer, dotted]{Input Module};
	
	\node(outHue)[layer, above left = of input] {Softmax};
	\node(outSaturation)[layer, above = of input] {Softmax};
	\node(outValue)[layer, above right = of input] {Softmax};
	
	\foreach \p in {Hue, Saturation, Value} 
	{
		\draw[->] (input) to (out\p);
		
		\node(plot\p)[above = of out\p, text width=\picwidth]{
			\includegraphics[width=\picwidth]{netdia/\p}
			\\
			\p
		};
		\draw[->] (out\p) to (plot\p);
	}
	
	\end{tikzpicture}
	
	\caption{The Distribution Output Module \label{fig:distoutmod}}
\end{figure}
	

\subsubsection{Point Estimation}\label{sec:point-estimation}
Our point estimation output model for the neural network is shown in \Cref{fig:pointoutmod}.
The hidden-layer from the top of the input module is feed to an 4 single output neurons.\footnote{Equivalently these 4 single neurons can be expressed as a layer with 4 outputs and 2 different activation functions.}
Two of these are used the sigmoid activation function (range 0:1) to produce the outputs for the saturation and value channels.
The other two use the tanh activation function (range -1:1), they produce the intermediate output that we call $y_{shue}$ and $y_{chue}$ for the sine and cosine of the hue channel respectively.
We use these intermediate values when calculated this loss function as it results in a loss function that is continuous and correctly handles the wrap-around nature of the hue channel.

During training we use the following loss function for each observation $y^\star$, and each corresponding prediction $y$.
\begin{align}
loss &= %
\frac{1}{2} \left(\sin(y^\star_{hue}) - y_{shue} \right)^2     \nonumber \\
&+ \frac{1}{2} \left(\cos(y^\star_{hue}) - y_{chue} \right)^2  \nonumber \\
&+ \left(y^\star_{sat} - y_{sat} \right)^2  \nonumber \\
&+ \left(y^\star_{val} - y_{val} \right)^2 %
\end{align}
This mean of this loss is taken over all observations in each mini-batch during training.





\begin{figure}
	\newcommand{\picwidth}{60pt}
	\begin{tikzpicture}
	
	\node (hiddenout)[layer, dotted]{Input Module};
	%\node (hiddenout)[above= 1 of input]{Affine with output width 4};
	%\draw[->](input) to (hiddenout);
	
	\node(outHue)[xshift=-1cm, above = 5 of hiddenout] {$y_{hue}$};
	\node(outSaturation)[above = 5 of hiddenout] {$y_{sat}$};
	\node(outValue)[xshift=1cm, above = 5 of hiddenout] {$y_{val}$};
	
	\node(Z)[above=-0.2cm of hiddenout.north]{};
	\node(Z2)[left=0cm of Z]{};%{$z_2$};
	\node(Z1)[left=0.3cm of Z2]{};%{$z_1$};
	\node(Z3)[right=0cm of Z]{};%{$z_3$};
	\node(Z4)[right=0.3cm of Z3]{};%{$z_4$};
	
	
	\node(s3)[layer, above=1 of Z3, xshift=0.5cm]{$\sigma$};
	\node(s4)[layer, above=1 of Z4, xshift=1cm]{$\sigma$};
	\draw[->](Z3) to (s3) to (outSaturation);
	\draw[->](Z4) to (s4) to (outValue);
	
	
	\node(AngHue)[layer, below = 1 of outHue, xshift=-0.7cm]{$ \mathrm{atan2} \left(y_{shue}, y_{chue} \right) $ };
	\draw[->](AngHue) to (outHue.south);
	
	\node(s1)[layer, above=1 of Z1, xshift=-1cm]{$\tanh$};
	\node(s2)[layer, above=1 of Z2, xshift=-0.5cm]{$\tanh$};
	\draw[->](Z1) to (s1);
	\draw[->](s1) to node[left]{$y_{shue}$} (AngHue);
	\draw[->](Z2) to (s2);
	\draw[->](s2) to node[right]{$y_{chue}$}  (AngHue.340);
	
	
	
	
	
	\end{tikzpicture}
	
	\caption{The Point Estimate Output Module.
		Here $\mathrm{atan2}$ is the quadrant preserving arctangent, outputting as a regularized angle (as per in all evaluations).
		 \label{fig:pointoutmod}}
\end{figure}


\section{Results}


\input{qualres.tex}
\input{quantres.tex}





\section{Conclusion}

The RNN is the standard type of model for this task.
However, we find its performance to be significantly exceeded by the SOWE and CNN.
The SOWE is an unordered model correspondent to a bag of words.
The CNN over word embeddings is correspondent to a bag of ngrams, in our case as bag of all 1,2,3,4 and 5-grams.
This means the CNN can readily take advantage of both fully ordered information, using the filters of length 5, down to unordered information using the filters of length one.
The RNN however must fully process the ordered nature of its inputs, as its output comes only from the final node.
It would be interesting to further contrast a bidirectional RNN.


We believe that to improve results further for the distribution estimation models,
it would benefit from not merging the training data into a single histogram per color.
But rather converting each observation into a one-hot representation of its bin.
We initially avoiding this solution as it precludes (directly) using KDE-based smoothing of the histograms.
However our results in this work indicate that the machine learning distribution estimation models is not substantially effected by the use of KDE-based smoothing or not.
While the single observations represented as one-hot would be less informative, there would be several orders of magnitude more of them, and the natural of training with random minibatches would result in a model that is less likely to get stuck in local minima, as so may achieve better results.

A further interesting avenue for investigation would condition the model not only on the words used but also on the speaker.
The original source of the data \textcite{Munroe2010XKCDdataset}, includes some demographic information which is not exploited by any known methods.
It is expected that color-term usage may vary with subcultures.



\clearpage
\bibliography{master}

\clearpage
\appendix
\input{supp.tex}



\end{document}
