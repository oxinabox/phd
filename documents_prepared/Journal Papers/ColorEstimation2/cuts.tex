

The Direct Histogram method simply returns the histogram as found for training using the method given by \Cref{sec:discretization}.
With smoothing, it returns the smoothed histogram as per \Cref{sec:kernel-density-based-smoothing}.
However, with two variation over what is used for the creation of the training data.
Firstly, they histograms are calculated using 64 bit IEEE Floating point rather than 32 bit (as the remainder of the models use), this means that when smoothing is applied the estimated nonzero area is wider.
Secondly, any empty bins in final histogram are replaced with ones containing an arbitrary small value.
We (arbitrarily) use the 64 bit machine epsilon$\approx 2^{-16}$ as this minimum value.
While this technically this change would cause the bins to not sum to a total of one, the effect is negligible and is well below the error from using 32 bit math in the other models (which also cause those models not to sum to one).
The motivation for both these changes was to ensure the direct models would not return a probability estimate of 0 for any input.
Without this the perplexity of any model that incorrectly reported a probability of zero would be undefined (IEEE \texttt{+Inf}).
We found that capping the minimum value to the machine epsilon the gives much better results than add-one smoothing, (also known as  Laplace or Lidstone smoothing) on the development dataset.
While this minimum cap penalizes results that would be zero more than add-one smoothing would, it does not reassign probability mass away from results that would be nonzero.
We consider giving the direct method this advantage is in-line with determining a rough upper bound on the result.

\subsection{Kernel-Density Estimation Based Smoothing}
As the distribution is not truly discrete, we investigate adding a Kernel-Density Estimation (KDE) based smoothing step.
KDE is a common approach in non-parametric estimation.

This is expected as a blurred discrete distribution captures some of the notions of continuity that a truly continuous output distribution would intrinsically feature.


We use the Fast Fourier Transform (FFT) based KDE method of the \textcite{silverman1982algorithm}.
We use a Gaussian kernel, and select the bandwidth per color description based on leave-one-out cross validation on the training data.
A known issue with the FFT based KDE method is that it has a wrap-around near the boundaries where mass that would be assigned outside the boundaries is instead assigned to the bin on the other side.
For the value and saturation channels we follow the standard solution of initially defining additional bins outside the true boundaries, then discard those bins and rescale the probability to one.
For the hue channel this wrap-around effect is exactly as desired.
