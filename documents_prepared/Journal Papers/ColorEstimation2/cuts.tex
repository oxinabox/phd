

The Direct Histogram method simply returns the histogram as found for training using the method given by \Cref{sec:discretization}.
With smoothing, it returns the smoothed histogram as per \Cref{sec:kernel-density-based-smoothing}.
However, with two variation over what is used for the creation of the training data.
Firstly, they histograms are calculated using 64 bit IEEE Floating point rather than 32 bit (as the remainder of the models use), this means that when smoothing is applied the estimated nonzero area is wider.
Secondly, any empty bins in final histogram are replaced with ones containing an arbitrary small value.
We (arbitrarily) use the 64 bit machine epsilon$\approx 2^{-16}$ as this minimum value.
While this technically this change would cause the bins to not sum to a total of one, the effect is negligible and is well below the error from using 32 bit math in the other models (which also cause those models not to sum to one).
The motivation for both these changes was to ensure the direct models would not return a probability estimate of 0 for any input.
Without this the perplexity of any model that incorrectly reported a probability of zero would be undefined (IEEE \texttt{+Inf}).
We found that capping the minimum value to the machine epsilon the gives much better results than add-one smoothing, (also known as  Laplace or Lidstone smoothing) on the development dataset.
While this minimum cap penalizes results that would be zero more than add-one smoothing would, it does not reassign probability mass away from results that would be nonzero.
We consider giving the direct method this advantage is in-line with determining a rough upper bound on the result.

\subsection{Kernel-Density Estimation Based Smoothing}
As the distribution is not truly discrete, we investigate adding a Kernel-Density Estimation (KDE) based smoothing step.
KDE is a common approach in non-parametric estimation.

This is expected as a blurred discrete distribution captures some of the notions of continuity that a truly continuous output distribution would intrinsically feature.


We use the Fast Fourier Transform (FFT) based KDE method of the \textcite{silverman1982algorithm}.
We use a Gaussian kernel, and select the bandwidth per color description based on leave-one-out cross validation on the training data.
A known issue with the FFT based KDE method is that it has a wrap-around near the boundaries where mass that would be assigned outside the boundaries is instead assigned to the bin on the other side.
For the value and saturation channels we follow the standard solution of initially defining additional bins outside the true boundaries, then discard those bins and rescale the probability to one.
For the hue channel this wrap-around effect is exactly as desired.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


When working with the hue, all measures need to take into account the wrap-around effect.
When ever we refer to mean squared error, mean or mode on the HSV space in this paper, we are referring to the angularly corrected forms given in \Cref{sec:angularly-correct}.

Unlike the RGB color space, the HSV channels do correspond to how humans perceive colors.
However, it is not designed to be a perceptionally uniform color space across hue (unlike CIELab), which does suggest using squared error for the point estimates is not necessarily finding the perceptual average of the color color.
However, given the other issues outlined above with point estimates that averaging at all is not ideal given the asymmetrical and multimodal nature, such that  even a more perceptual average would not solve this.
We thus we do not judge this a major concern, and focus on it as a modelling task.
Point estimates have only limited utility, and we include them as a secondary task for the assessment of the models capacities.
There are no such issues for our distribution estimations.

An important advantage of HSV over other color spaces is that it best meets the assumption that for a given color name each channel is statistically independent (see \Cref{sec:conditional-independence-assumption}).
