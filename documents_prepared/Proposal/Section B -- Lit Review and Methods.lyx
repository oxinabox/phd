#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass amsart
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Lit Review
\end_layout

\begin_layout Standard
The existing works have not focused on the reversibility of the embeddings
 becuase A and B are from Google Research and Microsoft Bing research respective
ly, and are thus focuses on the us of phrase embeddings for search engine
 query and indexing -- an aplication that only requires similarity measures
 between known documents.
 Sorcher's work formed part of his PhD thesis
\begin_inset CommandInset citation
LatexCommand cite
key "socher2014recursive"

\end_inset

, which covered both Natural Language Processing and Computer Vision, reversibli
ty for image embeddings would be expected to be substancially more complex
 than for natural language.
\end_layout

\begin_layout Section
Methods
\end_layout

\begin_layout Standard
The following methods of extending existing phrase embedding models will
 be considered:
\end_layout

\begin_layout Itemize
URAE
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

Morphing
\begin_inset Quotes erd
\end_inset

 URAE -- search over possible strucural trees
\end_layout

\begin_deeper
\begin_layout Itemize
After training a URAE conventionally
\end_layout

\begin_layout Itemize
Ask it to Unfold into a given structure, from a given phrase vector (generated
 using the URAE as normal)
\end_layout

\begin_layout Itemize
The Error of this structure is the sum of the distances of the nearest words
 to unfolded word embeddings
\end_layout

\begin_layout Itemize
Perform a search over a reasonable subset of all trees and find the best
\end_layout

\begin_deeper
\begin_layout Itemize
Madlibs: For some circumstancess, only a subset of trees need to be considered,
 or even just a single tree, for example if looking at reviews, forcing
 the sentence to be of the form: 
\begin_inset Quotes eld
\end_inset

The ____ was good, but the ____ was bad
\begin_inset Quotes erd
\end_inset

.
 Having a probability distribution over those two missing words would be
 useful.
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Itemize
PV-DM: Paragraph Vector Document Model.
\end_layout

\begin_deeper
\begin_layout Itemize
The PV-DM model is evaluated on new paragraphs which it has not seen by
 locking the word embeddings and the predictor weights, and training just
 the paragraph vector on its section until it converges.
 
\end_layout

\begin_deeper
\begin_layout Itemize
We could instead do the reverse -- lock the paragraph vector, and predictor
 weights, and train section of the word embeddings until they converge.
\end_layout

\begin_layout Itemize
However this require us to know next word after the words we are solving
 for.
\end_layout

\begin_layout Itemize
We can give ourselves this knowledge by terminating every training phrase
 with a pseudoword -- eg 
\begin_inset Quotes eld
\end_inset

*END*
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Itemize
We can also know when to stop by staring each training phase with 
\begin_inset Quotes eld
\end_inset

*START*
\begin_inset Quotes erd
\end_inset

 similar to the existing padding using in the paper
\end_layout

\begin_layout Itemize
in the case of the phrase 
\begin_inset Quotes eld
\end_inset

A B C D E F G
\begin_inset Quotes erd
\end_inset

, with window size 3, the cases are 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

*START* A B 
\begin_inset Formula $\to$
\end_inset

C
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

A B C 
\begin_inset Formula $\to$
\end_inset

D
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

B C D
\begin_inset Formula $\to$
\end_inset

E
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

C D E
\begin_inset Formula $\to$
\end_inset

F
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

D E F
\begin_inset Formula $\to$
\end_inset

G
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

E F G
\begin_inset Formula $\to$
\end_inset

*END*
\begin_inset Quotes erd
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
The issue with running the predictor backwards through these stages is that
 the label for the next word in PV-DM is catagorical -- it is the word (Implemen
ted as the words abriary but fixed index), which is not being reproduced
 as we step back.
 Consider the second last case: 
\begin_inset Quotes eld
\end_inset

D E F
\begin_inset Formula $\to$
\end_inset

G
\begin_inset Quotes erd
\end_inset

 -- we do not know the word that the G embedding which we reconstructed
 in the final step is.
 We need a probability distribution for that what the G index is.
 This could be estimated using nearest neighbours and a similarity metric
 (normalised over all words in a certain radius).
 How often to do this normalistation would require experementation.
\end_layout

\begin_deeper
\begin_layout Itemize
An alternive to having to solve this proplem would be to redescribe the
 model as predicting not the next word but the next word embedding.
\end_layout

\begin_deeper
\begin_layout Itemize
However this has the word embeddings as largely free variables, and a valid
 set of word embedding vectors would be to set them all equal.
 So it can not be done without further adaptoon
\end_layout

\begin_layout Itemize
There may be a method for doing this using Multimodal solvers -- as used
 in X to recognise speach using video and audio.
\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
The playoff of parameters is the window length and the paragraph embedding
 vs word embedding width.
 The larger the paragraph vector the better able it is to encode more informatio
n.
 The longer the word embedding the easier to distigish between words.
 However increasing both of this increases the compuation time.
 The longer the windows the more context is directly available (so less
 short sighted ideas.), however that is more information to encode itno the
 phrase vector and again more computation time.
 
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "C:/Users/Frames/Documents/GitHub/phd/Resources/master_bibliography/master"
options "bibtotoc,IEEEtran"

\end_inset


\end_layout

\end_body
\end_document
