Work Completed: Semantic Evaluation
Current methods for assessing the quality of representations are not sufficient to a certain whether they are sufficiently semantically consistent for use in resythisis.
The predominant assement method is the use as a classifier for sentiment analysis. While sentiment analysis does have  semantic componant, it is far too loose grained to reveal if sentences with the same meaning are colocated in vector space.
Thus a new methodfor direct semantic evaluation was created.
To enablle the checking of the semantic localisation sentences from real world corpora were categorised into groups of paraphrases -- sentences with the same semantic meaning.
They groups were straterfied into testing and training splits and a linear support vector machine was used to classify the sentences in their paraphrase groups.
The SVM was given embeddings from PVDM, DBOW, URAE, and the Mean of Word Embeddings each in turn.
Being able to correctly classify the sentences indicase that the embeddings and meanings consistently align and do not overlap.
Unexpectedly, the Mean of Word Embeddings performed substainally better than the more complex models.
This result does line up with the recent result of Ritter Et Al for Sum of Word Embedddings.


Sum Of Word Embeddings
A sentence as the sum of its words is a neat, and unexpected result. It bears investigation and development.
Resythisising a sentence from it's sum is a theoretically intractable problem, but with suitable approximations and heuristics may be feasible.
The problem of working out which word embeddings add up to a particular vector is equivalent to the Subset Sum problem which is proven NP-Complete.
The second problem is the loss of word order.
Both may be solved through the use of classical statical NLP:
collecting co-occurance, ngram (Probability of next word, given previous n-words) statisics from a suitable corpus.
The co-occurance statics can be used to greedily prune the search space so that embeddings words which are only rarely found together are not considered as possible partial sums.
The ngrams can be used to order the words, once the unordered set is solved for.
The problem would also require aggressive dimensionality reduction to reduce to time-complexity.
Preliminary results have shown that it is viable to preform substantial dimensionality reduction, without correspondent loss of information.
This is helped, as it the overall problem, by restricting the vocabulary -- for example only using the 1000 most common English words.
Given these constraints and heuristics, it should be feasible to tackle the problem using dynamic programming, 
or ant-colony optimisation.
Other investigations to be co-occurring with the development of resythisis, would be extending word embeddings to use the syntactic parts of speech labels.
Current word embeddings are based on only the word -- that means that there is one embedding for bank as in to "bank a plane", as for bank as in "the bank of a river".
Using current systems it is possible to reliably parts of speech tag, which will allow the separation of such homonyms.
However it does not completely solve the problem, A financial "bank" and a river bank are still the same. A complete solution would be require word sense disambiguation. 
Currently, the best word sense disambiguation algorithms barely perform above always choose the most common word sense.
This is worse than assigning a combined embedding for all uses with position given as  a frequency occurrence weighted centeroid of the true positions.

