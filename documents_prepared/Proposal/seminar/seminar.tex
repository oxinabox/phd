\makeatletter
\newif\ifGm@compatii
\makeatother
\documentclass[12pt,landscape,english]{beamer}



\usepackage{bbding}
\usepackage{etoolbox, verbatim}

\usepackage{graphicx}
\graphicspath{{../figs/}}

%%%%%%%%%%%%%%%TIKZ

\usepackage{tikz}
\usetikzlibrary{positioning, fit,arrows,chains,shapes.geometric}
\usetikzlibrary{graphs,graphdrawing}
%\usetikzlibrary{decorations.text}
\usetikzlibrary{petri}
\usegdlibrary{force, layered, trees}
%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%Bibliography
\usepackage[backend=bibtex, url=false,
bibstyle=ieee,firstinits=true]{biblatex}
\bibliography{../../../Resources/master_bibliography/master.bib}
\renewcommand*{\thefootnote}{} %No symbol or marker
\renewcommand{\footnotesize}{\scriptsize}
%%%%%%%%%%%%%%%%%




\mode<presentation>
%\setbeameroption{show notes}

%%%%%%%%%%%%%%%Template
\setbeamertemplate{navigation symbols}{}
\setbeamercolor{palette sidebar secondary}{fg=black,bg=blue!30!gray}
\setbeamercolor{sidebartitle}{fg=black,bg=blue!30!gray}
\useoutertheme{sidebar_custom}
\usebackgroundtemplate{\includegraphics[width=\paperwidth,height=\paperheight]{uwa_eng.png}}

\setbeamertemplate{itemize item}{\raisebox{-2px}{\JackStarBold} \hskip -.2em}
\setbeamertemplate{itemize subitem}{\raisebox{-2px}{\FourAsterisk} \hskip -.2em}
\setbeamertemplate{itemize subsubitem}{\raisebox{-2px}{\JackStar} \hskip -.2em}


\setbeamercolor{title}{fg=black}
\usefonttheme{structurebold}
\addtobeamertemplate{frametitle}{}{\vskip-1.2cm}

\setbeamercolor{sidebar}{fg=darkgray!80!black}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\makeatletter





\begin{document}
	
% Titlepage info

\title[Semantic Vectors]{Semantic Vector Representations for Natural Language}
\author[L.~White]{
	Lyndon White \texorpdfstring{\\\medskip{\scriptsize Supervised by:\\ Prof Roberto Togneri\\ Dr Wei Liu\\W/Prof Mohammed Bennamoun\\ }}{}}

\date{} % show no date
%\titlegraphic{\includegraphics[width=2.1cm]{balls}}

\begin{frame}[plain]
	\vfill
	\vspace{2cm}
	\titlepage
\end{frame}
}

%%%%%%%%%%%%%%%%%%%%Definitions

\def\x{\tilde{x}}
\def\y{\tilde{y}}
\def\b{\tilde{b}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}
\begin{frame} {Natural Language} 
	
	\note{Shown are two lists of information. On one side are examples of informaition in a form easily accessable by software. On the other is information recored digitally in natural language.
		}
\begin{columns}[T]
	\begin{column}{0.5\textwidth}
		\centering\resizebox{0.4\textwidth}{!}{\includegraphics{pres/people.png}}
		\begin{itemize}
			\item $10^7$ {  StackOverflow QAs}
			\item $10^7$ Digitalised Books \note[item]{Google has digitalisd over 20 million books alone}
			\item $10^7$ Wikipedia Articles
			\item $10^8$ Amazon Reviews \note[item]{Amazon reviews from the Amazon Review dataset}
			\item $10^8$ Journal Articles \note[item]{Journal Articles from Web of Science extrapolated}
			\item ...

		\end{itemize}		
	\end{column}
		\begin{column}{0.5\textwidth}
			\centering\resizebox{0.4\textwidth}{!}{\includegraphics{pres/PC.png}}
			\begin{itemize}
				\item $10^6$ {  Cyc: Common- sense Facts}
				\item $10^6$ DBpedia Entries
				\item $10^8$ DNA Sequences \note[item]{GenBank Nucleotide}
%				\item $10^9$ Weather Records
				\item $10^{10}$ Census Records
				\item ...
			\end{itemize}		
		\end{column}
\end{columns}
	\note{NLU is the subarea of NLP which is base around creating software which in some sense comprehends the natural language.}
\end{frame}


\begin{frame} {Applications: Embeddings} 
		\vspace{1ex}
	\centering\resizebox{!}{0.5\textheight}{\includegraphics{./embed}}
	\vspace{1ex}
	\begin{itemize}
		\item Extractive Summarization
		\item Similarity Measurement \note{More over if the vector space can be made common between two languages, then we get biligaul sentence similarity, which has applications for machine translation.}
		\item Searching for Images with Descriptions \note{Historically Image search worked by looking for key words on a website and then returning that websites pictures. If we convert both the Embed both images and text to a common vector space then we can do similarity matching as just discussed}
		\item Feature Detector For Classifiers
		\note{\begin{itemize}
			\item Sentiment Analysis
			\item Topic Classification
			\item Paraphrase Detection
		\end{itemize}}
	\end{itemize}
\end{frame}

\begin{frame} {Applications: Full Cycle} 
	\centering\resizebox{!}{0.5\textheight}{\includegraphics{./workflow}}
	\begin{itemize}
		\item Abstractive Summarization
		\item Paraphrase Generation \note{and this means using the multilanguage common vector space from before, Phrasal translation.}
		\item Generating Image Descriptions
		\item Semantically Lossy Text Compression \note{This is neat. Like a JPG image can be compressed by 2 orders of magnitude, at the cost of intoducting a few visual inaccuracies and blurs. We could compress text, at the cost of introducing artifacts in meaning.}
	\end{itemize}
\end{frame}


\section{Current Methods}
\begin{frame} {2 minute Intro To NNs}
	\begin{columns} 
	\begin{column}{0.5\textwidth}
		\centering\input{../figs/NN.pgf}
	\end{column}
	\begin{column}{0.5\textwidth}
	
			$f:\x\mapsto\y$
			$$NN\big(W_1,\b_1,..;\x\big)\approx f \big(\x\big)$$

		\colorbox{lime}{Tune:	$W_i,\b_i$}

	\end{column}
	\end{columns}
	\note[item]{But how to we get the vector representation $\x$ for the input in question?}
	\note[item]{Lets just assume we already knew it}
\end{frame}

\begin{frame} {NN with Embedding Look Up}
	\centering\resizebox{!}{0.7\textheight}{\input{../figs/LU_NN2.pgf}}
	
	\rightskip 1ex \colorbox{lime}{Tune:	$W_i,\b_i,\x_j$}
\end{frame}

\begin{frame} {Neural Probabalistic LM}
	\centering\resizebox{!}{0.7\textheight}{\input{../figs/NPLM1.pgf}}
	\footfullcite{NPLM}
\end{frame}

\begin{frame} {Neural Probabalistic LM}
	\centering\resizebox{!}{0.7\textheight}{\input{../figs/NPLM2.pgf}}
	\footfullcite{NPLM}
\end{frame}

\begin{frame} {Neural Probabalistic LM}
	\centering\resizebox{!}{0.7\textheight}{\input{../figs/NPLM3.pgf}}
	\footfullcite{NPLM}
	
	\note{\begin{itemize}
		\item By tweaking this general structure we can produce a variety of models
		\item replace the simple classifier with a Convolutional Neural Net and share the word embedding look-up table across several tasks, and you get the Colbert and Weston Embeddings
		\item remove the hidden layer and average instead of concatenating and you get Continuous Bag of Word Embeddings
		\item make the task to use one word embedding to work out all the context works and you get Skip nGrams.
		\item There are a lot of word-embedding methods based on this idea
		
		\item That there is the limitation though, we get word embeddings, vector representations for the words. But most of the documents we want to use NLU  to understand are not words. They are sentences. Which is what the rest of this presentation will focus on.
		
		\item One issue is that we are not looking at all the content all the time. If a word at the start of a sentence has a lot of interaction with a word at the end, then the windowed model can't hand that.
		
		
		\end{itemize}

		}
	
	
\end{frame}


\begin{frame} {PV DM} 
	\centering\resizebox{!}{0.7\textheight}{\input{../figs/PVDM1.pgf}}
	\footfullcite{le2014distributed}
\end{frame}


\begin{frame} {PV DM} 
	\centering\resizebox{!}{0.7\textheight}{\input{../figs/PVDM2.pgf}}
	\footfullcite{le2014distributed}
\end{frame}


\begin{frame} {PV DM} 
	\centering\resizebox{!}{0.7\textheight}{\input{../figs/PVDM3.pgf}}
	\footfullcite{le2014distributed}
	
	\note[item]{This is a nice method. Now we have vectors for sentences, or paragraphs. As well as for words.}
\end{frame}


\begin{frame} {RvNN} 
	\centering\resizebox{!}{0.7\textheight}{\input{../figs/RvNN.pgf}}
	\note[item]{ constituency-based parse tree, Not Reoccurant}
	\note[item]{Here is an alternitive, the Recurssive Neural Net, merges its inputs, pairwise in a tree}
	\note[item]{At each level we get a new embedding. Which is concatenated with another and merged again until we reach the top}
	\note[item]{It reuses the same embedding matrix}
	\note[item]{In the black box can go more or less anything, probably a more traditional neural network classifier, so long as it produces a error signal which can be backproperated.}
	\footfullcite{socher2010PhraseEmbedding}
\end{frame}

\begin{frame} {URAE}
		\vskip 2ex
		\centering\resizebox{!}{0.8\textheight}{\input{../figs/URAE.pgf}}
		\footfullcite{SocherEtAl2011:PoolRAE}
\end{frame}

\begin{frame} {Semisupervised URAE} 
		\vskip 2ex
		\centering\resizebox{!}{0.8\textheight}{\input{../figs/ssURAE.pgf}}
		\footfullcite{zhang2014BRAE}
\end{frame}

\section{Future Work}
\begin{frame}{Not reversible}
	Even The URAE which looks reversible, requires you to know the way out to get there.
	For RvNN decended models, we end up with a tree search problem
\end{frame}

\begin{frame}{Not semantic, enough}
	
	S/MOWE
	BRAE
\end{frame}


\section{Conclusion}
\begin{frame}{Conclusion: There}
	\begin{itemize}
		\item Too much information for people to handle
		\item but written only for people
		\item need to make machines handle Natural Language
		\item machine are good at numbers
		\item so we turn sentences into numbers.
	\end{itemize}
\end{frame}

\begin{frame}{Conclusion: and Back Again}
	\begin{itemize}
		\item Once they are vectors, operate on them
		\item Go back to sentences
		\item to go back to sentences, position needs to corespond to meaning
		\item and must have method to get back
	\end{itemize}
\end{frame}


	


\end{document}
