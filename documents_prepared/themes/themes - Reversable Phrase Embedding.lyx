#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass scrartcl
\begin_preamble
\usepackage{url}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Reversable Phrase Embedding
\end_layout

\begin_layout Standard
These three themes are broadened and generalized from discussed topics.
\end_layout

\begin_layout Standard
featuring: Re-Synthesis of Text from Vector Representations
\end_layout

\begin_layout Standard
See also: Natural Language Generation (the opposite of natural language
 understanding.)
\end_layout

\begin_layout Section
Applications / Possible Tasks/Projects within
\end_layout

\begin_layout Itemize
Summarization of a paragraph into a sentence
\end_layout

\begin_deeper
\begin_layout Itemize
Abstraction summarization
\end_layout

\end_deeper
\begin_layout Itemize
Production of a Short Title, via trade-off function between length and capture
 of meaning
\end_layout

\begin_deeper
\begin_layout Itemize
Again Abstractive summarization
\end_layout

\end_deeper
\begin_layout Itemize
Translation
\end_layout

\begin_deeper
\begin_layout Itemize
Using two stocatically reversible mappings from different language spaces
 into the same Meaning space
\end_layout

\end_deeper
\begin_layout Itemize
Robustness/Flow/Re-Synthesis cross over: Finding corrupted sentences/paragraphs
 and inserting correct word/sentence.
\end_layout

\begin_layout Itemize
The creation of a generative, rather than a discriminative model for phrase
 embeddings.
\end_layout

\begin_layout Section
Existing Work
\end_layout

\begin_layout Subsection
RvNN: Recursive Neural Networks -- Parser 
\end_layout

\begin_layout Standard
At Stanford, Socher et al, produced several papers around the development
 of a Recursive Neural Network.
 Abbreviated in throse works as RNN, in other works and here it is abbreviated
 RvNN to distinguish it for Reoccurrent Neural Networks.
 
\end_layout

\begin_layout Subsubsection
In Practice
\end_layout

\begin_layout Paragraph
The Neural Network
\end_layout

\begin_layout Standard
The general premise of the RvNN is for a neural network with several layers
 as shown in the figure.
 Note the multiple output layers.
 Note also the tied weights on the two 
\begin_inset Formula $L$
\end_inset

s.
 
\begin_inset Formula $W$
\end_inset

 takes 
\begin_inset Formula $c_{1}$
\end_inset

 concatenated with 
\begin_inset Formula $c_{2}$
\end_inset

as input and produced a merged parent with 
\begin_inset Formula $p_{1,2}$
\end_inset

.
\end_layout

\begin_layout Standard
Which is given to a scoring matrix to score how good the merge is,
\end_layout

\begin_layout Standard
and to a labeling matrix which assigns it a Part of Speech (POS) label.
\end_layout

\begin_layout Standard
The correct POS label is provided as training data.
\end_layout

\begin_layout Standard
It is applied Recursively across pairs of inputs (children) to create a
 total merged output.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename figs/RvNN_single.png
	width 50page%

\end_inset


\end_layout

\begin_layout Subparagraph
Max-Margin
\end_layout

\begin_layout Standard
TODO: Workout if explict corrupted words are actually ever involved in training
 using max margin, or if infact they are only part of the motivating logic.
 
\end_layout

\begin_layout Standard
If they are involved then certain tweaks can be done to do things like Corrupt
 replacing 
\emph on

\begin_inset Quotes eld
\end_inset

basketball player
\begin_inset Quotes erd
\end_inset


\emph default
 with 
\emph on

\begin_inset Quotes eld
\end_inset

basketballer
\begin_inset Quotes erd
\end_inset


\emph default
 and use the converse 
\begin_inset Quotes eld
\end_inset

Min-Margin
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Paragraph
The Tree
\end_layout

\begin_layout Subsubsection
Variations
\end_layout

\begin_layout Standard
The variation in 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2013:CVG"

\end_inset

 added a Context Grammar, to change between several 
\begin_inset Formula $W$
\end_inset

 matrices depending on the POS.
\end_layout

\begin_layout Standard
This Compositional Vector Grammar is now used in the Stanford Parser, a
 gold standard for parsing.
\end_layout

\begin_layout Subsubsection
Assessment
\end_layout

\begin_layout Standard
The success of the algorithm is measures on how well it is able to parse
 text (assign correct trees with correct POS tags).
\end_layout

\begin_layout Standard
Non-rigorous observations are made in 
\begin_inset CommandInset citation
LatexCommand cite
key "socher2010PhraseEmbedding"

\end_inset

, after embedding many sentences from the Wall Street Journal corpus, that
 similar phrase embeddings were located near each other.
\end_layout

\begin_layout Subsection
Phrasal Translations
\end_layout

\begin_layout Standard
Work has been done on translating phrases, using word embeddings
\begin_inset CommandInset citation
LatexCommand cite
key "zou2013bilingual"

\end_inset

.
 However this is using word embeddings only.
\end_layout

\begin_layout Standard
It seems to be quiet tied to actually mapping between words in word space
 (see also 
\begin_inset CommandInset citation
LatexCommand cite
key "phrasaltranslationtool"

\end_inset

), rather than moving to a semantic space and then back again.
\end_layout

\begin_layout Subsection
Single Word Embedding
\end_layout

\begin_layout Standard
The alot of work on Word Embeddings note the existence of linear substructures
 in single-word embeddings for example: 
\begin_inset Formula $v("king")-v("man")+v("woman")$
\end_inset

 has nearest neighbor 
\begin_inset Formula $v("queen")$
\end_inset

.
 The paper which first established this seems to be 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013linguisticsubstructures"

\end_inset

.
 Single word generation is fairly simple via nearest neighbor for word embedding.
\end_layout

\begin_layout Standard
Word Embedding spaces seem to combine syntax and semantics, when combining
 the words from a sentence it would be better to end up in a syntax free
 space.
 
\end_layout

\begin_layout Subsection
Summarization
\end_layout

\begin_layout Standard
Notably this would be Abstractive Summarization, of building a model then
 generating text from it.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Kagebaeck2014"

\end_inset

(on Extractive Multidocument Summarization) using word and phrase embedding
 stated that the authors were not aware of any other papers which used continuou
s vector space models for summarization tasks.
 Which is indicative that this area is not well explored, as summarization
 is a very natural use to put any Re-Synthesis system to.
\end_layout

\begin_layout Subsection
Knowledge base systems
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "MemoryNN"

\end_inset

 is quiet a complicated system for performing full natural language queries
 with natural language output, and with memorisation.
\end_layout

\begin_layout Standard
It has several componants, notable for this section is the R (Response).
 In most of there examples the response is set up to just returns a single
 word response, but some example are shown where they used a RNN.
 Details are not given, but I believe they used a RNN language model similar
 to 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2011RnnLM"

\end_inset

 (which they reference).
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2011RnnLM"

\end_inset

 does not make use of word embeedings, and is very similar to n-gram based
 model.
\end_layout

\begin_layout Standard
There is also 
\begin_inset CommandInset citation
LatexCommand cite
key "Socher2013TensorReasoning"

\end_inset

, which models word relationships with tensors, in a way a much more advanced
 version of 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013linguisticsubstructures"

\end_inset

, though I have not seen comparasons (They were both presented at the same
 conference I believe).
\end_layout

\begin_layout Section
Datasets
\end_layout

\begin_layout Subsection
Penn TreeBank
\end_layout

\begin_layout Subsection
AMR SemBank
\end_layout

\begin_layout Standard
Abstract Meaning Representation (AMR) is a structured representation of
 the meaning of a sentence
\begin_inset CommandInset citation
LatexCommand cite
key "Banarescu13abstractmeaning"

\end_inset

.
\end_layout

\begin_layout Standard
the AMR for two sentences with the same meaning is the same.
\end_layout

\begin_layout Standard
For example:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

(d / destroy-01 
\end_layout

\begin_layout Plain Layout

	:arg0 (b / boy) 
\end_layout

\begin_layout Plain Layout

	:arg1 (r / room))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

For all of:
\end_layout

\begin_layout Plain Layout

the destruction of the room by the boy ...
 
\end_layout

\begin_layout Plain Layout

the boy’s destruction of the room ...
 
\end_layout

\begin_layout Plain Layout

The boy destroyed the room.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
There is a freely available small dataset of 1,562 sentences from the novel,
 
\emph on

\begin_inset Quotes eld
\end_inset

The Little Prince
\begin_inset Quotes erd
\end_inset


\emph default
 available from 
\begin_inset CommandInset href
LatexCommand href
name "http://amr.isi.edu/download.html"
target "http://amr.isi.edu/download.html"

\end_inset

.
 
\end_layout

\begin_layout Standard
LDC publishes a dataset of a further 13,051 sentences at 
\begin_inset CommandInset href
LatexCommand href
name "https://catalog.ldc.upenn.edu/LDC2014T12"
target "https://catalog.ldc.upenn.edu/LDC2014T12"

\end_inset

.
 This is available to LDC subscribers (or $300 for nonmember) -- does UWA
 have a LDC membership?
\end_layout

\begin_layout Standard
In total 20,434 sentences have been annotated, meaning several thousand
 are unreleased (According to 
\begin_inset CommandInset href
LatexCommand href
name "http://amr.isi.edu/download.html"
target "http://amr.isi.edu/download.html"

\end_inset

).
\end_layout

\begin_layout Standard
According to 
\begin_inset CommandInset href
LatexCommand href
name "https://github.com/jflanigan/jamr"
target "https://github.com/jflanigan/jamr"

\end_inset

 there exists LDC2014E41, Released: May 30 2014, with 18,779 sentences.
\end_layout

\begin_layout Standard
JAMR (
\begin_inset CommandInset href
LatexCommand href
name "https://github.com/jflanigan/jamr"
target "https://github.com/jflanigan/jamr"

\end_inset

) is a parser that outputs AMR.
\end_layout

\begin_layout Subsection
FrameNet
\end_layout

\begin_layout Subsection
SemEval
\end_layout

\begin_layout Standard
SemEval is a annual workshop.
 Each year there are several competitive tasks, for which corpia are provided.
 These corpia tend to have on going use even in publications not associated
 with the workshop.
\end_layout

\begin_layout Subsection
WordNet
\end_layout

\begin_layout Section
Projects
\end_layout

\begin_layout Subsection
RvNN on AMR, for a Semantic Phrase Embedding
\begin_inset CommandInset label
LatexCommand label
name "sub:RvNN-on-AMR"

\end_inset


\end_layout

\begin_layout Standard
As the work of 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2013:CVG,Socher2011ParsingPhrases,socher2010PhraseEmbedding"

\end_inset

 utilized (a subset of) the Penn Treebank to learn Parts of Speech (POS)
 tags for a syntactical trained phrase embedding,
\end_layout

\begin_layout Standard
It could be replicated using the AMR SemBank
\begin_inset CommandInset citation
LatexCommand cite
key "Banarescu13abstractmeaning"

\end_inset

 for semantically trained phrase embedding.
\end_layout

\begin_layout Standard
This would not be reversible, but would be good in and of itself.
\end_layout

\begin_layout Subsubsection
Why would this be good
\end_layout

\begin_layout Subsubsection
Issues:
\end_layout

\begin_layout Standard
The major challenge is that the 
\end_layout

\begin_layout Standard
A second issue is that the SemBank is small <20,000 sentences.
 However, showing a proof of concept that this method works, and indicating
 that with more training data it would work better is likely a sufficient
 result.
\end_layout

\begin_layout Paragraph
TODO:
\end_layout

\begin_layout Standard
I need to examine the work of 
\begin_inset CommandInset citation
LatexCommand cite
key "Grefenstette2014"

\end_inset

.
 It does not use RvNN.
\end_layout

\begin_layout Subsection
ReSynthesis into a Provided Structure
\end_layout

\begin_layout Standard
There are two steps to directly ReSythisising from a phrase embedding of
 the form created by 
\begin_inset CommandInset citation
LatexCommand cite
key "Socher2011ParsingPhrases"

\end_inset

 (Or indeed by something created like 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:RvNN-on-AMR"

\end_inset

).
 One: the creation of the structure of the resynthesised text: eg 
\emph on

\begin_inset Quotes eld
\end_inset

Noun do Verb
\begin_inset Quotes erd
\end_inset


\emph default
, or 
\emph on
The 
\begin_inset Quotes eld
\end_inset

Adjective Noun Verb The Nou
\emph default
n
\begin_inset Quotes erd
\end_inset

.
 Second is filling said structure will values, eg 
\begin_inset Quotes eld
\end_inset

Cats do chase
\begin_inset Quotes erd
\end_inset

 or 
\emph on

\begin_inset Quotes eld
\end_inset

The fast cat chased the mouse.
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
In this project, we could assume the first is given, and solve the second.
 A not unreasonable, nor un-useful assumption.
\end_layout

\begin_layout Standard
For example is attempting to summarize reviews of hotels, it might be desirable
 to attempt to pull out statements that are only of the form 
\begin_inset Quotes eld
\end_inset

Noun Is Adjective
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Subsubsection
How
\end_layout

\begin_layout Standard
It is possible (I have a working proof of concept) to use linear algebra
 to reverse the operation of the RvNN.
 The basic operation on two input words is 
\begin_inset Formula $p_{1,2}=\tanh\left(W[Le_{1},\,Le_{2}L]\right)$
\end_inset

, where 
\begin_inset Formula $c_{i}=Le_{i}$
\end_inset

 where 
\begin_inset Formula $e_{i}$
\end_inset

 is a one-hot word identifier.
 Using atan, to reverse the nonlinearity.
 Using the pseudo inverse of 
\begin_inset Formula $W$
\end_inset

 to recover a least-squares values for recovering 
\begin_inset Formula $\hat{c}_{i}$
\end_inset

.
 Then using the cosign difference to obtain a similarity measure for each
 candidate 
\begin_inset Formula $\hat{e}_{i}$
\end_inset

.
 This should be generalisable for 
\begin_inset Formula $c_{i}=p_{j,k}$
\end_inset

 where it is a deeper structure.
\end_layout

\begin_layout Standard
When testing the basic operation on recovering two words as discussed it
 seems successful.
 This was performed using a 
\begin_inset Formula $L$
\end_inset

 which is a 50 dimensional matrix of Collobert & Weston word embeddings
 and a randomly set 
\begin_inset Formula $W$
\end_inset

.
 The input words were normally top ranked, and if not were generally in
 to top 5, with other highly ranked words similar (Eg President instead
 of King).
 This was with the random 
\begin_inset Formula $W$
\end_inset

 which does not encode useful information; a trained , 
\begin_inset Formula $W$
\end_inset

 should perform better.
\end_layout

\begin_layout Subsubsection
Issues 
\end_layout

\begin_layout Standard
This being successful and useful, hinges on being able to reconstitute words
 into a format that they were not in originally, but that comes close to
 preserving meaning.
 This relies on the operations of the 
\begin_inset Formula $W$
\end_inset

 matrix functioning to project into a space containing semantic (rather
 than syntactic) information.
 For example it would be good if 
\emph on

\begin_inset Quotes eld
\end_inset

The basketball player
\begin_inset Quotes erd
\end_inset

 
\emph default
was projected closer to 
\emph on

\begin_inset Quotes eld
\end_inset

The basketballer
\begin_inset Quotes erd
\end_inset

 
\emph default
than to 
\emph on

\begin_inset Quotes eld
\end_inset

The soccer player
\begin_inset Quotes erd
\end_inset

.
 
\emph default
Solving this kind of issue would require improving the way W is trained.
 Such as discussed in
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:RvNN-on-AMR"

\end_inset

.
\end_layout

\begin_layout Standard
That may not be sufficient, results in 
\begin_inset CommandInset citation
LatexCommand cite
key "RvNNLogicalSemantics"

\end_inset

 (preprint), showed that for logical semantics, separation of different
 sentiments was not well preserved for longer expressions by the RvNN.
 They also considered the recursive neural tensor networks (RvNTN) and found
 it did much better.
\end_layout

\begin_layout Subsubsection
Extension
\end_layout

\begin_layout Standard
The naïve way to extend this to work more generally, is to perform a search
 (making use of dynamic programming potentially) across a space of possible
 structures.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "../../Resources/master_bibliography/master"
options "IEEEtran"

\end_inset


\end_layout

\end_body
\end_document
