#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass scrartcl
\begin_preamble
\usepackage{url}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Reversible Phrase Embedding
\end_layout

\begin_layout Standard
featuring: Re-Synthesis of Text from Vector Representations
\end_layout

\begin_layout Standard
See also: Natural Language Generation (the opposite of natural language
 understanding.)
\end_layout

\begin_layout Subsection
Motivation
\end_layout

\begin_layout Standard
High quality phrase embeddings will allow a large number of filtering techniques
 to be brought to bare on NLP problems.
 To truly see many of the benefits it is required to be able to resynthesise
 from the semantic vector space back to the English sentence space.
\end_layout

\begin_layout Subsection
Premise
\end_layout

\begin_layout Standard
There exists plethora of signal processing techniques, that can be brought
 to bare upon NLP problems, if natural language is represented as a numerical
 vector.
 Several applications of such techniques are of much less relevance if the
 resulting vector can not be restored back to natural language.
\end_layout

\begin_layout Standard
Several models for creating word embeddings exists which have proved useful
 in many applications.
 All such word embeddings are reversible, either by a generative model,
 or by embedding the entire vocabulary and finding the nearest word in vector
 space.
 The same can not be done when embedding phrases, the phrasal vocabulary
 is essentially infinite.
\end_layout

\begin_layout Standard
Recently several techniques have come out for creating phrase embeddings
 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:RAE,SocherEtAl2013:CVG,Socher2011ParsingPhrases,SocherEtAl2011:PoolRAE"

\end_inset

 (All from the same first author).
 However none of these are truly reversible; though 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

 comes close.
 The reversing is a hard issue as there is no generative model for structure,
 and as mention above, the of all phrases is too large to embed in its entirety
 for a nearest neighbor search.
 The proposed issue to be tackled here is the design of a mechanism for
 creating an embedding, and for performing the resynthesis back from the
 embedding to a natural language sentence of similar meaning.
\end_layout

\begin_layout Section
Applications / Possible Tasks/Projects within
\end_layout

\begin_layout Itemize
Summarization of a paragraph into a sentence
\end_layout

\begin_deeper
\begin_layout Itemize
abstractive summarization
\end_layout

\end_deeper
\begin_layout Itemize
Production of a Short Title, via trade-off function between length and capture
 of meaning
\end_layout

\begin_deeper
\begin_layout Itemize
Again abstractive summarization
\end_layout

\end_deeper
\begin_layout Itemize
Translation
\end_layout

\begin_deeper
\begin_layout Itemize
Using two stochastically reversible mappings from different language spaces
 into the same semantic space
\end_layout

\begin_layout Itemize
People have been doing this with nonreversible mappings, for word embeddings,
 where it works fine because it is possible to project the whole vocabulary
 into the space, and then use the nearest neightbour to workout which word
 to translate to.
\end_layout

\end_deeper
\begin_layout Itemize
Robustness/Flow/Re-Synthesis cross over: Finding corrupted sentences/paragraphs
 and inserting correct word/sentence.
\end_layout

\begin_layout Itemize
The creation of a generative, rather than a discriminative model for phrase
 embeddings.
 (Is an ends in and of itself)
\end_layout

\begin_layout Itemize
Apply signal processing techniques like particle filters
\end_layout

\begin_layout Section
Approaches
\end_layout

\begin_layout Subsection
ReSynthesis into a Provided Structure
\end_layout

\begin_layout Standard
There are two steps to directly resynthesise from a phrase embedding of
 the form created by 
\begin_inset CommandInset citation
LatexCommand cite
key "Socher2011ParsingPhrases"

\end_inset

 (Or indeed by something created like 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:RvNN-on-AMR"

\end_inset

).
 One: the creation of the structure of the resynthesised text: eg 
\emph on

\begin_inset Quotes eld
\end_inset

Noun do Verb
\begin_inset Quotes erd
\end_inset


\emph default
, or 
\emph on
The 
\begin_inset Quotes eld
\end_inset

Adjective Noun Verb The Nou
\emph default
n
\begin_inset Quotes erd
\end_inset

.
 Second is filling said structure will values, eg 
\begin_inset Quotes eld
\end_inset

Cats do chase
\begin_inset Quotes erd
\end_inset

 or 
\emph on

\begin_inset Quotes eld
\end_inset

The fast cat chased the mouse.
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
In this project, we could assume the first is given, and solve the second.
 A not unreasonable, nor un-useful assumption.
\end_layout

\begin_layout Standard
For example is attempting to summarize reviews of hotels, it might be desirable
 to attempt to pull out statements that are only of the form 
\begin_inset Quotes eld
\end_inset

Noun Is Adjective
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Subsubsection
How
\end_layout

\begin_layout Standard
It is possible (I have a working proof of concept) to use linear algebra
 to reverse the operation of the RvNN.
 The basic operation on two input words is 
\begin_inset Formula $p_{1,2}=\tanh\left(W[Le_{1},\,Le_{2}L]\right)$
\end_inset

, where 
\begin_inset Formula $c_{i}=Le_{i}$
\end_inset

 where 
\begin_inset Formula $e_{i}$
\end_inset

 is a one-hot word identifier.
 Using atan, to reverse the nonlinearity.
 Using the pseudo inverse of 
\begin_inset Formula $W$
\end_inset

 to recover a least-squares values for recovering 
\begin_inset Formula $\hat{c}_{i}$
\end_inset

.
 Then using the cosign difference to obtain a similarity measure for each
 candidate 
\begin_inset Formula $\hat{e}_{i}$
\end_inset

.
 This should be generalisable for 
\begin_inset Formula $c_{i}=p_{j,k}$
\end_inset

 where it is a deeper structure .
\end_layout

\begin_layout Standard
When testing the basic operation on recovering two words as discussed it
 seems successful.
 This was performed using a 
\begin_inset Formula $L$
\end_inset

 which is a 50 dimensional matrix of Collobert & Weston word embeddings
 and a randomly set 
\begin_inset Formula $W$
\end_inset

.
 The input words were normally top ranked, and if not were generally in
 to top 5, with other highly ranked words similar (Eg President instead
 of King).
 This was with the random 
\begin_inset Formula $W$
\end_inset

 which does not encode useful information; a trained , 
\begin_inset Formula $W$
\end_inset

 should perform better.
 
\end_layout

\begin_layout Standard
A further experiment again with a random matrix showed it couldn't do the
 deeper structures, but that the math did work and returned a result.
 It is not unreasonable that it performed poorly with random 
\begin_inset Formula $W$
\end_inset

.
 
\end_layout

\begin_layout Standard
It is expected that the same would hold true, using RAE
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:RAE"

\end_inset

, which already have a reconstruction layer, in the place of taking the
 pseudo-inverse of 
\begin_inset Formula $W$
\end_inset

.
 Indeed this is very similar to the approach used Unfolding Recursive Auto-encod
ers
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

 -- which are trained to recreate there whole input (matching structure)
 this way during training.
\end_layout

\begin_layout Subsubsection
Issues 
\end_layout

\begin_layout Standard
This being successful and useful, hinges on being able to reconstitute words
 into a format that they were not in originally, but that comes close to
 preserving meaning.
 This relies on the operations of the 
\begin_inset Formula $W$
\end_inset

 matrix functioning to project into a space containing semantic (rather
 than syntactic) information.
 For example it would be good if 
\emph on

\begin_inset Quotes eld
\end_inset

The basketball player
\begin_inset Quotes erd
\end_inset

 
\emph default
was projected closer to 
\emph on

\begin_inset Quotes eld
\end_inset

The basketballer
\begin_inset Quotes erd
\end_inset

 
\emph default
than to 
\emph on

\begin_inset Quotes eld
\end_inset

The soccer player
\begin_inset Quotes erd
\end_inset

.
 
\emph default
Solving this kind of issue would require improving the way W is trained.
 Such as discussed in
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:RvNN-on-AMR"

\end_inset

.
\end_layout

\begin_layout Standard
That may not be sufficient, results in 
\begin_inset CommandInset citation
LatexCommand cite
key "RvNNLogicalSemantics"

\end_inset

 (preprint), showed that for logical semantics, separation of different
 sentiments was not well preserved for longer expressions by the RvNN.
 They found they need to use the more powerful recursive neural tensor networks
 (RvNTN).
\end_layout

\begin_layout Standard
Further too much information may be lost in the compression.
\end_layout

\begin_layout Subsubsection
Extension
\end_layout

\begin_layout Standard
The naïve way to extend this to work more generally, is to perform a search
 (making use of dynamic programming potentially) across a space of possible
 structures.
 
\end_layout

\begin_layout Subsection
Structural ReSynthesis (The Core Problem)
\end_layout

\begin_layout Standard
The general issue of above, that training these models piece by piece then
 using a nongenerative method to combine them.
 The merging in the RvNN can be generative (as in RAE), but the method which
 decides on the order is not.
 This does not allow the tree structure itself to be regenerated.
 The reconstruction needs to be in the structure, not (just) in the nodes.
\end_layout

\begin_layout Standard
Otherwise the RAE of 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:RAE"

\end_inset

 would have solved this long ago.
\end_layout

\begin_layout Standard
The Unfolding RAE
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

 does come close to tackling this problem, but is only trying to recreate
 the same structure it was trained on.
 Not necessarily the best structure for expressing its meaning.
 For example the sentence 
\begin_inset Quotes eld
\end_inset

I am not un-hungry
\begin_inset Quotes erd
\end_inset

 may be better expressed as 
\begin_inset Quotes eld
\end_inset

I am hungry
\begin_inset Quotes erd
\end_inset

.
 Here we capture the notion of best as closest to the embedded representation
 of the input, rather than closeness to the input.
\end_layout

\begin_layout Subsubsection
Tree Problem Detail 
\end_layout

\begin_layout Standard
When decomposing an embedding into a binary parse tree (the reverse of 
\begin_inset CommandInset citation
LatexCommand cite
key "socher2014recursive,SocherEtAl2013:CVG"

\end_inset

).
\end_layout

\begin_layout Standard
At each step the algorithm needs to decide between 4 choices: for both the
 left and right nodes it can expand either each, into a output word, or
 into an nonterminal node (for further decomposition).
\end_layout

\begin_layout Standard
When doing this it has an objective of generating a sentence which when
 converted back into an embedding will be as close as possible to the original
 embedding.
\end_layout

\begin_layout Standard
It also has the constraint that the sentence must be valid English (and
 ideally be natural sounding english).
\end_layout

\begin_layout Standard
When being used for summarization it has a further constraint on the length
 of the response (either in number of words, or at a more general scale
 in number of sentences.)
\end_layout

\begin_layout Subsubsection
Training to make the decision
\end_layout

\begin_layout Standard
It see
\end_layout

\begin_layout Section
Existing Work
\end_layout

\begin_layout Standard
The existing models for phrase embeddings by Socher et all share many commonalit
ies.
 All are based around applying some form of neural network to pairs of embedding
s, which outputs a merged embedding, and thusly creating a binary tree where
 at each stage the best such merge is performed.
 The major difference is in how the best merge is defined.
 In all cases this neural network is trained via back propagation through
 structure (BPTS)
\begin_inset CommandInset citation
LatexCommand cite
key "goller1996BPstructure"

\end_inset

.
\end_layout

\begin_layout Subsection
RvNN 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/RvNN.PNG
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:RvNN"

\end_inset

Left is an example of RvNN (from 
\begin_inset CommandInset citation
LatexCommand cite
key "socher2014recursive"

\end_inset

).
 The blue merge section of the right image is applied at each pair of edges
 where a semantic representation is merged, with the parent embedding 
\begin_inset Formula $p_{1,2}$
\end_inset

 is passed to the next stage as the next child embedding like 
\begin_inset Formula $c_{1}$
\end_inset

or 
\begin_inset Formula $c_{2}$
\end_inset

.
 Where an indexed word is added (the section with orange dots in boxes on
 the right) the section of the image on the left in red is performed.
 The left hand image in its entirety shows what happens for the merge of
 Historic and Church to form a NP,
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the RvNN a neural net is applied at each merge.
 It is a more complicated than usual network in that it has several separate
 output layers as shown to the right on 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:RvNN"

\end_inset

.
 It its heart though it is normal shallow neural net.
 Taking an input of 
\begin_inset Formula $c_{1}$
\end_inset

 concatenated with 
\begin_inset Formula $c_{2}$
\end_inset

where 
\begin_inset Formula $c_{1}$
\end_inset

 and 
\begin_inset Formula $c_{2}$
\end_inset

 are child word/(sub)phrase embeddings, and outputting a (sub)phrase embedding
 
\begin_inset Formula $p_{1,2}$
\end_inset

 of dimensionality equal to that of either of its inputs.
 This merged parent embedding is passed to the next stage.
\end_layout

\begin_layout Standard
The order in which to do the merge is determined by a trained scoring matrix:
 
\begin_inset Formula $W_{score}$
\end_inset

.
 The scoring matrix (and via BPTS, 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $L$
\end_inset

) is trained using the max-margin method.
 Max Margin finds a subgraident for ensuring the correct tree score better
 than their incorrect counterparts.
 An incorrect tree can be created from a correct example by corrupting on
 of the features.
 This does require having labeled correct trees to train from.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Socher2011ParsingPhrases,socher2010PhraseEmbedding"

\end_inset

 use the Wall Street Journal section of the Penn TreeBank for this.
\end_layout

\begin_layout Standard
The
\begin_inset Formula $W_{label}$
\end_inset

(and via BPTS 
\begin_inset Formula $W$
\end_inset

and 
\begin_inset Formula $L$
\end_inset

) is trained to predict using the parent embedding the correct Part of Speech
 (POS) annotation for that tree pointion, and it also detailed in the Treebank
 training data.
 This is extra discriminative information.
 
\end_layout

\begin_layout Standard
Non-rigorous observations are made in 
\begin_inset CommandInset citation
LatexCommand cite
key "socher2010PhraseEmbedding"

\end_inset

, after embedding many sentences from the Wall Street Journal corpus, that
 similar phrase embeddings were located near each other.
\end_layout

\begin_layout Standard
An variation on the RvNN
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2013:CVG"

\end_inset

 is used in the gold standard Stanford Parser for POS annotation.
 
\end_layout

\begin_layout Standard
This RvNN actually learns the correct linguistic structure of the sentence,
 unlike the Recursive Autoencoder models.
\end_layout

\begin_layout Subsection
RAE 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/RAE_uRAE.PNG
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:RAE"

\end_inset

Left is an example of RAE.
 Right is an example of an Unfolding RAE ( both from 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

).
 In this figure 
\begin_inset Formula $x_{1},x_{2,}x_{3}$
\end_inset

 are all word embeddings.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The Recursive Autoencoder (RAE)
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:RAE"

\end_inset

 is very similar to the RvNN, with Autoencoder rather than a feedforward
 neural network at each merge.
 However in the RAE, the loss function for determining the quality (and
 thus order) an a merge is how low it's reconstruction error is, and can
 be seen for the node 
\begin_inset Formula $y_{2}$
\end_inset

 in the left of 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:RAE"

\end_inset

.
 It is trained, again with BPTS, to minimize the sum of all the reconstruction
 errors in the network.
\end_layout

\begin_layout Standard
Structure is chosen inline only with this goal, not to replicated real linguisti
c structures (unlike in the RvNN above).
\end_layout

\begin_layout Standard
It is worth noting that the auto-encoders used are not denoising, and are
 bottle-necked specifically to have representation width of the same length
 as the input word embeddings.
\end_layout

\begin_layout Standard
It can be extended in a similar way to the RvNN by adding additional trained
 outputs at each node (allowing it to be 
\begin_inset Quotes eld
\end_inset

semi-supervised
\begin_inset Quotes erd
\end_inset

).
\end_layout

\begin_layout Subsection
Unfolding RAE
\end_layout

\begin_layout Standard
The Unfolding Recursive Autoencoder (URAE) is a variation on the RAE, which
 uses a different loss function
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

.
 In its case it regenerates the whole tree from the top, (as shown to the
 right of 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:RAE"

\end_inset

) and uses the sum of errors on the regenerated inputs as its loss function.
 
\end_layout

\begin_layout Standard
However it is not truly reversible to go from the embedding back to a sentence,
 as it needs the structure provided.
 In this case it can get the structure as it defined it.
 Further the form structure provided is determined by the URAE itself --
 Unlike the structure in
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2013:CVG,Socher2011ParsingPhrases,socher2010PhraseEmbedding"

\end_inset

 this structure is not directly related to any traditional linguistic relations.
 Thus it can not be used in general to resynthesise sentences from embeddings
 as it is impossible to intuit the structure without already having the
 sentence to embed.
\end_layout

\begin_layout Section
Datasets
\end_layout

\begin_layout Subsection
Penn TreeBank
\end_layout

\begin_layout Standard
Consisting of POS annotated: Wall Street Journal, Brown, and Switchboard
 Datasets.
 Is documented in seemingly only in postscript.
\end_layout

\begin_layout Subsection
AMR SemBank
\end_layout

\begin_layout Standard
Abstract Meaning Representation (AMR) is a structured representation of
 the meaning of a sentence
\begin_inset CommandInset citation
LatexCommand cite
key "Banarescu13abstractmeaning"

\end_inset

.
 Its currently in spec 1.2.1
\begin_inset CommandInset citation
LatexCommand cite
key "AMRspec"

\end_inset

.
\end_layout

\begin_layout Standard
the AMR for two sentences with the same meaning is the same.
\end_layout

\begin_layout Standard
For example:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

(d / destroy-01 
\end_layout

\begin_layout Plain Layout

	:arg0 (b / boy) 
\end_layout

\begin_layout Plain Layout

	:arg1 (r / room))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

For all of:
\end_layout

\begin_layout Plain Layout

the destruction of the room by the boy ...
 
\end_layout

\begin_layout Plain Layout

the boy’s destruction of the room ...
 
\end_layout

\begin_layout Plain Layout

The boy destroyed the room.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
There is a freely available small dataset of 1,562 sentences from the novel,
 
\emph on

\begin_inset Quotes eld
\end_inset

The Little Prince
\begin_inset Quotes erd
\end_inset


\emph default
 available from 
\begin_inset CommandInset href
LatexCommand href
name "http://amr.isi.edu/download.html"
target "http://amr.isi.edu/download.html"

\end_inset

.
 
\end_layout

\begin_layout Standard
LDC publishes a dataset of a further 13,051 sentences at 
\begin_inset CommandInset href
LatexCommand href
name "https://catalog.ldc.upenn.edu/LDC2014T12"
target "https://catalog.ldc.upenn.edu/LDC2014T12"

\end_inset

.
 This is available to LDC subscribers (or $300 for nonmember) -- does UWA
 have a LDC membership?In total 20,434 sentences have been annotated, meaning
 several thousand are unreleased (According to 
\begin_inset CommandInset href
LatexCommand href
name "http://amr.isi.edu/download.html"
target "http://amr.isi.edu/download.html"

\end_inset

).
 According to 
\begin_inset CommandInset href
LatexCommand href
name "https://github.com/jflanigan/jamr"
target "https://github.com/jflanigan/jamr"

\end_inset

 there exists LDC2014E41, Released: May 30 2014, with 18,779 sentences.
\end_layout

\begin_layout Standard
AMR is closely related to OntoNotes and to PropBank.
\end_layout

\begin_layout Subsection
OntoNotes
\end_layout

\begin_layout Subsection
PropBank
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "kingsbury2002treebank"

\end_inset


\end_layout

\begin_layout Subsection
FrameNet
\end_layout

\begin_layout Subsection
WordNet
\end_layout

\begin_layout Standard
It is an lexical database of words and their relationships.
 It can be accessed via python NLTK.
\end_layout

\begin_layout Subsection
SemEval
\end_layout

\begin_layout Standard
SemEval is a annual workshop.
 Each year there are several competitive tasks, for which corpia are provided.
 These corpia tend to have on-going use long after the workshop is over.
 Some of the corpia come from existing sources.
\end_layout

\begin_layout Subsection
Microsoft Research Paraphrase corpus
\end_layout

\begin_layout Standard
5800 pairs of sentences with the same meaning.
 Available from 
\begin_inset CommandInset href
LatexCommand href
name "http://research.microsoft.com/en-us/downloads/607d14d9-20cd-47e3-85bc-a2f65cd28042/"
target "http://research.microsoft.com/en-us/downloads/607d14d9-20cd-47e3-85bc-a2f65cd28042/"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "../../Resources/master_bibliography/master"
options "IEEEtran"

\end_inset


\end_layout

\begin_layout Section
Other Related Works
\end_layout

\begin_layout Subsection
Phrasal Translations
\end_layout

\begin_layout Standard
Work has been done on translating phrases, using word embeddings
\begin_inset CommandInset citation
LatexCommand cite
key "zou2013bilingual"

\end_inset

.
 However this is using word embeddings only.
\end_layout

\begin_layout Standard
It seems to be quiet tied to actually mapping between words in word space
 (see also 
\begin_inset CommandInset citation
LatexCommand cite
key "phrasaltranslationtool"

\end_inset

), rather than moving to a semantic space and then back again.
\end_layout

\begin_layout Standard
There is also 
\begin_inset CommandInset citation
LatexCommand cite
key "gao2014learning"

\end_inset

, seems to be based on bag of words (which has Li Deng as a coauthor).
\end_layout

\begin_layout Subsection
Single Word Embedding
\end_layout

\begin_layout Standard
The a lot of work on Word Embeddings note the existence of linear substructures
 in single-word embeddings for example: 
\begin_inset Formula $v("king")-v("man")+v("woman")$
\end_inset

 has nearest neighbor 
\begin_inset Formula $v("queen")$
\end_inset

.
 The paper which first established this seems to be 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013linguisticsubstructures"

\end_inset

.
 Single word generation is fairly simple via nearest neighbor for word embedding.
\end_layout

\begin_layout Standard
Word Embedding spaces seem to combine syntax and semantics, when combining
 the words from a sentence it would be better to end up in a syntax free
 space.
 
\end_layout

\begin_layout Subsection
Sum of Word Embeddings
\end_layout

\begin_layout Standard
I don't think anyone actually does this (despite what 
\begin_inset CommandInset citation
LatexCommand cite
key "ExtractiveSummaristation"

\end_inset

 says, see below).
 It doesn't work well.
 We expect it not to work well because of the results in 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013linguisticsubstructures"

\end_inset

 which shows that only 40% of simple relationships are represented in local
 structure.
\end_layout

\begin_layout Subsection
Summarization
\end_layout

\begin_layout Standard
Notably this would be Abstractive Summarization, of building a model then
 generating text from it.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "ExtractiveSummaristation"

\end_inset

(on Extractive Multidocument Summarization) using word and phrase embedding
 stated that the authors were not aware of any other papers which used continuou
s vector space models for summarization tasks.
 Which is indicative that this area is not well explored, as summarization
 is a very natural use to put any Re-Synthesis system to.
\end_layout

\begin_layout Standard
However: 
\begin_inset CommandInset citation
LatexCommand cite
key "ExtractiveSummaristation"

\end_inset

 claims that 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolovSkip"

\end_inset

 suggested summing word embeddings to get phrase embeddings, but to my knowledge
 that is quiet the opposite of what 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolovSkip"

\end_inset

 was saying.
 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolovSkip"

\end_inset

's discussion on phrases focuses on treating short phrases eg "Austrian
 Airlines", "New York Times" as separate tokens from the words that make
 them up in training word embeddings.
\end_layout

\begin_layout Subsection
Knowledge base systems
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "MemoryNN"

\end_inset

 is quiet a complicated system for performing full natural language queries
 with natural language output, and with memorization.
\end_layout

\begin_layout Standard
It has several components, notable for this section is the R (Response).
 In most of there examples the response is set up to just returns a single
 word response, but some example are shown where they used a RNN.
 Details are not given, but I believe they used a RNN language model similar
 to 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2011RnnLM"

\end_inset

 (which they reference).
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2011RnnLM"

\end_inset

 does not make use of word embeddings, and is very similar to n-gram based
 model.
\end_layout

\begin_layout Standard
There is also 
\begin_inset CommandInset citation
LatexCommand cite
key "Socher2013TensorReasoning"

\end_inset

, which models word relationships with tensors, in a way it is a much more
 advanced version of 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013linguisticsubstructures"

\end_inset

, though I have not seen direct comparisons (They were both presented at
 the same conference.).
\end_layout

\begin_layout Subsection
Traditional Language Models
\end_layout

\begin_layout Subsection
RNN/Windowed NN Language Models
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "zhang2014chinese"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand cite
key "Grefenstette2014"

\end_inset


\end_layout

\begin_layout Subsection
Pennman
\end_layout

\begin_layout Standard
US DARPA project/lab, has been working on NLG continuously since the 70s.
\end_layout

\begin_layout Section
Related0 Projects
\end_layout

\begin_layout Subsection
RvNN on AMR, for a Semantic Phrase Embedding
\begin_inset CommandInset label
LatexCommand label
name "sub:RvNN-on-AMR"

\end_inset


\end_layout

\begin_layout Standard
As the work of 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2013:CVG,Socher2011ParsingPhrases,socher2010PhraseEmbedding"

\end_inset

 utilized (a subset of) the Penn Treebank to learn Parts of Speech (POS)
 tags for a syntactical trained phrase embedding.
\end_layout

\begin_layout Standard
The embeedings in 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2013:CVG,Socher2011ParsingPhrases,socher2010PhraseEmbedding"

\end_inset

 are described by Socher et al as semantic, and indeed may be as it does
 not 
\emph on
only
\emph default
 get trained on the POS information, but also on max margin for structure.
\end_layout

\begin_layout Standard
It could be replicated using the AMR SemBank
\begin_inset CommandInset citation
LatexCommand cite
key "Banarescu13abstractmeaning"

\end_inset

 for semantically trained phrase embedding.
\end_layout

\begin_layout Standard
This would not be reversible, but would be good in and of itself.
\end_layout

\begin_layout Subsubsection
Why would this be good
\end_layout

\begin_layout Standard
Sorcher et al's work on phrase embeddings used syntactic data (POS tags),
 for it discriminative training, with the aim to get a semantic representation
 as a biproduct.
 AMR is semantic information, as are its related resources (see below).
 It makes sense that using semantic information would be better for this
 task.
\end_layout

\begin_layout Subsubsection
Issues:
\end_layout

\begin_layout Paragraph
Format/Alignment
\end_layout

\begin_layout Standard
The major challenge is that AMR is not a nice lean format, unlike POS tags.
 While correct AMR assigns exactly one representation per meaning some is
 quiet complex.
 Also while it is nicely recursive, the number of relational tags is high.
 There is also the issues around the word tags.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

51.
 Here you may see the best portrait that,
\end_layout

\begin_layout Plain Layout

later , I was able to make of him .
 (lpp_1943.51)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

(p / possible
\end_layout

\begin_layout Plain Layout

  :domain (s / see-01
\end_layout

\begin_layout Plain Layout

            :ARG0 (y / you)
\end_layout

\begin_layout Plain Layout

            :ARG1 (p2 / portrait
\end_layout

\begin_layout Plain Layout

                    :mod (g / good
\end_layout

\begin_layout Plain Layout

                           :degree (m / most))
\end_layout

\begin_layout Plain Layout

                    :ARG1-of (m2 / make-01
\end_layout

\begin_layout Plain Layout

                               :ARG0 (i / i)
\end_layout

\begin_layout Plain Layout

                               :domain-of (p3 / possible)
\end_layout

\begin_layout Plain Layout

                               :time (l / late
\end_layout

\begin_layout Plain Layout

                                       :degree (m3 / more)))
\end_layout

\begin_layout Plain Layout

                    :topic (h / he))
\end_layout

\begin_layout Plain Layout

            :location (h2 / here)))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

52.
 But my drawing is certainly very much
\end_layout

\begin_layout Plain Layout

less charming than its model .
 (lpp_1943.52)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

(c3 / contrast-01
\end_layout

\begin_layout Plain Layout

      :ARG2 (c / charm-01
\end_layout

\begin_layout Plain Layout

            :ARG0 (t / thing
\end_layout

\begin_layout Plain Layout

                  :ARG1-of (d / draw-01
\end_layout

\begin_layout Plain Layout

                        :ARG0 (i / i)))
\end_layout

\begin_layout Plain Layout

            :quant (l / less
\end_layout

\begin_layout Plain Layout

                  :degree (m / much
\end_layout

\begin_layout Plain Layout

                        :degree (v / very)))
\end_layout

\begin_layout Plain Layout

            :mod (c2 / certain)
\end_layout

\begin_layout Plain Layout

            :compared-to (m2 / model
\end_layout

\begin_layout Plain Layout

                  :poss t)))
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In overcoming this a lot could be learned from the preprocessing steps of
 the JARM parser
\begin_inset CommandInset citation
LatexCommand cite
key "JARMparser"

\end_inset

.
\end_layout

\begin_layout Standard
While RvNN can have arbitrary number of inputs to each layer (2 in most
 work), it must have the same number of inputs to each layer (Further RvNN
 works on more general structures than trees -- it is only restricted to
 acyclic graphs).
\end_layout

\begin_layout Standard
There are certain ways of handling this.
 AMR has up to 5 arguments (ARG1,ARG2...) to each node (excluding labels, like
 :mod), Normally no more than 3 are filled, and on many (most) there are
 <5 maximum.
 The nonfilled, whether by being empty, or by being nonexistant for that
 node type could simply be filled with zeros -- this should be fine, but
 I many need to check that.
 Another is to use the approach used in 
\begin_inset CommandInset citation
LatexCommand cite
key "Socher2011ParsingPhrases"

\end_inset

, for allowing n-ary trees of image segments to be represented as taller
 binary trees and allowing multiple valid solutions.
\end_layout

\begin_layout Paragraph
Amount of Training Data 
\end_layout

\begin_layout Standard
A second issue is that the SemBank is small <20,000 sentences.
 However, showing a proof of concept that this method works, and indicating
 that with more training data it would work better is likely a sufficient
 result.
 
\end_layout

\begin_layout Subsection
Parsing to ARM (without RvNN)
\end_layout

\begin_layout Standard
Closely linked to the former, would be parsing to AMR.
\end_layout

\begin_layout Standard
A AMR is producing a graph, with edges labeled with semantic operators,
 and nodes labeled with concepts.
\end_layout

\begin_layout Standard
The JARM parser is likely the only parser for Sentences to AMR.
 I'm not aware of other work in this area.
\end_layout

\begin_layout Standard
There a several places in parser like JARM that could benefit from using
 word-embeddings and related technologies.
 Word embeddings could, for example be used as the feature vectors.
\end_layout

\begin_layout Subsubsection
JARM Summery
\end_layout

\begin_layout Standard
A summery of 
\begin_inset CommandInset citation
LatexCommand cite
key "JARMparser"

\end_inset

 follows.
 
\end_layout

\begin_layout Standard
JARM is a parser for converting English sentences into Abstract Meaning
 Representations.
\end_layout

\begin_layout Standard
Evaluation is two steps: 
\end_layout

\begin_layout Enumerate
Mapping words to concept nodes, done with a linear feature scoring 
\end_layout

\begin_layout Enumerate
Determining which relationship edges exist, done with a maximum spanning
 subgraph algorithm which maximized the scores from a different linear features
 scoring.
\end_layout

\begin_layout Standard
Both feature scorers much be trained.
 For the word to concept mapping a "Automatic Aligner" is used to know which
 concepts (in AMR space), align to which words.
 That is to say the automatic aligner is used to determine the ground truth
 from the training data (of sentences paired wit their AMR rep) to which
 the concept mapping is trained to replicate.
 The Aligner itself is based on fairly simple hand engineered heuristics.
 
\end_layout

\end_body
\end_document
