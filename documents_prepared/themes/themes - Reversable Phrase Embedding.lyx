#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass scrartcl
\begin_preamble
\usepackage{url}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Reversable Phrase Embedding
\end_layout

\begin_layout Standard
featuring: Re-Synthesis of Text from Vector Representations
\end_layout

\begin_layout Standard
See also: Natural Language Generation (the opposite of natural language
 understanding.)
\end_layout

\begin_layout Subsection
Motivation
\end_layout

\begin_layout Standard
High quality phrase embeddings will allow a large number of filtering techniques
 to be brought to bare on NLP problems.
 To truly see many of the benefits it is required to be able to resynthesise
 from the semantic vector space back to the English sentence space.
\end_layout

\begin_layout Subsection
Premise
\end_layout

\begin_layout Standard
We are good at manipulating vectors, so it is desirable to map language
 into vectors.
\end_layout

\begin_layout Standard
However, if we can not turn vectors back into language then several applications
 are limited.
\end_layout

\begin_layout Standard
We have had good word embeddings for quiet a while.
 There are many models for creating them.
 We know they are good, because similar words are located close together
 in the vector space.
 We know which are good because there are 6ish competing models.
 We can reverse the mapping either by using a model with this as an intrinsic
 feature (Eg an Autoencoder), or by embedding the whole vocabulary and finding
 the nearest word in vector space.
 We know they are useful because people have done cool things with them
\begin_inset Note Note
status open

\begin_layout Plain Layout
citations here
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Since 2013, we have had a way to embed phrases.
 We know it is good because similar phrases are mapped near to it
\begin_inset CommandInset citation
LatexCommand cite
key "socher2010PhraseEmbedding"

\end_inset

.
 We don't know if it is very good because it has not been compare directly
 with anything.
 We know it is cool because it has had some neat applications
\begin_inset Foot
status open

\begin_layout Plain Layout
I can't find any right now
\end_layout

\end_inset

.
 We can not reverse it as the model is not reversible and it is not possible
 to embed every sentence that could be said to find which are near.
 We would like a phrase embedding method that could be reversed.
\end_layout

\begin_layout Section
Applications / Possible Tasks/Projects within
\end_layout

\begin_layout Itemize
Summarization of a paragraph into a sentence
\end_layout

\begin_deeper
\begin_layout Itemize
abstractive summarization
\end_layout

\end_deeper
\begin_layout Itemize
Production of a Short Title, via trade-off function between length and capture
 of meaning
\end_layout

\begin_deeper
\begin_layout Itemize
Again abstractive summarization
\end_layout

\end_deeper
\begin_layout Itemize
Translation
\end_layout

\begin_deeper
\begin_layout Itemize
Using two stochastically reversible mappings from different language spaces
 into the same semantic space
\end_layout

\begin_layout Itemize
People have been doing this with nonreversible mappings, for word embeddings,
 where it works fine because it is possible to project the whole vocabulary
 into the space, and then use the nearest neightbour to workout which word
 to translate to.
\end_layout

\end_deeper
\begin_layout Itemize
Robustness/Flow/Re-Synthesis cross over: Finding corrupted sentences/paragraphs
 and inserting correct word/sentence.
\end_layout

\begin_layout Itemize
The creation of a generative, rather than a discriminative model for phrase
 embeddings.
 (Is an ends in and of itself)
\end_layout

\begin_layout Section
Existing Work
\end_layout

\begin_layout Subsection
RvNN: Recursive Neural Networks -- Parser 
\end_layout

\begin_layout Standard
At Stanford, Socher et al, produced several papers around the development
 of a Recursive Neural Network.
 Abbreviated in those works as RNN, in other works (Eg 
\begin_inset CommandInset citation
LatexCommand cite
key "Kagebaeck2014"

\end_inset

) and here it is abbreviated RvNN to distinguish it from Recurrent Neural
 Networks.
 
\end_layout

\begin_layout Subsubsection
Implementation
\end_layout

\begin_layout Paragraph
The Neural Network
\end_layout

\begin_layout Standard
The general premise of the RvNN is for a neural network with as shown in
 the figure, which is applied to every node in a parse tree.
 Note the multiple output layers.
 Note also the tied weights on the two 
\begin_inset Formula $L$
\end_inset

s.
 
\begin_inset Formula $W$
\end_inset

 takes 
\begin_inset Formula $c_{1}$
\end_inset

 concatenated with 
\begin_inset Formula $c_{2}$
\end_inset

as input and produced a merged parent with 
\begin_inset Formula $p_{1,2}$
\end_inset

.
\end_layout

\begin_layout Standard
the parent representation is then processed by a scoring matrix to score
 how good the merge is, and also labeling matrix which assigns it a Part
 of Speech (POS) label.
 The correct POS label is provided as training data.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/RvNN_single.png
	width 50page%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:The-Neural-Net"

\end_inset

The Neural Net applied recurssively generating the tree for the RvNN
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
This neural network is applied across each pair of adjacent child word-embedding
s (
\begin_inset Formula $c_{i}$
\end_inset

, 
\begin_inset Formula $c_{j}$
\end_inset

) , and the highest scoring merge is done.
 When the mere is done this best parent embeddings replaces both of the
 child embeddings (sharing adjacencies of the children).
 So so through this greedy search a high scory tree structure is found for
 the input phrase.
\end_layout

\begin_layout Subparagraph
Training Max-Margin
\end_layout

\begin_layout Standard
Max Margin training is done, to find a subgraident for ensuring correct
 trees score better than their incorrect counterparts.
\end_layout

\begin_layout Standard
An incorrect tree can be created from a correct example by corrupting on
 of the features (and ideally ensuring that in doing so it does not mistakenly
 create an correct tree.)
\end_layout

\begin_layout Subsubsection
Assessment
\end_layout

\begin_layout Standard
The success of the algorithm is measures on how well it is able to parse
 text (assign correct trees with correct POS tags).
\end_layout

\begin_layout Standard
Non-rigorous observations are made in 
\begin_inset CommandInset citation
LatexCommand cite
key "socher2010PhraseEmbedding"

\end_inset

, after embedding many sentences from the Wall Street Journal corpus, that
 similar phrase embeddings were located near each other.
\end_layout

\begin_layout Subsubsection
Variations
\end_layout

\begin_layout Standard
The variation in 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2013:CVG"

\end_inset

 added a Context Grammar, to change between several 
\begin_inset Formula $W$
\end_inset

 matrices depending on the POS.
 This Compositional Vector Grammar is now used in the Stanford Parser, a
 gold standard for parsing.
\end_layout

\begin_layout Standard
Recursive Tensor Neural Networks are related to RvNN: at every stage, both
 a vector and a matrix representation are handled.
 They were used in 
\begin_inset CommandInset citation
LatexCommand cite
key "Socher2013TensorReasoning"

\end_inset

 (which is a really neat idea).
 They performed better than RNN in They performed better in 
\begin_inset CommandInset citation
LatexCommand cite
key "RvNNLogicalSemantics"

\end_inset


\end_layout

\begin_layout Subsection
RAE: Recursive Auto Encoders
\end_layout

\begin_layout Standard
Similar to the RvNN, the Recursive Auto Encoder applies an Auto Encoder
 while generating the tree, in the place of the neural network shown in
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:The-Neural-Net"

\end_inset

.
 Roughly replacing the Max-Margin derived scoring matrix with a reconstruction
 error.
\end_layout

\begin_layout Standard
The tree selected is selecting much the say way as for the 
\end_layout

\begin_layout Subsection
Phrasal Translations
\end_layout

\begin_layout Standard
Work has been done on translating phrases, using word embeddings
\begin_inset CommandInset citation
LatexCommand cite
key "zou2013bilingual"

\end_inset

.
 However this is using word embeddings only.
\end_layout

\begin_layout Standard
It seems to be quiet tied to actually mapping between words in word space
 (see also 
\begin_inset CommandInset citation
LatexCommand cite
key "phrasaltranslationtool"

\end_inset

), rather than moving to a semantic space and then back again.
\end_layout

\begin_layout Standard
There is also 
\begin_inset CommandInset citation
LatexCommand cite
key "gao2014learning"

\end_inset

, seems to be based on bag of words (which has Li Deng as a coauthor).
\end_layout

\begin_layout Subsection
Single Word Embedding
\end_layout

\begin_layout Standard
The a lot of work on Word Embeddings note the existence of linear substructures
 in single-word embeddings for example: 
\begin_inset Formula $v("king")-v("man")+v("woman")$
\end_inset

 has nearest neighbor 
\begin_inset Formula $v("queen")$
\end_inset

.
 The paper which first established this seems to be 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013linguisticsubstructures"

\end_inset

.
 Single word generation is fairly simple via nearest neighbor for word embedding.
\end_layout

\begin_layout Standard
Word Embedding spaces seem to combine syntax and semantics, when combining
 the words from a sentence it would be better to end up in a syntax free
 space.
 
\end_layout

\begin_layout Subsection
Sum of Word Embeddings
\end_layout

\begin_layout Standard
I don't think anyone actually does this (despite what 
\begin_inset CommandInset citation
LatexCommand cite
key "ExtractiveSummaristation"

\end_inset

 says, see below).
 It doesn't work well.
 We expect it not to work well because of the results in 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013linguisticsubstructures"

\end_inset

 which shows that only 40% of simple relationships are represented in local
 structure.
\end_layout

\begin_layout Subsection
Summarization
\end_layout

\begin_layout Standard
Notably this would be Abstractive Summarization, of building a model then
 generating text from it.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "ExtractiveSummaristation"

\end_inset

(on Extractive Multidocument Summarization) using word and phrase embedding
 stated that the authors were not aware of any other papers which used continuou
s vector space models for summarization tasks.
 Which is indicative that this area is not well explored, as summarization
 is a very natural use to put any Re-Synthesis system to.
\end_layout

\begin_layout Standard
However: 
\begin_inset CommandInset citation
LatexCommand cite
key "ExtractiveSummaristation"

\end_inset

 claims that 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolovSkip"

\end_inset

 suggested summing word embeddings to get phrase embeddings, but to my knowledge
 that is quiet the opposite of what 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolovSkip"

\end_inset

 was saying.
 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolovSkip"

\end_inset

's dicsussion on phrases focuses on treating short phrases eg "Austrian
 Airlines", "New York Times" as separate tokens from the words that make
 them up in training word embeddings.
\end_layout

\begin_layout Subsection
Knowledge base systems
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "MemoryNN"

\end_inset

 is quiet a complicated system for performing full natural language queries
 with natural language output, and with memorisation.
\end_layout

\begin_layout Standard
It has several components, notable for this section is the R (Response).
 In most of there examples the response is set up to just returns a single
 word response, but some example are shown where they used a RNN.
 Details are not given, but I believe they used a RNN language model similar
 to 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2011RnnLM"

\end_inset

 (which they reference).
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2011RnnLM"

\end_inset

 does not make use of word embeedings, and is very similar to n-gram based
 model.
\end_layout

\begin_layout Standard
There is also 
\begin_inset CommandInset citation
LatexCommand cite
key "Socher2013TensorReasoning"

\end_inset

, which models word relationships with tensors, in a way a much more advanced
 version of 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013linguisticsubstructures"

\end_inset

, though I have not seen comparasons (They were both presented at the same
 conference I believe).
\end_layout

\begin_layout Section
Datasets
\end_layout

\begin_layout Subsection
Penn TreeBank
\end_layout

\begin_layout Standard
Consisting of POS annotated: Wall Street Journal, Brown, and Switchboard
 Datasets.
 Is documented in seemingly only in postscript.
\end_layout

\begin_layout Subsection
AMR SemBank
\end_layout

\begin_layout Standard
Abstract Meaning Representation (AMR) is a structured representation of
 the meaning of a sentence
\begin_inset CommandInset citation
LatexCommand cite
key "Banarescu13abstractmeaning"

\end_inset

.
 Its currently in spec 1.2.1
\begin_inset CommandInset citation
LatexCommand cite
key "AMRspec"

\end_inset

.
\end_layout

\begin_layout Standard
the AMR for two sentences with the same meaning is the same.
\end_layout

\begin_layout Standard
For example:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

(d / destroy-01 
\end_layout

\begin_layout Plain Layout

	:arg0 (b / boy) 
\end_layout

\begin_layout Plain Layout

	:arg1 (r / room))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

For all of:
\end_layout

\begin_layout Plain Layout

the destruction of the room by the boy ...
 
\end_layout

\begin_layout Plain Layout

the boy’s destruction of the room ...
 
\end_layout

\begin_layout Plain Layout

The boy destroyed the room.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
There is a freely available small dataset of 1,562 sentences from the novel,
 
\emph on

\begin_inset Quotes eld
\end_inset

The Little Prince
\begin_inset Quotes erd
\end_inset


\emph default
 available from 
\begin_inset CommandInset href
LatexCommand href
name "http://amr.isi.edu/download.html"
target "http://amr.isi.edu/download.html"

\end_inset

.
 
\end_layout

\begin_layout Standard
LDC publishes a dataset of a further 13,051 sentences at 
\begin_inset CommandInset href
LatexCommand href
name "https://catalog.ldc.upenn.edu/LDC2014T12"
target "https://catalog.ldc.upenn.edu/LDC2014T12"

\end_inset

.
 This is available to LDC subscribers (or $300 for nonmember) -- does UWA
 have a LDC membership?In total 20,434 sentences have been annotated, meaning
 several thousand are unreleased (According to 
\begin_inset CommandInset href
LatexCommand href
name "http://amr.isi.edu/download.html"
target "http://amr.isi.edu/download.html"

\end_inset

).
 According to 
\begin_inset CommandInset href
LatexCommand href
name "https://github.com/jflanigan/jamr"
target "https://github.com/jflanigan/jamr"

\end_inset

 there exists LDC2014E41, Released: May 30 2014, with 18,779 sentences.
\end_layout

\begin_layout Standard
JAMR
\begin_inset CommandInset citation
LatexCommand cite
key "jarmparser"

\end_inset

 (
\begin_inset CommandInset href
LatexCommand href
name "https://github.com/jflanigan/jamr"
target "https://github.com/jflanigan/jamr"

\end_inset

) is a parser that outputs AMR.
\end_layout

\begin_layout Standard
AMR is closely related to OntoNotes and to PropBank.
\end_layout

\begin_layout Subsection
OntoNotes
\end_layout

\begin_layout Subsection
PropBank
\end_layout

\begin_layout Subsection
FrameNet
\end_layout

\begin_layout Subsection
WordNet
\end_layout

\begin_layout Standard
It is an lexical database of words and their relationships.
 It can be accessed via python NLTK.
\end_layout

\begin_layout Subsection
SemEval
\end_layout

\begin_layout Standard
SemEval is a annual workshop.
 Each year there are several competitive tasks, for which corpia are provided.
 These corpia tend to have on going use even in publications not associated
 with the workshop.
\end_layout

\begin_layout Section
Projects
\end_layout

\begin_layout Subsection
RvNN on AMR, for a Semantic Phrase Embedding
\begin_inset CommandInset label
LatexCommand label
name "sub:RvNN-on-AMR"

\end_inset


\end_layout

\begin_layout Standard
As the work of 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2013:CVG,Socher2011ParsingPhrases,socher2010PhraseEmbedding"

\end_inset

 utilized (a subset of) the Penn Treebank to learn Parts of Speech (POS)
 tags for a syntactical trained phrase embedding.
\end_layout

\begin_layout Standard
The embeedings in 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2013:CVG,Socher2011ParsingPhrases,socher2010PhraseEmbedding"

\end_inset

 are described by Socher et al as semantic, and indeed may be as it does
 not 
\emph on
only
\emph default
 get trained on the POS information, but also on max margin for structure.
\end_layout

\begin_layout Standard
It could be replicated using the AMR SemBank
\begin_inset CommandInset citation
LatexCommand cite
key "Banarescu13abstractmeaning"

\end_inset

 for semantically trained phrase embedding.
\end_layout

\begin_layout Standard
This would not be reversible, but would be good in and of itself.
\end_layout

\begin_layout Subsubsection
Why would this be good
\end_layout

\begin_layout Standard
Sorcher et al's work on phrase embeddings used syntactic data (POS tags),
 for it discriminative training, with the aim to get a semantic representation
 as a biproduct.
 AMR is semantic information, as are its related resources (see below).
 It makes sense that using semantic information would be better for this
 task.
\end_layout

\begin_layout Subsubsection
Issues:
\end_layout

\begin_layout Paragraph
Format/Alignment
\end_layout

\begin_layout Standard
The major challenge is that AMR is not a nice lean format, unlike POS tags.
 While correct AMR assigns exactly one representation per meaning some is
 quiet complex.
 Also while it is nicely recursive, the number of relational tags is high.
 There is also the issues around the word tags.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

51.
 Here you may see the best portrait that , later , I was able to make of
 him .
 (lpp_1943.51)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

(p / possible
\end_layout

\begin_layout Plain Layout

  :domain (s / see-01
\end_layout

\begin_layout Plain Layout

            :ARG0 (y / you)
\end_layout

\begin_layout Plain Layout

            :ARG1 (p2 / portrait
\end_layout

\begin_layout Plain Layout

                    :mod (g / good
\end_layout

\begin_layout Plain Layout

                           :degree (m / most))
\end_layout

\begin_layout Plain Layout

                    :ARG1-of (m2 / make-01
\end_layout

\begin_layout Plain Layout

                               :ARG0 (i / i)
\end_layout

\begin_layout Plain Layout

                               :domain-of (p3 / possible)
\end_layout

\begin_layout Plain Layout

                               :time (l / late
\end_layout

\begin_layout Plain Layout

                                       :degree (m3 / more)))
\end_layout

\begin_layout Plain Layout

                    :topic (h / he))
\end_layout

\begin_layout Plain Layout

            :location (h2 / here)))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

52.
 But my drawing is certainly very much less charming than its model .
 (lpp_1943.52)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

(c3 / contrast-01
\end_layout

\begin_layout Plain Layout

      :ARG2 (c / charm-01
\end_layout

\begin_layout Plain Layout

            :ARG0 (t / thing
\end_layout

\begin_layout Plain Layout

                  :ARG1-of (d / draw-01
\end_layout

\begin_layout Plain Layout

                        :ARG0 (i / i)))
\end_layout

\begin_layout Plain Layout

            :quant (l / less
\end_layout

\begin_layout Plain Layout

                  :degree (m / much
\end_layout

\begin_layout Plain Layout

                        :degree (v / very)))
\end_layout

\begin_layout Plain Layout

            :mod (c2 / certain)
\end_layout

\begin_layout Plain Layout

            :compared-to (m2 / model
\end_layout

\begin_layout Plain Layout

                  :poss t)))
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In overcoming this a lot could be learned from the preprocessing steps of
 the JARM parser
\begin_inset CommandInset citation
LatexCommand cite
key "JARMparser"

\end_inset

.
\end_layout

\begin_layout Standard
While RvNN can have arbitrary number of inputs to each layer (2 in most
 work), it must have the same number of inputs to each layer (Further RvNN
 works on more general structures than trees -- it is only restricted to
 acyclic graphs).
\end_layout

\begin_layout Standard
There are certain ways of handling this.
 AMR has up to 5 arguments (ARG1,ARG2...) to each node (excluding labels, like
 :mod), Normally no more than 3 are filled, and on many (most) there are
 <5 maximum.
 The nonfilled, whether by being empty, or by being nonexistant for that
 node type could simply be filled with zeros -- this should be fine, but
 I many need to check that.
 Another is to use the approach used in 
\begin_inset CommandInset citation
LatexCommand cite
key "Socher2011ParsingPhrases"

\end_inset

, for allowing n-ary trees of image segments to be represented as taller
 binary trees and allowing multiple valid solutions.
\end_layout

\begin_layout Paragraph
Amount of Training Data 
\end_layout

\begin_layout Standard
A second issue is that the SemBank is small <20,000 sentences.
 However, showing a proof of concept that this method works, and indicating
 that with more training data it would work better is likely a sufficient
 result.
 
\end_layout

\begin_layout Subsection
Parsing to ARM (without RvNN)
\end_layout

\begin_layout Standard
Closely linked to the former, would be parsing to AMR.
\end_layout

\begin_layout Standard
A AMR is producing a graph, with edges labeled with semantic operators,
 and nodes labeled with concepts.
\end_layout

\begin_layout Standard
The JARM parser is likely the only parser for Sentences to AMR.
 I'm not aware of other work in this area.
\end_layout

\begin_layout Standard
There a several places in parser like JARM that could benefit from using
 word-embeddings and related technologies.
 Word embeddings could, for example be used as the feature vectors.
\end_layout

\begin_layout Subsubsection
JARM Summery
\end_layout

\begin_layout Standard
A summery of 
\begin_inset CommandInset citation
LatexCommand cite
key "JARMparser"

\end_inset

 follows.
 
\end_layout

\begin_layout Standard
JARM is a parser for converting English sentences into Abstract Meaning
 Representations.
\end_layout

\begin_layout Standard
Evaluation is two steps: 
\end_layout

\begin_layout Enumerate
Mapping words to concept nodes, done with a linear feature scoring 
\end_layout

\begin_layout Enumerate
Determining which relationship edges exist, done with a maximum spanning
 subgraph algorithm which maximized the scores from a different linear features
 scoring.
\end_layout

\begin_layout Standard
Both feature scorers much be trained.
 For the word to concept mapping a "Automatic Aligner" is used to know which
 concepts (in AMR space), align to which words.
 That is to say the automatic aligner is used to determine the ground truth
 from the training data (of sentences paired wit their AMR rep) to which
 the concept mapping is trained to replicate.
 The Aligner itself is based on fairly simple hand engineered heuristics.
 
\end_layout

\begin_layout Subsection
ReSynthesis into a Provided Structure via Pseudo-inverse weightings
\end_layout

\begin_layout Standard
There are two steps to directly ReSythisising from a phrase embedding of
 the form created by 
\begin_inset CommandInset citation
LatexCommand cite
key "Socher2011ParsingPhrases"

\end_inset

 (Or indeed by something created like 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:RvNN-on-AMR"

\end_inset

).
 One: the creation of the structure of the resynthesised text: eg 
\emph on

\begin_inset Quotes eld
\end_inset

Noun do Verb
\begin_inset Quotes erd
\end_inset


\emph default
, or 
\emph on
The 
\begin_inset Quotes eld
\end_inset

Adjective Noun Verb The Nou
\emph default
n
\begin_inset Quotes erd
\end_inset

.
 Second is filling said structure will values, eg 
\begin_inset Quotes eld
\end_inset

Cats do chase
\begin_inset Quotes erd
\end_inset

 or 
\emph on

\begin_inset Quotes eld
\end_inset

The fast cat chased the mouse.
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
In this project, we could assume the first is given, and solve the second.
 A not unreasonable, nor un-useful assumption.
\end_layout

\begin_layout Standard
For example is attempting to summarize reviews of hotels, it might be desirable
 to attempt to pull out statements that are only of the form 
\begin_inset Quotes eld
\end_inset

Noun Is Adjective
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Subsubsection
How
\end_layout

\begin_layout Standard
It is possible (I have a working proof of concept) to use linear algebra
 to reverse the operation of the RvNN.
 The basic operation on two input words is 
\begin_inset Formula $p_{1,2}=\tanh\left(W[Le_{1},\,Le_{2}L]\right)$
\end_inset

, where 
\begin_inset Formula $c_{i}=Le_{i}$
\end_inset

 where 
\begin_inset Formula $e_{i}$
\end_inset

 is a one-hot word identifier.
 Using atan, to reverse the nonlinearity.
 Using the pseudo inverse of 
\begin_inset Formula $W$
\end_inset

 to recover a least-squares values for recovering 
\begin_inset Formula $\hat{c}_{i}$
\end_inset

.
 Then using the cosign difference to obtain a similarity measure for each
 candidate 
\begin_inset Formula $\hat{e}_{i}$
\end_inset

.
 This should be generalisable for 
\begin_inset Formula $c_{i}=p_{j,k}$
\end_inset

 where it is a deeper structure .
\end_layout

\begin_layout Standard
When testing the basic operation on recovering two words as discussed it
 seems successful.
 This was performed using a 
\begin_inset Formula $L$
\end_inset

 which is a 50 dimensional matrix of Collobert & Weston word embeddings
 and a randomly set 
\begin_inset Formula $W$
\end_inset

.
 The input words were normally top ranked, and if not were generally in
 to top 5, with other highly ranked words similar (Eg President instead
 of King).
 This was with the random 
\begin_inset Formula $W$
\end_inset

 which does not encode useful information; a trained , 
\begin_inset Formula $W$
\end_inset

 should perform better.
 
\end_layout

\begin_layout Standard
A further experiment again with a random matrix showed it couldn't do the
 deeper structures, but that the math did work and returned a result.
 It is not unreasonable that it performed poorly with random 
\begin_inset Formula $W$
\end_inset

.
 
\end_layout

\begin_layout Subsubsection
Issues 
\end_layout

\begin_layout Standard
This being successful and useful, hinges on being able to reconstitute words
 into a format that they were not in originally, but that comes close to
 preserving meaning.
 This relies on the operations of the 
\begin_inset Formula $W$
\end_inset

 matrix functioning to project into a space containing semantic (rather
 than syntactic) information.
 For example it would be good if 
\emph on

\begin_inset Quotes eld
\end_inset

The basketball player
\begin_inset Quotes erd
\end_inset

 
\emph default
was projected closer to 
\emph on

\begin_inset Quotes eld
\end_inset

The basketballer
\begin_inset Quotes erd
\end_inset

 
\emph default
than to 
\emph on

\begin_inset Quotes eld
\end_inset

The soccer player
\begin_inset Quotes erd
\end_inset

.
 
\emph default
Solving this kind of issue would require improving the way W is trained.
 Such as discussed in
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:RvNN-on-AMR"

\end_inset

.
\end_layout

\begin_layout Standard
That may not be sufficient, results in 
\begin_inset CommandInset citation
LatexCommand cite
key "RvNNLogicalSemantics"

\end_inset

 (preprint), showed that for logical semantics, separation of different
 sentiments was not well preserved for longer expressions by the RvNN.
 They also considered the recursive neural tensor networks (RvNTN) and found
 it did much better.
\end_layout

\begin_layout Standard
Further too much information may be lost due to the squeeze function being
 applied to the the results.
\end_layout

\begin_layout Subsubsection
Extension
\end_layout

\begin_layout Standard
The naïve way to extend this to work more generally, is to perform a search
 (making use of dynamic programming potentially) across a space of possible
 structures.
\end_layout

\begin_layout Subsection
Structural ReSynthesis (The Core Problem)
\end_layout

\begin_layout Standard
The general issue of above, that training these models piece by piece then
 using a nongenerative method to combine them (As in done in the node merging
 in RvNN etc), does not allow the tree structure itself to be regenerated.
 The Reconstruction needs to be in the structure, not (just) in the nodes.
\end_layout

\begin_layout Standard
Otherwise the RAE of 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE,SocherEtAl2011:RAE"

\end_inset

 would have solved this long ago.
\end_layout

\begin_layout Standard
Doing this with something from a purely generative model like the boltzmann
 machine would be pleasing.
 A Generative model for strucure.
 But may not be possible.
\end_layout

\begin_layout Subsubsection
Tree Problem Detail 
\end_layout

\begin_layout Standard
When decomposing an embedding into a binary parse tree (the reverse of 
\begin_inset CommandInset citation
LatexCommand cite
key "socher2014recursive,SocherEtAl2013:CVG"

\end_inset

).
\end_layout

\begin_layout Standard
At each step the algorith needs to decide between 4 choices: for both the
 left and right nodes it can expand either (or both), in to a Word, or into
 an embedding node (for further decomposition).
\end_layout

\begin_layout Standard
When doing this it has an objective of generating a sentence which when
 converted back into an embedding will be as close as possible to the original
 embedding.
\end_layout

\begin_layout Standard
It also has the constraint that the sentence must be valid English (and
 ideally be natural sounding english).
\end_layout

\begin_layout Standard
When being used for summarization it has a further constraint on the length
 of the response (either in number of words, or at a more general scale
 in number of sentences.)
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "../../Resources/master_bibliography/master"
options "IEEEtran"

\end_inset


\end_layout

\end_body
\end_document
