#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass scrartcl
\begin_preamble
\usepackage{url}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Reversable Phrase Embedding
\end_layout

\begin_layout Standard
These three themes are broadened and generalized from discussed topics.
\end_layout

\begin_layout Standard
featuring: Re-Synthesis of Text from Vector Representations
\end_layout

\begin_layout Standard
See also: Natural Language Generation (the opposite of natural language
 understanding.)
\end_layout

\begin_layout Section
Applications / Possible Tasks/Projects within
\end_layout

\begin_layout Itemize
Summarization of a paragraph into a sentence
\end_layout

\begin_deeper
\begin_layout Itemize
Abstraction summarization
\end_layout

\end_deeper
\begin_layout Itemize
Production of a Short Title, via trade-off function between length and capture
 of meaning
\end_layout

\begin_deeper
\begin_layout Itemize
Again Abstractive summarization
\end_layout

\end_deeper
\begin_layout Itemize
Translation
\end_layout

\begin_deeper
\begin_layout Itemize
Using two stocatically reversible mappings from different language spaces
 into the same Meaning space
\end_layout

\end_deeper
\begin_layout Itemize
Robustness/Flow/Re-Synthesis cross over: Finding corrupted sentences/paragraphs
 and inserting correct word/sentence.
\end_layout

\begin_layout Itemize
The creation of a generative, rather than a discriminative model for phrase
 embeddings.
\end_layout

\begin_layout Section
Existing Work
\end_layout

\begin_layout Subsection
RvNN: Recursive Neural Networks -- Parser 
\end_layout

\begin_layout Standard
At Stanford, Socher et al, produced several papers around the development
 of a Recursive Neural Network.
 Abbreviated in throse works as RNN, in other works and here it is abbreviated
 RvNN to distinguish it for Reoccurrent Neural Networks.
 
\end_layout

\begin_layout Subsubsection
In Practice
\end_layout

\begin_layout Paragraph
The Neural Network
\end_layout

\begin_layout Standard
The general premise of the RvNN is for a neural network with several layers
 as shown in the figure.
 Note the multiple output layers.
 Note also the tied weights on the two 
\begin_inset Formula $L$
\end_inset

s.
 
\begin_inset Formula $W$
\end_inset

 takes 
\begin_inset Formula $c_{1}$
\end_inset

 concatenated with 
\begin_inset Formula $c_{2}$
\end_inset

as input and produced a merged parent with 
\begin_inset Formula $p_{1,2}$
\end_inset

.
\end_layout

\begin_layout Standard
Which is given to a scoring matrix to score how good the merge is,
\end_layout

\begin_layout Standard
and to a labeling matrix which assigns it a Part of Speech (POS) label.
\end_layout

\begin_layout Standard
The correct POS label is provided as training data.
\end_layout

\begin_layout Standard
It is applied Recursively across pairs of inputs (children) to create a
 total merged output.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename figs/RvNN_single.png
	width 50page%

\end_inset


\end_layout

\begin_layout Paragraph
The Tree
\end_layout

\begin_layout Subsubsection
Variations
\end_layout

\begin_layout Standard
The variation in 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2013:CVG"

\end_inset

 added a Context Grammar, to change between several 
\begin_inset Formula $W$
\end_inset

 matrices depending on the POS.
\end_layout

\begin_layout Standard
This Compositional Vector Grammar is now used in the Stanford Parser, a
 gold standard for parsing.
\end_layout

\begin_layout Subsubsection
Assessment
\end_layout

\begin_layout Standard
The success of the algorithm is measures on how well it is able to parse
 text (assign correct trees with correct POS tags).
\end_layout

\begin_layout Standard
Non-rigorous observations are made in 
\begin_inset CommandInset citation
LatexCommand cite
key "socher2010PhraseEmbedding"

\end_inset

, after embedding many sentences from the Wall Street Journal corpus, that
 similar phrase embeddings were located near each other.
\end_layout

\begin_layout Subsection
Phrasal Translations
\end_layout

\begin_layout Standard
Work has been done on translating phrases, using word embeddings
\begin_inset CommandInset citation
LatexCommand cite
key "zou2013bilingual"

\end_inset

.
 However this is using word embeddings only.
\end_layout

\begin_layout Standard
It seems to be quiet tied to actually mapping between words in word space
 (see also 
\begin_inset CommandInset citation
LatexCommand cite
key "phrasaltranslationtool"

\end_inset

), rather than moving to a semantic space and then back again.
\end_layout

\begin_layout Subsection
Single Word Embedding
\end_layout

\begin_layout Standard
The alot of work on Word Embeddings note the existence of linear substructures
 in single-word embeddings for example: 
\begin_inset Formula $v("king")-v("man")+v("woman")$
\end_inset

 has nearest neighbor 
\begin_inset Formula $v("queen")$
\end_inset

.
 The paper which first established this seems to be 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013linguisticsubstructures"

\end_inset

.
 Single word generation is fairly simple via nearest neighbor for word embedding.
\end_layout

\begin_layout Standard
Word Embedding spaces seem to combine syntax and semantics, when combining
 the words from a sentence it would be better to end up in a syntax free
 space.
 
\end_layout

\begin_layout Subsection
Summarization
\end_layout

\begin_layout Standard
Notably this would be Abstractive Summarization, of building a model then
 generating text from it.
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "Kagebaeck2014"

\end_inset

(on Extractive Multidocument Summarization) using word and phrase embedding
 stated that the authors were not aware of any other papers which used continuou
s vector space models for summarization tasks.
 Which is indicative that this area is not well explored, as summarization
 is a very natural use to put any Re-Synthesis system to.
\end_layout

\begin_layout Subsection
Knowledge base systems
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "MemoryNN"

\end_inset

 is quiet a complicated system for performing full natural language queries
 with natural language output, and with memorisation.
\end_layout

\begin_layout Standard
It has several componants, notable for this section is the R (Response).
 In most of there examples the response is set up to just returns a single
 word response, but some example are shown where they used a RNN.
 Details are not given, but I believe they used a RNN language model similar
 to 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2011RnnLM"

\end_inset

 (which they reference).
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2011RnnLM"

\end_inset

 does not make use of word embeedings, and is very similar to n-gram based
 model.
\end_layout

\begin_layout Standard
There is also 
\begin_inset CommandInset citation
LatexCommand cite
key "Socher2013TensorReasoning"

\end_inset

, which models word relationships with tensors, in a way a much more advanced
 version of 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013linguisticsubstructures"

\end_inset

, though I have not seen comparasons (They were both presented at the same
 conference I believe).
\end_layout

\begin_layout Section
Datasets
\end_layout

\begin_layout Subsection
Brown Corpus
\end_layout

\begin_layout Subsection
PennTreeBank
\end_layout

\begin_layout Subsection
AMR SemBank
\end_layout

\begin_layout Standard
Abstract Meaning Representation (AMR) is a structured representation of
 the meaning of a sentence
\begin_inset CommandInset citation
LatexCommand cite
key "Banarescu13abstractmeaning"

\end_inset

.
\end_layout

\begin_layout Standard
the AMR for two sentences with the same meaning is the same.
\end_layout

\begin_layout Standard
For example:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

(d / destroy-01 
\end_layout

\begin_layout Plain Layout

	:arg0 (b / boy) 
\end_layout

\begin_layout Plain Layout

	:arg1 (r / room))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

For all of:
\end_layout

\begin_layout Plain Layout

the destruction of the room by the boy ...
 
\end_layout

\begin_layout Plain Layout

the boyâ€™s destruction of the room ...
 
\end_layout

\begin_layout Plain Layout

The boy destroyed the room.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
There is a freely available small dataset of 1,562 sentences from the novel,
 
\emph on

\begin_inset Quotes eld
\end_inset

The Little Prince
\begin_inset Quotes erd
\end_inset


\emph default
 available from 
\begin_inset CommandInset href
LatexCommand href
name "http://amr.isi.edu/download.html"
target "http://amr.isi.edu/download.html"

\end_inset

.
 
\end_layout

\begin_layout Standard
LDC publishes a dataset of a further 13,051 sentences at 
\begin_inset CommandInset href
LatexCommand href
name "https://catalog.ldc.upenn.edu/LDC2014T12"
target "https://catalog.ldc.upenn.edu/LDC2014T12"

\end_inset

.
 This is available to LDC subscribers (or $300 for nonmember) -- does UWA
 have a LDC membership?
\end_layout

\begin_layout Standard
In total 20,434 sentences have been annotated, meaning several thousand
 are unreleased.
\end_layout

\begin_layout Subsection
SemEval
\end_layout

\begin_layout Standard
SemEval is a annual workshop.
 Each year there are several competitive tasks, for which corpia are provided.
 These corpia tend to have on going use even in publications not associated
 with the workshop.
\end_layout

\begin_layout Subsection
WordNet
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "../../Resources/master_bibliography/master"
options "IEEEtran"

\end_inset


\end_layout

\end_body
\end_document
