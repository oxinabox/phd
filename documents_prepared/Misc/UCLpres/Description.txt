Title: Natural language is unreasonably simple, unreasonably often
Subtitle: Adding up word embeddings works far too well, why is that?

Description:
Word Embeddings have, in a big way, changed how we do natural language processing.
We use them (implicitly or explicitly) in a variety of powerful systems:
from LSTMs to complicated tree structured models.
These models are very exciting, and work well to push the envelope of what can be done.
But there are also simpler models: unreasonably simple systems based on just summing the word embeddings.
On a variety of tasks, these work unreasonably well.
They clearly do not capture compositional semantics (...or do they?).
So what is going on?

This talk will discuss why they work so well.
What sorts of problems they work well on; 
what this says about natural language
and the implications for evaluations.
There performance is not truly unreasonable:
there are reasons and this talk will cover them.


About the speaker:
Lyndon White is a PhD Candidate at the University of Western Australia.
His research area is natural language processing via machine learning.
His particular recent interests have been around marginally exotic classification-like problems involving natural language input -- one of the areas where summing word embeddings works really well.
He is under the supervision of Prof Roberto Togneri, Dr Wei Liu and W/Prof Mohammed Bennamoun.
He is a significant contributor to the julia programming language ecosystem,
is the co-maintainer of TensorFlow.jl and of numerous other packages.
He is currently in London for the julia language conference.