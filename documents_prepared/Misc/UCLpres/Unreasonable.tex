\RequirePackage{luatex85,shellesc}
\documentclass[dvipsnames]{beamer}
\usepackage[subpreambles=false]{standalone}

\usepackage[author={Lyndon White}]{pdfcomment}

\usepackage{fancyvrb}

\usepackage{microtype}
\usepackage{adjustbox}


%\newlength\xunit
\input{brownbeamer}
\setbeamercolor{math text}{fg=bluewrite}
\setbeamercolor{math text displayed}{fg=bluewrite}

\usefonttheme[onlymath]{serif}

\usepackage{amsmath}
\usepackage{mathtools}
\input{preamble-math}


\usepackage[at]{easylist}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{master.bib}
\let\oldcite=\cite                                                              
\renewcommand{\cite}[1]{\textcolor{bluewrite}{\oldcite{#1}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{pgfplots}
\usepackage{pgfplotstable}

\pgfplotsset{
	barbase/.style={
		ybar,ymajorgrids, 
		width=0.95\textwidth, height= 5cm,
		bar width=0.6cm,
		axis line style={draw=none},
		x tick style={draw=none},
		ytick = {0.10, 0.15, ..., 0.4},
		ytick pos=left,
	}
}


%%%%%%%%%%%%%%%%%%

\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{chains}
\usetikzlibrary{decorations.pathmorphing}
%\usetikzlibrary{graphs,graphs.standard,graphdrawing,arrows}
%\usegdlibrary{layered, trees, force}



\usepackage{graphicx}
\graphicspath{{./figs/}, {./}}
\usepackage[space]{grffile}

\usepackage{trimclip}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TIKZ stuff

\input{preamble-tikz}

\tikzset{%
	->,
	align=center,
	node distance=30mm, sibling distance=25mm, level distance=40mm,
	every edge/.append style = {thick},
	package/.style={draw, circle, very thick},
	mypackage/.style={package, blue},
	repo/.style={draw, very thick, purple},
	info/.style={Green},
	supports/.style={dotted, very thick},
	indirect/.style={decoration={snake}, decorate, very thick, purple}
}

\newcommand{\pkg}[2][]{\tikzset{#2/.append style={}}; \node[#1,package, #2](#2) {#2.jl}}
\newcommand{\repo}[2][]{\node[#1,repo](#2) {#2}}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% all notes should just be simple item notes.
% I neither need nor want more functionality
\let\oldnote\note
\renewcommand{\note}{\oldnote[item]}

\renewcommand{\emph}{\alert}

%%%%%%%%%%%%%%%%
\newcommand{\countertitle}[1]{\vfill\vfill{\centering \Large \color{bluewrite} #1}\vspace{-0.5cm}}


%%%%%%%%

\newcommand{\mquote}[1]{\textit{``#1''}}

%%%%%

\newcommand{\natlang}[1]{\textcolor{Purple}{\texttt{#1}}}

%%%%%%%%%%%%%%%%%%%%%


\institute{Department of Electical, Electronic and Computer Engineering\\The University of Western Australia}
\date{}
\title{Natural language is \\ unreasonably simple, unreasonably often}
\subtitle{Adding up word embeddings works far too well,\\ why is that?}
\author{
%	\includegraphics[height=2cm]{juliaml}
%	\includegraphics[height=2cm]{juliatext}
%	\includegraphics[height=2cm]{juliastring}
%	\\
%	\vspace{5mm}
	\textbf{Lyndon White}}

\logo{\hfill\includegraphics[scale=0.12]{uwa}\hfill\hspace{0.5cm}}

\begin{document}

\centering %Center everywhere
\frame{\maketitle}
\logo{}

\begin{frame}{Australia, really it is quiet far away}
	\note{A considerable distance you might say}
	\centering
	\includegraphics[height=0.8\pageheight]{world}
\end{frame}

\begin{frame}{The University of Western Australia}
	\includegraphics[width=0.6\textwidth]{uwaphoto}\hfill\null
	\vspace{-0.1\pageheight}
	\raggedleft
%	\null\hfill%
	\includegraphics[width=0.6\textwidth]{perthphoto}
\end{frame}

\begin{frame}{We like to think language is very complicated}
	
	\includegraphics[width=\textwidth]{parseegone}
	\vfill
	\includegraphics[width=\textwidth]{parseegtwo}
	\vfill	
	\vfill	
	\countertitle{This is what we do, and complicated models make us feel good and publish well.}
\end{frame}

\begin{frame}{and sometimes language is}
	\vfill
	\natlang{This movie is a truly excellent example of the quality of cinematography this century; bring back the good old days of real cinema!}
	\vfill
	\natlang{\textcolor{Green}{You shouldn't miss this,\\ that would be the worst mistake.}}
	\vfill
	\natlang{\textcolor{Orange}{It's not that it is was bad,\\ but it wasn't what I hoped for.}}
	
	\countertitle{And so we need the complicated models.}
	\vfill
\end{frame}

\begin{frame}{but sometimes language isn't}
	\begin{itemize}
		\item \natlang{The girl stands on the tennis court.}
		\item Not: \natlang{The tennis court stands on the girl.}
	\end{itemize}
	\vfill
	\pause
	\structure{How do we know?}
	\begin{description}
		\item[World Knowledge:] \natlang{girl} is an \emph{agent}, that can take \emph{actions}% like \natlang{standing}.
		\\
		\hfill OR\hfill\hfill\hfill\null
		\item[Language Modelling:] the trigram \natlang{tennis court stands} never occurs in the Google Books corpus.
	\end{description}
	\countertitle{And so simple methods work}
\end{frame}



\newcommand{\mathtextcolor}{\color{brownwrite}}
\newcommand{\ut}[2]{\underbrace{\text{\mathtextcolor{}{#1}}}_{\mathclap{\text{#2}}}}
\newcommand{\ot}[2]{\overbrace{\text{\mathtextcolor{}{#1}}}^{\mathclap{\text{#2}}}}
\begin{frame}{and often it looks like it is complicated\\ but isn't}
	\vfill 
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\renewcommand{\mathtextcolor}{\color[rgb]{0.043,0.535,0.529}}
			\begin{equation*}
				\ut{dark}{basic modifier}
				\quad \ot{green}{color modifier}
				\!\!\ut{ish}{meta modifier}
				\quad \ot{blue}{head color}
			\end{equation*}
			
			\vfill
			\renewcommand{\mathtextcolor}{\color[rgb]{0.063,0.651,0.455}}
			
			\begin{equation*}
				\ut{dark}{basic modifier}
				\qquad \; \ot{blu}{color modifier}
				\!\!\ut{ish}{meta modifier}
				\quad \ot{green}{head color}
			\end{equation*}
		\end{column}
	
		\begin{column}{0.5\textwidth}
			\vfill
			\renewcommand{\mathtextcolor}{\color[rgb]{1,0.398,0.314}}
			\begin{equation*}
			\quad \ot{rudd}{\shortstack{color modifier\\ (red)}}
			\!\!\!\ut{y}{meta modifier}
			\quad \ot{coral}{\shortstack{head color\\ (from noun) }}
			\end{equation*}
			\vfill	
		\end{column}
	\end{columns}


	\note{When it comes to color names, you can define a sophisticated grammar, ISCC-NBS defines a fairly basic one. But it isn't really sufficient to capture everything going on with a realistically sized vocabulary}
	
	\note{I'll be back for this example later}
	
	\countertitle{and so using complicated methods leads to worse performance.}
	\vfill
\end{frame}


\begin{frame}{You can just add up word embeddings and uses it as a representation}
	\begin{block}{\normalsize \citehere{acl2018bleuopposedmeaning}}
		\mquote{On classification tasks,  our [LSTM encoder-decoder] models
			are outperformed even by GloVe-BOW, except
			for the NLI tasks\ldots}
	\end{block}
	
	\begin{block}{\normalsize \citehere{ac2018probingsentencevectors}}
		\mquote{Our first striking result is the good overall performance
			of Bag-of-Vectors, confirming early insights
			that aggregated word embeddings capture
			surprising amounts of sentence information\ldots}
	\end{block}
\end{frame}

\begin{frame}{Content is king}
	\vspace{-0.5cm}
	Considering further \cite{ac2018probingsentencevectors}
	\only<1>{\begin{itemize}
			\item They looked at $3 \times 8  + 5$ models \note{8 different training varients of 3 types of sophisticaed models, plus 5 baselines}
			\item \emph{10 probing tasks} were used to evaluate the models.
			\item Of the \emph{baseline} models SOWE was often the best.
			\item It was rarely better than the best performing \emph{sophisticated model}
			\item \emph{Except} on Word Content \note{Actually, most of the baseline models are better than the fancy models on the word content task, but SOWE beats them on most of tasks.}
		\end{itemize}
	}
	\only<2>{
		When the probing task results are correlated against 12 down stream tasks.\\
		\includegraphics[height=0.6\pageheight]{probing-cor}
	}
	\note{This is not the say that SOWE wins at downstream tasks; I've not seen those results. But it does highlight just how very important capturing surface information is -- lose track of that, and your losing important information.}
	
	\vspace{-0.5cm}
	
	\countertitle{The words people use are the most important part of what they are saying}
\end{frame}

\begin{frame}{SkipGram is the most well known of recent word embedding methods}
	
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\begin{tikzpicture}[]
			
			
			\coordinate(w1) at (0,0);
			\node[draw,circle,green,left=0.8 of w1](ww1) at (0,0) {$\n w_{i}$};
			
			
			\coordinate[above left=of w1, xshift=-0.95cm, yshift=-1cm] (CC0);% at (Cn);
			\embtable{C}{CC0}{\tiny Input Embs}
			
			\draw[->] (ww1) -- ({C}3.east);
			
			
			\node(L2)[layer, above = 3 of w1,
			%	label={[label distance=0] 0: i.e. \\ $\i \softmax(\ldots)_{\n w_j}$}
			]{
				$\frac{\exp{\i V_{\n w_j, :} \i C_{\n w_i}}}%
				{\sum_{\forall v \in \set V} \exp{\i V_{v, :} \i C_{\n w_i}}}$
			};
			
			\draw[->]  ({C}3.east) -- node[labe]{$\i C_{\n w_i}$} (L2);
			
			
			\node(out)[above = 1.5 of L2]{$P^\star(\n w_j \mid \n w_i)$};
			\draw[->] (L2) edge (out);
			
			
			\coordinate[above = 3 of {C}n] (VV0);
			\embtable[T]{V}{VV0}{\tiny Output Embs}
			
			
			%	\pgfmathsetmacro\ang{(\ii * -2 + -180+20 };
			%	\draw[->] (V\ii.east) -- (L2.\ang);
			%\draw[->] ({V}n.east) -- (L2.south west);
			
			
			%\draw[->] (out.204) -- ({V}6.east);
			\node(ww2)[draw,circle, green, left=0.2 of out]{$\n w_j$};
			\draw[->] (ww2) -- ({V}6.east);
			\draw[->] ({V}6.east)-- node[auto, labe]{$\i V_{\n w_j,:}$} (L2.170);
			\end{tikzpicture}
		\end{column}
		
		\begin{column}{0.5\textwidth}
			\only<1>{
				\begin{align*}
				P^\star&(\n w_j \mid \n w_i) = \i {\softmax \left( V\,\i C_{\n w_i}) \right)}_{\n w_j} \label{equ:skipgram}\\
				&=\frac{\exp{\i V_{\n w_j, :} \i C_{\n w_i}}}%
				{\sum_{\forall k \in \set V} \exp{\i V_{k, :} \i C_{\n w_i}}}
				\end{align*}\\
				\vspace{1cm}
				Maximise $P^\star(\n w_j \mid \n w_i)$
				for combinations of words\\ $\n w_j$, $\n w_i$
				that actually co-occur.\\
				\vspace{1cm}
			}
			\only<2>{
				$P^\star(\n w_j \mid \n w_i)$ is maximised
				when $\i V_{\n w_j, :} \i C_{\n w_i}$
				is maximized.\\
				\vspace{1.7cm}
				i.e. when the \emph{dot-product} of the \emph{input embeddings} and the \emph{output embeddings} of collocated words\\ approaches $1$
			}
		\end{column}
	\end{columns}
	
\end{frame}

\newcommand{\monoto}{\varpropto}%{\raisebox{-0.2em}{\overset{\approx}{\propto}}}
\begin{frame}{SkipGram is a iterative algorithm for weighted collocation matrix factorization.}
	\begin{columns}
		\begin{column}{0.65\textwidth}
			$V$ is a $300\times |\set V|$ {\small input embeddings matrix}\\
			$C$ is a $|\set V|\times 300$ {\small output embeddings matrix}\\
			$X$ is a $|\set V|\times |\set V|$ {\small collocation count matrix}\\
			$f$ is some monotonic weighting function.\\
			(\cite{levy2015lsaisbasicallyskipgramswithexperimentstoprove})
		\end{column}
		\begin{column}{0.3\textwidth}
			\begin{align*}
			Loss & \monoto -X \odot \exp{V C}\\
			& \monoto{} VC - \log X\\
			& \monoto{} VC - f(X)
			\end{align*}
			Loss is minimized when  $VC \approx f(X)$
		\end{column}
	\end{columns}
	
	
	\note{Chasing down exactly what that weighting function $f$ is is a bit hairy, as there are many implementation factors like negative sampling, subsampling, etc that go into it. Levy et al, and several others have done it, and it comes out as a proxy for mutual information}
	
	\note{Oh look is is GloVE}
	
	\countertitle{When trying to factorize very large matrices numerical linear algebraticians often use iterative methods.}
	\vfill
\end{frame}



\begin{frame}{So SkipGrams are a \\ dimensionality reduction algorithm,\\ that tries remember collocated words}
	\begin{itemize}
		\item Contrast: \emph{PCA} is a dimensionality reduction algorithm, that tries to \emph{remember the most variant factors}
		\item Contrast: \emph{t-SNE} is a dimensionality reduction algorithm, that tries to \emph{preserve similarity as distance}
		\item Compressing knowledge of collocated words into a dense vector, gives us \emph{Firth's Criterion}.
	\end{itemize}
	
\end{frame}


\begin{frame}{Matrix product with onehot vector is\\ indexed slicing}
	Consider the \emph{onehot} representation of some word $w$,\\
	as $\iv e_w=\big[0, \ldots, \underbrace{1}_{\mathclap{w\text{th position}}},0 \ldots, 0\big]$\\
	\vspace{0.5cm}
	It's word embedding is given by
	$\i C_{w} = C^\intercal e_w$
\end{frame}

\begin{frame}{Sum of word embeddings is the same as \\ matrix product with bag of words}
	A bag of words can be represented as a vector of the counts of each word in the vocabulary.\\ \vspace{0.5cm}
	For a sequence of words: $\left(\n w_1, \n w_2, \ldots \right)$ \\\vspace{0.5cm}
	The\emph{ bag of words} can be given by\\ $\v x = \sum_{\forall i} \iv e_{\n w_i}$.\\\vspace{0.5cm}
	
	The \emph{sum of word embeddings} for the same sequence is:
	$\sum_{\forall i} \i C_{\n w_i} = C^\intercal \sum_{\forall i} \iv e_{\n w_i}$
\end{frame}

\begin{frame}{Concatenation followed by matrix product\\ is the same as \\matrix product followed by addition}
	\vspace{-0.7cm}
	\begin{equation*}
	\left[\begin{array}{cc}
	U & V\end{array}\right]\left[\begin{array}{c}
	\v a\\
	\v b
	\end{array}\right] = U\v a + V \v b
	\end{equation*}
	Thus
	\begin{align*}
	C (\v a + \v b) &= 
	C\v a + C \v b \\&=
	\left[\begin{array}{cc}
	C & C\end{array}\right]\left[\begin{array}{c}
	\v a\\
	\v b
	\end{array}\right]
	\end{align*}
	\countertitle{A summed input is the same as a concatenated input\\ with blockwise weight tying.}
	\vfill
	\vfill
	\vfill
	\vfill
	\vfill
\end{frame}


\pgfplotstableread[col sep=comma,header=has colnames]{data/selection_len_scores.csv}{\sellenscores}
\begin{frame}[fragile]{Bag of words information is not lost in sums of word embeddings}
	\only<1>{
		\begin{itemize}
			\item A bag of words captures all unigram information in a sentence/document etc
			\item A greedy method can (mostly) recover the BOW from a SOWE\\
			\citehere{White2015BOWgen}\vspace{-0.5cm}
			\begin{enumerate}
				\item Greedily add nearest word to remaining the SOWE to bag
				\item Check each word in bag and swap it for a better one
				\item Repeat until no change.
			\end{enumerate}
			\normalsize
			\item Thus, SOWE captures similar \emph{most} unigram information.
			%\item little cancellation or obscuring sums.
			\note{Further, the SOWE space is (in some sense) \emph{nice}}
		\end{itemize}
	}
	\only<2>{
		\begin{tikzpicture}
		\begin{axis}[xlabel=Ground Truth Sentence Length,
		ylabel=Mean Jaccard Index,
		legend style={at={(0.5,1)}, anchor=south,legend columns=3, draw=none},
		width=10cm,height=4cm,cycle list name=exotic]
		\addplot table [y=brown_glove50_jaccard_mean,x=ground_len]{\sellenscores};
		\addplot table [y=brown_glove100_jaccard_mean,x=ground_len]{\sellenscores};
		\addplot table [y=brown_glove200_jaccard_mean,x=ground_len]{\sellenscores};
		\addplot table [y=brown_glove300_jaccard_mean,x=ground_len]{\sellenscores};%
		\addplot table [y=books_0_01_glove300_jaccard_mean,x=ground_len]{\sellenscores};
		\legend{\small 50D Brown, \small 100D Brown, \small 200D Brown, \small 300D Brown, \small 300D Books}					
		\end{axis}
		\end{tikzpicture}
	}
	\vspace{-0.5cm}
	
	\countertitle{We can reliably recover all words from a sum of word embeddings}
\end{frame}

\begin{frame}{Sentence embeddings space should partition readily according to paraphrases}
	\begin{itemize}
		\item \emph{Paraphrases} are defined by \emph{bidirectional entailment}.
		\item This is an \emph{equivalence relation}
		\item It thus gives rise to a \emph{partition} of \emph{natural language space}.
	\end{itemize}
	\countertitle{}
\end{frame}

\begin{frame}{What does it mean to partition readily?}
	There are many ways one could define the quality of a space, on its ability to be partitioned according to anther linked space's partitions.\\
	%	Without restrictions an arbitrary bijection could be allowed.\\
	%	But that would not be a 
	\note{This is a bit like assessing quality of clusters. And there is prove-ably no ideal cluster qualuty measure}
	
	\begin{description}%[labelsep=1em]
		\item[Convex]\qquad No twists, bulges, holes, jumps etc.
		\item[Seperable]\qquad Should not overlap, should be separate      			
		\item[Concentrated]\quad small area		
	\end{description}
	\countertitle{Notice: these are the same criteria needed linear SVM to work well.}
\end{frame}

\pgfplotstableread[col sep=comma]{
	Name,MSRP,Opiniosis, void
	PV--DM, 78, 38.26,0
	PV--DBOW, 89.93, 32.19,0
	URAE, 51.14, 20.87,0
	%	MOWE, 97.91, 69.30,0
	SOWE, 98.02, 68.75,0
	BOW, 98.37, 65.23,0
	PCA--BOW, 97.96, 54.43,0
}\resultstable

\begin{frame}[fragile]{When assessing ability to match partitions using a linear SVM classification task}
	\begin{tikzpicture}
	\begin{axis}[barbase,
	nodes near coords, nodes near coords align={vertical},
	nodes near coords={\small \hspace{1mm} \pgfmathprintnumber[precision=0]{\pgfplotspointmeta}\%},
	ytick= {0,25,50,75,100},
	yticklabels={0\%,25\%,50\%,75\%,100\%},%\pgfmathprintnumber{\tick}\,\%,
	ybar=1pt,% interval= 0.7,
	ylabel=Accuracy,
	ymin=0, ymax=115,
	%    		enlarge y limits=0.2,
	enlarge x limits=0.1,
	xtick=data,
	xticklabels={PV\\DM,PV\\DBOW,URAE,SOWE,BOW,PCA\\BOW},
	xticklabel pos=lower,
	xticklabel style={align=center},
	legend style={at={(0.5,-0.4)}, anchor=north,legend columns=2, draw=none},
	legend image code/.code={%
		\draw[#1] (0cm,-0.2cm) rectangle (0.5cm,0.3cm);
	}
	]
	\addplot[style={top color=Red, bottom color=Green, draw=none, mark=none}] table [y=MSRP,x expr=\coordindex, meta=Name] {\resultstable};
	\addplot [style={top color=Orange, bottom color=Blue, draw=none, mark=none}] table [y=Opiniosis,x expr=\coordindex]{\resultstable};
	\legend{MSRP\hspace{1cm}, Opinosis};
	\end{axis}
	\end{tikzpicture}
	\vspace{-0.7cm}
	\countertitle{Knowing word content is really useful}
\end{frame}



\begin{frame}{What is going on here?}
	\begin{block}{PV-DM / PV-DBOW \\ \citehere{le2014distributed}}
		\mquote{It shows that the paragraph vectors, when evaluated correctly, do not work better than bag-of-words (bag-of-ngrams being even better)}\\
		\hfill -- Tomas Mikolov (23/11/17) w.r.t \cite{mesnil2014ensemble}
	\end{block}
	
	\begin{block}{URAE\\
			\citehere{SocherEtAl2011:PoolRAE}
		}
		Only 200D so not entirely fair.\\
		Based on binary constituency parse structure\\
		Deep, compressing each layer, 2 inputs to one output.
	\end{block}
\end{frame}

\begin{frame}{What is going on here?}
	\begin{block}{BOW \\ Bag of Words}
		Several thousand dimensions\\
		1000D spaces tend to be very separable\\
		No synonym capacity\\
	\end{block}
	\begin{block}{PCA BOW \\ Principle Component Analysis \cite{hotelling1933analysis}}
		Drop down to 300D to match others\\
		PCA is inferior to GloVE as a dimensionality reduction technique,
		for semantic preservation.
	\end{block}
\end{frame}


\begin{frame}[fragile]{Consider machine captioning evaluation}
	Correlation with human ranking in the COMPOSITE captioning evaluation dataset. \cite{Aditya2017}
	
	\begin{itemize}
		\item For each image, there are 5 captions.
		\item A mix of \emph{human} generated
		\item  and \emph{machine} generated.
		\item this is an evaluation of \emph{evaluation metrics}
		
	\end{itemize}
	\note{To steal someone else's joke. This is a second order evaluation. A first order evaluation is the application of a metric; a third order evaluation is what you are doing now.}
	\countertitle{Task is to rate the captions in the same order as the human rankers.}
\end{frame}


\begin{frame}[fragile]{Look what wins}
	\vspace{-0.5cm}
	\pgfplotstableread[header=has colnames,
	columns/Method/.style={string type},
	columns/Type/.style={string type}
	]{%
		Method	Kendall	Type
		BLEU	0.191	{basic ngram}
		ROGUE-L	0.213	{basic ngram}
		\emph{GloVE}	0.337	{MOWE}
		METEOR	0.341	{fancy ngram}
		CIDEr	0.342	{fancy ngram}
		SPICE	0.349	{semantic parsing}
		\emph{Word2Vec}	0.349	{MOWE}
		%SPICE+CIDEr	0.357	{semantic parsing}
		\emph{FastText}	0.364	{MOWE}
	}{\compositeCaptionEval}
	
	
	
	\begin{tikzpicture}
	\begin{axis}[barbase,
	xticklabels from table={\compositeCaptionEval}{Method},
	legend style={at={(0.5,-0.7)}, anchor=north,legend columns=2, draw=none},
	legend image post style={xscale=1, yscale=1},
	%	enlargelimits=0.3,
	ylabel = {Kendall's $\tau$},
	ymin=0.1, ymax=0.4,
	enlarge x limits=0.2,
	xticklabel style={rotate=90},
	xtick = data,
	%yticklabel={$\mathsf{\pgfmathprintnumber{\tick}}$\small\%},
	%nodes near coords={$\mathsf{\pgfmathprintnumber[precision=2]{\pgfplotspointmeta}}$},
	]
	
	\addplot[style={top color=Orange, bottom color=Blue, draw=none, mark=none}] table[x expr=\coordindex, y=Kendall]{\compositeCaptionEval};
	
	\end{axis}
	\end{tikzpicture}
	
	{\small
		\textasteriskcentered Forthcoming publication \emph{Naeha Sharif, Lyndon White, Mohammed Bennamoun and Syed Afaq Ali Shah}.
	}
	\vspace{0.01cm}
%	
%	\note{This is a hard task, somewhere between abstractive summarization and machine translation, and the assessment is also difficult.}
%	\note{The similarity to abstractive summarization makes it particular bad -- it is not clear what should be said}
%	\note{Shown here is a comparason of minimum cosine similarity to reference MOWE for different embeddings with more traditional metrics}
%	\note{See that the performance of the traditional ngram based metrics BLEU for MT and ROGUE for summarization kinda suck}
%	\note{Next best is the worst performing out of word embedding models GloVE.}
%	\note{It gets out performed by METEOR and CIDEr, which I would summarize as fancy ngram methods, with synonyms and weights.}
%	\note{Then comes SPICE. SPICE is very sophisticated, it does parsing and builds this semantic graph, then does graph comparasons.}
%	\note{But it is beated by Word2Vec, and by FastText}
%	\note{There are a few reasons why FastText is better than word2vec -- it could be the capturing of morphemes for example. But I'd probably attribute it to a few more years of engineering and a whole lot more data.}
%	
	\countertitle{Captioning quality can be assessed on \textbf{fluency} and on \textbf{adequacy}}
\end{frame}





\begin{frame}{All captions in COMPOSITE are \textbf{fluent}}
	\begin{itemize}
		\item We are really good at language modelling now.
		\item In theory our RNN language models can capture all needed state
		\item COMPOSITE captions are a mix of \emph{human generated} and \emph{state-of-the-art machine generated}.
	\end{itemize}
	\vfill
	\countertitle{Trying to capture fluency in your captioning metric is thus not important}
\end{frame}


\begin{frame}{The proper way to look at adequacy is to build a semantic graph, and apply reasoning to it}
	% TODO replace this with nice figures I made myself
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\includegraphics[width=\textwidth]{spicegraph}
		\end{column}
		\begin{column}{0.5\textwidth}
			This is what SPICE does \note{The best performing model other than a MOWE}.
			\cite{spice2016}
			
			You could use AMR \cite{Banarescu13abstractmeaning}, or ERS\cite{bender2015ERS}.
			
			To get to a form that reasoning can be applied on.
			\note{Or more generally that a neural network can leverage.}
		\end{column}
	\end{columns}
	
	\countertitle{This semantic graph must be derived from the \textbf{right words} in the \textbf{right order}}
\end{frame}


\begin{frame}{Semantic graph comes from syntactic graph}
	\begin{itemize}
		\item The syntactic graph comes from the \emph{word order} and \emph{word content}.
		\item In theory, \emph{different} words in \emph{different} orders could give the \emph{same} semantic graph
		\item and the \emph{same} words in a \emph{different} order could give a \emph{different} semantic graph.
	\end{itemize}
	\countertitle{syntactic graph comes from token order}
\end{frame}

\begin{frame}{Due to ambiguity in possible word order semantic meaning should not be derivable from averaged lexical meaning representation}
	\begin{itemize}
		\item Well written sentences are short: \emph{14-17 words}
		\item They don't have complicated clauses and negations.
		\item Words are used in consistent phrases:
		\begin{itemize}
			\item \natlang{The girl stands on the tennis court}
			\item Not: \natlang{The tennis court stands on the girl}
		\end{itemize}
		\item Good captions are such good sentences.
		
	\end{itemize}
	\countertitle{But in-practice, it probably is}
\end{frame}

\begin{frame}{Word order is more predictable than you may think}
	
	\countertitle{Further, other orderings are likely (near) paraphrases.}
\end{frame}


\begin{frame}{Some might say the problems we are assessing on are not sufficiently difficult}
	\note{But that is a bit of a meaningly statement}
	The problems are exactly as difficult as they are. \note{Tautology}
	\begin{itemize}
		\item real world problems on real data.
	\end{itemize}
	
\end{frame}

\begin{frame}{Maybe we are not really doing natural language understanding}
	\countertitle{but practically we are certainly doing something useful}
\end{frame}



\end{document}