\RequirePackage{luatex85,shellesc}
\documentclass[dvipsnames]{beamer}
\usepackage[subpreambles=false]{standalone}

\usepackage[author={Lyndon White}]{pdfcomment}

\usepackage{fancyvrb}

\usepackage{microtype}
\usepackage{adjustbox}


%\newlength\xunit
\input{brownbeamer}
\setbeamercolor{math text}{fg=bluewrite}
\setbeamercolor{math text displayed}{fg=bluewrite}

\usefonttheme[onlymath]{serif}

\usepackage{amsmath}
\usepackage{mathtools}
\input{preamble-math}


\usepackage[at]{easylist}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{master.bib}
\let\oldcite=\cite                                                              
\renewcommand{\cite}[1]{\textcolor{bluewrite}{\oldcite{#1}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{pgfplots}
\usepackage{pgfplotstable}

\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{chains}
\usetikzlibrary{decorations.pathmorphing}
%\usetikzlibrary{graphs,graphs.standard,graphdrawing,arrows}
%\usegdlibrary{layered, trees, force}

\usepackage{graphicx}
\graphicspath{{./figs/}, {./}}
\usepackage[space]{grffile}

\usepackage{trimclip}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TIKZ stuff

\input{preamble-tikz}

\tikzset{%
	->,
	align=center,
	node distance=30mm, sibling distance=25mm, level distance=40mm,
	every edge/.append style = {thick},
	package/.style={draw, circle, very thick},
	mypackage/.style={package, blue},
	repo/.style={draw, very thick, purple},
	info/.style={Green},
	supports/.style={dotted, very thick},
	indirect/.style={decoration={snake}, decorate, very thick, purple}
}

\newcommand{\pkg}[2][]{\tikzset{#2/.append style={}}; \node[#1,package, #2](#2) {#2.jl}}
\newcommand{\repo}[2][]{\node[#1,repo](#2) {#2}}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% all notes should just be simple item notes.
% I neither need nor want more functionality
\let\oldnote\note
\renewcommand{\note}{\oldnote[item]}

\renewcommand{\emph}{\alert}

%%%%%%%%%%%%%%%%
\newcommand{\countertitle}[1]{\vfill\vfill{\centering \Large \color{bluewrite} #1}\vspace{-0.5cm}}

%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\natlang}[1]{\textcolor{Purple}{\texttt{#1}}}

%%%%%%%%%%%%%%%%%%%%%


\institute{School of Electical, Electronic and Computer Engineering\\The University of Western Australia}
\date{}
\title{Natural language is \\ unreasonably simple, unreasonably often}
\subtitle{Adding up word embeddings works far too well, why is that?}
\author{
%	\includegraphics[height=2cm]{juliaml}
%	\includegraphics[height=2cm]{juliatext}
%	\includegraphics[height=2cm]{juliastring}
%	\\
%	\vspace{5mm}
	\textbf{Lyndon White}}

\logo{\hfill\includegraphics[scale=0.12]{uwa}\hfill\hspace{0.5cm}}

\begin{document}

\centering %Center everywhere
\frame{\maketitle}
\logo{}

\begin{frame}{Australia, really it is quiet far away}
	\note{A considerable distance you might say}
	\centering
	\includegraphics[height=0.8\pageheight]{world}
\end{frame}

\begin{frame}{The University of Western Australia}
	\includegraphics[width=0.6\textwidth]{uwaphoto}\hfill\null
	\vspace{-0.1\pageheight}
	\raggedleft
%	\null\hfill%
	\includegraphics[width=0.6\textwidth]{perthphoto}
\end{frame}

\begin{frame}{We like to think language is very complicated}
	
	\includegraphics[width=\textwidth]{parseegone}
	\vfill
	\includegraphics[width=\textwidth]{parseegtwo}
	\vfill	
	\vfill	
	\countertitle{This is what we do, and complicated models make us feel good and publish well.}
\end{frame}

\begin{frame}{and sometimes language is}
	\vfill
	\natlang{This movie is a truly excellent example of the quality of cinematography this century; bring back the good old days of real cinema!}
	\vfill
	\natlang{\textcolor{Green}{You shouldn't miss this,\\ that would be the worst mistake.}}
	\vfill
	\natlang{\textcolor{Orange}{It's not that it is was bad,\\ but it wasn't what I hoped for.}}
	
	\countertitle{And so we need the complicated models.}
	\vfill
\end{frame}

\begin{frame}{but sometimes language isn't}
	\begin{itemize}
		\item \natlang{The girl stands on the tennis court.}
		\item Not: \natlang{The tennis court stands on the girl.}
	\end{itemize}
	\vfill
	\pause
	\structure{How do we know?}
	\begin{description}
		\item[World Knowledge:] \natlang{girl} is an \emph{agent}, that can take \emph{actions}% like \natlang{standing}.
		\\
		\hfill OR\hfill\hfill\hfill\null
		\item[Language Modelling:] the trigram \natlang{tennis court stands} never occurs in the Google Books corpus.
	\end{description}
	\countertitle{And so simple methods work}
\end{frame}



\newcommand{\mathtextcolor}{\color{brownwrite}}
\newcommand{\ut}[2]{\underbrace{\text{\mathtextcolor{}{#1}}}_{\mathclap{\text{#2}}}}
\newcommand{\ot}[2]{\overbrace{\text{\mathtextcolor{}{#1}}}^{\mathclap{\text{#2}}}}
\begin{frame}{and often it looks like it is complicated\\ but isn't}
	\vfill 
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\renewcommand{\mathtextcolor}{\color[rgb]{0.043,0.535,0.529}}
			\begin{equation*}
				\ut{dark}{basic modifier}
				\quad \ot{green}{color modifier}
				\!\!\ut{ish}{meta modifier}
				\quad \ot{blue}{head color}
			\end{equation*}
			
			\vfill
			\renewcommand{\mathtextcolor}{\color[rgb]{0.063,0.651,0.455}}
			
			\begin{equation*}
				\ut{dark}{basic modifier}
				\qquad \; \ot{blu}{color modifier}
				\!\!\ut{ish}{meta modifier}
				\quad \ot{green}{head color}
			\end{equation*}
		\end{column}
	
		\begin{column}{0.5\textwidth}
			\vfill
			\renewcommand{\mathtextcolor}{\color[rgb]{1,0.398,0.314}}
			\begin{equation*}
			\quad \ot{rudd}{\shortstack{color modifier\\ (red)}}
			\!\!\!\ut{y}{meta modifier}
			\quad \ot{coral}{\shortstack{head color\\ (from noun) }}
			\end{equation*}
			\vfill	
		\end{column}
	\end{columns}


	\note{When it comes to color names, you can define a sophisticated grammar, ISCC-NBS defines a fairly basic one. But it isn't really sufficient to capture everything going on with a realistically sized vocabulary}
	
	\note{I'll be back for this example later}
	
	\countertitle{and so using complicated methods leads to worse performance.}
	\vfill
\end{frame}

\begin{frame}{SkipGram is the most well known of recent word embedding methods}
	
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\begin{tikzpicture}[]
			
			
			\coordinate(w1) at (0,0);
			\node[draw,circle,green,left=0.8 of w1](ww1) at (0,0) {$\n w_{i}$};
			
			
			\coordinate[above left=of w1, xshift=-0.95cm, yshift=-1cm] (CC0);% at (Cn);
			\embtable{C}{CC0}{\tiny Input Embs}
			
			\draw[->] (ww1) -- ({C}3.east);
			
			
			\node(L2)[layer, above = 3 of w1,
			%	label={[label distance=0] 0: i.e. \\ $\i \softmax(\ldots)_{\n w_j}$}
			]{
				$\frac{\exp{\i V_{\n w_j, :} \i C_{\n w_i}}}%
				{\sum_{\forall v \in \set V} \exp{\i V_{v, :} \i C_{\n w_i}}}$
			};
			
			\draw[->]  ({C}3.east) -- node[labe]{$\i C_{\n w_i}$} (L2);
			
			
			\node(out)[above = 1.5 of L2]{$P^\star(\n w_j \mid \n w_i)$};
			\draw[->] (L2) edge (out);
			
			
			\coordinate[above = 3 of {C}n] (VV0);
			\embtable[T]{V}{VV0}{\tiny Output Embs}
			
			
			%	\pgfmathsetmacro\ang{(\ii * -2 + -180+20 };
			%	\draw[->] (V\ii.east) -- (L2.\ang);
			%\draw[->] ({V}n.east) -- (L2.south west);
			
			
			%\draw[->] (out.204) -- ({V}6.east);
			\node(ww2)[draw,circle, green, left=0.2 of out]{$\n w_j$};
			\draw[->] (ww2) -- ({V}6.east);
			\draw[->] ({V}6.east)-- node[auto, labe]{$\i V_{\n w_j,:}$} (L2.170);
			\end{tikzpicture}
		\end{column}
	
		\begin{column}{0.5\textwidth}
			\only<1>{
				\begin{align*}
				P^\star&(\n w_j \mid \n w_i) = \i {\softmax \left( V\,\i C_{\n w_i}) \right)}_{\n w_j} \label{equ:skipgram}\\
				&=\frac{\exp{\i V_{\n w_j, :} \i C_{\n w_i}}}%
					{\sum_{\forall k \in \set V} \exp{\i V_{k, :} \i C_{\n w_i}}}
				\end{align*}\\
				\vspace{1cm}
				Maximise $P^\star(\n w_j \mid \n w_i)$
				for combinations of words\\ $\n w_j$, $\n w_i$
				that actually co-occur.\\
				\vspace{1cm}
			}
			\only<2>{
				$P^\star(\n w_j \mid \n w_i)$ is maximised
				 when $\i V_{\n w_j, :} \i C_{\n w_i}$
				 is maximized.\\
				 \vspace{1.7cm}
				 i.e. when the \emph{dot-product} of the \emph{input embeddings} and the \emph{output embeddings} of collocated words\\ approaches $1$
			}
		\end{column}
	\end{columns}
	
\end{frame}

\newcommand{\monoto}{\varpropto}%{\raisebox{-0.2em}{\overset{\approx}{\propto}}}
\begin{frame}{SkipGram is a iterative algorithm for weighted collocation matrix factorization.}
	\begin{columns}
		\begin{column}{0.65\textwidth}
			$V$ is a $300\times |\set V|$ {\small input embeddings matrix}\\
			$C$ is a $|\set V|\times 300$ {\small output embeddings matrix}\\
			$X$ is a $|\set V|\times |\set V|$ {\small collocation count matrix}\\
			$f$ is some monotonic weighting function.\\
			(\cite{levy2015lsaisbasicallyskipgramswithexperimentstoprove})
		\end{column}
		\begin{column}{0.3\textwidth}
			\begin{align*}
				Loss & \monoto -X \odot \exp{V C}\\
				& \monoto{} VC - \log X\\
				& \monoto{} VC - f(X)
			\end{align*}
			Loss is minimized when  $VC \approx f(X)$
		\end{column}
	\end{columns}

	
	\note{Chasing down exactly what that weighting function $f$ is is a bit hairy, as there are many implementation factors like negative sampling, subsampling, etc that go into it. Levy et al, and several others have done it, and it comes out as a proxy for mutual information}
	
	\note{Oh look is is GloVE}
	
	\countertitle{When trying to factorize very large matrices numerical linear algebraticians often use iterative methods.}
	\vfill
\end{frame}

\begin{frame}{So SkipGrams are a \\ dimensionality reduction algorithm,\\ that tries remember collocated words}
	\begin{itemize}
		\item Contrast: \emph{PCA} is a dimensionality reduction algorithm, that tries to \emph{remember the most variant factors}
		\item Contrast: \emph{t-SNE} is a dimensionality reduction algorithm, that tries to \emph{preserve similarity as distance}
		\item Compressing knowledge of collocated words into a dense vector, gives us \emph{Firth's Criterion}.
	\end{itemize}
	
\end{frame}

\begin{frame}{Matrix product with onehot vector is\\ indexed slicing}
	Consider the \emph{onehot} representation of some word $w$,\\
	as $\iv e_w=\big[0, \ldots, \underbrace{1}_{\mathclap{w\text{th position}}},0 \ldots, 0\big]$\\
	\vspace{0.5cm}
	It's word embedding is given by
	$\i C_{w} = C^\intercal e_w$
\end{frame}

\begin{frame}{Sum of word embeddings is the same as \\ matrix product with bag of words}
	A bag of words can be represented as a vector of the counts of each word in the vocabulary.\\ \vspace{0.5cm}
	For a sequence of words: $\left(\n w_1, \n w_2, \ldots \right)$ \\\vspace{0.5cm}
	The\emph{ bag of words} can be given by\\ $\v x = \sum_{\forall i} \iv e_{\n w_i}$.\\\vspace{0.5cm}
	
	The \emph{sum of word embeddings} for the same sequence is:
	$\sum_{\forall i} \i C_{\n w_i} = C^\intercal \sum_{\forall i} \iv e_{\n w_i}$
\end{frame}

\begin{frame}{Concatenation followed by matrix product\\ is the same as \\matrix product followed by addition}
	\vspace{-0.7cm}
	\begin{equation*}
		\left[\begin{array}{cc}
		U & V\end{array}\right]\left[\begin{array}{c}
		\v a\\
		\v b
		\end{array}\right] = U\v a + V \v b
	\end{equation*}
	Thus
	\begin{align*}
	C (\v a + \v b) &= 
	C\v a + C \v b \\&=
	\left[\begin{array}{cc}
	C & C\end{array}\right]\left[\begin{array}{c}
	\v a\\
	\v b
	\end{array}\right]
	\end{align*}
	\countertitle{A summed input is the same as a concatenated input\\ with blockwise weight tying.}
	\vfill
	\vfill
	\vfill
	\vfill
	\vfill
\end{frame}

\begin{frame}{Bag of words information is not lost in sums of word embeddings}
	\begin{itemize}
		\item A bag of words captures all unigram information in a sentence/document etc
		
	\end{itemize}
	\countertitle{We can reliably recover all words from a sum of word embeddings}
\end{frame}




\begin{frame}[fragile]{Consider machine captioning evaluation}
	Correlation with human ranking in the COMPOSITE captioning evaluation dataset. \cite{Aditya2017}
	
	\pgfplotstableread[header=has colnames,
		columns/Method/.style={string type},
		columns/Type/.style={string type}
	]{%
	Method	Kendall	Type
	BLEU	0.191	{basic ngram}
	ROGUE-L	0.213	{basic ngram}
	\emph{GloVE}	0.337	{MOWE}
	METEOR	0.341	{fancy ngram}
	CIDEr	0.342	{fancy ngram}
	SPICE	0.349	{semantic parsing}
	\emph{Word2Vec}	0.349	{MOWE}
	%SPICE+CIDEr	0.357	{semantic parsing}
	\emph{FastText}	0.364	{MOWE}
	}{\compositeCaptionEval}
	
	
	\begin{tikzpicture}
	\begin{axis}[ybar,ymajorgrids,
	xticklabels from table={\compositeCaptionEval}{Method},
	legend style={at={(0.5,-0.7)}, anchor=north,legend columns=2, draw=none},
	legend image post style={xscale=1, yscale=1},
	%	enlargelimits=0.3,
	ymin=0.1, ymax=0.4,
	enlarge x limits=0.2,
	xticklabel style={rotate=90},
	xtick = data,
	width=0.85\textwidth, height= 5cm,
	bar width=0.6cm,
	%yticklabel={$\mathsf{\pgfmathprintnumber{\tick}}$\small\%},
	ylabel = {Kendall's $\tau$},
	axis line style={draw=none},
	x tick style={draw=none},
	ytick = {0.10, 0.15, ..., 0.4},
	ytick pos=left,
	%nodes near coords={$\mathsf{\pgfmathprintnumber[precision=2]{\pgfplotspointmeta}}$},
	]
	
	\addplot[style={top color=Orange, bottom color=Blue, draw=none, mark=none}] table[x expr=\coordindex, y=Kendall]{\compositeCaptionEval};
	
	\end{axis}
	\end{tikzpicture}
	
	{\small
	\textasteriskcentered Forthcoming publication \emph{Naeha Sharif, Lyndon White, Mohammed Bennamoun and Syed Afaq Ali Shah}.
	}
	
	\note{This is a hard task, somewhere between abstractive summarization and machine translation, and the assessment is also difficult.}
	\note{The similarity to abstractive summarization makes it particular bad -- it is not clear what should be said}
	\note{Shown here is a comparason of minimum cosine similarity to reference MOWE for different embeddings with more traditional metrics}
	\note{See that the performance of the traditional ngram based metrics BLEU for MT and ROGUE for summarization kinda suck}
	\note{Next best is the worst performing out of word embedding models GloVE.}
	\note{It gets out performed by METEOR and CIDEr, which I would summarize as fancy ngram methods, with synonyms and weights.}
	\note{Then comes SPICE. SPICE is very sophisticated, it does parsing and builds this semantic graph, then does graph comparasons.}
	\note{But it is beated by Word2Vec, and by FastText}
	\note{There are a few reasons why FastText is better than word2vec -- it could be the capturing of morphemes for example. But I'd probably attribute it to a few more years of engineering and a whole lot more data.}

\end{frame}

\begin{frame}{What is going on? How can a unigram method be beating everything?}
	
	\countertitle{Captioning quality can be assessed on \textbf{fluency} and on \textbf{adequacy}}
\end{frame}

\begin{frame}{All captions in COMPOSITE are \textbf{fluent}}
	\begin{itemize}
		\item We are really good at language modelling now.
		\item In theory our RNN language models can capture all needed state
		\item COMPOSITE captions are a mix of \emph{human generated} and \emph{state-of-the-art machine generated}.
	\end{itemize}
	\vfill
	\countertitle{Trying to capture fluency in your captioning metric is thus not important}
\end{frame}

\begin{frame}{The proper way to look at adequacy is to build a semantic graph, and apply reasoning to it}
	% TODO replace this with nice figures I made myself
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\includegraphics[width=\textwidth]{spicegraph}
		\end{column}
		\begin{column}{0.5\textwidth}
			This is what SPICE does \note{The best performing model other than a MOWE}.
			\cite{spice2016}
			
			You could use AMR \cite{Banarescu13abstractmeaning}, or ERS\cite{bender2015ERS}.
			
			To get to a form that reasoning can be applied on.
			\note{Or more generally that a neural network can leverage.}
		\end{column}
	\end{columns}

	\countertitle{This semantic graph must be derived from the \textbf{right words} in the \textbf{right order}}
\end{frame}

\begin{frame}{Semantic graph comes from syntactic graph}
	\begin{itemize}
		\item The syntactic graph comes from the \emph{word order} and \emph{word content}.
		\item In theory, \emph{different} words in \emph{different} orders could give the \emph{same} semantic graph
		\item and the \emph{same} words in a \emph{different} order could give a \emph{different} semantic graph.
	\end{itemize}
\end{frame}

\begin{frame}{Due to ambiguity in possible word order semantic meaning should not be derivable from averaged lexical meaning representation}
	\begin{itemize}
		\item Well written sentences are short: \emph{14-17 words}
		\item They don't have complicated clauses and negations.
		\item Words are used in consistent phrases:
		\begin{itemize}
			\item \natlang{The girl stands on the tennis court}
			\item Not: \natlang{The tennis court stands on the girl}
		\end{itemize}
		\item Good captions are such good sentences.
		
	\end{itemize}
	\countertitle{But in-practice, it probably is}
\end{frame}


\begin{frame}{Some might say the problems we are assessing on are not sufficiently difficult}
	\note{But that is a bit of a meaningly statement}
	The problems are exactly as difficult as they are. \note{Tautology}
	\begin{itemize}
		\item real world problems on real data.
	\end{itemize}
	
\end{frame}

\begin{frame}{Maybe we are not really doing natural language understanding}
	\countertitle{but practically we are certainly doing something useful}
\end{frame}


\end{document}