\RequirePackage{luatex85,shellesc}
\documentclass[dvipsnames]{beamer}
\usepackage[author={Lyndon White}]{pdfcomment}

\usepackage{fancyvrb}

\usepackage{microtype}
\usepackage{adjustbox}

\usepackage{amsmath}
\usepackage{mathtools}

\usepackage[subpreambles=false]{standalone}


\usepackage[at]{easylist}

%\newlength\xunit
\input{brownbeamer}
\setbeamercolor{math text}{fg=bluewrite}
\setbeamercolor{math text displayed}{fg=bluewrite}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{master.bib}
\let\oldcite=\cite                                                              
\renewcommand{\cite}[1]{\textcolor{bluewrite}{\oldcite{#1}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{pgfplots}
\usepackage{pgfplotstable}

\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{chains}
\usetikzlibrary{decorations.pathmorphing}
%\usetikzlibrary{graphs,graphs.standard,graphdrawing,arrows}
%\usegdlibrary{layered, trees, force}

\usepackage{graphicx}
\graphicspath{{./figs/}, {./}}
\usepackage[space]{grffile}

\usepackage{trimclip}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TIKZ stuff

\tikzset{%
	->,
	align=center,
	node distance=30mm, sibling distance=25mm, level distance=40mm,
	every edge/.append style = {thick},
	package/.style={draw, circle, very thick},
	mypackage/.style={package, blue},
	repo/.style={draw, very thick, purple},
	info/.style={Green},
	supports/.style={dotted, very thick},
	indirect/.style={decoration={snake}, decorate, very thick, purple}
}

\newcommand{\pkg}[2][]{\tikzset{#2/.append style={}}; \node[#1,package, #2](#2) {#2.jl}}
\newcommand{\repo}[2][]{\node[#1,repo](#2) {#2}}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% all notes should just be simple item notes.
% I neither need nor want more functionality
\let\oldnote\note
\renewcommand{\note}{\oldnote[item]}

\renewcommand{\emph}{\alert}

%%%%%%%%%%%%%%%%
\newcommand{\countertitle}[1]{\vfill\vfill{\centering \Large \color{bluewrite} #1}\vspace{-0.5cm}}

%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\natlang}[1]{\textcolor{Purple}{\texttt{#1}}}

%%%%%%%%%%%%%%%%%%%%%


\institute{School of Electical, Electronic and Computer Engineering\\The University of Western Australia}
\date{}
\title{Natural language is \\ unreasonably simple, unreasonably often}
\subtitle{Adding up word embeddings works far too well, why is that?}
\author{
%	\includegraphics[height=2cm]{juliaml}
%	\includegraphics[height=2cm]{juliatext}
%	\includegraphics[height=2cm]{juliastring}
%	\\
%	\vspace{5mm}
	\textbf{Lyndon White}}

\logo{\hfill\includegraphics[scale=0.12]{uwa}\hfill\hspace{0.5cm}}

\begin{document}

\centering %Center everywhere
\frame{\maketitle}
\logo{}

\begin{frame}{Australia, really it is quiet far away}
	\note{A considerable distance you might say}
	\centering
	\includegraphics[height=0.8\pageheight]{world}
\end{frame}

\begin{frame}{The University of Western Australia}
	\includegraphics[width=0.6\textwidth]{uwaphoto}\hfill\null
	\vspace{-0.1\pageheight}
	\raggedleft
%	\null\hfill%
	\includegraphics[width=0.6\textwidth]{perthphoto}
\end{frame}

\begin{frame}{We like to think language is very complicated}
	
	\includegraphics[width=\textwidth]{parseegone}
	\vfill
	\includegraphics[width=\textwidth]{parseegtwo}
	\vfill	
	\vfill	
	\countertitle{This is what we do, and complicated models make us feel good and publish well.}
\end{frame}

\begin{frame}{and sometimes language is}
	\vfill
	\natlang{This movie is a truly excellent example of the quality of cinematography this century; bring back the good old days of real cinema!}
	\vfill
	\natlang{\textcolor{Green}{You shouldn't miss this,\\ that would be the worst mistake.}}
	\vfill
	\natlang{\textcolor{Orange}{It's not that it is was bad,\\ but it wasn't what I hoped for.}}
	
	\countertitle{And so we need the complicated models.}
	\vfill
\end{frame}

\begin{frame}{but sometimes language isn't}
	\begin{itemize}
		\item \natlang{The girl stands on the tennis court.}
		\item Not: \natlang{The tennis court stands on the girl.}
	\end{itemize}
	\vfill
	\pause
	\structure{How do we know?}
	\begin{description}
		\item[World Knowledge:] \natlang{girl} is an \emph{agent}, that can take \emph{actions}% like \natlang{standing}.
		\\
		\hfill OR\hfill\hfill\hfill\null
		\item[Language Modelling:] the trigram \natlang{tennis court stands} never occurs in the Google Books corpus.
	\end{description}
	\countertitle{And so simple methods work}
\end{frame}



\newcommand{\ut}[2]{\underbrace{\text{\textcolor{brownwrite}{#1}}}_{\mathclap{\text{#2}}}}
\newcommand{\ot}[2]{\overbrace{\text{\textcolor{brownwrite}{#1}}}^{\mathclap{\text{#2}}}}
\begin{frame}{and often it looks like it is complicated\\ but isn't}
	\vfill 
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\begin{equation*}
				\ut{dark}{basic modifier}
				\quad \ot{green}{color modifier}
				\!\!\ut{ish}{meta modifier}
				\quad \ot{blue}{head color}
			\end{equation*}
			
			\vfill
			
			\begin{equation*}
				\ut{dark}{basic modifier}
				\qquad \; \ot{blu}{color modifier}
				\!\!\ut{ish}{meta modifier}
				\quad \ot{green}{head color}
			\end{equation*}
		\end{column}
	
		\begin{column}{0.5\textwidth}
			\vfill
			\begin{equation*}
			\quad \ot{rudd}{\shortstack{color modifier\\ (red)}}
			\!\!\!\ut{y}{meta modifier}
			\quad \ot{coral}{\shortstack{head color\\ (from noun) }}
			\end{equation*}
			\vfill	
		\end{column}
	\end{columns}

	
	

	\note{When it comes to color names, you can define a sophisticated grammar, ISCC-NBS defines a fairly basic one. But it isn't really sufficient to capture everything going on with a realistically sized vocabulary}
	
	\note{I'll be back for this example later}
	
	\countertitle{and so using complicated methods leads to worse performance.}
	\vfill
\end{frame}


\begin{frame}{SkipGram etc. are basically iterative algorithms for weighted collocation matrix factorization.}
	\countertitle{When trying to factorize very large matrices numerical linear algebraticians often use iterative methods.}
\end{frame}



\begin{frame}{Bag of words information is not lost in sums of word embeddings}
	\begin{itemize}
		\item A bag of words captures all unigram information in a sentence/document etc
		
	\end{itemize}
	\countertitle{We can reliably recover all words from a sum of word embeddings}
\end{frame}




\begin{frame}[fragile]{Consider machine generation captioning evaluation}
	Correlation with human ranking in the COMPOSITE captioning evaluation dataset. \cite{Aditya2017}
	
	\pgfplotstableread[header=has colnames,
		columns/Method/.style={string type},
		columns/Type/.style={string type}
	]{%
	Method	Kendall	Type
	BLEU	0.191	{basic ngram}
	ROGUE-L	0.213	{basic ngram}
	\emph{GloVE}	0.337	{MOWE}
	METEOR	0.341	{fancy ngram}
	CIDEr	0.342	{fancy ngram}
	SPICE	0.349	{semantic parsing}
	\emph{Word2Vec}	0.349	{MOWE}
	%SPICE+CIDEr	0.357	{semantic parsing}
	\emph{FastText}	0.364	{MOWE}
	}{\compositeCaptionEval}
	
	
	\begin{tikzpicture}
	\begin{axis}[ybar,ymajorgrids,
	xticklabels from table={\compositeCaptionEval}{Method},
	legend style={at={(0.5,-0.7)}, anchor=north,legend columns=2, draw=none},
	legend image post style={xscale=1, yscale=1},
	%	enlargelimits=0.3,
	ymin=0.1, ymax=0.4,
	enlarge x limits=0.2,
	xticklabel style={rotate=90},
	xtick = data,
	width=0.85\textwidth, height= 5cm,
	bar width=0.6cm,
	%yticklabel={$\mathsf{\pgfmathprintnumber{\tick}}$\small\%},
	ylabel = {Kendall's $\tau$},
	axis line style={draw=none},
	x tick style={draw=none},
	ytick = {0.10, 0.15, ..., 0.4},
	ytick pos=left,
	%nodes near coords={$\mathsf{\pgfmathprintnumber[precision=2]{\pgfplotspointmeta}}$},
	]
	
	\addplot[style={top color=Orange, bottom color=Blue, draw=none, mark=none}] table[x expr=\coordindex, y=Kendall]{\compositeCaptionEval};
	
	\end{axis}
	\end{tikzpicture}
	
	{\small
	\textasteriskcentered Forthcoming publication Naeha Sharif, Lyndon White, Mohammed Bennamoun and Syed Afaq Ali Shah.
	}
	
	\note{This is a hard task, somewhere between abstractive summarization and machine translation, and the assessment is also difficult.}
	\note{The similarity to abstractive summarization makes it particular bad -- it is not clear what should be said}
	\note{Shown here is a comparason of minimum cosine similarity to reference MOWE for different embeddings with more traditional metrics}
	\note{See that the performance of the traditional ngram based metrics BLEU for MT and ROGUE for summarization kinda suck}
	\note{Next best is the worst performing out of word embedding models GloVE.}
	\note{It gets out performed by METEOR and CIDEr, which I would summarize as fancy ngram methods, with synonyms and weights.}
	\note{Then comes SPICE. SPICE is very sophisticated, it does parsing and builds this semantic graph, then does graph comparasons.}
	\note{But it is beated by Word2Vec, and by FastText}
	\note{There are a few reasons why FastText is better than word2vec -- it could be the capturing of morphemes for example. But I'd probably attribute it to a few more years of engineering and a whole lot more data.}

\end{frame}

\begin{frame}{What is going on? How can a unigram method be beating everything?}
	
	\countertitle{Captioning quality can be assessed on \textbf{fluency} and on \textbf{adequacy}}
\end{frame}

\begin{frame}{All captions in COMPOSITE are \textbf{fluent}}
	\begin{itemize}
		\item We are really good at language modelling now.
		\item In theory our RNN language models can capture all needed state
		\item COMPOSITE captions are a mix of \emph{human generated} and \emph{state-of-the-art machine generated}.
	\end{itemize}
	\vfill
	\countertitle{Trying to capture fluency in your captioning metric is thus not important}
\end{frame}

\begin{frame}{The proper way to look at adequacy is to build a semantic graph, and apply reasoning to it}
	% TODO replace this with nice figures I made myself
	\begin{columns}
		\begin{column}{0.5\textwidth}
			\includegraphics[width=\textwidth]{spicegraph}
		\end{column}
		\begin{column}{0.5\textwidth}
			This is what SPICE does \note{The best performing model other than a MOWE}.
			\cite{spice2016}
			
			But you could use AMR etc \cite{Banarescu13abstractmeaning}.
			
			To get to a form that reasoning can be applied on.
			\note{Or more generally that a neural network can leverage.}
		\end{column}
	\end{columns}

	\countertitle{This semantic graph must be derived from the \textbf{right words} in the \textbf{right order}}
\end{frame}

\begin{frame}{Semantic graph comes from syntactic graph}
	\begin{itemize}
		\item The syntactic graph comes from the \emph{word order} and \emph{word content}.
		\item In theory, \emph{different} words in \emph{different} orders could give the \emph{same} semantic graph
		\item and the \emph{same} words in a \emph{different} order could give a \emph{different} semantic graph.
	\end{itemize}
\end{frame}

\begin{frame}{Due to ambiguity in possible word order semantic meaning should not be derivable from averaged lexical meaning representation}
	\begin{itemize}
		\item Well written sentences are short: \emph{14-17 words}
		\item They don't have complicated clauses and negations.
		\item Words are used in consistent phrases:
		\begin{itemize}
			\item \natlang{The girl stands on the tennis court}
			\item Not: \natlang{The tennis court stands on the girl}
		\end{itemize}
		\item Good captions are such good sentences.
		
	\end{itemize}
	\countertitle{But in-practice, it probably is}
\end{frame}


\begin{frame}{Some might say the problems we are assessing on are not sufficiently difficult}
	\note{But that is a bit of a meaningly statement}
	The problems are exactly as difficult as they are. \note{Tautology}
	\begin{itemize}
		\item real world problems on real data.
	\end{itemize}
	
\end{frame}


\end{document}