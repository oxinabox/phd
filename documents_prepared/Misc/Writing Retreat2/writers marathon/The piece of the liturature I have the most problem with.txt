I don't know what the piece of the liturature I have the most problem with is?

Is it the one that is too hard, and incomprehasible?
I don't find too many papers in comprehesible,
but rather the paper are comprehesible but what they descibe 
so consisely and understandable can have 
varst amount more word required in translating to executable code.
They are beguilingly comprehesible.

Is it something that is wrong?
But so little research in my area is wrong,
becuase it is mostly aparent.


Is it something that is unpublished?
The odd work that looks so close to mine,
but on deeperlook is scaresly realed.
That  has never been referred to by any article,
and doesn't seem to have been published or released at all?
I don't think I have a problem with it,
its just badly done.


I guess the worst is probably Le and Mikolov's paper.
It has come out that the parameters that were reported in the paper,
were certainly not the parameters used in the evaluation/results reported in the paper.
And that is just poor form.
Suggestions have come that the true parameters used,
and now available, burried deep on a messagebord/email chain,
had months of trialling and adjustements, to get the results they did.
And that would mean that there was information leaked from the testset into the hyperparameters.
Which is "cheating"  or at least allowing the algorithm to cheat.
That would mean the results reported would not generalise.
Further it may indicate their algorithm is so hyper-parameter sensitive,
that it may generally perform poorly.

My experience is that it performs alright,
but older simpler methods work better.

But having a big name auther like Mikolov, 
means that it get cite, and used as the basis for hundreds of future work.
I think it already has thousands.
And those works are potentially not performing as well as they could be,
if they were using another underling technology.

It is the fear that they have screwed up;
compounded by the knowedge they did.

There are further alegations that they 
trained on the test set.
A mistake I've made, but caught before it got too far.
Training on the test set would be bad, and should result in a retraction.

I believe the paper is only on Arvix, so it may have no peer review at all.
THart is a problem I have with industry researchers in general:
They have no pressure to publish in traditional venues,
which means they may not get any peer review.

Of course the general connection to this is that things only published on arvix 
would get cited less etc.
But this does not hold when you are dealling with authors,
of such repute that they almost established the field.

The notion I've heard, and the impression I get from the aformentioned email/messageboard thread,
is the Mikolov -- the very well known author, was meerly supervising the work of Le, who I think was an intern at the time.
And while he provided the general idea, and some of the other works.
The experimentations and hyperparameter setting was does without his direct oversite.
Which may be a problem, when it is Mikolov's fame that attracts the attention.
(though the general ideas in the paper are by no means bad).
Mikolov does say that he thought there was a simpler way to on the same idea line
to accomplish the same task.

It is a interesting idea line.

So I guess that paper bothers me the most.


But other papers bother me.
The paper on hierachiucal softmax whuich barely describes it, at all.
