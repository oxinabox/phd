\documentclass{book}
\input{preamble}


\begin{document}
\bookmarksetup{startatroot}
\addtocontents{toc}{\protect\vspace{12pt}}
\chapter{Conclusion}
Current research in natural language understanding relies on creating computer manipulatable representations of natural language for purposes of making inferences about meaning.
This thesis has focused on one particular type of representation:
linear combinations of embeddings.
This is a very simple representation, closely related to a bag of words.
There is a machine learning adage: that given enough data and a model with sufficiently high representational capacity any problem can be solved.
However, we seem to have found a sweet spot, 
where a model seemingly without sufficiently high representational capacity,
never-the-less performs excellently on tasks with the amount of data that we have.
It seems clear that there will always exist low-medium resource settings where linear combinations of embeddings will remain an ideal method for many practical problems.


The research presented here on linear combinations of embeddings has shown that this simple input representation technique is surprisingly powerful.
This  power is related to the fact that surface level information plays a significant role in practically giving human understandable meaning to a natural language utterance.
Word content is the most obvious surface level information, and is effectively captured by a LCOWE.
The LCOWE represented this in a dense, but informative vector.
While the LCOWE loses word order information, it preserves the aggregated content very well,
making it very useful for the tasks considered in this research.


We considered a number of tasks to identify the utility of this representation.
\Cref{SentVecMeaning} investigated classifying paraphrases as a means to investigate the quality of SOWE as a sentence embedding method.
\Cref{ColorEst} defined models for color estimation from short phrases.
\Cref{RefittingSenses} considered if we could use weighted combinations of sense embeddings to better capture the sense used in a particular example.
\Cref{NovelPerspective} considered taking the mean of the embeddings adjacent to named entity tokens across a fictional text as a feature to characterize how the named entity token was being used.
We followed up these practical demonstrations of capacity,
with further investigations into what can be recovered from the SOWE in the important area of sentence representations.
\Cref{BOWgen} demonstrated a method that could partially recover bags of words from a given SOWE.
\Cref{SOWE2Sent} extended this work by attempting to order those bags of words into sentences.
This demonstrated that a surprising amount of information is still available in the summed embeddings; which helps to explain why they work so well.



Linear combinations of embedding are not perfect for representing all meaning,
as they do not encode any information about word order.
It is thus clear that there exists sentences and phrases that are ambiguous when represented this way.
However, we note that such sentences are rare: often there is only one likely ordering, particularly in any given text with a restricted domain.
Most sentences are relatively short; multiple similarly likely word ordering occur more often in longer sentences.
Many reorderings are paraphrases, or near paraphrases, particularly when done at the clause level.
Though some orderings, such as noun swaps of nouns with similar ontological classification (e.g. Agents, Objects) do exist at almost all lengths:
many are paraphrases \natlang{The banana is next to the orange} \emph{vs.} \natlang{The orange is next to the banana};
and others are similar in meaning: \natlang{The banana is to the left of the orange} \emph{vs.} \natlang{The orange is to the left of the banana}.
It is desirable that such sentences are nearby in a representational of the semantic space.


\section{Future work}

\subsection{Adversarial Test cases}
A limitation of the LCOWE representations is that they have no ability to represent word order.
This is in-contrast to RNNs and other commonly used neural network based representations of multi-word natural language input.
It is possibly to construct adversarial test cases, that no LCOWE can succeed on.
This can be done by selecting sentences with multiple reasonable word orders with very different meanings.
It is worth consideration,
that such adversarial test cases allow advancement of the state of the art to increase the capacity of models to represent all possible inputs.
However, they do not necessarily advance the practical state of the art in representing real inputs that occur in a particular domain. 
Thus it is  is essential to understand how common such adversarial test cases are in practice.

Future work in this area requires not just the construction of adversarial examples; but of the determination of how common they are in practice.
Adversarial examples are not ubiquitous in real world tasks.
It is important not to succeed on only these cases, while failing on the more common simple cases.

It is also important to consider how challenging an adversarial test case is.
In \Cref{ColorEst}, the ordered task which was to make predictions for colors for which the different words in the name could appear in different orders to describe different colors.
For example \natlang{bluish green} and \natlang{greenish blue} are different colors.
However, they are very \emph{similar} colors.
As such the error from discarding word order, is less than the error from using a more complicated model such as an RNN.
Such a more complex model is harder to train, and those practical difficulties can dominate over a small amount of theoretical lack of capacity.



\subsection{Language Models and Orderless Representations}

There is a complementary aspect to LCOWE and language models.
While LCOWE have no capacity to handle word order, but they have an excellent ability to capture word content;
whereas pure language models have no ability to capture word content, but have an excellent ability to capture word order.
Language modelling based models incorporating a representation stage, such as encoder-decoders \citep{cho-EtAl:2014:EMNLP2014}, do not capture word content as well as LCOWE \citep{ac2018probingsentencevectors}.
They do, however, have state of the art order representation.
An interesting combination of the two,
would be an encoder-decoder model,
where the intermediate state between the encoder and the decoder,
is augmented by concatenating the final RNN output, with a sum of word embeddings, for all the input words to the encoder.
This corresponds to a bypass of the RNN units.
A similar bypass of intermediate layers has been used in feed-forward networks including the notable neural probabilistic language model \citep{NPLM}.



%
%Future work in this area would be to enhance an encoder-decode model by concatenating to the shared state a SOWE.
%\todo{REDO this figure with the Embedding layers shown and the SOWE shared representation laye radded}
%
%
%
%\begin{figure}
%\begin{tikzpicture}
%	\numdef{\N}{8}
%	\numdef{\labelwidth}{5.5cm}
%
%	\node(lbl)[text width= \labelwidth] {\textbf{SOWE Enhanced\\RNN Encoder-Decoder}\\%
%		Variable $n$ inputs: $\nv x_t$\\%
%		Variable $m$ outputs $\nn \hat{y}_t$\\%
%		Prompts: $\nv r_t$ (often $y_{t-1}$)
%	};
%	
%	\coordinate[yshift = -3.5cm] (L0) at (lbl.west);
%	\numdef{\NN}{4}
%	\foreach \I[count=\j from 0] in {1,...,\NN}{
%		\ifnumequal{\I}{\NN - 1}{%
%			\node(L\I)[dashed, layer, right = of L\j] {\ldots};
%			\node(w\I)[below = of L\I]{\ldots};
%		}%
%		{
%			\node(L\I)[layer, right = of L\j] {$\mathrm{RU_E}$};
%			\node(w\I)[below = of L\I]{\ifnumequal{\I}{\NN}{$\nv x_n$}{$\nv x_\I$}};
%			\draw[->] (w\I) -- (L\I);
%		}
%	}
%	\foreach \I[count=\j from 1] in {2,...,\NN} {
%		\draw[->] (L\j) edge node[labe] {state} (L\I);
%	}
%	
%	
%	
%	
%	\coordinate[above = 3 of L\NN] (Lp\NN);
%	\numdef{\NP}{\N - 1}
%	\foreach \j in {\NN,...,\NP}{
%		\numdef{\I}{\j+1}
%		\numdef{\y}{\I - \NN}
%		\ifnumequal{\I}{\N-1}{%
%			\node(Lp\I)[dotted, layer, right = of Lp\j] {\ldots};
%			\node(w\I)[below = of Lp\I]{\ldots};
%			\node(y\I)[above = of Lp\I]{\ldots};
%			\draw[->,dotted] (w\I) -- (Lp\I);
%			\draw[->,dotted] (Lp\I) -- (y\I);
%			\path[->,dotted] (L\NN.north) edge node[labe]{$\vv z$} (w\I.south west);
%		}%
%		{
%			\node(Lp\I)[layer, right = of Lp\j] {$\mathrm{RU_D}$};
%			\ifnumequal{\I}{\N}{
%				\node(w\I)[below = of Lp\I]{$[\vv z; \nv r_m]$};
%				\node(y\I)[above = of Lp\I]{$\nn \hat{y}_m$};
%			}
%			{
%				\node(w\I)[below = of Lp\I]{$[\vv z; \nv r_\y]$};
%				\node(y\I)[above = of Lp\I]{$\nn \hat{y}_\y$};
%			}
%			
%			\draw[->] (w\I) -- (Lp\I);
%			\draw[->] (Lp\I) -- (y\I);
%			\path[->] (L\NN.north) edge node[labe]{$\vv z$} (w\I.south west);
%		}
%	}
%	
%	
%	\numdef{\NNp1}{\NN + 1}
%	\foreach \I in {\NNp1,...,\NP} {
%		\numdef{\j}{\I+1}
%		\draw[->] (Lp\I) edge node[labe] {state} (Lp\j);
%	}
%\end{tikzpicture}
%
%\end{figure}




\end{document}