\documentclass{book}
\input{preamble}


\begin{document}
\part{Conclusion}
\chapter{Conclusion}
Current research in natural language understanding relies on creating computer manipulatable representations of natural language for purposes of making inferences about meaning.


While, the normal machine learning adage that given enough data and a model with sufficiently high representational capacity any problem can be solved always applies,
we seem to have found a sweet spot, 
where a model seemingly without sufficiently high representational capacity,
never-the-less performs excellently on tasks with the amount of data that we have.


The research presented here on linear combinations of embeddings has shown that this simple input representation technique is surprisingly powerful.
This surprising power come from the value of surface level information in many practical natural language understanding tasks.
The word content being the most obvious of these.
This word content is presented in a dense and informative form in a LCOWE; in a way that captures and preserves lexical similarity information.
While the LCOWE loses word order information, it very well preserves the word content.
That content proved more useful for the tasks considered in this research.

We considered a number of tasks to identify the utility of this representation in for different uses.
\Cref{SentVecMeaning} investigated classifying paraphrases as means to investigate quality of SOWE as a sentence embedding method.
\Cref{ColorEst} defined models for color estimation from short phrases.
\Cref{RefittingSenses} considered if we could use weighted combinations of sense embeddings to better capture the sense used in a particular example.
\Cref{NovelPerspective} considered taking the mean of the embeddings adjacent to named entity tokens across a fictional text as a feature to characterize how the named entity token was being used.
We followed up these practical demonstrations of capacity,
with further investigations into what can be recovered from the SOWE for the important category of sentence representations.
\Cref{BOWgen} demonstrated a method that could partially recover bags of words.
\Cref{SOWE2Sent} extended this to attempt to order those bags of words into sentences.
This demonstrated that a surprising amount of information is still available in the summed embeddings; which helps to explain why they work so well.



\section{Future work}

\subsection{Adversarial Test cases}
It is worth consideration,
that adversarial test cases allow advancement of the state of the art to increase the capacity of models to represent all possibly inputs.
However, understanding how common they are are is essential.

Future work in this area requires not just the construction of adversarial examples; but of the determination of how common they are in practice.
Adversarial examples are not ubiquitous in real world tasks.
It is important not to succeed on only these cases, while failing on the more common simple cases.

It is also important to consider how adversarial such a challenging case is.
In \Cref{ColorEst}, the ordered task which was to make predictions for colors for which the different words in the name could appear in different orders to describe different colors.
For example \natlang{bluish green} and \natlang{greenish blue} are different colors.
However, they are very \emph{similar} colors.
As such the error from discarding word order, is less than the error from using a more complicated model such as an RNN.
Such a more complex model is harder to train, and those practical difficulties can dominate over a small amount of theoretical lack of capacity.


\section{Language Models and Orderless Representations}

There is a complementary aspect to LCOWE and language models.
While LCOWE have no capacity to handle word order but excellent ability to capture word content;
Pure language models have no ability to capture word content, but excellent ability to capture word order.
Language modelling based tasks incorporating a representation stage, such as encoder-decoders \citep{cho-EtAl:2014:EMNLP2014}, do not capture word content as well as LCOWE \citep{ac2018probingsentencevectors}.
They do, however, have state of the art order order representation.
\todo{I would really like to talk about how word embeddings are useful for caption evaluation here.}
%
%Future work in this area would be to enhance an encoder-decode model by concatenating to the shared state a SOWE.
%\todo{REDO this figure with the Embedding layers shown and the SOWE shared representation laye radded}
%
%
%
%\begin{figure}
%\begin{tikzpicture}
%	\numdef{\N}{8}
%	\numdef{\labelwidth}{5.5cm}
%
%	\node(lbl)[text width= \labelwidth] {\textbf{SOWE Enhanced\\RNN Encoder-Decoder}\\%
%		Variable $n$ inputs: $\nv x_t$\\%
%		Variable $m$ outputs $\nn \hat{y}_t$\\%
%		Prompts: $\nv r_t$ (often $y_{t-1}$)
%	};
%	
%	\coordinate[yshift = -3.5cm] (L0) at (lbl.west);
%	\numdef{\NN}{4}
%	\foreach \I[count=\j from 0] in {1,...,\NN}{
%		\ifnumequal{\I}{\NN - 1}{%
%			\node(L\I)[dashed, layer, right = of L\j] {\ldots};
%			\node(w\I)[below = of L\I]{\ldots};
%		}%
%		{
%			\node(L\I)[layer, right = of L\j] {$\mathrm{RU_E}$};
%			\node(w\I)[below = of L\I]{\ifnumequal{\I}{\NN}{$\nv x_n$}{$\nv x_\I$}};
%			\draw[->] (w\I) -- (L\I);
%		}
%	}
%	\foreach \I[count=\j from 1] in {2,...,\NN} {
%		\draw[->] (L\j) edge node[labe] {state} (L\I);
%	}
%	
%	
%	
%	
%	\coordinate[above = 3 of L\NN] (Lp\NN);
%	\numdef{\NP}{\N - 1}
%	\foreach \j in {\NN,...,\NP}{
%		\numdef{\I}{\j+1}
%		\numdef{\y}{\I - \NN}
%		\ifnumequal{\I}{\N-1}{%
%			\node(Lp\I)[dotted, layer, right = of Lp\j] {\ldots};
%			\node(w\I)[below = of Lp\I]{\ldots};
%			\node(y\I)[above = of Lp\I]{\ldots};
%			\draw[->,dotted] (w\I) -- (Lp\I);
%			\draw[->,dotted] (Lp\I) -- (y\I);
%			\path[->,dotted] (L\NN.north) edge node[labe]{$\vv z$} (w\I.south west);
%		}%
%		{
%			\node(Lp\I)[layer, right = of Lp\j] {$\mathrm{RU_D}$};
%			\ifnumequal{\I}{\N}{
%				\node(w\I)[below = of Lp\I]{$[\vv z; \nv r_m]$};
%				\node(y\I)[above = of Lp\I]{$\nn \hat{y}_m$};
%			}
%			{
%				\node(w\I)[below = of Lp\I]{$[\vv z; \nv r_\y]$};
%				\node(y\I)[above = of Lp\I]{$\nn \hat{y}_\y$};
%			}
%			
%			\draw[->] (w\I) -- (Lp\I);
%			\draw[->] (Lp\I) -- (y\I);
%			\path[->] (L\NN.north) edge node[labe]{$\vv z$} (w\I.south west);
%		}
%	}
%	
%	
%	\numdef{\NNp1}{\NN + 1}
%	\foreach \I in {\NNp1,...,\NP} {
%		\numdef{\j}{\I+1}
%		\draw[->] (Lp\I) edge node[labe] {state} (Lp\j);
%	}
%\end{tikzpicture}
%
%\end{figure}



\section{Some reflections upon semantic spaces}
\begin{figure}
	\centering
	\begin{tikzpicture}[
			every node/.style = {text width=60mm,  align=center},
			pointdot/.style = {fill=purple, circle, minimum size=4pt,text width=0mm, inner sep=0pt}			
			]
		\node[draw](idea){Idea Space};
		% Objects: ideas, thoughts
		\node[draw, below = of idea](nl){Natural Language Space};
		% Objects: utterances, sentences, words
		\node[draw, below = of nl](emb){Representation Space};
		% Objects: Embeddings
		
		\draw[purple] (idea.200) to (nl.north);
		\draw[purple] (idea.-20) to (nl.north);
		\node[pointdot] at (nl.north) {};
		
		\draw[] (nl) to (emb);
		\draw[dashed,purple] (emb.20) to (nl.south);
		\draw[dashed,purple] (emb.-200) to (nl.south);
		\node[pointdot] at (nl.south) {};
		
		\node[pointdot,black] at (emb.north) {};
		
	\end{tikzpicture}
	\caption{\label{fig:spaces} The representation space is a computationally manipulate representation of the idea space.
	The natural language utterances come from points in the idea space; though due to ambiguity we can only truly hope to estimate distributions when the interpret them. A single point embedding as an approximation to a distribution with a single tight peak.}
\end{figure}

We can consider that there is a true semantic space of ideas.
When speaking, this space is projected down to natural languages space.
To again quote Webber: ``A sentence is a group of words expressing a complete thought.'', it is not a complete thought, only the \emph{expression} of one.
This projection from idea to utterance is imperfect -- it is lossy.
Many ideas are expressed the same way, and language thus has a lot of ambiguity.
When we try to understand the meaning of a natural language utterance we are trying to find the point in idea space that the speaker intend.
Some times the natural language space alone, is enough to recover a good idea of the the point in idea space the speaker intends,
but other times it is not.

The preimage of a point in natural language space (e.g. a sentence),
is a probability distribution over idea space that could have lead to that utterance -- $P(meaning \mid utterance)$.
This distribution could be combined with other factors (in a Bayesian way); either from that natural language context, or the enviroment more broadly.
For example, to use a meaning that centres around word sense:
we can identify two (of the many) senses of the word \natlang{apples}:
one in reference to the fruit, the other in reference to the computers made by the eponymous company.
Thus, on its own the sentence \natlang{Apples are good.}
suggests a distribution with at least two peaks in idea space.
Combine that utterance, with the context of being in a computer store, rather than a grocer, and the probability of one peak can be increased, though  the other not entirely removed.
Further around each peak remains adjacent closely related possible meanings.
For example the statement could be in relation to only computers, or also to other products.
idea space is a continuous space, with every utterance corresponding to a unique point. It is an uncountably large space.
In contrast natural language space is countably large, being composed of finite length combinations of of symbols taken from a finite alphabet.
An uncountable number of points in idea space are projected down to a single point in natural language space.


When designing a embedding method, for sentences, words or other structures,
we seek to define a representation space
that has good 
In particular it should have a continuous mapping from to and from embedding space.
A neighbourhood in representation space, should correspond to a neighbourhood in idea space.
\Cref{SentVecMeaning} investigated this for sentence embeddings.
By taking points in natural language space known to come from very near by points in idea space, that is to say paraphrases,
and checking that they belong to near points in embedding space.


As each point in natural language space is defines a distribution over idea space of what may be meant;
and representation space is attempting to be in correspondence to idea space;
it is such that each point in natural language space should project to a distribution over embedding space.
Instead, most methods project to natural language points to single points in embeddings space.
This is viable when the region in idea space that the natural language point could have come from is small -- in particular that the distribution in idea space is narrow variance and mono-modal.
In that case the single point estimate in embedding space is an useful approximation.


This has particularly clear utility for word sense embeddings,
which are defined by multimodal distributions,
with large peaks for each homonym, and smaller nearby peaks for each polyseme.
Furthermore we can't rule out the speaker using the word incorrectly or metaphorically which gives rise to nonzero values elsewhere in idea space.
Word-sense embeddings produce multiple sense embeddings -- ideally one corresponding to each peak in idea space.
We know these peaks are only rough approximations to the true point in idea space for a given usage of a word.
\Cref{RefittingSenses} attempts find other points in the embeddings space, that better corresponds to the true point in idea space for the particular use.


The color understanding task considered in \Cref{ColorDist} is interesting.
While it is a typical natural language understanding system,
which takes a point in natural language space (a color name),
moves through a representation space (the output of one of the input modules: SOWE, CNN, or RNN) using supervision to output something from meaning space.
In this case the meaning space is explicit, it is the HSV color space.
Using point estimation it outputs a point in meaning space

Unsupervised methods, in particular word embeddings, but also more generally, are ungrounded.
They are based only on natural language space observations.
The goal is not to capture I meaning in this space,
but rather to create a space that is a good input to a supervised system that can learn a good correspondence from natural language space to idea space. 
It is for this reason that principles such as Firth's distributional hypothesis, that words occurring in similar contexts have similar meaning, 
are so useful as a basis.
While it does not allow the encoding of meaning, it allows thus encoding of similarity of meaning.
This is ideally suited for creating a space that will make a good source representation for a supervised method applied for natural language understanding task.
Were that task accomplished with a neural network, the later hidden layers, or the fine-turned embeddings would form a grounded representation of idea space.


While the research presented in this dissertation has made use of the idea that we are working with a sample from a distribution over a proxy for meaning space,
it is our belief that further advancements would benefit from fully considering
word embeddings and other objects from representation spaces, not as discrete points but as distributions.




\printbib


\end{document}