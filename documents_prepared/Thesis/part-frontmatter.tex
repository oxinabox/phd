\documentclass{book}

\input{preamble.tex}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
\centering

{
	\Huge \sffamily 
	On the surprising capacity\\
	of linear combinations of embeddings\\
	for natural language processing\\
}
\vspace{1cm}
{\large Lyndon White\\
	{\normalsize BCM in Computation and Pure Mathematics; \\
	BE in Electrical and Electronic Engineering\\
	}
}
{\large\today\\}
\vspace{2cm}
\includegraphics[width=.50\linewidth]{figs/uwa}
\vfill
{
		\large
	This thesis is presented for the degree of\\
	Doctor of Philosophy\\
	of The University of Western Australia\\
}
\vspace{3cm}
\end{titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% See the structure of https://api.research-repository.uwa.edu.au/portalfiles/portal/9709033/THESIS_DOCTOR_OF_PHILOSOPHY_KHAN_Salman_Hameed_2016_Part_1.PDF


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Thesis Declaration}
\vskip 0.8cm
\begin{flushleft}
	I, Lyndon White, certify that:
	\vskip 0.5cm 
	This thesis has been substantially accomplished during enrolment in the degree.
	\vskip 0.25cm
	This thesis does not contain material which has been accepted for the award of any other degree or diploma in my name, in any university or other tertiary institution.
	\vskip 0.25cm
	No part of this work will, in the future, be used in a submission in my name, for any other degree or diploma in any university or other tertiary institution without the prior approval of The University of Western Australia and where applicable, any partner institution responsible for the joint-award of this degree.
	\vskip 0.25cm
	This thesis does not contain any material previously published or written by another person, except where due reference has been made in the text. 
	\vskip 0.25cm
	The work(s) are not in any way a violation or infringement of any copyright, trademark, patent, or other rights whatsoever of any person.
	\vskip 0.25cm
	The work described in this thesis was partially funded by 
	Australian Research Council grants DP150102405 and LP110100050;
	and by a Australian Government Research Training Program (RTP) Scholarship.
	\vskip 0.25cm
	This thesis contains published work and/or work prepared for publication, some of which has been co-authored. 
	\vskip 0.5cm
	Signature: \todo{[Sign here]}
	\vskip 0.25cm
	Date: \today
\end{flushleft}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{% Dedication
	\clearpage
	\thispagestyle{empty}
	\vspace*{\stretch{1}}
	\centering
	{
		In memoriam of\\
		Laurie White\\
		\emph{1927--2018}\\
	}
	\vspace{\stretch{3}}
	\clearpage
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Abstract}
As Webber's classic 1929 text \textit{English Composition and Literature} states:
``A sentence is a group of words expressing a complete thought.''
People use natural language is used to represent thoughts.
Thus the representation of natural language, in turn, is of fundamental importance in the field of artificial intelligence.
Natural language understanding is an area which fundamentally revolves around how to represent text in a form that an algorithm can manipulate in such a way as to mimic the ability of a human to truly understand the text's meaning.
In this dissertation, we aim to extend the practical reach of this area,
by exploring a commonly overlooked method for natural language representation: linear combinations (i.e. weighted sums) of embedded representations.
This dissertation is organised as a collection of research publications: with the novel contributions published as in conference proceedings or journals; and with the literature review having been published as part of a book.

When considering how to represent English input into a natural language processing system,
a common response is to consider modelling it as a sequential modelling problem: time-series of words.
A more complex alternative is to base the input model on the grammatical tree structures used by linguists.
But there are also simpler models: systems based on just summing the word embeddings.
On a variety of tasks, these work very well -- often better than the more complex models.
This dissertation examines these linear combinations of embeddings for natural language understanding tasks.

In brief, it is found that a sum of embeddings is a particularly effective dimensionality-reduced representation of a bag of words.
The dimensionality reduction is carried out at the word level via the implicit matrix factorization 
on the collocation probability matrix.
It thus captures into the dense word embeddings the key features of lexical semantics:
words that occur in similar contexts have similar meanings.
We find that summing these representations of words gives us a very useful representation of structures built upon words.

A limitation of the sum of embedding representation is that it is unable to represent word order.
This representation does not capture any order related information; unlike for example a recurrent neural network.
Recurrent neural networks, and other more complex models, are out performed by sums of embeddings in tasks where word order is not highly significant.
It is found that even in tasks were word order does matter to an extent, the improved training capacity of the simpler model still can mean that it performs better than more complex models.
This limitation thus hurts surprisingly little.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Acknowledgements}
Lorem Ipsum \todo{Write this}

\chapter*{Authorship declaration}

\DeclareDocumentCommand{\publication}{m m %
	O{Devised problem. Designed and implemented algorithms. Conducted experiments. Created figures. Wrote publication. Supervisors reviewed and provided useful feedback for improvement.} %
	}{
	\vskip 0.5cm
	\textbf{Details of the work:} \\
	\AtNextCite{\defcounter{maxnames}{99}} % Set for unlimitted names to be show.
	\fullcite{#2} \\
	
	\textbf{Location in thesis:} \Cref{#1}\\
	\textbf{Student contribution to work:} \\
	#3
	\\
	\textbf{Co-author signatures and dates:} 
	\vspace{1cm}
	\\
	
}


\begin{flushleft}
	This thesis contains work that has been published and/or prepared for publication.

	\publication{LitRev}{NRoNL}[Determined content. Created figures. Wrote book. Supervisors reviewed and provided useful feedback for improvement.]
	\publication{SentVecMeaning}{White2015SentVecMeaning}
	\publication{ColorEst}{White2018ColorEst}
	\publication{RefittingSenses}{WhiteRefittingSenses}
	\publication{NovelPerspective}{novelperspective}
	\publication{BOWgen}{White2015BOWgen}
	\publication{SOWE2Sent}{White2016a}
	\publication{DataDeps}{DataDeps}[Primary author of software. Created figures. Wrote publication. Supervisors reviewed and provided useful feedback for improvement.]
	\publication{DataDepsGenerators}{DataDepsGenerators}[Original author of software. Provided direction, guidance, and code review for its enhancement. Wrote publication.]
	
	\vfill
	\vskip 0.5cm
	\todo{Dr Togneri to sign}
	\begin{tabular}{l}
		I, Roberto Togneri certify that the student statements \\ regarding their contribution to each of the works listed above are correct. \\
		Coordinating supervisor signature: {[insert signature]}\\
		Date: {[insert date]}
	\end{tabular}
	\vspace{3cm}
\end{flushleft}
%%%%%%%%%%%%%%%

\end{document}
