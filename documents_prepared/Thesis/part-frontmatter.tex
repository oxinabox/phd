\documentclass{book}

\input{preamble.tex}

\begin{document}
		
% See the structure of https://api.research-repository.uwa.edu.au/portalfiles/portal/9709033/THESIS_DOCTOR_OF_PHILOSOPHY_KHAN_Salman_Hameed_2016_Part_1.PDF
\chapter*{Abstract}
As Webber's classic 1929 text \textit{English Composition and Literature} states:
``A sentence is a group of words expressing a complete thought.''
Human's use natural language is used to represent thoughts.
Thus the representation of natural language, in turn, is of fundamental importance in the field of artificial intelligences.
Natural language understanding is an area which fundamentally revolves around how to represent text in a form that an algorithm can manipulate in such a way as to mimic the ability of a human to truly understand the text's meaning.
In this dissertation, we aim to extend the practical reach of this area,
by exploring a commonly overlooked method for natural language representation: linear combinations (i.e. weighted sums) of embedded representations.
This dissertation is organised as a collection of research publications: with the novel contributions published as in conference proceedings or journals; and with the literature review having been published as part of a book.

When considering how to represent English input into a natural language processing system,
a common response is to consider modelling it as a sequential modelling problem: time-series of words.
A more complex alternative is to base the input model the grammatical tree structures used by linguists.




Word Embeddings have, in a big way, changed how we do natural language processing.
We use them (implicitly or explicitly) as a component in a variety of powerful systems:
from LSTMs to complicated tree structured models.
These models are very exciting, and work well to push the envelope of what can be done.
But there are also simpler models: systems based on just summing the word embeddings.
On a variety of tasks, these work very well -- often better than the more complex models.

This dissertation examines linear combinations of embeddings for natural language understanding tasks.
In particular simple sums, but also some weightings such as means.
The dissertation contains published works demonstrating the utility of the linear combinations of embeddings
for representing sentences, short phrases, word senses and usage contexts.
It also contains works investigating the extent that the original input words and sentences can be recovered from the summed embedding representation.

In brief, it is found that a sum of embeddings is a particularly effective dimensionality-reduced representation of a bag of words.
The dimensionality reduction is carried out at the word level via the implicit matrix factorization 
on the collocation probability matrix.
It thus captures into the dense word embeddings the key features of lexical semantics:
words that occur in similar contexts have similar meanings.
We find that summing these representations of words gives us a very useful representation of structures built upon words.

A limitation of the sum of embedding representation is that it is unable to represent word order.
This representation does not capture any order related information; unlike for example a recurrent neural network.
Recurrent neural networks, and other more complex models, are out performed by sums of embeddings in tasks where word order is not highly significant.
It is found that even in tasks were word order does matter to an extent, the improved training capacity of the simpler model still can mean that it performs better than more complex models.
This limitation thus hurts surprisingly little.

{% Dedication
\clearpage
\thispagestyle{empty}
\vspace*{\stretch{1}}
\centering
	{
	In memoriam of\\
	Laurie White\\
	\emph{1927--2018}\\
	}
\vspace{\stretch{3}}
\clearpage
}

\end{document}
