\documentclass{book}
\input{preamble.tex}


\begin{document}


\chapter{TensorFlow.jl: An Idiomatic Julia Front End for TensorFlow}
\preamble{This paper is currently under review for the Journal of Open Source Software.}

\section{Summary}

TensorFlow.jl is a Julia \citep{Julia} client library for the
TensorFlow deep-learning framework
\citep{tensorflow2015-whitepaper,tensorflow2016}. It allows users to define
TensorFlow graphs using Julia syntax, which are interchangable with the
graphs produced by Google's first-party Python TensorFlow client and can
be used to perform training or inference on machine-learning models.

Graphs are primarily defined by overloading native Julia functions to
operate on a TensorFlow.jl \texttt{Tensor} type, which represents a node
in a TensorFlow computational graph. This overloading is powered by
Julia's powerful multiple-dispatch system, which in turn allows allows
the vast majority of Julia's existing array-processing functionality to
work as well on the new \texttt{Tensor} type as they do on native Julia
arrays. User code is often unaware and thereby reusable with respect to
whtether its inputs are TensorFlow tensors or native Julia arrays by
utilizing \emph{duck-typing}.

TensorFlow.jl has an elegant, idiomatic Julia syntax. It allows all the
usual infix operators such as \texttt{+}, \texttt{-}, \texttt{*} etc. It
works seamlessly with Julia's broadcast syntax as well, such as the
\texttt{.*} operator. This \texttt{*} can correspond to matrix
multiplication while \texttt{.*} corresponds to element-wise
multiplication, while Python clients needs distinct \texttt{@} (or
\texttt{matmul}) and \texttt{*} (or \texttt{multiply}) functions. It
also allows Julia-style indexing (e.g.
\texttt{x{[}:,\ ii\ +\ end√∑2{]}}), and concatenation (e.g.
\texttt{{[}A\ B{]}}, \texttt{{[}x;\ y;\ 1{]}}). Its goal is to be
idiomatic for Julia users, while still preserving all the power and
maturity of the TensorFlow system. For example, it allows Julia code to
operate on TPUs by virtue of using the same TensorFlow graph syntax as
Python's TensorFlow client, even though there is no native Julia TPU
compiler.

\newpage

TensorFlow.jl to carefully balance between matching the Python
TensorFlow API and Julia conventions. In turn, the Python TensorFlow
client is itself designed to closely mirror numpy. Some examples are
shown in the table below.

{	\renewcommand{\arraystretch}{1.5}
\begin{tabularx}{\textwidth}{XXX}
	\toprule
	\textbf{Julia} & \textbf{Python TensorFlow} & \textbf{TensorFlow.jl}\\
	\midrule
	1-based indexing & 0-based indexing & 1-based indexing \\
	Column Major & Row Major & Row Major \\
	explicit broadcasting & implicit broadcasting & implicit or explicit broadcasting \\
	last index at \texttt{end} 2nd last in \texttt{end-1} & last index at \texttt{-1} second last in \texttt{-2} & last index at \texttt{end} 2nd last in \texttt{end-1} \\
	Operations in Julia ecosystem namespaces. (\texttt{SVD} in \texttt{LinearAlgebra}, \texttt{erfc} in \texttt{SpecialFunctions}, \texttt{cos} in \texttt{Base}) & All operations TensorFlow's namespaces (\texttt{SVD} in \texttt{tf.linalg}, \texttt{erfc} in \texttt{tf.math}, \texttt{cos} in \texttt{tf.math}, and all reexported from \texttt{tf}) & All hand imported Operations in the Julia ecosystems namespaces. (\texttt{SVD} in \texttt{LinearAlgebra}, \texttt{erfc} in \texttt{SpecialFunctions}, \texttt{cos} in \texttt{Base}) Ops that have no other place are in \texttt{TensorFlow}. Automatically generated ops are in \texttt{Ops} \\

	Container types are parametrized by number of dimensions and element type & N/A: does not have a parametric type system & Tensors are parametrized by element type, enabling easy specialization of algorithms for different types. \\
	\bottomrule
\end{tabularx}
}

Defining TensorFlow graphs in the Python TensorFlow client can be viewed
as metaprogramming, in the sense that a host language (Python) is being
used to generate code in a different embedded language (the TensorFlow
computational graph) \citep{MLandPL}. This often comes with some
awkwardness, as the syntax and the semantics of the embedded language by
definition do not match the host language or there would be no need for
two languages to begin with. Using TensorFlow.jl is similarly a form of
meta-programming for the same reason. However, the flexibility and
meta-programming facilities offered by Julia's macro system makes Julia
especially well-suited as a host language, as macros implemented in
TensorFlow.jl can syntactically transform idiomatic Julia code into
Julia code that constructs TensorFlow graphs. This permits users to
reuse their knowledge of Julia, while users of the Python TensorFlow
client essentially need to learn both Python and TensorFlow.

One example of our ability to leverage the increased expressiveness of
Julia is using \texttt{@tf} macro blocks implemented in TensorFlow.jl to
automatically name nodes in the TensorFlow computatioal graph. Nodes in
a TensorFlow graph have names; these correspond to variable names in a
traditional programming language. Thus every operation, variable and
placeholder takes a \texttt{name} parameter. In most TensorFlow
bindings, these must be specified manually resulting in a lot of code
that includes duplicate information such as
\texttt{x\ =\ tf.placeholder(tf.float32,\ name="x")} or they are
defaulted to an uninformative value such as \texttt{Placeholder\_1}. In
TensorFlow.jl, prefixing a lexical block (such as a \texttt{function} or
a \texttt{begin} block) with the \texttt{@tf} macro will cause the
\texttt{name} parameter on all operations occurring on the right-hand
side of an assignment to be filled in using the left-hand side. For
example, the TensorFlow.jl equivalent of the above example is
\texttt{@tf\ x\ =\ placeholder(Float32)}. Note how \texttt{x} is named
only once instead of twice, as is redundantly required in the Python
example. Since all nodes in the computational graph can automatically be
assigned the same name as the corresponding Julia variable with no
additional labor from TensorFlow.jl users, users get for free more
intuitive debugging and graph visualisation.


to

Another example of the use of Julia's metaprogramming is in the
automatic generation of Julia code for each operation defined by the
official TensorFlow C implementation (for example, convolutions of two
TensorFlow tensors). The C API can be queried to return definitions of
all operations as protobuffer descriptions, which includes the expected
TensorFlow type and arity of its inputs and outputs, as well as
documentation. This described the operations at a sufficient level to
generate the Julia code to bind to the functions in the C API and
automatically generate a useful docstring for the function,. One
challenge in this is that such generated code must correct the indices
to be 1-based instead of 0-based to accord with Julia convention.
Various heuristics are employed by TensorFlow.jl to guess which input
arguments represent indicies and so should be converted.

TensorFlow.jl ships by default with bindings for most operations, but
any operation can be dynamically imported at runtime using
\texttt{@tfimport\ OperationName}, which will generate the binding and
load it immediately. Additionally, for operations that correspond to
native Julia operations (for example, \texttt{sin}), we overload the
native Julia operation to call the proper binding.

We also use Julia's advanced parametric type system to enable elegant
implementations of array operations not easily possible in other client
libraries. TensorFlow.jl represents all nodes in the computational graph
as parametric \texttt{Tensor} types which are parametrised by their
element type, e.g. \texttt{Tensor\{Int\}}, \texttt{Tensor\{Float64\}} or
\texttt{Tensor\{Bool\}}. This allows Julia's dispatch system to be used
to simplify defining some bindings. For example, indexing a
\texttt{Tensor} with an \texttt{Int}-like Tensor will ultimately create
a node corresponding to a TensorFlow ``gather'' operation, and indexing
with a \texttt{Bool}-like Tensor will correspond to a ``boolean\_mask''
operation. It is also used to cast inputs in various functions to
compatible shapes.
\hypertarget{challenges}{%
	\subsection{Challenges}\label{challenges}}

A significant difficulty in implementing the TensorFlow.jl package for
Julia is that in the upstream TensorFlow version 0 and 1 distributions,
the C API is primarily designed for the execution of pretrained models
and does not include many conveients for the definition of training of
graphs.

The C API primarily exposes low-level array operations such as matrix
multiplication or reductions. Graidnet descent ptimizers, RNNs
functionality, and (until recently) shape-inference all required
reimplementation on the Julia side. Most challengingly, the symbolic
differentiation implemented in the \texttt{gradients} function is not
available from the C API for all operations. To work around this, we
currently use Julia's \href{https://github.com/JuliaPy/PyCall.jl}{Python
	interop} library to generate the gradient nodes using the Python client
for those operations not supported by the C API. This requires
serializing and deserializing TensorFlow graphs on both the Julia and
Python side.

This has been improving over time, both due to Google moving more
functionality from the Python TensorFlow client to the C API which can
reused by Julia, and with more reimplementations of other aspects of the
Python client from our own volunteer efforts. There nevertheless remains
a large number of components from the upstream \texttt{contrib}
submodule that remain unimplemented, including various efforts around
probabilistic programming.

\hypertarget{other-deep-learning-frameworks-in-julia}{%
\subsection{Other deep learning frameworks in
		Julia}\label{other-deep-learning-frameworks-in-julia}}

Julia also has bespoke neural network packages such as Mocha
\citep{mocha2014}, Knet \citep{knet2016} and Flux \citep{flux}, as
well as bindings to other frameworks such as MxNet \citep{mxnet2015}.
While not having the full-capacity to directly leverage some of the
benefits of the language and its ecosystem present in the pure julia
frameworks such as Flux, TensorFlow.jl provides an interface to one of
the most mature and widely deployed deep learning environments. It thus
trivially supports technologies such as TPUs and visualization libraries
like TensorBoard. It also gains the benefits from the any optimisations
made in the graph execution engine of the underlying TensorFlow C
library, which includes extensive support for automatically
distributing computations over multiple host machines which each
have multiple GPUs.

\subsection{Acknowledgements}

\begin{itemize}
	\item
	We gratefully acknowledge the 30 contributors to the TensorFlow.jl
	Github repository.
	\item
	We especially thank Katie Hyatt for contributing tests and
	documentation.
	\item
	We thank members of Julia Computing and the broader Julia Community
	for various discussions, especially Mike Innes and Keno Fischer.
\end{itemize}


\end{document}
