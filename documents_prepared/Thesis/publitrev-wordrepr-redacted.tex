%\documentclass[parskip]{komatufte}
%\include{preamble}
%
%
%
%
%\begin{document}
%{

\chapter{Word Representations}\label{sec:word-representations}
\preamble{This chapter originally appeared as Chapter 3 of the book ``Neural Representations of Natural Language'', published by by Springer.}

\aside[Word Vector or Word Embedding?]{Some literature uses the term \emph{word~vector}, or \emph{vector-space model} to refer to representations from LDA and LSA etc.
	Other works use the terms are used synonymously with \emph{word~embedding}.
	Word embeddings are vectors, in any case.
}

\dictum[J.R. Firth, 1957]
{
	You shall know a word by the company it keeps.%
	\aside{The epigraph at the beginning of this section is over-used. However, it is obligatory to include it in a work such as this, as it so perfectly sums up why representations useful for language modelling are representations that capture semantics (as well as syntax).}
}


\begin{abstract}
Word embeddings are the core innovation that has brought machine learning to the forefront of natural language processing.
This chapter discusses how one can create a numerical vector that captures the salient features (e.g. semantic meaning) of a word.
Discussion begins with the classic language modelling problem.
By solving this, using a neural network-based approach, word-embeddings are created.
Techniques such as CBOW and skip-gram models (\texttt{word2vec}), and more recent advances in relating this to common linear algebraic reductions on co-locations as discussed.
The chapter also includes a detailed discussion of the often confusing hierarchical softmax, and negative sampling techniques.
It concludes with a brief look at some other applications and related techniques.
\end{abstract}

\redactedbody{}
%\end{document}
