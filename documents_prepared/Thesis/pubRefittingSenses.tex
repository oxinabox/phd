%\documentclass[]{book}
%\input{preamble.tex}
%
%
%
%\begin{document}
{

%Hositontal scaling of tikz
\newlength\xunit
\xunit=1cm


%user 
\newcommand{\W}{\mathcal{W}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\s}{\mathbf{s}}
\renewcommand{\l}{\mathbf{l}}
\renewcommand{\u}{\mathbf{u}}
\newcommand{\ci}{\perp\!\!\!\perp} % from Wikipedia


\newcommand{\wordquote}[1]{\enquote{\texttt{#1}}}

	

\chapter{Finding Word Sense Embeddings Of Known Meaning}
\preamble{This paper was presented at the 19th Conference on Intelligent Text Processing and Computational Linguistics, in 2018.}


\begin{abstract}
Word sense embeddings are vector representations of polysemous words -- words with multiple meanings.
These induced sense embeddings, however, do not necessarily correspond to any dictionary senses of the word.
To overcome this, we propose a method to find new sense embeddings with known meaning.
We term this method refitting, as the new embedding is fitted to model the meaning of a target word in an example sentence.
The new lexically refitted embeddings are learnt using the probabilities of the existing induced sense embeddings, as well as their vector values.
Our contributions are threefold:
(1) The refitting method to find the new sense embeddings;
 (2) a novel smoothing technique, for use with the refitting method;
and (3) a new similarity measure for words in context, defined by using the refitted sense embeddings.
We show how our techniques improve the performance of the Adaptive Skip-Gram sense embeddings for word similarly evaluation; and how they allow the embeddings to be used for lexical word sense disambiguation.
\end{abstract}


\section{Introduction}

Popular word embedding vectors, such as Word2Vec, represent a word's semantic meaning and its syntactic role as a point in a vector space \parencite{mikolov2013efficient,pennington2014glove}.
As each word is only given one embedding, such methods are restricted to the representation of only a single combined sense, or meaning, of the word.
\emph{Word sense embeddings} generalise word embeddings to handle polysemous and homonymous  words.
Often these sense embeddings are learnt through unsupervised Word Sense Induction (WSI) \parencite{Reisinger2010,Huang2012,tian2014probabilistic,AdaGrams}.
The induced sense embeddings are unlikely to directly coincide with any set of human defined meaning at all, i.e. they will not match lexical senses such as those defined in a lexical dictionary, e.g. WordNet \parencite{miller1995wordnet}.
These induced senses may be more specific, more broad, or include the meanings of jargon not in common use.


One may argue that WSI systems can capture better word senses than human lexicographers do manually.
However, this does not mean that induced senses can replace standard lexical senses.
It is important to appreciate the vast wealth of existing knowledge defined around lexical senses.
Methods to link induced senses to lexical senses allow us to take advantage of both worlds.


We propose \emph{a refitting method} to generate a sense embedding vector that matches with a labelled lexical sense.
Given an example sentence with the labelled lexical sense of a particular word, the refitting method algorithmically combines the induced sense embeddings of the target word such that the likelihood of the example sentence is maximised.
We find that in doing so, the sense of the word in that sentence is captured.
With the refitting, the induced sense embeddings are now able to be used in more general situations where standard senses, or user defined senses are desired.

Refitting word sense vectors to match a lexicographical sense inventory, such as WordNet or a translator's dictionary, is possible if the sense inventory features at least one example of the target sense's use.
Our method allows this to be done very rapidly, and from only the single example of use this has with possible applications in low-resource languages.

%\microtypesetup{protrusion=false, tracking=false}
Refitting can also be used to fit to a user provided example, giving a specific sense vector for that use.
This has strong applications in information retrieval.
The user can provide an example of a use of the word they are interested in.
For example, searching for documents about 
\wordquote{banks} as in \enquote{the river banks were very muddy}.
By generating an embedding for that specific sense, and by comparing with the generated embeddings in the indexed documents, we can not only pick up on suitable uses of other-words for example \wordquote{beach} and \wordquote{shore},
but also exclude different usages, for example of a financial bank.
The method we propose, using our refitted embeddings, has lower time complexity than AvgSimC \parencite{Reisinger2010}, the current standard method for evaluating the similarity of words in context.
This is detailed in \Cref{RefittedSimVsAvgSimC}.
%\microtypesetup{protrusion=true, tracking=true}


We noted during refitting, that a single induced sense would often dominate the refitted representation.
It is rare in natural language for the meaning to be so unequivocal.
Generally, a significant overlap exists between the meaning of different lexical senses, and there is often a high level of disagreement when humans are asked to annotate a corpus \parencite{veronis1998study}.
We would expect that during refitting there would likewise be contention over the most likely induced sense.
Towards this end, we develop a smoothing method, which we call \emph{geometric smoothing} that de-emphasises the sharp decisions made by the (unsmoothed) refitting method.
We found that this significantly improves the results.
This suggests that the sharpness of sense decisions is an issue with the language model, which smoothing can correct.
The geometric smoothing method is presented in \Cref{smoothing}.


We demonstrate the refitting method on sense embedding vectors induced using Adaptive Skip-Grams (AdaGram) \parencite{AdaGrams}, as well as our own simple greedy word sense embeddings.
The method is applicable to any skip-gram-like language model that can take a sense vector as its input, and can output the probability of a word appearing in that sense's context.


The rest of the paper is organised as follows: \Cref{relatedwords} briefly discusses two areas of related works.
\Cref{Framework} presents our refitting method, as well as our proposed geometric smoothing method.
\Cref{Models} describes the WSI embedding models used in the evaluations.
\Cref{SimilarityInContext} defines the RefittedSim measure for word similarity in context, and presents its results.
\Cref{lexicalWSD} shows how the refitted sense vectors can be used for lexical WSD.
Finally, the paper concludes in \Cref{conclusionRS}.

\section{Related Works} \label{relatedwords}

\subsection{Directly Learning Lexical Sense Embeddings}
In this area of research, the induction of word sense embeddings is treated as a supervised, or semi-supervised task, that requires sense labelled corpora for training.

Iacobacci et al. \parencite{iacobacci2015sensembed} use a Continuous Bag of Word language model \parencite{mikolov2013efficient}, using word senses as the labels rather than words.
This is a direct application of word embedding techniques.
To overcome the lack of a large sense labelled corpus, Iacobacci et al. use a 3rd party WSD tool, BabelFly \parencite{Moro2014}, to add sense annotations to a previously unlabelled corpus. 

Chen et al. \parencite{Chen2014} use a supervised approach to train sense vectors, with an unsupervised WSD labelling step.
They partially disambiguate their training corpus, using  word sense vectors based on WordNet; and use these labels to train their embeddings.
This relabelled data is then used as training data, for finding sense embeddings using skip-grams.

Our refitting method learns a new sense embedding as a weighted sum of existing induced sense embeddings of the target word.
Refitting is a one-shot learning solution, as compared to the approaches used in the works discussed above.
A notable advantage is the time taken to add a new sense.
Adding a new sense is practically instantaneous, and replacing the entire sense inventory, of several hundred thousand senses, is only a matter of a few hours.
Whereas for the existing approaches this would require repeating the training process, which will often take several days.
Refitting is a process done to word sense embeddings, rather than a method for finding sense embeddings from a large corpus. 

\subsection{Mapping induced senses to lexical senses}\label{mapping}
By defining a stochastic map between the induced and lexical senses, Agirre et al. \parencite{agirre2006}, propose a general method for allowing WSI systems to be used for WSD.
Their work was used in SemEval-2007 Task 02 \parencite{SemEval2007WSIandWSD} to evaluate all entries. 
Agirre et al. use a mapping corpus to find the probability of a lexical sense, given the induced sense according to the WSI system.
This is more general than the approach we propose here, which only works for sense embedding based WSI.
By exploiting the particular properties of sense embedding based WSI systems we propose a system that can better facilitate the use of this subset of WSI systems for WSD.

\section{Proposed Refitting Framework} \label{refitting} \label{Framework}

The key contribution of this work is to provide a way to synthesise a word sense embedding given only a single example sentence and a set of pretrained sense embedding vectors. 
We termed this \emph{refitting} the sense vectors.
By refitting the unsupervised vectors we define a new vector, that lines up with the specific meaning of the word from the example sentence.

This can be looked at as a one-shot learning problem, analogous to regression.
The training of the induced sense, and of the language model, can be considered an unsupervised pre-training step.
The new word sense embedding should give a high value for the likelihood of the example sentence, according to the language model.
It should also generalise to give a high likelihood of other contexts where this word sense occurs.

We initially attempted to directly optimise the sense vector to predict the example.
We applied the L-BFGS \parencite{nocedal1980updating} optimisation algorithm with the sense vector being the parameter being optimised over, and the objective being to maximise the probability of the example sentence according to the language model.
This was found to generalise poorly, due to over-fitting, and to be very slow.
Rather than a direct approach, we instead take inspiration from the locally linear relationship between meaning and vector position that has been demonstrated for word embeddings \parencite{mikolov2013efficient,mikolovSkip,mikolov2013linguisticsubstructures}.

To refit the induced sense embeddings to a particular meaning of a word, we express that a new embedding as as a weighted combination of the induced sense vectors.
The weight is determined by the probability of each induced sense given the context.

Given a collection of induced (unlabelled) embeddings $\u={u_1,...,u_{n_u}}$, and an example sentence $\c={w_1,...,w_{n_c}}$ we define a function $l(\u \mid \c )$ which determines the refitted sense vector, from the unsupervised vectors and the context as:
\begin{equation} \label{eq:synth}
l(\u \mid \c ) = \sum_{\forall u_i \in \u} u_i P(u_i \mid \c)
\end{equation}
Bayes' Theorem can be used to estimate the posterior predictive distribution $P(u_i \mid \c)$.

Bengio et al. \parencite{NPLM} describe a similar method to \Cref{eq:synth} for finding  (single sense) word embeddings for words not found in their vocabulary.
The formula they give is as per \Cref{eq:synth}, but summing over the entire vocabulary of words (rather than just $\u$).


\subsection{A General WSD method} \label{generalwsd}
Using the language model and application of Bayes' theorem, we define a general word sense disambiguation method that can be used for refitting (\Cref{eq:synth}), and for lexical word sense disambiguation (see \Cref{lexicalWSD}).
This is a standard approach of using Bayes' theorem \parencite{tian2014probabilistic,AdaGrams}.
We present it here for completeness.

The context is used to determine which sense is the most suitable for this use of the \emph{target word} (the word being disambiguated).
Let $\s=(s_{1},...,s_{n})$, be the collection of senses for the target word\footnote{As this part of our method is used with both the unsupervised senses and the lexical senses, referred to as $\u$ and $\l$ respectively in other parts of the paper, here we use a general sense $\s$ to avoid confusion.}.

Let $\c=(w_{1},...,w_{n_c})$ be a sequence of words making up the context of the target word.
For example for the target word \emph{kid}, the context could be $\c=($ \emph{ wow the wool from the, is, so, soft, and, fluffy}$)$, where \emph{kid} is the central word taken from between \emph{the} and \emph{fluffy}.

For any particular sense, $s_i$, the multiple sense skip-gram language model can be used to find the probability of a word $w_j$ occurring in the context: $P(w_j \mid s_i)$.
By assuming the conditional independence of each word $w_j$ in the context, given the sense embedding $s_i$, the probability of the context can be calculated:
\begin{equation} \label{eq:contextprobtrue}
P(\c \mid s_{i})=\prod_{\forall w_{j}\in\c}P(w_{j} \mid s_{i})
\end{equation}
The correctness of the conditional independence assumption depends on the quality of the representation -- the ideal sense representation would fully capture all information about the contexts it can appear in -- thus  the other contexts elements would not present any additional information, and so  $P(w_a \mid w_b,s_i)=P(w_a \mid s_i)$.
Given this, we have an estimate of $P(\c \mid s_{i})$ which can be used to find $P(s_i \mid \c)$.
However, a false assumption of independence contributes towards overly sharp estimates of the posterior distribution \cite{rosenfeld2000two}, which we seek to address in \Cref{smoothing} with geometric smoothing.


Bayes' Theorem is applied to this context likelihood function  $P(\c \mid s_{i})$ and a prior for the sense $P(s_i)$ to allow the posterior probability to be found:
\begin{equation} \label{eq:generalwsd}
P(s_{i} \mid \c) =
\dfrac{P(\c \mid s_{i})P(s_{i})}
{\sum_{s_{j}\in\s} P(\c \mid s_{j})P(s_{j})}
\end{equation}
This is the probability of the sense given the context.


\subsection{Geometric Smoothing for General WSD} \label{smoothing}
During refitting, we note that often one induced sense would be calculated as having much higher probability of occurring than the others (according to \Cref{eq:generalwsd}).
This level of certainty is not expected to occur in natural languages, ambiguity is almost always possible. 
To resolve such dominance problems, we propose a new \emph{geometric smoothing} method.
This is suitable for smoothing posterior probability estimates derived from products of conditionally independent likelihoods.
It smooths the resulting distribution, by shifting all probabilities to be closer to the uniform distribution.

We hypothesize that the sharpness of probability estimates from \Cref{eq:generalwsd} is a result of data sparsity, and of a false independence assumption in \Cref{eq:contextprobtrue}.
This is well known to occur for n-gram language models \cite{rosenfeld2000two}.
Word-embeddings language models largely overcome the data sparsity problem due to weight sharing effects \parencite{NPLM}.
We suggest that the problem remains for word sense embeddings, where there are many more classes.
Thus the training data must be split further between each sense than it was when split for each word. 
The power law distribution of word use \parencite{zipf1949human} is compounded by word senses within those used also following the a power law distribution \parencite{Kilgarriff2004}.
Rare senses are liable to over-fit to the few contexts they do occur in, and so give disproportionately high likelihoods to contexts that those are similar to.
We propose to handle these issues through additional smoothing.

We consider replacing the unnormalised posterior with its $n_c$-th root, where $n_c$ is the length of the context.
We replace the likelihood of \Cref{eq:contextprobtrue} with 
\(P_S(\c \mid s_{i})=\prod_{\forall w_{j}\in\c}\sqrt[n_c]{P(w_{j} \mid s_{i})}\).
Similarly, we replace the prior with:
\(P_S(s_{i})= \sqrt[n_c]{P(w_{j} \mid s_{i})}\)
When this is substituted into \Cref{eq:generalwsd}, it becomes a smoothed version of $P(s_{i} \mid \c)$.
\begin{equation} \label{eq:generalwsdsmoothed}
P_S(s_{i}\mid\c) %
%&=\dfrac{P_{S}(\c\mid s_{i})P_S(s_{i})}
%{\sum_{s_{j}\in\s} P_{S}(\c \mid s_{j}) P_S(s_{j})} \\
%
%&
=\dfrac{\sqrt[n_c]{P(\c\mid s_{i})P(s_{i})}}
{\sum_{s_{j}\in\s} \sqrt[n_c]{P(\c \mid s_{j})P(s_{j})}} 
%
%&=\dfrac{\prod_{\forall w_{j}\in\c} \sqrt[|\c|]{P(w_{j}\mid s_{})P(s_{j})}}%
%{\sum_{s_{j}\in\s}\prod_{\forall w_{k}\in\c}\sqrt[|\c|]{P(w_{k}\mid s_{j})P(s_{j})}}
\end{equation}
The motivation for taking the $n_c$-th root comes from considering the case of the uniform prior.
In this case $P_S(\c \mid s_{i})$ is the geometric mean of the individual word probabilities $P_S(w_j \mid s_{i})$.
Consider, if one has two context sentences, $\c=\{w_1,...,w_{n_c}\}$ and $\c^\prime=\{w_1^\prime,...,w^\prime_{n_{c^\prime}}\}$, such that $n_c^\prime > n_c^\prime$
then using \Cref{eq:contextprobtrue} to calculate $P(\c \mid s_{i})$ and $P(\c^\prime \mid s_{i})$ will  result in incomparable results as additional number of probability terms will dominate -- often significantly more than the relative values of the probabilities themselves.
The number of words that can occur in the context of any given sense is very large -- a large portion of the vocabulary.
We would expect, averaging across all words, that each addition word in the context would decrease the probability by a factor of $\frac{1}{V}$, where  $V$ is the vocabulary size. 
The expected probabilities for \mbox{$P(\c \mid s_{i})$ is $\frac{1}{V^{n_c}}$} and for \mbox{$P(\c^\prime \mid s_{i})$ is $\frac{1}{V^{n_{c^\prime}}}$}.
As $n_{c^\prime} > n_c$, thus we expect $P(\c^\prime \mid s_{i}) \ll P(\c \mid s_{i})$.
Taking the $n_{c}$-th and $n_{c^\prime}$-th roots of $P(\c \mid s_{i})$ and $P(\c \mid s_{i})$ normalises these probabilities so that they have the same expected value; thus making a context-length independent comparison possible.
When this normalisation is applied to \Cref{eq:generalwsd}, we get the smoothing effect.


\section{Experimental Sense Embedding Models} 
\label{Models}
We trained two sense embedding models, AdaGram \parencite{AdaGrams} and our own Greedy Sense Embedding method. 
During training we use the Wikipedia dataset as used by Huang et al. \parencite{Huang2012}.
However, we do not perform the extensive preprocessing used in that work.

%\subsection{AdaGram}
Most of our evaluations are carried out on Adaptive SkipGrams (AdaGram) \parencite{AdaGrams}.
AdaGram is a non-parametric Bayesian extension of Skip-gram. It learns a number of different word senses, as are required to properly model the language.

We use the implementation\footnote{\url{https://github.com/sbos/AdaGram.jl}} provided by the authors with minor adjustments for Julia \parencite{Julia} v0.5 compatibility.

%\subsubsection{Model Parameters}

The AdaGram model was configured to have up to 30 senses per word, where each sense is represented by a 100 dimension vector. 
The sense threshold was set to $10^{-10}$ to encourage many senses.
Only words with at least 20 occurrences are kept, this gives a total vocabulary size of 497,537 words.


%\pdfcomment{
%	Dict\{String,Any\} with 18 entries:
%	"prototypes" => 30
%	"nprocessors" => 13
%	"output\_fn" => "../models/adagram/more\_senses.adagram\_model"
%	"sense\_treshold" => 1.0e-10
%	"remove\_top\_k" => 0
%	"context\_cut" => true
%	"initcount" => 1.0
%	"train\_fn" => "../data/corpora/WikiCorp/tokenised\_lowercase\_WestburyLab.wikicorp.201004.txt"
%	"d" => 0.0
%	"alpha" => 0.25
%	"subsample" => 1.0e-5
%	"epochs" => 1
%	"window" => 10
%	"min\_freq" => 20
%	"save\_treshold" => 0.0
%	"dim" => 100
%	"stopwords" => Set\{AbstractString\}()
%	"dict\_fn" => "../data/corpora/WikiCorp/tokenised\_lowercase\_WestburyLab.wikicorp.201004.1gram
%}

%\subsection{Greedy Word Sense Embeddings}

To confirm that our techniques are not merely a quirk of the AdaGram method or its implementation, we implemented a new simple baseline word sense embedding method.
This method starts with a fixed number of randomly initialised embeddings, then greedily assigns each training case to the sense which predicts it with the highest probability (using \Cref{eq:generalwsd}).
The task remains the same: using skip-grams with hierarchical softmax to predict the context words for the input word sense.
This is similar to \cite{neelakantan2015efficient}, however it is using collocation probability, rather than distance in vector-space as the sense assignment measure.
Our implementation is based on a heavily modified version of Word2Vec.jl\footnote{\url{https://github.com/tanmaykm/Word2Vec.jl/}}.

This method is intrinsically worse than AdaGram.
Nothing in the model encourages diversification and specialisation of the embeddings.
Manual inspection reveals that a variety of senses are captured, though with significant repetition of common senses, and with rare senses being missed.
Regardless of its low quality, it is a fully independent method from AdaGram, and so is suitable for our use in checking the generalisation of the refitting techniques.

The vocabulary used is smaller than for the AdaGram model.
Words with at least 20,000 occurrences are allocated 20 senses.
Words with at least 250 occurrences are restricted to a single sense.
The remaining rare words are discarded. 
This results in a vocabulary size of 88,262, with 2,796 words having multiple senses.
We always use a uniform prior, as the model does not facilitate easy calculation of the prior.

%\pdfcomment{
%Vocab size: 88,262
%n\_senses = 20
%min\_count = 250
%min\_count for multiple senses = 20\_000
%multisense word count = 2796
%dimensions=300
%}


\section{Similarity of Words in Context} \label{SimilarityInContext}
Estimating word similarity with context is the task of determining how similar words are, when presented with the context they occur in.
The goal of this task is to match human judgements of word similarity.
For each of the target words and contexts; we use refitting on the target word to create a word sense embedding specialised for the meaning in the context provided.
Then the similarity of the refitted vectors can be measured using cosine distance (or similar).
By measuring similarity this way, we are defining a new similarity measure.

%\subsection{AvgSimC}
Reisinger and Mooney \parencite{Reisinger2010} define a number of measures for word similarity suitable for use with sense embeddings.
The most successful was AvgSimC, which has become the gold standard method for use on similarity tasks. It has been used with great success in many works \cite{Huang2012,Chen2014,tian2014probabilistic}. 


AvgSimC is defined using distance metric $d$ (normally cosine distance) as: 
\begin{equation} \label{eq:avgsimc}
\mathrm{AvgSimC}((\u,\c),(\u^{\prime},\c^{\prime})) 
=  \frac{1}{n \times n^{\prime}}
\sum_{u_{i}\in\u}
\sum_{u_{j}^{\prime}\in\u^{\prime}}
P(u_{i}\mid\c)\,P(u_{j}^{\prime}\mid\c^{\prime})\,d(u_{i},u_{j}^{\prime})
\end{equation}
for contexts $\c$ and $\c^\prime$, the contexts of the two words to be compared, and for $\u=\{u_1,...,u_n\}$ and $\u^\prime=\{u^\prime_1,...,u\prime_{n^\prime}\}$ the respective sets of induced senses of the two words.


\subsection{A New Similarity Measure: RefittedSim}\label{RefittedSimVsAvgSimC}
\begin{figure}
	\caption{Block diagram for RefittedSim similarity measure} \label{diaRefittedSim}
	\centering
	\begin{adjustbox}{max width=0.7\linewidth}
	\input{figs/refittedsim.tex}
	\end{adjustbox}
\end{figure}
We define a new similarity measure, RefittedSim, as the distance between the refitted sense embeddings.
As shown in \Cref{diaRefittedSim} the example contexts are used to refit the induced sense embeddings of each word.
This is a direct application of  \Cref{eq:synth}.

Using the same definitions as in \Cref{eq:avgsimc}, RefittedSim is defined as:
\begin{equation} \label{eq:refittedsim}
\resizebox{\linewidth}{!}{
	$\mathrm{RefittedSim}((\u,\c),(\u^{\prime},\c^{\prime}))
	\:=\: d(l(\u \mid \c), l(\u^\prime \mid \c^\prime)
	= d\left(
	\sum_{u_{i}\in\u}u_{i}P(u_{i}\mid\c),\:
	\sum_{u_{j}^{\prime}\in\u^{\prime}}u_{i}P(u_{j}^{\prime}\mid\c^{\prime})\right)
	$
}
\end{equation}

AvgSimC is a probability weighted average of pairwise computed distances for each sense vector.
Whereas RefittedSim is a single distance measured between the two refitted vectors -- which are the probability weighted averages of the original unsupervised sense vectors.


There is a notable difference in time complexity between AvgSimC and RefittedSim.
AvgSimC has time complexity $O(n\left\Vert \c\right\Vert +n^{\prime}\left\Vert \c^{\prime}\right\Vert +n\times n^{\prime})$,
while RefittedSim has $O(n\left\Vert \c\right\Vert +n^{\prime}\left\Vert \c^{\prime}\right\Vert)$.
The product of the number of senses of each word $n \times n^\prime$, may be small for dictionary senses, but it is often large for induced senses. Dictionaries tend to define only a few senses per word -- the average\footnote{It should be noted, though, that the number of meanings is not normally distributed \parencite{zipf1945meaning}.} number of senses per word in WordNet is less than three \parencite{miller1995wordnet}.
For induced senses, however, it is often desirable to train many more senses, to get better results using the more fine-grained information.
Reisinger and Mooney \parencite{Reisinger2010} found optimal results in several evaluations near 50 senses.
In such cases the $O(n \times n^\prime)$ is significant, avoiding it with RefittedSim makes the similarity measure more useful for information retrieval.

\subsection{Experimental Setup}
We evaluate our refitting method using Stanford's Contextual Word Similarities (SCWS) dataset \parencite{Huang2012}.
During evaluation, each context paragraph is limited to 5 words to either side of the target word, as in the training.


\subsection{Results}
\begin{table}
	\caption{Spearman rank correlation $\rho \times 100$  when evaluated on the SCWS task, for varying hyper-parameters.} \label{swscres}
	\pgfplotstabletypeset[col sep=comma, header=has colnames, string type,
		columns/Smoothing/.style={column name={$\substack{\mathrm{Geometric}\\\mathrm{Smoothing}}$}},
		columns/Use Prior/.style={column name={$\substack{\mathrm{Use}\\\mathrm{Prior}}$}},
		%		columns/Use Prior/.style={column name={\small{Use Prior}}},
		columns/AvgSimC/.style={
			%column name={\small{AvgSimC}},
			numeric type,
			precision=1,
			fixed zerofill=true,
			preproc/expr={100*##1},
			column type=c
		},
		columns/RefittedSim/.style={
			%column name={\small{RefittedSim}},
			numeric type,
			precision=1,
			fixed zerofill=true,
			preproc/expr={100*##1},
			column type=c
		},
		every row 1 column RefittedSim/.style={
			postproc cell content/.style={
				@cell content/.add={$\bf}{$}
			}
		},
		every row 0 column AvgSimC/.style={
			postproc cell content/.style={
				@cell content/.add={$\bf}{$}
			}
		},
		every head row/.style={after row = {\toprule}}
		]{data/swsc-grid.csv}
	\end{table}

\begin{table}
	\caption{Spearman rank correlation $\rho \times 100$  when evaluated on the SCWS task, compared to other methods \label{swscEvery}.
	RefittedSim-S is with smoothing, and RefittedSim-SU is with uniform prior}
	\pgfplotstabletypeset[col sep=comma, header=has colnames, string type,
	%		columns/Smoothing/.style={column name={\small{Smoothing}}},
	%		columns/Use Prior/.style={column name={\small{Use Prior}}},
	columns/rho/.style={
		column name={$\rho \times 100$},
		numeric type,
		precision=1,
		fixed zerofill=true,
		preproc/expr={100*##1},
		column type=c
	},
	every row 7 column 3/.style={
		postproc cell content/.style={
			@cell content/.add={$\bf}{$}
		}
	},
	every head row/.style={after row = {\toprule}}
	]{data/swsc.csv}
\end{table}


\Cref{swscres} shows the results of our evaluations on the SCWS similarity task.
A significant improvement can be seen by applying our techniques.

The RefittedSim method consistently outperforms AvgSimC across all configurations.
Similarly geometric smoothing consistently improves performance both for AvgSimC and for RefittedSim.
The improvement is significantly more for RefittedSim than for AvgSimC results.
In general using the unsupervised sense prior estimate from the AdaGram model, improves performance -- particularly for AvgSimC.
The exception to this is with RefittedSim with smoothing, where it makes very little difference.
Unsurprisingly, given its low quality, the Greedy embeddings are always outperformed by AdaGram.
It is not clear if these improvements will transfer to clustering based methods due to the differences in how the sense probability is estimated, compared to the language model based method evaluated on in \Cref{swscres}.



\Cref{swscEvery} compares our results with those reported in the literature using other methods.
These results are not directly comparable, as each method uses a different training corpus, with different preprocessing steps,  which can have significant effects on performance.
It can been seen that by applying our techniques we bring the results of our AdaGram model from very poor ($\rho \times 100 = 43.8$) when using normal AvgSimC without smoothing, 
up to being competitive with other models, when using RefittedSim with smoothing.
The method of Chen et al. \parencite{Chen2014}, has a significant lead on the other results presented.
This can be attributed to its very effective semi-supervised fine-tuning method.
This suggests a possible avenue for future development in using refitted sense vectors to  relabel a corpus, and then performing fine-tuning similar to that done by Chen et al.


\section{Word Sense Disambiguation}\label{lexicalWSD}

\subsection{Refitting for Word Sense Disambiguation} 
\begin{figure}
	\centering
	\begin{adjustbox}{max width=0.9\linewidth}
		\input{figs/wsd.tex}
	\end{adjustbox}
	\caption{Block diagram for performing WSD using refitting.\label{WSDBlock}} 
\end{figure}
Once refitting has been used to create sense vectors for lexical word senses, an obvious used of them is to perform word sense disambiguation.
In this section we refer to the lexical word sense disambiguation problem, i.e. to take a word and find its dictionary sense;
whereas the methods discussed in \Cref{eq:generalwsd,eq:generalwsdsmoothed} consider the more general problem, as applicable to disambiguating lexical or induced word senses depending on the inputs.
Our overall process shown in \Cref{WSDBlock} uses both: first disambiguating the induced senses as part of refitting, then using the refitted sense vectors to find the most likely dictionary sense.

First, refitting is used to transform the induced sense vectors into lexical sense vectors.
We use the targeted word's lemma (i.e. base form), and part of speech (POS) tag to retrieve all possible definitions of the word (Glosses) from WordNet; there is one gloss per sense.
These glosses are used as the example sentence to perform refitting (see \Cref{refitting}).
We find embeddings, $\l=\{l_1,..., l_{n_l}\}$ for each of the lexical word senses using \Cref{eq:synth}.
These lexical word senses are still supported by the language model, which means one can apply the general WSD method to determine the posterior probability of a word sense, given an observed context. 

When given a sentence $\c_{T}$, containing a target word to be disambiguated, 
the probability of each lexical word sense $P(l_i \mid \c_{T})$, can be found using \Cref{eq:generalwsd} (or the smoothed version \Cref{eq:generalwsdsmoothed}), over the lexically refitted sense embeddings.
Then, selecting the correct sense is simply selecting the most likely sense:
\begin{equation}
\label{eq:lexicalwsd}
l^\star (\l, \c_T) 
= \argmax_{\forall l_i \in \l} P(l_i|\c_T) 
= \argmax_{\forall l_i \in \l} \frac{P(\c_T \mid l_i)P(l_i)}{\sum_{\forall l_j \in \l} P(\c_T \mid l_j)P(l_j)}
\end{equation}


\subsection{Lexical Sense Prior}
WordNet includes frequency counts for each word sense based on Semcor \parencite{tengi1998design}.
These form a prior for $P(l_i)$.
The comparatively small size of Semcor means that many word senses do not occur at all.
We apply add-one smoothing to remove any zero counts.
This is in addition to using our proposed geometric smoothing as an optional part of the general WSD.
Geometric smoothing serves a different (but related) purpose, of decreasing the sharpness of the likelihood function -- not of removing impossibilities from the prior.

\subsection {Experimental Setup}
The WSD performance is evaluated on the SemEval 2007 Task 7.

We use the weighted mapping method of Agirre et al. \parencite{agirre2006}, (see \Cref{mapping}) as a baseline alternative method for using WSI senses for WSD.
We use Semcor as the mapping corpus, to derive the mapping weights.

The second baseline we use is the Most Frequent Sense (MFS).
This method always disambiguates any word as having its  most common meaning.
Due to the power law distribution of word senses, this is a very effective heuristic \parencite{Kilgarriff2004}.
We also consider the results when using a backoff to MFS when a method is unable to determine the word sense the method can report the MFS instead of returning no result (a non-attempt).


\subsection{Word Sense Disambiguation Results} \label{WSDtask}
\pgfplotstableset{
	nhundred/.style={
 		numeric type,
		precision=3,
		fixed zerofill=true,
		column type=r
%		preproc/expr={100*##1}
	}
}
\begin{table}
	\centering
	\begin{adjustbox}{max width=\columnwidth}
		\pgfplotstabletypeset[col sep=comma, header=has colnames, string type,
		every head row/.style={after row = {\toprule}},
%
		columns/Method/.style={ 
			column type=l
		},
%
		columns/Attempted/.style={ 
			column type=r
		},
%
		columns/F1/.style={nhundred},
		columns/Precision/.style={nhundred},
		columns/Recall/.style={nhundred},
%
		every row 0 column F1/.style={
			postproc cell content/.style={
				@cell content/.add={$\bf}{$}
			}
		}
%				
		]{data/semeval2007t7-short.csv}
	\end{adjustbox}

	\caption{Results on SemEval 2007 Task 7 -- course-all-words disambiguation.
	The \mbox{\emph{-S} marks} results using geometric smoothing.
	The \mbox{\emph{\textasteriskcentered } marks} results with MFS backoff.
	} \label{samevalres}
\end{table}

The results of employing our method for WSD , are shown in \Cref{samevalres}.
Our results using smoothed refitting, both with AdaGram and Greed Embeddings with backoff, outperform the MFS baseline  \cite{Navigli:2007:STC:1621474.1621480}  -- noted as a surprisingly hard baseline to beat \parencite{Chen2014}.

The mapping method \parencite{agirre2006}  was not up to the task of mapping unsupervised senses to supervised senses, on this large scale task.
The Refitting method works better.
Though refitting is only usable for language-model embedding WSI, the mapping method is suitable for all WSI systems.

While not directly comparable due to the difference in training data, we note that our Refitted results, are similar in performance, as measured by F1 score, to the results reported by Chen et al. \parencite{Chen2014}.
AdaGram with smoothing, and Greedy embeddings with backoff have close to the same result as reported for L2R with backoff -- with the AdaGram slightly better and the Greedy embeddings slightly worse.
They are exceeded by the best method reported in that paper: S2C method with backoff.
Comparison to non-embedding based methods is not discussed here for brevity.
Historically state of the art systems have functioned very differently; normally by approaching the WSD task by more direct means. %\parencite{Navigli:2007:STC:1621474.1621480,moro2015semeval,navigli2013semeval}.


Our results are not strong enough for Refitted AdaGram to be used as a WSD method on its own, but do demonstrate that the senses found by refitting are capturing the information from lexical senses.
It is now evident that the refitted sense embeddings are able to perform WSD, which was not possible with the unsupervised senses. 

\section{Conclusion}\label{conclusionRS}

A new method is proposed for taking unsupervised word embeddings, and adapting them to align to particular given lexical senses, or user provided usage examples. 
This refitting method thus allows us to find word sense embeddings with known meaning.
This method can be seen as a one-shot learning task, where only a single labelled example of each class is available for training.
We show how our method can be used to create embeddings to evaluate the similarity of words, given their contexts.

This allows us to propose a new similarity measuring method, RefittedSim.
The performance of RefittedSim on AdaGram is comparable to the results reported by the researchers of other sense embeddings techniques using AvgSimC, but its time complexity is significantly lower.
We also demonstrate how similar refitting principles can be used to create a set of vectors that are aligned to the meanings in a sense inventory, such as WordNet.

We show how this can be used for word sense disambiguation.
On this difficult task, it performs marginally better than the hard to beat MFS baseline, and significantly better than a general mapping method used for working with WSI senses on lexical WSD tasks.
As part of our method for refitting, we present a geometric smoothing to overcome the issues of overly dominant senses probability estimates.
We show that this significantly improves the performance.
Our refitting method provides effective bridging between the vector space representation of meaning, and the traditional discrete lexical representation.
More generally it allows a sense embedding to be created to model the meaning of a word in any given sentence.
Significant applications  of sense embeddings in tasks such as more accurate information retrieval thus become possible.

}
%\end{document}
