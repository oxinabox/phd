\documentclass{book}
\include{preamble}


\begin{document}

\part {Introduction}
	
\chapter{Introduction}

It has been a continual surprise, that simple combinations of embeddings performs so well for a variety of tasks in natural language processing.
At first glance, such simple methods capturing only unordered word use should have little capacity in the rich and highly structured nature of human language as we linguistically understand it.
However at a second glance, similar surface information has been used in information retrieval with great success since the inception of the field \citep{maron1961automatic}.
Linear combinations of embeddings can be considered as a dimensionality reduction of a bag of words, with various weightings.
Dimensionality reduction can be characterised as finding the best lower dimensional representation of an input according to some quality criterion.
In the case of word embeddings, that quality criterion is generally along the lines the ability to predict the co-occurring words -- an salient quality of lexical semantics.
As such, linear combinations of embeddings take bag of words which is a strong surface form representation,
take reduce it to a dense representation that captures lexical semantics.


\citet{acl2018bleuopposedmeaning} found that taking a mean of word embeddings outperformed almost all their more sophisticated machine-translation-based sentence representations when used on classification and paraphrase detection tasks.
This is not to say that linear combinations of embeddings are ideal models for all tasks.
They clearly can not truly handle all the complexities of language.
But rather that the occurrence of the complexities they can not handle is rarer in practice in many tasks than is often expected.

\citet{ac2018probingsentencevectors} constructed 10 probing tasks to isolate the some of the information captured by sentence representations.
They found the strong performance of averaged word embeddings on sentence level tasks to be striking.
They attribute it to the sentence level information being redundantly encoded in the word-forms: the surface level information is surprisingly useful for what at first looks like a sophisticated tasks.
With the exception of their word-content task, they did find more sophisticated models able to perform better the the averaged word embeddings.
However, when correlating the performance of their probing task against real world tasks,
they found that the word-content probing task was by far the most positively correlated with the real word tasks.
This makes it clear how valuable surface information is in practical tasks.

In the work presented this dissertation, 
we find that that even in tasks where it would seem that non-surface information incorporating word-order is required,
we find in practice other issues cause the more powerful models that are (theoretically) able to handle these situations correctly may them to be never-the-less outperformed.
This is particularly the case where the theoretical improvement from incorporating this information is small, relative to the practical complexity of the techniques required to leverage it.
Such a case where where word order matters but the error from ignoring it is small, is particular illustrated in \Cref{ColorEst}.

At a high-level the success of these techniques comes down to most human language being easy to understand and simple.
This expectation of language being easily understood is highlight by \citep{grice1975logic}, which brings the expectation the communication is conducted following the cooperative principle
The overall supermaxim for Grice's cooperative principle is the speakers should ``be perspicuous'' or more perspicuously, should use speech that is clearly expressed and easily understood.
The particular relevant maxims within the principle are:
the maxim of quantity, that one should should make contributions no more nor less informative than required;
and the maxim of manner: to avoid ambiguity and obscurity of expression, and to make contributions that are brief and orderly.
While Grice originally proposed these are exceptions upon conversation, the general principle applies more broadly to natural language communication.
This general principle being that language used is normally expected to be understood easily -- thus fulfilling the goal of communicating.


Adversarial examples are reasonably easy to construct.
An adversarial example to a linear combination of word embeddings is any text where the word order significantly effects that meaning;
and where multiple possible word orders exist.
For such an adversary to be significant, both word orders must be reasonably likely to occur.
However; such cases are rarer than one might expect, as was found indirectly in \Cref{SOWE2Sent}.
Particularly when punctuation is included, which it reasonably can be as a token embedding.
As such, while these cases certainly exist, we find that for real applications
they are sufficiently rare that the simplicity of the linear combinations of embeddings type approach can work very well.



The when applied in sentence/phrase representation contexts,
such as discussed in \Cref{SentVecMeaning}, and \Cref{ColorEst}
this gives support to the notion that word order is often not a very significant feature in determining meaning.
It seems clear that word order, and other factors of linguistic structure must contribute to the meaning of the phrase.
However, our result suggest that it is often in a minor way, and that for many tasks these linear combinations are superior due to their simplicity and effectiveness.
While taking into account greater linguistic structure may be the key to bridging the between "almost perfect" and "true perfection", the current state of the field for many tasks has not reached "almost perfect", and as such simpler methods still form an important part.

To further understand the relationship between SOWE and BOW, and the extent to which word order matters the capacity to reverse the conversion from phrase to SOWE in investigated in \Cref{BOWgen} and \Cref{SOWE2Sent}.
The results in \Cref{BOWgen} show that it is indeed largely possible to reconstruct bags of words from SOWE, suggesting that when considered as a a dimensionality reduction technique SOWE does not lose much information.
This is extended in \Cref{SOWE2Sent} to order those bags of words back to sentences via a simple tri-gram language model.
This had some success at outright reconstructing the sentences.
This highlights the idea that for many  bags of words (which can be reconstructed form a sum of word embeddings) there may truly be only one reasonable sentence from which they might have come.
This would explain why SOWE, and BOW, ignorance of word order does not prevent them from being useful representation of sentence.


The successes of the sums of word embeddings discussed in \Cref{SentVecMeaning}, and \Cref{ColorEst} leads us to consider other uses of linear combinations for representation.
\Cref{NovelPerspective} and \Cref{RefittingSenses} consider tasks well outside of phrase representation where the order clearly does not matter.




On the complexity of models.
One of the attractive features of these linear combinations is there simplicity
This is true both in an implementation sense, and in the sense of gradient descent.
For example, the vanishing gradient problem in deep networks, especially RNNs and RvNNs
simply does not exist for a sum of word embeddings due to it not being as an input structure.
This in contrast to RNNs which are deep in time, and RvNNs which are deep in structure.
Deep networks can be placed upon the input processing as represented by a RNN, RvNN or linear combination of embeddings,
but for the RNN, and RvNN the network is already depth even with only one hidden layer on top.



TODO: INSERT RNN FAIL PAPER REFERENCE HERE




\section{Thesis Overview}
This thesis tackles a number of natural language understanding problems, and in the solutions draws conclusions on the capacity of linear combinations of embeddings.

The representation of \emph{sentences} is investigated in \Cref{SentVecMeaning}, through a paraphrase grouping tasks.
Similarly, the representation of \emph{phrases} is investigated in \Cref{ColorEst} through a color understanding (estimation) task.
Given the observed properties found by sums of word embeddings,
this leads to the investigation of if weighted sums of word sense embeddings might better resplendent a particular usage of a word in \Cref{RefittingSenses}.
The capacity also lends to the investigation of using a sum of word embeddings to represent the contexts of all usages of a named entity, for the point of view character detection task investigated in \Cref{NovelPerspective}.
We conclude with a complementary pair of works in  \Cref{BOWgen,SOWE2Sent}, which investigate the ability to recover bags of words and sentences, from sums of word embeddings representing sentences.
These final works illustrate some of the reasons why the linear combinations work so well.

\begin{table}
	\begin{tabularx}{\textwidth}{llXX}
		\toprule
		\textbf{Chapter} & \textbf{Structure} & Task & \textbf{Embeddings}\\
		\midrule
		\Cref{SentVecMeaning} & Sentences & Paraphrase grouping &  Word2Vec \citep{mikolovSkip}\\
		\Cref{ColorEst} & Short Phrases & Color understanding & FastText \citep{bojanowski2016enriching}\\
		\Cref{RefittingSenses} & Word Senses & Similarity with context \& Word sense disambiguation %
			& AdaGram \citep{AdaGrams} \& Bespoke greedy sense embeddings\\
		\Cref{NovelPerspective} & Adj. Contexts & POV character detection & FastText \citep{bojanowski2016enriching}\\
		\Cref{BOWgen} & Sentences & Recovering bags of words & GLoVE \citep{pennington2014glove} \\
		\Cref{SOWE2Sent} & Sentences & Recovering sentences & GLoVE \citep{pennington2014glove} \\
		\bottomrule
	\end{tabularx}
	\caption{Summary of the investigations published within this dissertation.}
\end{table}


\subsection{\Cref{SentVecMeaning}}
We begin by examining methods for representing sentences.
Sentences are a fundamental unit of communication --
a sentence is a single complete idea.

The core goal is to determine if an sentence embedding clearly separated the different ideas.
Paraphrases are defined by a bidirectional entailment relationship between two sentences.
This is a equivalence relationship, it thus gives rise to a partitioning of all sentences in natural language space.
If a sentence embedding is of high quality, it will be easy to define a corresponding partitioning of the embedding space.
One way to determine how easy it is to define the corresponding partitioning it to attempt to do just that as a supervised classification task using a weak classifier.
A weak classifier, such as the linear SVM we used, is required as a more powerful classifier (such as a deep neural network) could learn arbitary transforms.
The classification task is to take in a sentence embedding and predict which group of paraphrases it belongs to.
Where the target paraphrase group is defined using other paraphrases with the same meaning as the candidate.

Under this course of evaluation it was found that the sum and mean of word embeddings performed very well as a sentence representation.
These LCOWEs were the best performing models under evaluation.
They were closely followed by the bag of words, which is advantaged by being much higher dimensionality than other models.
The LCOWEs outperform the bag of words as they also capture  synonyms and other features of lexical relatedness.
Slightly worse than the bag of words was the bag of words with PCA dimensionality reduction to 300 dimensions.
This confirms our expectation that LCOWEs are a better form of dimensionality reduction for preserving meaning from a bag of words than PCA.
 


A limitation of this work is that it does not include the examination of any encoder-decoder based methods, such as Skip-Thought \citep{DBLP:journals/corr/KirosZSZTUF15}, or machine translation models.
Another limmitation of the work is that the URAE use a pretrained model with only 200 dimensions, rather than 300 dimensions as was used in the other evaluations.


Paraphrases provide one source of grounding for evaluation of sentences.
Color names are a subset of short phrases which also have a ground truth for meaning -- the color.
They are thus useful for evaluating the performance of LICOWE on short phrases.

\subsection{\Cref{ColorEst}}
To evaluated the performance of input representations for short phrases, we considered a color understanding task.
Color understanding is considered as grounded microcosm of natural language understanding \citep{2016arXiv160603821M}.
It appears as a complicated sub-domain, with many of the same issues that plague natural language understanding in general:
it features a lot of ambiguity, substantial morphological and syntax structure, and depends significantly on context that is not made available to the natural language understanding algorithms.
Unlike natural language more generally, it has a comparatively small vocabulary, and it has grounded meaning.
The meaning of a particular utterance, say \natlang{bluish green}, can be grounded to a point in color space, say in HSV (192°, 93\%, 72\%), based on the questioning the speaker.
The general meaning of the a color phrase can be grounded to a distribution over color space, based on surveying the population of speakers.

Models were thus created to learn a mapping from natural language space, to points or distributions in color space.
Three input representations were considered: a sum of word embeddings (SOWE), a convolutional neural network (CNN), and a recurrent neural network (RNN).
The SOWE corresponds to a bag of words -- no knowledge of order.
The CNN corresponds to a bag of ngrams -- it includes features of all length, thus can encode order.
The RNN is a fully sequential model -- all inputs are processed in order and it must remember previous inputs.

It was expected that this task would benefit significantly from a knowledge of word order.
For example, \natlang{bluish green} and \natlang{greenish blue} are visibly different colors.
The former being greener than the later.
However, it was found that the SOWE was the best performing input representation, 
followed closely by the CNN , with the RNN performing much worse.
This was even the case when the test set was restricted to only contain colors names for which multiple different word orders (representing different colors) were found in the training set.
This can be attributed to the difficulty in training the more complicated models.
In contrast to a simple feed-forward SOWE, in a RNN the gradient must propagate further from the output,
and there is more weights to be learned in the gates.
This difficulty dominated over the limitation in being able to model the color names correctly.
We note that while \natlang{bluish green} and \natlang{greenish blue} are different colors, but they are never the less similar colors.
As such, the error from treating them as the same, is less than the error caused by training difficulties.


The solving problem of color estimation from natural language color name, has pragmatic uses.
Color estimation from description has utility as a tool for improving human-computer interaction.
For example allowing free(-er) text for specifying colors in plotting software, using point estimation.
It also has utility as an education tool: people from different cultures, especially non-native English speakers, may not know exactly what color range is described by \natlang{dark salmon}, and our model allows for tools to be created to answer such queries using distribution estimation.


A limitation of this study is the metrics used.
For distribution estimation, the perplexity of the discretized distributions in color space is reported.
It would be preferable to uses Kullback–Leibler divergence, which would allow comparisons to future works that output truly continuous distributions.
Kullback–Leibler divergence is monotonically related the to discretized perplexity, however.
For point estimation, using an evaluation metric such as a Delta-E, which is controlled for the varying sensitivity of human perception for different hues.
Neither limitation has direct bearing on the assessment of the input representations.



% also considers the use of word embeddings as features, here to predict probability distributions and point estimates for colors given the color name. A Sum of Word Embeddings as an input layer is contrasted with processing the input (as word embeddings) through an RNN or CNN, as well as to a baseline. While this is a task where word order is significant, (``Brownish Green'' is a slightly different shade to ``Greenish Brown'') never the less SOWE do very well, likely due to the ease of training.



\subsection{\Cref{RefittingSenses}}
With the demonstrated utility of linear combinations of embeddings for representing the meanings of larger structures made from words,
it is worth investigating their utility for representing the possible different meanings of words.
When it comes to representing word senses, it may be desirable to find a representation for the exact sense of a word being used in a particular example.
A a very fine grained word sense for just that one use.
If one has a collection of induced word senses, it seems reasonable to believe that the ideal word sense for a particular use, must lay somewhere between them in embedding space.
Further more, if one knows the probably of each of the coarse induced senses being the correct sense for this use,
then it makes sense that the location of the fine grained sense embedding would be closer to the more likely coarse sense,
and further from the less likely coarse sense.
As such we propose a method to define these specific case word senses based on a probability weighted sum of coarser word sense embeddings.
We say that we \emph{refit} the original sense embeddings, using the single example sentence to induce the fine grained sense embedding.

Using this we define a similarity measure which we call RefittedSim, which we find to work better than AvgSimC \citep{Reisinger2010}.
AvgSimC is a probability-weighted average of all the pairwise similarity scores for each sense embedding.
In contrast RefittedSim is a single similarity score as measured between the two refitted vectors -- which are the probability weighted averages of the coarser sense vectors.
On the embeddings used in our evaluations this gave a solid improvement over AvgSimC.
It is also asymptotically faster to evaluate.

We also evaluated using refitting for word sense disambiguation (WSD).
Normally, induced senses can not be used for word sense disambiguation, as they do not correspond to standard dictionary word senses.
By using the WordNet gloss (definition) as an example sentence, we are able to use refitting to create a new set of sense embeddings suitable for WSD.
Using this we can use the skip-gram formulation for probability of the context given the  refitted sense, and so apply Baye's theorem to find the most-likely sense.
However, we found that the results were only marginally better than the baseline.
Nearly unsupervised WSD is a very difficult problem; with a strong baseline of simply reporting the most-common sense.
Our results do suggest that our refitting method does not learn features that are antithetical to WSD.
However, they do incorporate the most frequent sense as a prior and seem to provide little benefit beyond that.

A limitation of this study was that it did not perform the evaluation on state-of-the-art word-sense embeddings.
As such, while it's comparisons between these embeddings are valid they
can not be readily compared to the current state-of-the-art on the tasks.


\subsection{\Cref{NovelPerspective}}

Given the success of LCOWEs for representing meaningful linquistic structures (sentences and phrases),
a natural follow up question is on its capacity to represent combinations of words that do not feature this natural kind of structure.
These would be more arbitrary bags of words; that never the less may be useful features for a particular task.
The task investigated in this work was about identifying point of view characters in a novel.

Given some literary text written in third person limited point of view, such as Robert Jordan's popular \textit{``Wheel of Time''} series of novels,
it is useful to a reader (or person analysing the text), to identify which sections are from the perspective of which character.
That is to say, we would like to classify the chapters of a book according to which character they are from the perspective of.
This at first looks like a multiclass classification problem;
however it is in-fact an information extraction problem.
The set of possible classes for any given chapter is the set of all named entities in the book.
Different booked have different characters,
thus the set of named entities in the training data will not match that of an arbitrary book selected by a user.
As such, the named entity tokens themselves can not be used in training for this task.
Instead, it must be determined whether or not a named entity is the point of view character, based on how the named entity token is used.
To do this, an representation of the context of use is needed.


The task can be treated as a binary classification problem.
Given some feature vector representing how a particular named entity token was used throughout a chapter,
find the probability of that named entity being the point of view character.
We considered two possible feature sets to use to generate the feature vectors for named entity token use.
Both feature sets consider the context primarily in terms of the token (word) immediately prior to, and the token (word) immediately after the named entity.
We define a 200 dimensional hand-crafted \emph{classical feature set} in terms of the counts of adjacent part of speech tags, position in the text, and token frequency.
We define a \emph{mean of word embedding based feature set} as the concatenation of the mean of the word embedding for the words occurring immediately prior, to the mean of the word occurring immediately after.
As this was using 300 dimensional embeddings, this gives a 600 dimensional feature vector.


It was found that the two feature sets performed similarly, with both working very well.
It seems like the primary difficulty was with the high dimensionality of the word embedding based feature set.
Without sufficient training data, it over-fit quiet easily.
It's performance dropped sharply on the testset, compared to it's oracle performance if trained on the testset,
when the largest book series was removed.
This likely could have been ameliorated by using lower dimensional embeddings.

The good performance of the word embedding based feature set is surprising here as it does not include any frequency information.
We used a mean, rather than a sum, of word embeddings to represent the context of named entity token use.
In the classical feature set, we found that by far the most important feature was how often that named entity token was used.
Indeed just reporting the most frequently mentioned named entity gave a very strong baseline.
The lexical information captured by the MOWE is clearly similarly useful to the part of speech tag counts, and almost certainly makes more fine grained information available to the classifier.
Thus allowing it to define good decisions boundaries for if the feature vector represents a point of view character or not.


A limitation of this study is that different binary classifiers were used for the two feature sets.
Ideally, the performance using a range of classifiers for both would have been reported.
Our preliminary results suggested that the classifier choice was not significant.
With logistic regression, SVM, and decision trees giving similarly high results for both feature sets.
 




%considers the use of word embeddings as features to a binary classifier to detect named entities are the Point of View character. The mean of the word embeddings of the adjacent words for each occurrence of each named-entity is used as a feature vector for a binary classifier. We contrast it's performance to that of hand-engineered lexical features. It is concluded that the difference in performance is primarily related to dimensionality and the amount of training data for the binary task.





\subsection{\Cref{BOWgen}}
proposes as method to extract the original bag of words from a sum of word embeddings. Thus placing a bound on the information lost in the transformation of BOW to SOWE. This is done via a simple greedy algorithm with a correction step.

\subsection{\Cref{SOWE2Sent}}
extends this, investigating to what extend can the BOW generated by \Cref{BOWgen} be reordered to determine the original sentence. This placing a bound on the information lost from Sentence to SOWE. This is carried out by transforming the word ordering problem into a mixed integer programming problem of finding the most likely sequence according to a trigram language model.
	




\section{Some Math}
\subsection{Word Embeddings}
Given a word represented by an integer $w$, from a vocabulary $\set{V} \subset \set{Z}$,
and a matrix of embeddings, represented as $C$:
its embedding can be found by slicing out the $w$th column:
$\ii C_w$.
For $\iv e_w$ the elementary unit vector, i.e. the one-hot vector representation of $w$
it can be seen that the word embedding can be represented as the product of the embedding matrix with the one-hot vector.

\begin{equation*}
	\ii C_w = C\,\iv e_w
\end{equation*}


\subsection{Sum of Word Embeddings}
For some sequence of (not necessarily unique words) words $\seq W \in {\set V}^{\set Z_0}$ represented $\seq W = \left(\nn w_1, \nn w_2, \ldots, \nn w_n \right)$, where $\nn w_i$ is an integer representing which word the $i$th word is.

The sum of word embeddings (SOWE) representation is written as:
\begin{equation*}
\sum_{i=1}^{i=n} \ii C_{\nn w_i}
\end{equation*}


The bag of word (BOW) representation is written as
a vector from $\set{Z^{\left\vert \set V \right\vert}}$.
\begin{equation*}
\vv x = \sum_{i=1}^{i=n} {\iv e_{\nn w_i}}
\end{equation*},


Using this the sum of word embeddings can be seen to be the product of the embedding matrix with a BOW vector.
\begin{equation*}
\sum_{i=1}^{i=n} \ii C_{\nn w_i}
= \sum_{i=1}^{i=n}C\, \iv e_{\nn w_i}
= C\,\sum_{i=1}^{i=n} \iv e_{\nn w_i}
= C\,\vv x
\end{equation*}


\subsection{Mean of Word Embeddings}

The mean of word embeddings representation is written as:
\begin{equation*}
\frac{1}{n} \sum_{i=1}^{i=n} \ii C_{\nn w_i}
\end{equation*}


Note that $n$ is equal to the element-wise sum of the BOW vector ($x$), i.e. to its l1-norm:

\begin{equation*}
	n = {\left\Vert \vv x \right\Vert}_1 = \sum_{\forall j \in \set{V}} \iv x_j
\end{equation*}

Thus the mean of word embeddings can be seen as the product of the embedding matrix with the l1-normalized BOW vector.

\begin{equation*}
\frac{1}{n} \sum_{i=1}^{i=n} \ii C_{\nn w_i}
= \frac{1}{n}  \sum_{i=1}^{i=n}C\, \iv e_{\nn w_i}
=  \frac{1}{n} C\, \sum_{i=1}^{i=n}x
= C\,\frac{\vv x}{\left\Vert \vv x \right\Vert}_{\!1}
\end{equation*}

\subsection{Linear Combination of Embeddings}
The full generalisation of this is that any linear combination of embeddings
can be seen as product of the embedding matrix, the a weighted bag of words.

A weighting function for linear combination scheme can be defined,
mapping from a given bag of words, and a word, to the weighting of that word.
$\alpha: {\set Z}^{\left\vert \set V \right\vert} \times \set{V} \to \set{R}$.

(For example for the mean of word embeddings $\alpha(\vv x, w) = \frac{1}{\left\Vert \vv x \right\Vert}_{\!1}$.)

From the weighting function, we can evaluated it for a given BOW, for each word in the vocabulary to define 
a weighting vector $\dv \alpha_{\vv x}$:
\begin{equation*}
\dv \alpha_{\vv x} = \left[ \alpha(\vv x, w) \right]_{w\in \set V}
\end{equation*}


Using $\odot$ as the Hadamard (i.e. element-wise) product,
we can thus write:

\begin{equation*}
\sum_{i=1}^{i=n} \alpha(\vv x, \nn w_i) \ii C_{\nn w_i}
= C\, \sum_{i=1}^{i=n} \alpha(\vv x, \nn w_i) \iv e_{\nn w_i}
= C\, \left( \dv \alpha_{\vv x} \odot \vv x   \right)
\end{equation*}

	
	
	
\printbib

\end{document}