\documentclass{book}
\input{preamble.tex}


\begin{document}
	
\chapter{Introduction}

It has been a continual surprise, that simple combinations of embeddings performs so well for a variety of tasks in natural language processing.

The when applied in sentence/phrase representation contexts,
such as discussed in \Cref{SentVecMeaning}, and \Cref{ColorDist}
this gives support to the notion that word order is often not a very significant feature in determining meaning.
It seems clear that word order, and other factors of linguistic structure must contribute to the meaning of the phrase.
However, our result suggest that it is often in a minor way, and that for many tasks these linear combinations are superior due to their simplicity and effectiveness.
While taking into account greater linguistic structure may be the key to bridging the between "almost perfect" and "true perfection", the current state of the field for many tasks has not reached "almost perfect", and as such simpler methods still form an important part.

To further understand the relationship between SOWE and BOW, and the extent to which word order matters the capacity to reverse the conversion from phrase to SOWE in investigated in \Cref{BOWgen} and \Cref{SOWE2Sent}.
The results in \Cref{BOWgen} show that it is indeed largely possible to reconstruct bags of words from SOWE, suggesting that when considered as a a dimensionality reduction technique SOWE does not lose much information.
This is extended in \Cref{SOWE2Sent} to order those bags of words back to sentences via a simple tri-gram language model.
This had some success at outright reconstructing the sentences.
This highlights the idea that for many  bags of words (which can be reconstructed form a sum of word embeddings) there may truly be only one reasonable sentence from which they might have come.
This would explain why SOWE, and BOW, ignorance of word order does not prevent them from being useful representation of sentence.


The successes of the sums of word embeddings discussed in \Cref{SentVecMeaning}, and \Cref{ColorDist} leads us to consider other uses of linear combinations for representation.
\Cref{NovelPerspective} and \Cref{RefittingSenses} consider tasks well outside of phrase representation where the order clearly does not matter.








\section{Works Within this Thesis}

\begin{itemize}
	\item \Cref{SentVecMeaning} examines a variety of sentence embedding methods on how well they create a space that can be partitioned according to the true partitioning of the sentences according to their meaning. It finds the surprising result that the linear combinations of embeddings (SOWE and Mean of Word Embeddings) significantly outperform the more sophisticated methods.
	\item \Cref{ColorDist} also considers the use of word embeddings as features, here to predict probability distributions and point estimates for colors given the color name. A Sum of Word Embeddings as an input layer is contrasted with processing the input (as word embeddings) through an RNN or CNN, as well as to a baseline. While this is a task where word order is significant, (``Brownish Green'' is a slightly different shade to ``Greenish Brown'') never the less SOWE do very well, likely due to the ease of training.
	\item \Cref{BOWgen} proposes as method to extract the original bag of words from a sum of word embeddings. Thus placing a bound on the information lost in the transformation of BOW to SOWE. This is done via a simple greedy algorithm with a correction step.
	\item \Cref{SOWE2Sent} extends this, investigating to what extend can the BOW generated by \Cref{BOWgen} be reordered to determine the original sentence. This placing a bound on the information lost from Sentence to SOWE. This is carried out by transforming the word ording problem into a mixed integer programming problem of finding the most likely sequence according to a trigram language model.
	\item \Cref{NovelPerspective} considers the use of word embeddings as features to a binary classifier to detect named entities are the Point of View character. The mean of the word embeddings of the adjacent words for each occurrence of each named-entity is used as a feature vector for a binary classifier. We contrast it's performance to that of hand-engineered lexical features. It is concluded that the difference in performance is primarily related to dimensionality and the amount of training data for the binary task.
	\item \Cref{RefittingSenses} defines a method to generate new word sense embeddings from an existing set of sense embeddings by using their a linear combination of the existing embeddings with weight based on how likely those senses are to occur in an example sentence. It finds the sense embeddings generated this way are more useful for similarity comparison, but not competitive for use in word sense disambiguation.

\end{itemize}


\section{Some Math}
\subsection{Word Embeddings}
Given a word represented by an integer $w$, from a vocabulary $\set{V} \subset \set{Z}$,
and a matrix of embeddings, represented as $C$:
its embedding can be found by slicing out the $w$th column:
$\i{C}_w$.
For $\iv e_w$ the elementary unit vector, i.e. the one-hot vector representation of $w$
it can be seen that the word embedding can be represented as the product of the embedding matrix with the one-hot vector.

\begin{equation*}
	\i C_w = C\,\iv e_w
\end{equation*}


\subsection{Sum of Word Embeddings}
For some sequence of (not necessarily unique words) words $\seq W \in {\set V}^{\set Z_0}$ represented $\seq W = \left(\n w_1, \n w_2, \ldots, \n w_n \right)$, where $\n w_i$ is an integer representing which word the $i$th word is.

The sum of word embeddings (SOWE) representation is written as:
\begin{equation*}
\sum_{i=1}^{i=n} \i C_{\n w_i}
\end{equation*}


The bag of word (BOW) representation is written as
a vector from $\set{Z^{\left\vert \set V \right\vert}}$.
\begin{equation*}
\v x = \sum_{i=1}^{i=n} {\iv e_{\n w_i}}
\end{equation*},


Using this the sum of word embeddings can be seen to be the product of the embedding matrix with a BOW vector.
\begin{equation*}
\sum_{i=1}^{i=n} \i C_{\n w_i}
= \sum_{i=1}^{i=n}C\, \iv e_{\n w_i}
= C\,\sum_{i=1}^{i=n} \iv e_{\n w_i}
= C\,\v x
\end{equation*}


\subsection{Mean of Word Embeddings}

The mean of word embeddings representation is written as:
\begin{equation*}
\frac{1}{n} \sum_{i=1}^{i=n} \i C_{\n w_i}
\end{equation*}


Note that $n$ is equal to the element-wise sum of the BOW vector ($x$), i.e. to its l1-norm:

\begin{equation*}
	n = {\left\Vert \v x \right\Vert}_1 = \sum_{\forall j \in \set{V}} \iv x_j
\end{equation*}

Thus the mean of word embeddings can be seen as the product of the embedding matrix with the l1-normalized BOW vector.

\begin{equation*}
\frac{1}{n} \sum_{i=1}^{i=n} \i C_{\n w_i}
= \frac{1}{n}  \sum_{i=1}^{i=n}C\, \iv e_{\n w_i}
=  \frac{1}{n} C\, \sum_{i=1}^{i=n}x
= C\,\frac{\v x}{\left\Vert \v x \right\Vert}_{\!1}
\end{equation*}

\subsection{Linear Combination of Embeddings}
The full generalisation of this is that any linear combination of embeddings
can be seen as product of the embedding matrix, the a weighted bag of words.

A weighting function for linear combination scheme can be defined,
mapping from a given bag of words, and a word, to the weighting of that word.
$\alpha: {\set Z}^{\left\vert \set V \right\vert} \times \set{V} \to \set{R}$.

(For example for the mean of word embeddings $\alpha(\v x, w) = \frac{1}{\left\Vert \v x \right\Vert}_{\!1}$.)

From the weighting function, we can evaluated it for a given BOW, for each word in the vocabulary to define 
a weighting vector $\dv \alpha_{\v x}$:
\begin{equation*}
\dv \alpha_{\v x} = \left[ \alpha(\v x, w) \right]_{w\in \set V}
\end{equation*}


Using $\odot$ as the Hadamard (i.e. element-wise) product,
we can thus write:

\begin{equation*}
\sum_{i=1}^{i=n} \alpha(\v x, \n w_i) \i C_{\n w_i}
= C\, \sum_{i=1}^{i=n} \alpha(\v x, \n w_i) \iv e_{\n w_i}
= C\, \left( \dv \alpha_{\v x} \odot \v x   \right)
\end{equation*}



	
	
	


\end{document}