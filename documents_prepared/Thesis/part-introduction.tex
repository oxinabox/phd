\documentclass{book}
\input{preamble.tex}


\begin{document}
	
\chapter{Introduction}

It has been a continual surprise, that simple combinations of embeddings performs so well for a variety of tasks in natural language processing.




\section{Works Within this Thesis}

\begin{itemize}
	\item \Cref{SentVecMeaning} examines a variety of sentence embedding methods on how well they create a space that can be partitioned according to the true partitioning of the sentences according to their meaning. It finds the surprising result that the linear combinations of embeddings (SOWE and Mean of Word Embeddings) significantly outperform the more sophisticated methods.
	\item \Cref{BOWgen} proposes as method to extract the original bag of words from a sum of word embeddings. Thus placing a bound on the information lost in the transformation of BOW to SOWE. This is done via a simple greedy algorithm with a correction step.
	\item \Cref{SOWE2Sent} extends this, investigating to what extend can the BOW generated by \Cref{BOWgen} be reordered to determine the original sentence. This placing a bound on the information lost from Sentence to SOWE. This is carried out by transforming the word ording problem into a mixed integer programming problem of finding the most likely sequence according to a trigram language model.
	\item \Cref{NovelPerspective} considers the use of word embeddings as features to a binary classifier to detect named entities are the Point of View character. The mean of the word embeddings of the adjacent words for each occurrence of each named-entity is used as a feature vector for a binary classifier. We contrast it's performance to that of hand-engineered lexical features. It is concluded that the difference in performance is primarily related to dimensionality and the amount of training data for the binary task.
	\item \Cref{ColorDist} also considers the use of word embeddings as features, here to predict probability distributions and point estimates for colors given the color name. A Sum of Word Embeddings as an input layer is contrasted with processing the input (as word embeddings) through an RNN or CNN, as well as to a baseline. While this is a task where word order is significant, (``Brownish Green'' is a slightly different shade to ``Greenish Brown'') never the less SOWE do very well, likely do to the ease of training.
	\item \Cref{RefittingSenses} defines a method to generate new word sense embeddings from an existing set of sense embeddings by using their a linear combination of the existing embeddings with weight based on how likely those senses are to occur in an example sentence. It finds the sense embeddings generated this way are more useful for similarity comparison, but not competitive for use in word sense disambiguation.



\section{Some Math}
\subsection{Word Embeddings}
Given a word represented by an integer $w$, from a vocabulary $\set{V} \subset \set{Z}$,
and a matrix of embeddings, represented as $C$:
its embedding can be found by slicing out the $w$th column:
$\i{C}_w$.
For $\iv e_w$ the elementary unit vector, i.e. the one-hot vector representation of $w$
it can be seen that the word embedding can be represented as the product of the embedding matrix with the one-hot vector.

\begin{equation*}
	\i C_w = C\,\iv e_w
\end{equation*}


\subsection{Sum of Word Embeddings}
For some sequence of (not necessarily unique words) words $\seq W \in {\set V}^{\set Z_0}$ represented $\seq W = \left(\n w_1, \n w_2, \ldots, \n w_n \right)$, where $\n w_i$ is an integer representing which word the $i$th word is.

The sum of word embeddings (SOWE) representation is written as:
\begin{equation*}
\sum_{i=1}^{i=n} \i C_{\n w_i}
\end{equation*}


The bag of word (BOW) representation is written as
a vector from $\set{Z^{\left\vert \set V \right\vert}}$.
\begin{equation*}
\v x = \sum_{i=1}^{i=n} {\iv e_{\n w_i}}
\end{equation*},


Using this the sum of word embeddings can be seen to be the product of the embedding matrix with a BOW vector.
\begin{equation*}
\sum_{i=1}^{i=n} \i C_{\n w_i}
= \sum_{i=1}^{i=n}C\, \iv e_{\n w_i}
= C\,\sum_{i=1}^{i=n} \iv e_{\n w_i}
= C\,\v x
\end{equation*}


\subsection{Mean of Word Embeddings}

The mean of word embeddings representation is written as:
\begin{equation*}
\frac{1}{n} \sum_{i=1}^{i=n} \i C_{\n w_i}
\end{equation*}


Note that $n$ is equal to the element-wise sum of the BOW vector ($x$), i.e. to its l1-norm:

\begin{equation*}
	n = {\left\Vert \v x \right\Vert}_1 = \sum_{\forall j \in \set{V}} \iv x_j
\end{equation*}

Thus the mean of word embeddings can be seen as the product of the embedding matrix with the l1-normalized BOW vector.

\begin{equation*}
\frac{1}{n} \sum_{i=1}^{i=n} \i C_{\n w_i}
= \frac{1}{n}  \sum_{i=1}^{i=n}C\, \iv e_{\n w_i}
=  \frac{1}{n} C\, \sum_{i=1}^{i=n}x
= C\,\frac{\v x}{\left\Vert \v x \right\Vert}_{\!1}
\end{equation*}

\subsection{Linear Combination of Embeddings}
The full generalisation of this is that any linear combination of embeddings
can be seen as product of the embedding matrix, the a weighted bag of words.

A weighting function for linear combination scheme can be defined,
mapping from a given bag of words, and a word, to the weighting of that word.
$\alpha: {\set Z}^{\left\vert \set V \right\vert} \times \set{V} \to \set{R}$.

(For example for the mean of word embeddings $\alpha(\v x, w) = \frac{1}{\left\Vert \v x \right\Vert}_{\!1}$.)

From the weighting function, we can evaluated it for a given BOW, for each word in the vocabulary to define 
a weighting vector $\dv \alpha_{\v x}$:
\begin{equation*}
\dv \alpha_{\v x} = \left[ \alpha(\v x, w) \right]_{w\in \set V}
\end{equation*}


Using $\odot$ as the Hadamard (i.e. element-wise) product,
we can thus write:

\begin{equation*}
\sum_{i=1}^{i=n} \alpha(\v x, \n w_i) \i C_{\n w_i}
= C\, \sum_{i=1}^{i=n} \alpha(\v x, \n w_i) \iv e_{\n w_i}
= C\, \left( \dv \alpha_{\v x} \odot \v x   \right)
\end{equation*}



	
	
	
\end{itemize}


\end{document}