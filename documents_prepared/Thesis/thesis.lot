\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {1.1}{\ignorespaces Summary of the investigations published within this dissertation.\relax }}{5}{table.caption.12}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {6.1}{\ignorespaces The semantic classification accuracy of the various models across the two evaluation corpora.\relax }}{113}{table.caption.26}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {8.1}{\ignorespaces The results for the \textbf {full distribution estimation task}. Lower perplexity (PP) is better.\relax }}{134}{table.caption.55}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {8.2}{\ignorespaces The results for the \textbf {full point estimation task}. Lower mean squared error (MSE) is better.\relax }}{135}{table.caption.56}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {8.3}{\ignorespaces The results for the \textbf {order distribution estimation task}. Lower perplexity (PP) is better. This is a subset of the full test set containing only tests where the order of the words matters.\relax }}{135}{table.caption.57}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {8.4}{\ignorespaces The results for the \textbf {order point estimation task}. Lower mean squared error (MSE) is better. This is a subset of the full test set containing only tests where the order of the words matters.\relax }}{135}{table.caption.58}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {8.5}{\ignorespaces The results for the \textbf {unseen combinations distribution estimation task}. Lower perplexity (PP) is better. This uses the extrapolation subset of the full test set. In the extrapolating results certain rare word combinations were removed from the training and development sets. In the non-extrapolating results the full training and development stet was used.\relax }}{135}{table.caption.59}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {8.6}{\ignorespaces The results for the \textbf {unseen combinations point estimation task}. Lower mean squared error (MSE) is better. This uses the extrapolation subset of the full test set. In the extrapolating results certain rare word combinations were removed from the training and development sets. In the non-extrapolating results the full training and development stet was used.\relax }}{136}{table.caption.60}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {8.7}{\ignorespaces The third quartile for the pairwise Spearman's correlation of the color channels given the color name.\relax }}{139}{table.caption.63}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {8.8}{\ignorespaces The results for the \textbf {full distribution estimation task} using smoothed training data. Lower perplexity (PP) is better. This corresponds to the main results in \Cref {tbl:distfull}.\relax }}{140}{table.caption.64}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {8.9}{\ignorespaces The results for the \textbf {order distribution estimation task} using smoothed training data. Lower perplexity (PP) is better. This is a subset of the full test set containing only tests where the order of the words matters. This corresponds to the main results in \Cref {tbl:distord}.\relax }}{140}{table.caption.65}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {8.10}{\ignorespaces The results for the \textbf {unseen combinations distribution estimation task} using smoothed training data. Lower perplexity (PP) is better. This uses the extrapolation subset of the full test set. In the extrapolating results certain rare word combinations were removed from the training and development sets. In the non-extrapolating results the full training and development stet was used. This corresponds to the main results in \Cref {tbl:distextrapo}.\relax }}{140}{table.caption.66}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {10.1}{\ignorespaces Spearman rank correlation $\rho \times 100$ when evaluated on the SCWS task, for varying hyper-parameters.\relax }}{148}{table.caption.68}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {10.2}{\ignorespaces Spearman rank correlation $\rho \times 100$ when evaluated on the SCWS task, compared to other methods . RefittedSim-S is with smoothing, and RefittedSim-SU is with uniform prior\relax }}{148}{table.caption.69}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {10.3}{\ignorespaces Results on SemEval 2007 Task 7 -- course-all-words disambiguation. The \unhbox \voidb@x \hbox {\emph {-S} marks} results using geometric smoothing. The \unhbox \voidb@x \hbox {\emph {\textasteriskcentered } marks} results with MSF backoff. \relax }}{150}{table.caption.71}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {12.1}{\ignorespaces The number of chapters and point of view characters for each dataset. \relax }}{158}{table.caption.78}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {12.2}{\ignorespaces The results of the character classifier systems. The best results are \textbf {bolded}. \relax }}{159}{table.caption.79}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {12.3}{\ignorespaces The training set accuracy of the machine learning character classifier systems. \relax }}{160}{table.caption.80}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {14.1}{\ignorespaces Examples of the BOW Produced by our method using the Books Corpus vocabulary, compared to the Correct BOW from the reference sentences. The P and C columns show the the number of occurrences of each word in the Produced and Correct bags of words, respectively. \textbf {Bolded} lines highlight mistakes. Examples\nobreakspace {}a-e were sourced from \textcite {iyyer2014generating}, Examples\nobreakspace {}f-h from \textcite {Bowman2015SmoothGeneration}. Note that in example\nobreakspace {}a, the \emph {``\_\_...\_(n)''} represents \emph {n} repeated underscores (without spaces).\relax }}{170}{table.caption.86}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {14.2}{\ignorespaces The performance of the BOW generation method. Note the final line is for the Books Corpus, where-as the preceding are or the Brown Corpus.\relax }}{171}{table.caption.87}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {16.1}{\ignorespaces The overall performance of the Sel.\nobreakspace {}BOW+Ord.{} sentence generation process when evaluated on the Books corpus. \relax }}{182}{table.caption.97}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {16.2}{\ignorespaces The performance of the word selection step, on the Books corpus. This table shows a subset of the results reported by \textcite {White2015BOWgen}.\relax }}{182}{table.caption.98}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {16.3}{\ignorespaces A comparison our method, to the example sentences generated by the DT-RAE method of \textcite {iyyer2014generating}. Ref.\nobreakspace {}BOW+Ord.{} shows the word ordering step on the reference BOW. the Sel. and Ord. columns indicate if the output had the correct words selected, and ordered respectively. With {\fontfamily {pzd}\fontencoding {U}\fontseries {m}\fontshape {n}\selectfont \char 51}{} indicating correct and {\fontfamily {pzd}\fontencoding {U}\fontseries {m}\fontshape {n}\selectfont \char 55}{} indicating incorrect. {\fontfamily {pzd}\fontencoding {U}\fontseries {m}\fontshape {n}\selectfont \char 54}{} indicates not only that ordering was not correct, but that the MIP problem had no feasible solutions at all. DT-RAE Ref.{} shows the result of the method of \textcite {iyyer2014generating}, when the dependency tree of the output is provided to the generating process, whereas in DT-RAE Para.{} an arbitrary dependency tree is provided to the generating process. Note that the reference used as input to Sel.\nobreakspace {}BOW+Ord.{} and Ref.\nobreakspace {}BOW+Ord.{} sentence was varied slightly from that used in \textcite {iyyer2014generating} and \textcite {White2015BOWgen}, in that terminating punctuation was not removed, and nor were multiword entity references grouped into single tokens.\relax }}{186}{table.caption.100}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {16.4}{\ignorespaces A comparison of the output of the Two Step process proposed in this paper, to the example sentences generated by the VAE method of \textcite {Bowman2015SmoothGeneration}.\relax }}{187}{table.caption.101}
\defcounter {refsection}{0}\relax 
\contentsline {table}{\numberline {16.5}{\ignorespaces A pair of example sentences, where the correct order is particularly ambiguous.\relax }}{187}{table.caption.102}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
