#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass acm-sigs
\begin_preamble
\usepackage{tikz}
\usetikzlibrary{positioning, fit,arrows,chains,shapes.geometric}
\usetikzlibrary{graphs,graphdrawing}
\usegdlibrary{force, layered, trees}


\renewcommand{\tabref}{\Tabref}
\renewcommand{\figref}{\Figref}
\renewcommand{\secref}{\Secref}

\interfootnotelinepenalty=10000

%%% Reenable Old Style Terms, as 1998 style linked from conference site
\def\terms{%\if@twocolumn
\section*{General Terms}
%\else \small
%\quotation\the\parskip
%\fi}
}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
title{How Well Sentence Embeddings Capture Meaning}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
def
\backslash
sharedaffiliation{%
\end_layout

\begin_layout Plain Layout


\backslash
end{tabular} 
\end_layout

\begin_layout Plain Layout


\backslash
begin{tabular}{c}}
\end_layout

\begin_layout Plain Layout

%
\end_layout

\begin_layout Plain Layout


\backslash
numberofauthors{4}
\end_layout

\begin_layout Plain Layout


\backslash
author{
\end_layout

\begin_layout Plain Layout


\backslash
alignauthor Lyndon White
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
email{lyndon.white@research.uwa.edu.au}
\end_layout

\begin_layout Plain Layout

%
\end_layout

\begin_layout Plain Layout


\backslash
alignauthor Roberto Togneri
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
email{roberto.togneri@uwa.edu.au}
\end_layout

\begin_layout Plain Layout

%
\end_layout

\begin_layout Plain Layout


\backslash
alignauthor Wei Liu
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
email{wei.liu@uwa.edu.au}
\end_layout

\begin_layout Plain Layout

%
\end_layout

\begin_layout Plain Layout


\backslash
and
\end_layout

\begin_layout Plain Layout


\backslash
alignauthor 
\end_layout

\begin_layout Plain Layout


\backslash
alignauthor Mohammed Bennamoun
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
email{mohammed.bennamoun@uwa.edu.au}
\end_layout

\begin_layout Plain Layout


\backslash
alignauthor 
\end_layout

\begin_layout Plain Layout

%
\end_layout

\begin_layout Plain Layout


\backslash
sharedaffiliation
\end_layout

\begin_layout Plain Layout


\backslash
affaddr{The University of Western Australia}
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
affaddr{35 Stirling Highway, Crawley, Western Australia}
\end_layout

\begin_layout Plain Layout

} 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcopyright{acmcopyright} 
\end_layout

\begin_layout Plain Layout


\backslash
conferenceinfo{ADCS,}{
\end_layout

\begin_layout Plain Layout

December 08-09, 2015, Parramatta, NSW, Australia
\end_layout

\begin_layout Plain Layout

} 
\end_layout

\begin_layout Plain Layout


\backslash
isbn{978-1-4503-4040-3/15/12}
\end_layout

\begin_layout Plain Layout


\backslash
acmPrice{
\backslash
$15.00} 
\end_layout

\begin_layout Plain Layout


\backslash
doi{http://dx.doi.org/10.1145/2838931.2838932}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
maketitle
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\entails}{\vDash}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Guide can be found at https://www.acm.org/sigs/publications/sigguide-v2.2sp
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
Several approaches for embedding a sentence into a vector space have been
 developed.
 However, it is unclear to what extent the sentence's position in the vector
 space reflects its semantic meaning, rather than other factors such as
 syntactic structure.
 Depending on the model used for the embeddings this will vary -- different
 models are suited for different down-stream applications.
 For applications such as machine translation and automated summarization,
 it is highly desirable to have semantic meaning encoded in the embedding.
 We consider this to be the quality of 
\emph on
semantic localization
\emph default
 for the model -- how well the sentences' meanings coincides with their
 embedding's position in vector space.
 Currently the semantic localization is assessed indirectly through practical
 benchmarks for specific applications.
 
\end_layout

\begin_layout Abstract
In this paper, we ground the semantic localization problem through a
\emph on
 
\emph default
semantic classification task.
 The task is to classify sentences according to their meaning.
 A SVM with a linear kernel is used to perform the classification using
 the sentence vectors as its input.
 The sentences from subsets of two corpora, the Microsoft Research Paraphrase
 corpus and the Opinosis corpus, were partitioned according to their semantic
 equivalence.
 These partitions give the target classes for the classification task.
 Several existing models, including URAE, PV--DM and PV--DBOW, were assessed
 against a bag of words benchmark.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{CCSXML}
\end_layout

\begin_layout Plain Layout

<ccs2012>
\end_layout

\begin_layout Plain Layout

<concept>
\end_layout

\begin_layout Plain Layout

<concept_id>10010147.10010178.10010179.10010184</concept_id>
\end_layout

\begin_layout Plain Layout

<concept_desc>Computing methodologies~Lexical semantics</concept_desc>
\end_layout

\begin_layout Plain Layout

<concept_significance>300</concept_significance>
\end_layout

\begin_layout Plain Layout

</concept> 
\end_layout

\begin_layout Plain Layout

<concept><concept_id>10010147.10010257.10010293.10010319</concept_id>
\end_layout

\begin_layout Plain Layout

<concept_desc>Computing methodologies~Learning latent representations</concept_d
esc>
\end_layout

\begin_layout Plain Layout

<concept_significance>300</concept_significance>
\end_layout

\begin_layout Plain Layout

</concept> 
\end_layout

\begin_layout Plain Layout

<concept> 
\end_layout

\begin_layout Plain Layout

<concept_id>10010147.10010257.10010293.10010294</concept_id>
\end_layout

\begin_layout Plain Layout

<concept_desc>Computing methodologies~Neural networks</concept_desc>
\end_layout

\begin_layout Plain Layout

<concept_significance>100</concept_significance> 
\end_layout

\begin_layout Plain Layout

</concept> 
\end_layout

\begin_layout Plain Layout

<concept> 
\end_layout

\begin_layout Plain Layout

<concept_id>10002951.10003317.10003359.10003360</concept_id>
\end_layout

\begin_layout Plain Layout

<concept_desc>Information systems~Test collections</concept_desc>
\end_layout

\begin_layout Plain Layout

<concept_significance>100</concept_significance> 
\end_layout

\begin_layout Plain Layout

</concept>
\end_layout

\begin_layout Plain Layout

</ccs2012> 
\end_layout

\begin_layout Plain Layout


\backslash
end{CCSXML}
\end_layout

\begin_layout Plain Layout


\backslash
ccsdesc[300]{Computing methodologies~Lexical semantics} 
\end_layout

\begin_layout Plain Layout


\backslash
ccsdesc[300]{Computing methodologies~Learning latent representations}
\end_layout

\begin_layout Plain Layout


\backslash
ccsdesc[100]{Computing methodologies~Neural networks} 
\end_layout

\begin_layout Plain Layout


\backslash
ccsdesc[100]{Information systems~Test collections}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
terms{Measurement, Performance, Experimentation}
\end_layout

\begin_layout Plain Layout


\backslash
category{H.3.1}{Information Storage and Retrieval}{Content Analysis and Indexing}[
Abstracting methods, Linguistic processing] 
\end_layout

\begin_layout Plain Layout


\backslash
category{I.2.7}{Artificial Intelligence}{Natural Language Processing}[Language
 parsing and understanding] 
\end_layout

\end_inset


\end_layout

\begin_layout Keywords
Semantic vector space representations, semantic consistency evaluation,
 sentence embeddings, word embeddings
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Sentence embeddings are often referred to as semantic vector space representatio
ns 
\begin_inset CommandInset citation
LatexCommand cite
key "iyyer2014generating"

\end_inset

.
 Embedding the meaning of a sentence into a vector space is expected to
 be very useful for natural language tasks.
 Vector representation of natural languages enables discourse analysis to
 take advantage of the array of tools available for computation in vector
 spaces.
 However, the embeddings of a sentence may encode a number of factors including
 semantic meaning, syntactic structure and topic.
 Since many of these embeddings are learned unsupervised on textual corpora
 using various models with different training objectives, it is not entirely
 clear the emphasis placed on each factor in the encoding.
 For applications where encoding semantic meaning is particularly desirable,
 such as machine translation and automatic summarization, it is crucial
 to be able to assess how well the embeddings capture the sentence's semantics.
 In other words, for successful application to these areas it is required
 that the embeddings generated by the models correctly encode meaning such
 that sentences with the same meaning are co-located in the vector space,
 and sentences with differing meanings are further away.
 However, few current models are directly trained to optimize for this criteria.
\end_layout

\begin_layout Standard
Currently sentence embeddings are often generated as a byproduct of unsupervised
, or semi-supervised, tasks.
 These tasks include: word prediction 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

; recreation of input, as in the auto-encoders of 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:RAE,SocherEtAl2011:PoolRAE"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "iyyer2014generating"

\end_inset

; and syntactic structural classification 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2013:CVG,socher2010PhraseEmbedding"

\end_inset

.
 As a result the vector representations of the input sentences learned by
 these models are tuned towards the chosen optimization task.
 When employing the embeddings produced as features for other tasks, the
 information captured by the embeddings often proved to be very useful:
 e.g.
 approaching or exceeding previous state-of-the-art results, in sentiment
 analysis 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:RAE,SocherMVRNN,le2014distributed"

\end_inset

 and paraphrase detection 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

.
 However these practical applications do not directly show how well meaning
 is captured by the embeddings.
\end_layout

\begin_layout Standard
This paper provides a new method to assess how well the models are capturing
 semantic information.
 A strict definition for the semantic equivalence of sentences is: that
 each sentence shall entail the other.
 Such mutually entailing sentences are called 
\emph on
paraphrases
\emph default
.
 In this paper we propose to use paraphrases to assess how well the true
 semantic space aligns with the vector space the models embed into.
 It thus assesses whether projecting a sentence via the models in to the
 vector space preserves meaning.
\end_layout

\begin_layout Standard
The evaluation corpora were prepared by grouping paraphrases from the Microsoft
 Research Paraphrase (MSRP) 
\begin_inset CommandInset citation
LatexCommand cite
key "msrParapharaCorpus"

\end_inset

 and Opinosis 
\begin_inset CommandInset citation
LatexCommand cite
key "ganesan2010opinosis"

\end_inset

 corpora.
 A semantic classification task was defined which assesses if the model's
 embeddings could be used to correctly classify sentences as belonging to
 the paraphrase group with semantically equivalent sentences.
 Ensuring that sentences of common meaning, but differing form are located
 in vector space together, is a challenging task and shows a model's semantic
 encoding strength.
 This assessment, together with out recent work in the area, allows for
 a better understanding of how these models work, and suggest new directions
 for the development in this area.
\end_layout

\begin_layout Standard
The assessment proposed in this paper adds to the recent work on semantic
 evaluation methods, such as the work of Gershamn and Tenenbaum 
\begin_inset CommandInset citation
LatexCommand cite
key "gershmanphrase"

\end_inset

 and of Ritter et.
 al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "RitterPosition"

\end_inset

.
 In particular, the real-world corpus based assessment in this paper is
 highly complementary to the structured artificial corpus based assessment
 of Ritter et.
 al.
 These methods are discussed in more detail in the next section.
\end_layout

\begin_layout Standard
The rest of the paper is organized into the following sections.
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Background"

\end_inset

 discusses the existing models being assessed, the traditional assessment
 methods, and the aforementioned more recent semantic correctness based
 assessments.
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Methodology"

\end_inset

 describes the processes by which the models are evaluated using our new
 method, and the parameters used in the evaluation.
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Corpus-Construction"

\end_inset

 continues into more details on the development of the evaluation corpora
 for the semantic classification evaluation task.
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Results-and-Discussion"

\end_inset

 details the results from evaluating the models and discusses the implications
 for their semantic consistency.
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Conclusion"

\end_inset

 closes the paper and suggests new directions for development.
\end_layout

\begin_layout Section
Background
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:Background"

\end_inset


\end_layout

\begin_layout Subsection
Models
\end_layout

\begin_layout Standard
Three well known sentence embedding methods are evaluated in this work.
 The compositional distributed model of the Unfoldering Recussive Autoencoder
 (URAE) by Socher et.
 al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

; and the two word content predictive models, Distributed Memory (PV-DM)
 and Distributed Bag of Words by Le and Mikolov 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

.
 In addition to these advanced sentence embedding models, a simple average
 of word embeddings, from Mikolov et.
 al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolovSkip"

\end_inset

, is also assessed.
 These models and their variant forms have been applied to a number of natural
 language processing tasks in the past, as detailed in the subsequent sections,
 but not to a real-sentence semantic classification task as described here.
\end_layout

\begin_layout Subsubsection
Unfolding Recursive Auto-Encoder (URAE)
\end_layout

\begin_layout Standard
The Unfolding Recursive Autoencoder (URAE) 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

 is an autoencoder based method.
 It functions by recursively using a single layer feedforward neural-network
 to combine embedded representations, following the parse tree.
 Its optimization target is to be be able to reverse (unfold) the merges
 and produce the original sentence.
 The central folding layer -- where the whole sentence is collapsed to a
 single embedding vector -- is the sentence representation.
\end_layout

\begin_layout Subsubsection
PV-DM
\end_layout

\begin_layout Standard
The Distributed Memory Paragraph Vectors (PV-DM) 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

 method is based on an extension of the Continuous Bag-of-Words word-embedding
 model 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013efficient"

\end_inset

.
 It is trained using a sliding window of words to predict the next word.
 The softmax predictor network is fed a word-embedding for each word in
 the window, plus an additional sentence embedding vector which is reused
 for all words in the sentence -- called the paragraph vector in 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

.
 These input embeddings can be concatenated or averaged; in the results
 below they were concatenated.
 During training both word and sentence vectors are allowed to vary, in
 evaluation (i.e.
 inference), the word vectors are locked and the sentence vector is trained
 until convergence on the prediction task occurs.
 
\end_layout

\begin_layout Subsubsection
PV-DBOW
\end_layout

\begin_layout Standard
Distributed Bag of Words Paragraph Vectors (PV-DBOW) 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

, is based on the Skip-gram model for word-embeddings, also from 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013efficient"

\end_inset

.
 In PV-DBOW a sentence vector is used as the sole input to a neural net.
 That network is tasked with predicting the words in the sentence.
 At each training iteration, the network is tasked to predict a number of
 words from the sentence, selected with a specified window size, using the
 sentence vector being trained as the input.
 As with PV-DM to infer embedding the rest of the network is locked, and
 only the sentence vector input allowed to vary, it is then trained to convergen
ce.
\end_layout

\begin_layout Subsubsection
Sum and Mean of Word Embeddings (SOWE and MOWE)
\end_layout

\begin_layout Standard
Taking the element-wise sum or mean of the word embeddings over all words
 in the sentence also produces a vector with the potential to encode meaning.
 Like traditional bag of words no order information is encoded, but the
 model can take into consideration word relations such as synonymity as
 encoded by the word vectors.
 The mean was used as baseline in 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

.
 The sum of word embeddings first considered in 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolovSkip"

\end_inset

 for short phrases, it was found to be an effective model for summarization
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "KaagebExtractiveSummaristation"

\end_inset

.
 The cosine distance, as is commonly used when comparing distances between
 embeddings, is invariant between sum and mean of word embeddings.
 Both sum and mean of word embeddings are computationally cheap models,
 particularly given pretrained word embeddings are available.
\end_layout

\begin_layout Subsection
General Evaluation Methods
\end_layout

\begin_layout Standard
As discussed in the introduction, current methods of evaluating the quality
 of embedding are on direct practical applications designed down-stream.
\end_layout

\begin_layout Standard
Evaluation on a Paraphrase Detection task takes the form of being presented
 with pairs of sentences and tasked with determining if the sentences are
 paraphrases or not.
 The MSRP Corpus, 
\begin_inset CommandInset citation
LatexCommand cite
key "msrParapharaCorpus"

\end_inset

 which we used in the semantic classification task, is intended for such
 use.
 This pairwise check is valuable, and does indicate to a certain extent
 if the embeddings are capturing meaning, or not.
 However, by considering groups of paraphrases, a deeper intuition can be
 gained on the arrangement of meaning within the vector space.
\end_layout

\begin_layout Standard
Sentiment Analysis is very commonly used task for evaluating embeddings.
 It was used both for the recursive autoencoder in 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:RAE"

\end_inset

 and for the paragraph vector models in 
\begin_inset CommandInset citation
LatexCommand cite
key "le2014distributed"

\end_inset

.
 Sentiment Analysis is classifying a text as positive or negative, or assigning
 a score as in the Sentiment Treebank 
\begin_inset CommandInset citation
LatexCommand cite
key "RvNTN"

\end_inset

.
 Determining the sentiment of a sentence is partially a semantic task, but
 it is lacking in several areas that would be required for meaning.
 For example, there is only an indirect requirement for the model to process
 the subject at all.
 Sentiment Analysis is a key task in natural language processing, but it
 is distinct from semantic meaning.
\end_layout

\begin_layout Standard
Document Classification is a classic natural language processing task.
 A particular case of this is topic categorization.
 Early work in the area goes back to 
\begin_inset CommandInset citation
LatexCommand cite
key "maron1961automatic"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "borko1963automatic"

\end_inset

.
 Much more recently it has been used to assess the convolution neural networks
 of 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/ZhangL15"

\end_inset

, where the articles of several news corpora were classified into categories
 such as 
\begin_inset Quotes eld
\end_inset

Sports
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

Business
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

Entertainment
\begin_inset Quotes erd
\end_inset

.
 A huge spectrum of different sentences are assigned to the same topic.
 It is thus too board and insufficiently specific to evaluate the consistency
 of meanings.
 Information retrieval can be seen as the inverse of the document classification
 task.
\end_layout

\begin_layout Standard
Information Retrieval is the task of identifying the documents which most
 match a query.
 Such document selection depends almost entirely on topic matching.
 Suitable results for information retrieval have no requirement to agree
 on meaning, though text with the same meaning are more likely to match
 the same queries.
\end_layout

\begin_layout Standard
The evaluation of semantic consistency requires a task which is fine grained,
 and preserving meaning.
 Document Classification and Information Retrieval are insufficiently fine-grain
ed.
 Sentiment Analysis does not preserve meaning, only semantic orientation.
 Paraphrase Detection is directly relevant to evaluating semantic constancy,
 however it is a binary choice based on a pairwise comparison -- a more
 spatial application is desirable for evaluating these vector spaces.
 Thus the current down-steam application tasks are not sufficient for assessing
 semantic consistency -- more specialized methods are required.
\end_layout

\begin_layout Subsection
Evaluations of Semantic Consistency
\begin_inset CommandInset label
LatexCommand label
name "sub:Evaluations-of-Semantic"

\end_inset


\end_layout

\begin_layout Standard
Semantic consistency for word embeddings is often measured using the analogy
 task.
 In an analogy the meta-relation: 
\family typewriter
A is to B as C is to D
\family default
.
 Mikolov et.
 al.
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013linguisticsubstructures"

\end_inset

 demonstrated that the word-embedding models are semantically consistent
 by showing that the semantic relations between words were reflected as
 a linear offset in the vector space.
 That is to say, for embeddings 
\begin_inset Formula $\tilde{x}_{a},\,\tilde{x}_{b},\,\tilde{x}_{c},\,\tilde{x}_{d}$
\end_inset

 corresponding to words A, B, C and D, respectively; it was tested that
 if for a strong relationship matching between A/B and C/D, then the offset
 vector would be approximately equal: 
\begin_inset Formula $\tilde{x}_{b}-\tilde{x}_{a}\approxeq\,\tilde{x}_{d}-\tilde{x}_{c}$
\end_inset

.
 Rearranging this in word space gets the often quoted example of 
\begin_inset Formula $\mathtt{King}-\mathtt{Man}+\mathtt{Woman}\approxeq\mathtt{Queen}$
\end_inset

, As man is to woman, king is to queen.
 In the rating task as described by 
\begin_inset CommandInset citation
LatexCommand cite
key "jurgens2012semeval"

\end_inset

, the goal is to rank such analogous word pairs based on the degree the
 relation matches.
 Thus to evaluate the word-embedding model using this task, it was a matter
 of sorting closeness of the corresponding offset vectors.
 Surprisingly strong results were found on this task
\begin_inset CommandInset citation
LatexCommand cite
key "mikolov2013linguisticsubstructures"

\end_inset

.
 It was thus demonstrated that word embeddings were not simply semantically
 consistent, but more so that this consistency was displayed as local linearity.
 This result gives confidence in the semantic quality of the word embeddings.
 However, this relationship analogy test cannot be performed for sentence
 embeddings.
\end_layout

\begin_layout Standard
Gershman et.
 al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "gershmanphrase"

\end_inset

, compares the distances of modified sentences in vector space, to the semantic
 distances ascribed to them by human raters.
 Like the analogy task for word vectors, this task requires ranking the
 targets based on the vector distance, however instead of rating on the
 strength of relationships it measures simply the similarities of the sentences
 to an original base sentence for each group.
 In that evaluation 30 simple base sentences of the form 
\family typewriter
A
\family default
 
\family typewriter
[adjective1]
\family default
 
\family typewriter
[noun1]
\family default
 
\family typewriter
[prepositional phrase]
\family default
 
\family typewriter
[adjective2]
\family default
 
\family typewriter
[noun2]
\family default
 were modified to produce 4 difference derived sentences.
 The derived sentences were produced by swapping the nouns, swapping the
 adjectives, reversing the positional phrase (so
\emph on
 
\family typewriter
\emph default
behind
\family default
 becomes 
\family typewriter
in front of
\family default
), and a paraphrase by doing all of the aforementioned changes.
 Human raters were tasked with sorting the transformed sentences in similarity
 to the base sentence.
 This evaluation found that the embedding models considered did not agree
 with the semantic similarity rankings placed by humans.
 While the sentence embedding models performed poorly on the distance ranking
 measure, it is also worth considering how they perform on a meaning classificat
ion task.
\end_layout

\begin_layout Standard
A meaning classification task was recently proposed by Ritter et.
 al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "RitterPosition"

\end_inset

, to classify sentences based on which spatial relationship was described.
 The task was to classify the sentence as describing: 
\emph on
Adhesion to Vertical Surface
\emph default
,
\emph on
 Support by Horizontal Surface
\emph default
,
\emph on
 Full Containment
\emph default
,
\emph on
 Partial Containment
\emph default
,
\emph on
 
\emph default
or
\emph on
 Support from Above.

\emph default
 In this evaluation also, the sentences took a very structured form: 
\family typewriter
There is a [noun1] [on/in] the [noun2]
\family default
.
 These highly structured sentences take advantage of the disconnection between
 word content and the positional relationship described to form a task that
 must be solved by a compositional understanding combining the understanding
 of the words.
 
\emph on

\begin_inset Quotes eld
\end_inset

The apple is on the refrigerator
\emph default

\begin_inset Quotes erd
\end_inset

 and 
\emph on

\begin_inset Quotes eld
\end_inset

The magnet is on the refrigerator
\begin_inset Quotes erd
\end_inset


\emph default
 belong to two separate spatial categories, even though the word content
 is very similar.
 Surprisingly, the simple model of adding word vectors outperformed compositiona
l models such as the recursive autoencoder.
 The result does have some limitation due to the highly artificial nature
 of the sentences, and the restriction to categorizing into a small number
 of classes based only on the meaning in terms of positional relationship.
 To generalize this task, in this paper we consider real world sentences
 being classed into groups according to their full semantic meaning.
\end_layout

\begin_layout Section
Methodology
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:Methodology"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement t
wide true
sideways false
status open

\begin_layout Plain Layout

\size footnotesize
\begin_inset CommandInset include
LatexCommand include
filename "figs/block_overview.pgf"

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Process-Diagram"

\end_inset

Process Diagram for the Evaluation of Semantic Consistency via our method
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
To evaluate how well a model's vectors capture the meaning of a sentence,
 a semantic classification task was defined.
 The task is to classify sentences into classes where each shares the same
 meaning.
 Each class is thus defined as a paraphrase groups.
 This is a far finer-grained task than topic classification.
 It is a multiclass classification problem, rather than the binary decision
 problem of paraphrase detection.
 Such multiclass classification requires the paraphrase groups to be projected
 into compact and distinct groups in the vector space.
 A model which produces such embeddings which are thus easily classifiable
 according to their meaning can been thus seen to have good semantic localizatio
n.
 
\end_layout

\begin_layout Standard
This semantic classification does not have direct practical application
 -- it is rare that the need will be to quantify sentences into groups with
 the same prior known meaning.
 Rather it serves as a measure to assess the models general suitability
 for other tasks requiring a model with consistency between meaning and
 embedding.
\end_layout

\begin_layout Standard
To evaluate the success at the task three main processes are involved, as
 shown in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:Process-Diagram"

\end_inset

: Corpus Preparation, Model Preparation, and the Semantic Classification
 task itself.
\end_layout

\begin_layout Subsection
Corpus Preparation
\end_layout

\begin_layout Standard
The construction of each of the corpora is detailed more fully in the next
 section.
 In brief: Two corpora were constructed by selecting subsets of the Microsoft
 Research Paraphrase (MSRP) 
\begin_inset CommandInset citation
LatexCommand cite
key "msrParapharaCorpus"

\end_inset

 and of the Opinosis 
\begin_inset CommandInset citation
LatexCommand cite
key "ganesan2010opinosis"

\end_inset

 corpora.
 The corpora were partitioned into groups of paraphrases -- sentences with
 the same meaning.
 Any paraphrase groups with less than three sentences were discarded.
 The paraphrase grouping was carried out manually for Opinosis, and automaticall
y for the MSRP corpus using the existing paraphrase pairings.
 The paraphrase groups divide the total semantic space of the corpora into
 discrete classes, where each class contains sentences sharing the same
 meaning.
\end_layout

\begin_layout Standard
It is by comparing the ability of the models to produce embeddings which
 can be classified back into these classes, that we can compare the real
 semantic space partitions to their corresponding vector embedding space
 regions.
\end_layout

\begin_layout Subsection
Model Preparation and Inferring Vectors
\end_layout

\begin_layout Standard
Prior to application to semantic classification, as with any task the models
 had to be pretrained.
 Here we use the term 
\emph on
pretraining
\emph default
 to differentiate the model training from the classifier training.
 The pretraining is not done using the evaluation corpora as they are both
 very small.
 Instead other data are used, and the inference/evaluation procedure given
 for each method was then used to produce the vectors for each sentence.
 The model parameters used are detailed below.
\end_layout

\begin_layout Subsubsection
Unfolding Recursive Auto-Encoder (URAE)
\end_layout

\begin_layout Standard
In this evaluation we make use of the pretrained network that Socher et.
 al.
 have graciously made available
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
name "http://www.socher.org/index.php/Main/DynamicPoolingAndUnfoldingRecursiveAutoencodersForParaphraseDetection"
target "http://www.socher.org/index.php/Main/DynamicPoolingAndUnfoldingRecursiveAutoencodersForParaphraseDetection"

\end_inset


\end_layout

\end_inset

, full information is available in the paper
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

.
 It is initialized on the unsupervised Collobert and Weston word embeddings
\begin_inset CommandInset citation
LatexCommand cite
key "collobert2008unified"

\end_inset

, and training on a subset of 150,000 sentences from the gigaword corpus.
 It produces embeddings with 200 dimensions.
 This pretrained model when used with dynamic pooling and other word based
 features performed very well on the MSRP corpus paraphrase detection.
 However in the evaluation below the dynamic pooling techniques are not
 used as they are only directly suitable for enhancing pairwise comparisons
 between sentences.
\end_layout

\begin_layout Subsubsection
Paragraph Vector Methods (PV-DM and PV-DBOW)
\end_layout

\begin_layout Standard
Both PV-DM and PV-DBOW, were evaluated using the GenSim implementation 
\begin_inset CommandInset citation
LatexCommand cite
key "rehurek_lrec"

\end_inset

 from the current 
\emph on
develop
\emph default
 branch
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
name "https://github.com/piskvorky/gensim/tree/develop/"
target "https://github.com/piskvorky/gensim/tree/develop/"

\end_inset


\end_layout

\end_inset

.
 Both were trained on approximately 1.2 million sentences from randomly selected
 Wikipedia articles, and the window size was set to 8 words, and the vectors
 were of 300 dimensions.
\end_layout

\begin_layout Subsubsection
Sum and Mean of Word Embeddings (SOWE and MOWE)
\end_layout

\begin_layout Standard
The word embeddings used for MOWE were taken from the Google News pretrained
 model
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
name "https://code.google.com/p/word2vec/"
target "https://code.google.com/p/word2vec/"

\end_inset


\end_layout

\end_inset

 based on the method described in 
\begin_inset CommandInset citation
LatexCommand cite
key "mikolovSkip"

\end_inset

.
 This has been trained on 100 million sentences from Google News.
 A small portion of the evaluation corpus did not have embeddings in the
 Google News model.
 These tokens were largely numerals, punctuation symbols, proper nouns and
 unusual spellings, as well as the stop-words: 
\begin_inset Quotes eld
\end_inset

and
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

a
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

of
\begin_inset Quotes erd
\end_inset

.
 These words were simply skipped.
 The resulting embeddings have 300 dimensions, like the word embeddings
 they were based on.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Subsubsection
Bag of Words (BOW and PCA BOW)
\end_layout

\begin_layout Standard
A bag of words (BOW) model is also presented as a baseline.
 There is a dimension in each vector embedding for the count of each token,
 including punctuation, in the sentence.
 In the Opinosis and MSRP subcorpora there were a total of 1,085 and 2,976
 unique tokens respectively, leading to BOW embeddings of corresponding
 dimensionality.
 As it is a distributional rather than distributed representation, the BOW
 model does not need any pretraining step.
 For comparison to the lower dimensional models Principle Component Analysis
 (PCA) was applied to the BOW embeddings to produce an additional baseline
 set of embeddings of 300 dimensions -- in line with PV-DM, PV-DBOW, SOWE,
 and MOWE models.
 It does not quite follow the steps shown in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:Process-Diagram"

\end_inset

, as the PCA pretraining step is performed on the training embeddings only
 during the SVM classification process, and it is used to infer the PCA
 BOW embeddings during the testing step.
 This avoids unfair information transfer where the PCA would otherwise be
 about to choose representations optimized for the whole set, including
 the test data.
 It was found that when the PCA model was allowed to cheat in this way it
 performed a few percentage points better.
 The bag of words models do not have any outside knowledge.
\end_layout

\begin_layout Subsection
Semantic Classification 
\end_layout

\begin_layout Standard
The core of this evaluation procedure is in the semantic classification
 step.
 A support vector machine (SVM), with a linear kernel, and class weighting
 was applied to the task of predicting which paraphrase group each sentence
 belongs to.
 Classification was verified using 3-fold cross-validation across different
 splits of the testing/training data, the average results are shown in this
 section.
 The splits were in proportion to the class size.
 For the smallest groups this means there were two training cases and one
 test case to classify.
 
\end_layout

\begin_layout Standard
In this paper, only a linear kernel was used, because a more powerful kernel
 such as RBF may be able to compensate for irregularities in the vector
 space, which makes model comparison more difficult.
 Scikit-learn 
\begin_inset CommandInset citation
LatexCommand cite
key "scikit-learn"

\end_inset

 was used to orchestrate the cross-validation and to interface with the
 LibLinear SVM implementation 
\begin_inset CommandInset citation
LatexCommand cite
key "LIBLIBEAR"

\end_inset

.
 As the linear SVM's classification success depends on how linearly separable
 the input data is, thus this assessed the quality of the localization of
 the paraphrase groupings embeddings.
\end_layout

\begin_layout Section
Corpus Construction
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:Corpus-Construction"

\end_inset


\end_layout

\begin_layout Subsection
Microsoft Research Paraphrased Grouped Subcorpus
\end_layout

\begin_layout Standard
The MSRP corpus is a very well established data set for the paraphrase detection
 task 
\begin_inset CommandInset citation
LatexCommand cite
key "msrParapharaCorpus"

\end_inset

.
 Sentences are presented as pairs which are either paraphrases, or not.
 A significant number of paraphrases appear in multiple different pairings.
 Using this information, groups of paraphrases can be formed.
\end_layout

\begin_layout Standard
The corpus was partitioned according to sentence meaning by taking the symmetric
 and transitive closures the set of paraphrase pairs.
 For example if sentences 
\emph on
A
\emph default
,
\emph on
 B
\emph default
,
\emph on
 C
\emph default
 and 
\emph on
D
\emph default
 were present in the original corpus as paraphrase pairs: 
\begin_inset Formula $A,\,B$
\end_inset

, 
\begin_inset Formula $D,\,A$
\end_inset

 and 
\begin_inset Formula $B,C$
\end_inset

 then the paraphrase group 
\begin_inset Formula $\{A,B,C,D\}$
\end_inset

 is found.
 Any paraphrase groups containing less than 3 phrases were discarded.
 The resulting sub-corpus has the breakdown as shown in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:msrp_corpus_hist"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement t
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/msrp_hist.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:msrp_corpus_hist"

\end_inset

 Break down of how many paraphrases groups are present in the MSRP subcorpus
 of which sizes.It contains a total of 859 unique sentences, broken up into
 273 paraphrase groups.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement t
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/opinosis_hist.pdf
	width 100col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:opinosis_corpus_hist"

\end_inset

 Break down of how many paraphrases groups are present in the Opinosis subcorpus
 of which sizes.
 It contains a total of 521 unique sentences, broken up into 89 paraphrase
 groups.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Opinosis Paraphrase Grouped Subcorpus
\end_layout

\begin_layout Standard
The Opinosis Corpus
\begin_inset CommandInset citation
LatexCommand cite
key "ganesan2010opinosis"

\end_inset

 was used as secondary source of original real-world text.
 It is sourced from several online review sites: Tripadvisor, Edmunds.com,
 and Amazon.com, and contains single sentence statements about hotels, cars
 and electronics.
 The advantage of this as a source for texts is that comments on the quality
 of services and products tend to be along similar lines.
 The review sentences are syntactically simpler than sentences from a news-wire
 corpus, and also contain less named entities.
 However, as they are from more casual communications, the adherence to
 grammar and spelling may be less formal.
 
\end_layout

\begin_layout Standard
Paraphrases were identified using the standard criterion: bidirectional
 entailment.
 For a paraphrase group 
\begin_inset Formula $\mathcal{S}$
\end_inset

 of sentences: 
\begin_inset Formula $\forall s_{1},\,s_{2}\in\mathcal{S},\quad s_{1}\entails s_{2}\quad\wedge\;s_{2}\entails s_{1}$
\end_inset

, every sentence in the group entails the every other sentence in the group.
 A stricter interpretation of bidirectional entailment was used, as compared
 to the 
\begin_inset Quotes eld
\end_inset

mostly bidirectional entailment
\begin_inset Quotes erd
\end_inset

 used in the MSRP corpus.
 The grouping was carried out manually.
 Where it was unclear as to the group a particular phrase should belong
 to it was left out of the corpus entirely.
 The general guidelines were as follows.
\end_layout

\begin_layout Itemize
Tense, Transitional Phrases, and Discourse and Pragmatic Markers were ignored.
\end_layout

\begin_layout Itemize
Statement intensity was coarsely quantized.
 
\end_layout

\begin_layout Itemize
Approximately equal quantitative and qualitative values were treated as
 synonymous.
\end_layout

\begin_layout Itemize
Sentences with entities mentioned explicitly were grouped separately from
 similar statements where they were implied.
\end_layout

\begin_layout Itemize
Sentences with additional information were grouped separately from those
 without that information.
\end_layout

\begin_layout Standard
The final point is the most significant change from the practices apparent
 in the construction of the MSRP corpus.
 Sentences with differing or additional information were classified as non-parap
hrases.
 This requirement comes from the definition of bidirectional entailment.
 For example, 
\emph on

\begin_inset Quotes eld
\end_inset

The staff were friendly and polite.
\begin_inset Quotes erd
\end_inset


\emph default
, 
\emph on

\begin_inset Quotes eld
\end_inset

The staff were polite.
\begin_inset Quotes erd
\end_inset


\emph default
 and 
\emph on

\begin_inset Quotes eld
\end_inset

The staff were friendly.
\begin_inset Quotes erd
\end_inset


\emph default
 are in three separate paraphrase groups.
 The creators of the MSRP corpus, however, note 
\begin_inset Quotes eld
\end_inset

...the majority of the equivalent pairs in this dataset exhibit `mostly bidirection
al entailments', with one sentence containing information `that differs'
 from or is not contained in the other.
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "msrParapharaCorpus"

\end_inset

.
 While this does lead to more varied paraphrases; it strays from the strict
 linguistic definition of a paraphrase, which complicates the evaluation
 of the semantic space attempted here.
 This stricter adherence to bidirectional entailment resulted in finer separatio
n of groups, which makes this a more challenging corpus.
\end_layout

\begin_layout Standard
After the corpus had been broken into paraphrase groups some simple post-process
ing was done.
 Several artifacts present in the original corpus were removed, such as
 substituting the ampersand symbol for 
\family typewriter
&amp
\family default
.
 Any paraphrase groups containing identical sentences were merged, and duplicate
s removed.
 Finally, any group with less than three phrases was discarded.
 With this complete the breakdown is as in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:opinosis_corpus_hist"

\end_inset

.
\end_layout

\begin_layout Standard
Further information on the construction of the corpora in this section,
 and download links are available online.
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset href
LatexCommand href
name "http://white.ucc.asn.au/resources/paraphrase_grouped_corpora/"
target "http://white.ucc.asn.au/resources/paraphrase_grouped_corpora/"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Results and Discussion
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:Results-and-Discussion"

\end_inset


\end_layout

\begin_layout Subsection
Classification Results and Discussion
\end_layout

\begin_layout Standard
The results of performing the evaluation method described in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sec:Methodology"

\end_inset

 are shown in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "tab:results"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="8" columns="3">
<features rotate="0" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
MSRP Subcorpus
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Opinosis Subcorpus
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 PV-DM
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
78.00%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
38.26%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 PV-DBOW
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
89.93%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
32.19%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 URAE
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
51.14%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
20.86%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 MOWE
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
97.91%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
69.30%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
SOWE
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
98.02%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
68.75%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 BOW
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
98.37%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
65.23%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 PCA BOW
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
97.96%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
54.43%
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:results"

\end_inset

The semantic classification accuracy of the various models across the two
 evaluation corpora.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
While the relative performance of the models is similar between the corpora,
 the absolute performance differs.
 On the absolute scale, all the models perform much better on the MSRP subcorpus
 than on the Opinosis subcorpus.
 This can be attributed to the significantly more distinct classes in the
 MSRP subcorpus.
 The Opinosis subcorpus draws a finer line between sentences with similar
 meanings.
 As discussed earlier, for example there is a paraphrase group for 
\emph on

\begin_inset Quotes eld
\end_inset

The staff were polite.
\begin_inset Quotes erd
\end_inset


\emph default
, another for 
\emph on

\begin_inset Quotes eld
\end_inset

The staff were friendly.
\begin_inset Quotes erd
\end_inset

,
\emph default
 and a third for 
\emph on

\begin_inset Quotes eld
\end_inset

The staff were friendly and polite.
\begin_inset Quotes erd
\end_inset


\emph default
.
 Under the guidelines used for paraphrases in MSRP, these would all have
 been considered the same group.
 Secondly, there is a much wider range of topics in the MSRP.
 Thus the paraphrase groups with different meanings in MSRP corpus are also
 more likely to have different topic entirely than those from Opinosis.
 Thus the the ground truth of the semantics separability of phrases from
 the MSRP corpus is higher than for Opinosis, making the semantic classification
 of the Opinosis subcorpus is a more challenging task.
\end_layout

\begin_layout Standard
The URAE model performs the worst of all models evaluated.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "KaagebExtractiveSummaristation"

\end_inset

 is was suggested that the URAE's poor performance at summarizing the Opinosis
 corpus could potentially be attributed to the less formally structured
 product reviews -- the URAE being a highly structured compositional model.
 However, here it also performed poorly on the MSRP -- which it was created
 for 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

.
 The exact same model from 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

 was used here -- though this did put it at a dimensional disadvantage over
 the other models having 200 dimensions to the other's 300.
 The key difference from 
\begin_inset CommandInset citation
LatexCommand cite
key "SocherEtAl2011:PoolRAE"

\end_inset

, beyond the changing to a multiclass classification problem, was the lack
 of the complementary word-level features as used in the dynamic pooling
 layer.
 This suggests the model could benefit from such world level features --
 as the very strong performance of the word-based model indicates.
\end_layout

\begin_layout Standard
The word based models, MOWE, SOWE, BOW and PCA BOW, performed very well.
 This suggests that word choice is a very significant factor in determining
 meaning; so much so that the models which can make use of word order informatio
n, URAE and PV-DM, were significantly outperformed by methods which made
 more direct use of the word content.
 
\end_layout

\begin_layout Standard
The very high performance of the BOW maybe attributed to its very high dimension
ality, though the MOWE and SOWE performed similarly.
 The PCA step can be considered as being similar to choosing an optimal
 set of words to keep so as to maximum variability in the bag of words.
 It loses little performance, even though decreasing vector size by an order
 of magnitude -- particularly on the easier MSRP dataset.
\end_layout

\begin_layout Subsection
Model Agreement
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/msrp_agree.pdf
	width 100col%

\end_inset


\begin_inset space \hspace*{\fill}
\end_inset


\begin_inset Graphics
	filename figs/opinosis_agree.pdf
	width 100col%

\end_inset


\begin_inset space \enspace{}
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:misclass_agreement"

\end_inset

 The misclassification agreement between each of the models for the MSRP
 (left) and Opinosis (right) subcorpora.
 Below each model name is the total mistakes made.
 The denominator of each fraction is the number of test cases incorrectly
 classified by both models.
 The numerator is the portion of those misclassifications which were classified
 in the same (incorrect) way by both models.
 The shading is in-proportion to that fraction.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The misclassifications of the models can be compared.
 By selecting one of the test/train folds from the classification task above,
 and comparing the predicted classifications for each test-set sentence,
 the similarities of the models were assessed.
 The heatmaps in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:misclass_agreement"

\end_inset

 show the agreement in errors.
 Here misclassification agreement is given as an approximation to 
\begin_inset Formula $P(m_{1}(x)=m_{2}(x)\,|\,m_{1}(x)\ne y\,\wedge\,m_{2}(x)\ne y)$
\end_inset

, for a randomly selected sentence 
\begin_inset Formula $x$
\end_inset

, with ground truth classification 
\begin_inset Formula $y$
\end_inset

, where the models 
\begin_inset Formula $m_{1}$
\end_inset

 and 
\begin_inset Formula $m_{2}$
\end_inset

 are used to produce classifications.
 Only considering the cases where both models were incorrect, rather than
 simple agreement, avoids the analysis being entirely dominated by the agreement
 of the models with the ground truth.
 
\end_layout

\begin_layout Standard
The word based models showed significant agreement.
 Unsurprisingly MOWE and SOWE have almost complete agreement in both evaluations.
 The other models showed less agreement -- while they got many of the same
 cases wrong the models produced different misclassifications.
 This overall suggests that the various full sentence models are producing
 substantially dissimilar maps from meaning to vector space.
 Thus it seems reasonable that using a ensemble approach between multiple
 sentence models and one word-based model would produce strong results.
 Yin and Schtze 
\begin_inset CommandInset citation
LatexCommand cite
key "Yin2015"

\end_inset

 found this successful when combining different word embedding models.
\end_layout

\begin_layout Subsection
Limitations
\end_layout

\begin_layout Standard
This evaluation has some limitations.
 As with all such empirical evaluations of machine learning models, a more
 optimal choice of hyper-parameters and training data will have an impact
 on the performance.
 In particular, if the model training was on the evaluation data the models
 would be expected to be better able to position their embedding.
 This was however unfeasible due to the small sizes of the datasets used
 for evaluation, and would not reflect real word application of the models
 to data not prior seen.
 Beyond the limitation of the use of the datasets is their contents.
\end_layout

\begin_layout Standard
The paraphrase groups were not selected to be independent of the word content
 overlap -- they were simply collected on commonality of meaning from real
 world sourced corpora.
 This is a distinct contrast to the the work of Ritter et.
 al.
\begin_inset CommandInset citation
LatexCommand cite
key "RitterPosition"

\end_inset

 discussed in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "sub:Evaluations-of-Semantic"

\end_inset

 where the classes were chosen to not have meaningful word overlap.
 However our work is complementary to theirs, and our findings are well
 aligned.
 The key difference in performance is the magnitude of the performance of
 the sum of word embeddings (comparable to the mean of word embeddings evaluated
 here).
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "RitterPosition"

\end_inset

 the word embedding model performed similarly to the best of the more complex
 models.
 In the results presented above we find that the word embedding based model
 performs significantly beyond the more complex models.
 This can be attributed to the word overlap in the paraphrase groups --
 in real-world speech people trying to say the same thing do in-fact use
 the same words very often.
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:Conclusion"

\end_inset


\end_layout

\begin_layout Standard
A method was presented, to evaluate the semantic localization of sentence
 embedding models.
 Semantically equivalent sentences are those which exhibit bidirectional
 entailment -- they each imply the truth of the other.
 Paraphrases are semantically equivalent.
 The evaluation method is a semantic classification task -- to classify
 sentences as belonging to a paraphrase group of semantically equivalent
 sentences.
 The datasets used were derived from subsets of existing sources, the MRSP
 and the Opinosis corpora.
 The relative performance of various models was consistent across the two
 tasks, though differed on an absolute scale.
\end_layout

\begin_layout Standard
The word embedding and bag of word models performed best, followed by the
 paragraph vector models, with the URAE trailing in both tests.
 The strong performance of the sum and mean of word embeddings (SOWE and
 MOWE) compared to the more advanced models aligned with the results of
 Ritter et.
 al.
\begin_inset CommandInset citation
LatexCommand cite
key "RitterPosition"

\end_inset

.
 The difference in performance presented here for real-word sentences, were
 more marked than for the synthetic sentence used by Ritter et.
 al.
 This may be attributed to real-world sentences often having meaning overlap
 correspondent to word overlap -- as seen also in the very strong performance
 of bag of words.
 Combining the result of this work with those of Ritter et.
 al., it can be concluded that summing word vector representations is a practical
 and surprisingly effective method for encoding the meaning of a sentence.
 
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Subsubsection*
Acknowledgement
\end_layout

\begin_layout Standard
This research is supported by the Australian Postgraduate Award, and partially
 funded by Australian Research Council DP150102405 and LP110100050.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "../../../Resources/master_bibliography/master"
options "abbrv"

\end_inset


\end_layout

\end_body
\end_document
