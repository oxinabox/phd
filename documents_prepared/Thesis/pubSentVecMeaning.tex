\documentclass[]{book}
\input{preamble.tex}


\begin{document}
\global\long\def\entails{\vDash}

\chapter{How Well Sentence Embeddings Capture Meaning}
\preamble{This paper was presented at the 20th Australasian Document Computing Symposium, in 2015.}

\begin{abstract}
Several approaches for embedding a sentence into a vector space have
been developed. However, it is unclear to what extent the sentence's
position in the vector space reflects its semantic meaning, rather
than other factors such as syntactic structure. Depending on the model
used for the embeddings this will vary -- different models are suited
for different down-stream applications. For applications such as machine
translation and automated summarization, it is highly desirable to
have semantic meaning encoded in the embedding. We consider this to
be the quality of \emph{semantic localization} for the model -- how
well the sentences' meanings coincides with their embedding's position
in vector space. Currently the semantic localization is assessed indirectly
through practical benchmarks for specific applications. 

In this paper, we ground the semantic localization problem through
a \emph{semantic classification} task. The task is to classify sentences
according to their meaning. A SVM with a linear kernel is used to
perform the classification using the sentence vectors as its input.
The sentences from subsets of two corpora, the Microsoft Research
Paraphrase corpus and the Opinosis corpus, were partitioned according
to their semantic equivalence. These partitions give the target classes
for the classification task. Several existing models, including URAE,
PV--DM and PV--DBOW, were assessed against a bag of words benchmark.
\end{abstract}


\section{Introduction}

Sentence embeddings are often referred to as semantic vector space
representations \cite{iyyer2014generating}. Embedding the meaning
of a sentence into a vector space is expected to be very useful for
natural language tasks. Vector representation of natural languages
enables discourse analysis to take advantage of the array of tools
available for computation in vector spaces. However, the embeddings
of a sentence may encode a number of factors including semantic meaning,
syntactic structure and topic. Since many of these embeddings are
learned unsupervised on textual corpora using various models with
different training objectives, it is not entirely clear the emphasis
placed on each factor in the encoding. For applications where encoding
semantic meaning is particularly desirable, such as machine translation
and automatic summarization, it is crucial to be able to assess how
well the embeddings capture the sentence's semantics. In other words,
for successful application to these areas it is required that the
embeddings generated by the models correctly encode meaning such that
sentences with the same meaning are co-located in the vector space,
and sentences with differing meanings are further away. However, few
current models are directly trained to optimize for this criteria.

Currently sentence embeddings are often generated as a byproduct of
unsupervised, or semi-supervised, tasks. These tasks include: word
prediction \cite{le2014distributed}; recreation of input, as in the
auto-encoders of \cite{SocherEtAl2011:RAE,SocherEtAl2011:PoolRAE}
and \cite{iyyer2014generating}; and syntactic structural classification
\cite{SocherEtAl2013:CVG,socher2010PhraseEmbedding}. As a result
the vector representations of the input sentences learned by these
models are tuned towards the chosen optimization task. When employing
the embeddings produced as features for other tasks, the information
captured by the embeddings often proved to be very useful: e.g. approaching
or exceeding previous state-of-the-art results, in sentiment analysis
\cite{SocherEtAl2011:RAE,SocherMVRNN,le2014distributed} and paraphrase
detection \cite{SocherEtAl2011:PoolRAE}. However these practical
applications do not directly show how well meaning is captured by
the embeddings.

This paper provides a new method to assess how well the models are
capturing semantic information. A strict definition for the semantic
equivalence of sentences is: that each sentence shall entail the other.
Such mutually entailing sentences are called \emph{paraphrases}. In
this paper we propose to use paraphrases to assess how well the true
semantic space aligns with the vector space the models embed into.
It thus assesses whether projecting a sentence via the models in to
the vector space preserves meaning.

The evaluation corpora were prepared by grouping paraphrases from
the Microsoft Research Paraphrase (MSRP) \cite{msrParapharaCorpus}
and Opinosis \cite{ganesan2010opinosis} corpora. A semantic classification
task was defined which assesses if the model's embeddings could be
used to correctly classify sentences as belonging to the paraphrase
group with semantically equivalent sentences. Ensuring that sentences
of common meaning, but differing form are located in vector space
together, is a challenging task and shows a model's semantic encoding
strength. This assessment, together with out recent work in the area,
allows for a better understanding of how these models work, and suggest
new directions for the development in this area.

The assessment proposed in this paper adds to the recent work on semantic
evaluation methods, such as the work of Gershamn and Tenenbaum \cite{gershmanphrase}
and of Ritter et. al. \cite{RitterPosition}. In particular, the real-world
corpus based assessment in this paper is highly complementary to the
structured artificial corpus based assessment of Ritter et. al. These
methods are discussed in more detail in the next section.

The rest of the paper is organized into the following sections. \Cref{sec:Background}
discusses the existing models being assessed, the traditional assessment
methods, and the aforementioned more recent semantic correctness based
assessnements. \Cref{sec:Methodology} describes the processes by which
the models are evaluated using our new method, and the parameters
used in the evaluation. \Cref{sec:Corpus-Construction} continues into
more details on the development of the evaluation corpora for the
semantic classification evaluation task. \Cref{sec:Results-and-Discussion}
details the results from evaluating the models and discusses the implications
for their semantic consistency. \Cref{sec:Conclusion} closes the paper
and suggests new directions for development.


\section{Background}

\label{sec:Background}


\subsection{Models}

Three well known sentence embedding methods are evaluated in this
work. The compositional distributed model of the Unfoldering Recussive
Autoencoder (URAE) by Socher et. al. \cite{SocherEtAl2011:PoolRAE};
and the two word content predictive models, Distributed Memory (PV-DM)
and Distributed Bag of Words by Le and Mikolov \cite{le2014distributed}.
In addition to these advanced sentence embedding models, a simple
average of word embeddings, from Mikolov et. al. \cite{mikolovSkip},
is also assessed. These models and their variant forms have been applied
to a number of natural language processing tasks in the past, as detailed
in the subsequent sections, but not to a real-sentence semantic classification
task as described here.


\subsubsection{Unfolding Recursive Auto-Encoder (URAE)}

The Unfolding Recursive Autoencoder (URAE) \cite{SocherEtAl2011:PoolRAE}
is an autoencoder based method. It functions by recursively using
a single layer feedforward neural-network to combine embedded representations,
following the parse tree. Its optimization target is to be be able
to reverse (unfold) the merges and produce the original sentence.
The central folding layer -- where the whole sentence is collapsed
to a single embedding vector -- is the sentence representation.


\subsubsection{PV-DM}

The Distributed Memory Paragraph Vectors (PV-DM) \cite{le2014distributed}
method is based on an extension of the Continuous Bag-of-Words word-embedding
model \cite{mikolov2013efficient}. It is trained using a sliding
window of words to predict the next word. The softmax predictor network
is fed a word-embedding for each word in the window, plus an additional
sentence embedding vector which is reused for all words in the sentence
-- called the paragraph vector in \cite{le2014distributed}. These
input embeddings can be concatenated or averaged; in the results below
they were concatenated. During training both word and sentence vectors
are allowed to vary, in evaluation (i.e. inference), the word vectors
are locked and the sentence vector is trained until convergence on
the prediction task occurs. 


\subsubsection{PV-DBOW}

Distributed Bag of Words Paragraph Vectors (PV-DBOW) \cite{le2014distributed},
is based on the Skip-gram model for word-embeddings, also from \cite{mikolov2013efficient}.
In PV-DBOW a sentence vector is used as the sole input to a neural
net. That network is tasked with predicting the words in the sentence.
At each training iteration, the network is tasked to predict a number
of words from the sentence, selected with a specified window size,
using the sentence vector being trained as the input. As with PV-DM
to infer embedding the rest of the network is locked, and only the
sentence vector input allowed to vary, it is then trained to convergence.


\subsubsection{Sum and Mean of Word Embeddings (SOWE and MOWE)}

Taking the element-wise sum or mean of the word embeddings over all
words in the sentence also produces a vector with the potential to
encode meaning. Like traditional bag of words no order information
is encoded, but the model can take into consideration word relations
such as synonymy as encoded by the word vectors. The mean was used
as baseline in \cite{le2014distributed}. The sum of word embeddings
first considered in \cite{mikolovSkip} for short phrases, it was
found to be an effective model for summarization in \cite{KaagebExtractiveSummaristation}.
The cosine distance, as is commonly used when comparing distances
between embeddings, is invariant between sum and mean of word embeddings.
Both sum and mean of word embeddings are computationally cheap models,
particularly given pretrained word embeddings are available.


\subsection{General Evaluation Methods}

As discussed in the introduction, current methods of evaluating the
quality of embedding are on direct practical applications designed
down-stream.

Evaluation on a Paraphrase Detection task takes the form of being
presented with pairs of sentences and tasked with determining if the
sentences are paraphrases or not. The MSRP Corpus, \cite{msrParapharaCorpus}
which we used in the semantic classification task, is intended for
such use. This pairwise check is valuable, and does indicate to a
certain extent if the embeddings are capturing meaning, or not. However,
by considering groups of paraphrases, a deeper intuition can be gained
on the arrangement of meaning within the vector space.

Sentiment Analysis is very commonly used task for evaluating embeddings.
It was used both for the recursive autoencoder in \cite{SocherEtAl2011:RAE}
and for the paragraph vector models in \cite{le2014distributed}.
Sentiment Analysis is classifying a text as positive or negative,
or assigning a score as in the Sentiment Treebank \cite{RvNTN}. Determining
the sentiment of a sentence is partially a semantic task, but it is
lacking in several areas that would be required for meaning. For example,
there is only an indirect requirement for the model to process the
subject at all. Sentiment Analysis is a key task in natural language
processing, but it is distinct from semantic meaning.

Document Classification is a classic natural language processing task.
A particular case of this is topic categorization. Early work in the
area goes back to \cite{maron1961automatic} and \cite{borko1963automatic}.
Much more recently it has been used to assess the convolution neural
networks of \cite{DBLP:journals/corr/ZhangL15}, where the articles
of several news corpora were classified into categories such as ``Sports'',
``Business'' and ``Entertainment''. A huge spectrum of different
sentences are assigned to the same topic. It is thus too board and
insufficiently specific to evaluate the consistency of meanings. Information
retrieval can be seen as the inverse of the document classification
task.

Information Retrieval is the task of identifying the documents which
most match a query. Such document selection depends almost entirely
on topic matching. Suitable results for information retrieval have
no requirement to agree on meaning, though text with the same meaning
are more likely to match the same queries.

The evaluation of semantic consistency requires a task which is fine
grained, and preserving meaning. Document Classification and Information
Retrieval are insufficiently fine-grained. Sentiment Analysis does
not preserve meaning, only semantic orientation. Paraphrase Detection
is directly relevant to evaluating semantic constancy, however it
is a binary choice based on a pairwise comparison -- a more spatial
application is desirable for evaluating these vector spaces. Thus
the current down-steam application tasks are not sufficient for assessing
semantic consistency -- more specialized methods are required.


\subsection{Evaluations of Semantic Consistency\label{sub:Evaluations-of-Semantic}}

Semantic consistency for word embeddings is often measured using the
analogy task. In an analogy the meta-relation: \texttt{A is to B as
C is to D}. Mikolov et. al.\cite{mikolov2013linguisticsubstructures}
demonstrated that the word-embedding models are semantically consistent
by showing that the semantic relations between words were reflected
as a linear offset in the vector space. That is to say, for embeddings
$\tilde{x}_{a},\,\tilde{x}_{b},\,\tilde{x}_{c},\,\tilde{x}_{d}$ corresponding
to words A, B, C and D, respectively; it was tested that if for a
strong relationship matching between A/B and C/D, then the offset
vector would be approximately equal: $\tilde{x}_{b}-\tilde{x}_{a}\approxeq\,\tilde{x}_{d}-\tilde{x}_{c}$.
Rearranging this in word space gets the often quoted example of $\mathtt{King}-\mathtt{Man}+\mathtt{Woman}\approxeq\mathtt{Queen}$,
As man is to woman, king is to queen. In the rating task as described
by \cite{jurgens2012semeval}, the goal is to rank such analogous
word pairs based on the degree the relation matches. Thus to evaluate
the word-embedding model using this task, it was a matter of sorting
closeness of the corresponding offset vectors. Surprisingly strong
results were found on this task\cite{mikolov2013linguisticsubstructures}.
It was thus demonstrated that word embeddings were not simply semantically
consistent, but more so that this consistency was displayed as local
linearity. This result gives confidence in the semantic quality of
the word embeddings. However, this relationship analogy test cannot
be performed for sentence embeddings.

Gershman et. al. \cite{gershmanphrase}, compares the distances of
modified sentences in vector space, to the semantic distances ascribed
to them by human raters. Like the analogy task for word vectors, this
task requires ranking the targets based on the vector distance, however
instead of rating on the strength of relationships it measures simply
the similarities of the sentences to an original base sentence for
each group. In that evaluation 30 simple base sentences of the form
\texttt{A} \texttt{{[}adjective1{]}} \texttt{{[}noun1{]}} \texttt{{[}prepositional
phrase{]}} \texttt{{[}adjective2{]}} \texttt{{[}noun2{]}} were modified
to produce 4 difference derived sentences. The derived sentences were
produced by swapping the nouns, swapping the adjectives, reversing
the positional phrase (so\emph{ }\texttt{behind} becomes \texttt{in
front of}), and a paraphrase by doing all of the aforementioned changes.
Human raters were tasked with sorting the transformed sentences in
similarity to the base sentence. This evaluation found that the embedding
models considered did not agree with the semantic similarity rankings
placed by humans. While the sentence embedding models performed poorly
on the distance ranking measure, it is also worth considering how
they perform on a meaning classification task.

A meaning classification task was recently proposed by \citet{RitterPosition}, to classify sentences based on which spatial
relationship was described. The task was to classify the sentence
as describing: \emph{Adhesion to Vertical Surface},\emph{ Support
by Horizontal Surface},\emph{ Full Containment},\emph{ Partial Containment},\emph{
}or\emph{ Support from Above.} In this evaluation also, the sentences
took a very structured form: \texttt{There is a {[}noun1{]} {[}on/in{]}
the {[}noun2{]}}. These highly structured sentences take advantage
of the disconnection between word content and the positional relationship
described to form a task that must be solved by a compositional understanding
combining the understanding of the words. \emph{``The apple is on
the refrigerator}'' and \emph{``The magnet is on the refrigerator''}
belong to two separate spatial categories, even though the word content
is very similar. Surprisingly, the simple model of adding word vectors
outperformed compositional models such as the recursive autoencoder.
The result does have some limitation due to the highly artificial
nature of the sentences, and the restriction to categorizing into
a small number of classes based only on the meaning in terms of positional
relationship. To generalize this task, in this paper we consider real
world sentences being classed into groups according to their full
semantic meaning.


\section{Methodology}

\label{sec:Methodology}

\begin{figure}
\resizebox{\textwidth}{!}{\input{figs/block_overview}}

\caption{\label{fig:Process-Diagram} Process Diagram for the Evaluation of Semantic Consistency via our method}
\end{figure}


To evaluate how well a model's vectors capture the meaning of a sentence,
a semantic classification task was defined. The task is to classify
sentences into classes where each shares the same meaning. Each class
is thus defined as a paraphrase groups. This is a far finer-grained
task than topic classification. It is a multiclass classification
problem, rather than the binary decision problem of paraphrase detection.
Such multiclass classification requires the paraphrase groups to be
projected into compact and distinct groups in the vector space. A
model which produces such embeddings which are thus easily classifiable
according to their meaning can been thus seen to have good semantic
localization. 

This semantic classification does not have direct practical application
-- it is rare that the need will be to quantify sentences into groups
with the same prior known meaning. Rather it serves as a measure to
assess the models general suitability for other tasks requiring a
model with consistency between meaning and embedding.

To evaluate the success at the task three main processes are involved,
as shown in \Cref{fig:Process-Diagram}: Corpus Preparation, Model Preparation,
and the Semantic Classification task itself.


\subsection{Corpus Preparation}

The construction of each of the corpora is detailed more fully in
the next section. In brief: Two corpora were constructed by selecting
subsets of the Microsoft Research Paraphrase (MSRP) \citep{msrParapharaCorpus}
and of the Opinosis \citep{ganesan2010opinosis} corpora. The corpora
were partitioned into groups of paraphrases -- sentences with the
same meaning. Any paraphrase groups with less than three sentences
were discarded. The paraphrase grouping was carried out manually for
Opinosis, and automatically for the MSRP corpus using the existing
paraphrase pairings. The paraphrase groups divide the total semantic
space of the corpora into discrete classes, where each class contains
sentences sharing the same meaning.

It is by comparing the ability of the models to produce embeddings
which can be classified back into these classes, that we can compare
the real semantic space partitions to their corresponding vector embedding
space regions.


\subsection{Model Preparation and Inferring Vectors}

Prior to application to semantic classification, as with any task
the models had to be pretrained. Here we use the term \emph{pretraining}
to differentiate the model training from the classifier training.
The pretraining is not done using the evaluation corpora as they are
both very small. Instead other data are used, and the inference/evaluation
procedure given for each method was then used to produce the vectors
for each sentence. The model parameters used are detailed below.


\subsubsection{Unfolding Recursive Auto-Encoder (URAE)}

In this evaluation we make use of the pretrained network that Socher
et. al. have graciously made available\footnote{\href{http://www.socher.org/index.php/Main/DynamicPoolingAndUnfoldingRecursiveAutoencodersForParaphraseDetection}{http://www.socher.org/index.php/Main/DynamicPoolingAndUnfoldingRecursiveAutoencodersForParaphraseDetection}},
full information is available in the paper \citep{SocherEtAl2011:PoolRAE}.
It is initialized on the unsupervised Collobert and Weston word embeddings \citep{collobert2008unified},
and training on a subset of 150,000 sentences from the gigaword corpus.
It produces embeddings with 200 dimensions. This pretrained model
when used with dynamic pooling and other word based features performed
very well on the MSRP corpus paraphrase detection. However in the
evaluation below the dynamic pooling techniques are not used as they
are only directly suitable for enhancing pairwise comparisons between
sentences.


\subsubsection{Paragraph Vector Methods (PV-DM and PV-DBOW)}

Both PV-DM and PV-DBOW, were evaluated using the GenSim implementation
\citep{rehurek_lrec} from the current \emph{develop} branch\footnote{\href{https://github.com/piskvorky/gensim/tree/develop/}{https://github.com/piskvorky/gensim/tree/develop/}}.
Both were trained on approximately 1.2 million sentences from randomly
selected Wikipedia articles, and the window size was set to 8 words,
and the vectors were of 300 dimensions.


\subsubsection{Sum and Mean of Word Embeddings (SOWE and MOWE)}

The word embeddings used for MOWE were taken from the Google News
pretrained model\footnote{\href{https://code.google.com/p/word2vec/}{https://code.google.com/p/word2vec/}}
based on the method described in \citep{mikolovSkip}. This has been
trained on 100 million sentences from Google News. A small portion
of the evaluation corpus did not have embeddings in the Google News
model. These tokens were largely numerals, punctuation symbols, proper
nouns and unusual spellings, as well as the following stop-words: ``and'',
``a'' and ``of''. These words were simply skipped. The resulting
embeddings have 300 dimensions, like the word embeddings they were
based on.



\subsubsection{Bag of Words (BOW and PCA BOW)}

A bag of words (BOW) model is also presented as a baseline. There
is a dimension in each vector embedding for the count of each token,
including punctuation, in the sentence. In the Opinosis and MSRP subcorpora
there were a total of 1,085 and 2,976 unique tokens respectively,
leading to BOW embeddings of corresponding dimensionality. As it is
a distributional rather than distributed representation, the BOW model
does not need any pretraining step. For comparison to the lower dimensional
models Principle Component Analysis (PCA) was applied to the BOW embeddings
to produce an additional baseline set of embeddings of 300 dimensions
-- in line with PV-DM, PV-DBOW, SOWE, and MOWE models. It does not
quite follow the steps shown in \Cref{fig:Process-Diagram}, as the
PCA pretraining step is performed on the training embeddings only
during the SVM classification process, and it is used to infer the
PCA BOW embeddings during the testing step. This avoids unfair information
transfer where the PCA would otherwise be about to choose representations
optimized for the whole set, including the test data. It was found
that when the PCA model was allowed to cheat in this way it performed
a few percentage points better. The bag of words models do not have
any outside knowledge.


\subsection{Semantic Classification }

The core of this evaluation procedure is in the semantic classification
step. A support vector machine (SVM), with a linear kernel, and class
weighting was applied to the task of predicting which paraphrase group
each sentence belongs to. Classification was verified using 3-fold
cross-validation across different splits of the testing/training data,
the average results are shown in this section. The splits were in
proportion to the class size. For the smallest groups this means there
were two training cases and one test case to classify. 

In this paper, only a linear kernel was used, because a more powerful
kernel such as RBF may be able to compensate for irregularities in
the vector space, which makes model comparison more difficult. Scikit-learn
\citep{scikit-learn} was used to orchestrate the cross-validation
and to interface with the LibLinear SVM implementation \citep{LIBLIBEAR}.
As the linear SVM's classification success depends on how linearly
separable the input data is, thus this assessed the quality of the
localization of the paraphrase groupings embeddings.


\section{Corpus Construction}

\label{sec:Corpus-Construction}


\subsection{Microsoft Research Paraphrased Grouped Subcorpus}

The MSRP corpus is a very well established data set for the paraphrase
detection task \citep{msrParapharaCorpus}. Sentences are presented
as pairs which are either paraphrases, or not. A significant number
of paraphrases appear in multiple different pairings. Using this information,
groups of paraphrases can be formed.

The corpus was partitioned according to sentence meaning by taking
the symmetric and transitive closures the set of paraphrase pairs.
For example if sentences \emph{A},\emph{ B},\emph{ C} and \emph{D}
were present in the original corpus as paraphrase pairs: $A,\,B$,
$D,\,A$ and $B,C$ then the paraphrase group $\{A,B,C,D\}$ is found.
Any paraphrase groups containing less than 3 phrases were discarded.
The resulting sub-corpus has the breakdown as shown in \Cref{fig:msrp_corpus_hist}.

\begin{figure}[t]
\includegraphics[width=1\columnwidth]{figs/msrp_hist}

\caption{\label{fig:msrp_corpus_hist} Break down of how many paraphrases groups
are present in the MSRP subcorpus of which sizes.It contains a total
of 859 unique sentences, broken up into 273 paraphrase groups.}
\end{figure}


\begin{figure}[t]
\includegraphics[width=1\columnwidth]{figs/opinosis_hist}

\caption{\label{fig:opinosis_corpus_hist} Break down of how many paraphrases
groups are present in the Opinosis subcorpus of which sizes. It contains
a total of 521 unique sentences, broken up into 89 paraphrase groups.}
\end{figure}



\subsection{Opinosis Paraphrase Grouped Subcorpus}

The Opinosis Corpus \citep{ganesan2010opinosis} was used as secondary
source of original real-world text. It is sourced from several online
review sites: Tripadvisor, Edmunds.com, and Amazon.com, and contains
single sentence statements about hotels, cars and electronics. The
advantage of this as a source for texts is that comments on the quality
of services and products tend to be along similar lines. The review
sentences are syntactically simpler than sentences from a news-wire
corpus, and also contain less named entities. However, as they are
from more casual communications, the adherence to grammar and spelling
may be less formal. 

Paraphrases were identified using the standard criterion: bidirectional
entailment. For a paraphrase group $\mathcal{S}$ of sentences: $\forall s_{1},\,s_{2}\in\mathcal{S},\quad s_{1}\entails s_{2}\quad\wedge\;s_{2}\entails s_{1}$,
every sentence in the group entails the every other sentence in the
group. A stricter interpretation of bidirectional entailment was used,
as compared to the ``mostly bidirectional entailment'' used in the
MSRP corpus. The grouping was carried out manually. Where it was unclear
as to the group a particular phrase should belong to it was left out
of the corpus entirely. The general guidelines were as follows.
\begin{itemize}
\item Tense, Transitional Phrases, and Discourse and Pragmatic Markers were
ignored.
\item Statement intensity was coarsely quantized. 
\item Approximately equal quantitative and qualitative values were treated
as synonymous.
\item Sentences with entities mentioned explicitly were grouped separately
from similar statements where they were implied.
\item Sentences with additional information were grouped separately from
those without that information.
\end{itemize}
The final point is the most significant change from the practices
apparent in the construction of the MSRP corpus. Sentences with differing
or additional information were classified as non-paraphrases. This
requirement comes from the definition of bidirectional entailment.
For example, \emph{``The staff were friendly and polite.''}, \emph{``The
staff were polite.''} and \emph{``The staff were friendly.''} are
in three separate paraphrase groups. The creators of the MSRP corpus,
however, note ``...the majority of the equivalent pairs in this dataset
exhibit `mostly bidirectional entailments', with one sentence containing
information `that differs' from or is not contained in the other.''
\citep{msrParapharaCorpus}. While this does lead to more varied paraphrases;
it strays from the strict linguistic definition of a paraphrase, which
complicates the evaluation of the semantic space attempted here. This
stricter adherence to bidirectional entailment resulted in finer separation
of groups, which makes this a more challenging corpus.

After the corpus had been broken into paraphrase groups some simple
post-processing was done. Several artifacts present in the original
corpus were removed, such as substituting the ampersand symbol for
\texttt{\&amp}. Any paraphrase groups containing identical sentences
were merged, and duplicates removed. Finally, any group with less
than three phrases was discarded. With this complete the breakdown
is as in \Cref{fig:opinosis_corpus_hist}.

Further information on the construction of the corpora in this section,
and download links are available online.\footnote{\href{http://white.ucc.asn.au/resources/paraphrase_grouped_corpora/}{http://white.ucc.asn.au/resources/paraphrase\_{}grouped\_{}corpora/}}


\section{Results and Discussion}

\label{sec:Results-and-Discussion}


\subsection{Classification Results and Discussion}

The results of performing the evaluation method described in \Cref{sec:Methodology}
are shown in \Cref{tab:results}.

\begin{table}
\begin{tabular}{|c|c|c|}
\hline 
 & MSRP Subcorpus & Opinosis Subcorpus\tabularnewline
\hline 
\hline 
 PV-DM & 78.00\% & 38.26\%\tabularnewline
\hline 
 PV-DBOW & 89.93\% & 32.19\%\tabularnewline
\hline 
 URAE & 51.14\% & 20.86\%\tabularnewline
\hline 
 MOWE & 97.91\% & \textbf{69.30\%}\tabularnewline
\hline 
SOWE & 98.02\% & 68.75\%\tabularnewline
\hline 
 BOW & \textbf{98.37\%} & 65.23\%\tabularnewline
\hline 
 PCA BOW & 97.96\% & 54.43\%\tabularnewline
\hline 
\end{tabular}\caption{\label{tab:results}The semantic classification accuracy of the various
models across the two evaluation corpora.}
\end{table}


While the relative performance of the models is similar between the
corpora, the absolute performance differs. On the absolute scale,
all the models perform much better on the MSRP subcorpus than on the
Opinosis subcorpus. This can be attributed to the significantly more
distinct classes in the MSRP subcorpus. The Opinosis subcorpus draws
a finer line between sentences with similar meanings. As discussed
earlier, for example there is a paraphrase group for \emph{``The
staff were polite.''}, another for \emph{``The staff were friendly.'',}
and a third for \emph{``The staff were friendly and polite.''}.
Under the guidelines used for paraphrases in MSRP, these would all
have been considered the same group. Secondly, there is a much wider
range of topics in the MSRP. Thus the paraphrase groups with different
meanings in MSRP corpus are also more likely to have different topic
entirely than those from Opinosis. Thus the the ground truth of the
semantics separability of phrases from the MSRP corpus is higher than
for Opinosis, making the semantic classification of the Opinosis subcorpus
is a more challenging task.

The URAE model performs the worst of all models evaluated. In \citep{KaagebExtractiveSummaristation}
is was suggested that the URAE's poor performance at summarizing the
Opinosis corpus could potentially be attributed to the less formally
structured product reviews -- the URAE being a highly structured compositional
model. However, here it also performed poorly on the MSRP -- which
it was created for \citet{SocherEtAl2011:PoolRAE}. The exact same
model used by the authors of the paper was used here -- though
this did put it at a dimensional disadvantage over the other models
having 200 dimensions to the other's 300. The key difference from
the evaluation in \citet{SocherEtAl2011:PoolRAE}, beyond the changing to a multiclass
classification problem, was the lack of the complementary word-level
features as used in the dynamic pooling layer. This suggests the model
could benefit from such world level features -- as the very strong
performance of the word-based model indicates.

The word based models, MOWE, SOWE, BOW and PCA BOW, performed very
well. This suggests that word choice is a very significant factor
in determining meaning; so much so that the models which can make
use of word order information, URAE and PV-DM, were significantly
outperformed by methods which made more direct use of the word content. 

The very high performance of the BOW maybe attributed to its very
high dimensionality, though the MOWE and SOWE performed similarly.
The PCA step can be considered as being similar to choosing an optimal
set of words to keep so as to maximum variability in the bag of words.
It loses little performance, even though decreasing vector size by
an order of magnitude -- particularly on the easier MSRP dataset.


\subsection{Model Agreement}

\begin{figure*}
\includegraphics[width=0.45\textwidth]{figs/msrp_agree}\hspace*{\fill}\includegraphics[width=0.45\textwidth]{figs/opinosis_agree}\enspace{}

\caption{\label{fig:misclass_agreement} The misclassification agreement between
each of the models for the MSRP (left) and Opinosis (right) subcorpora.
Below each model name is the total mistakes made. The denominator
of each fraction is the number of test cases incorrectly classified
by both models. The numerator is the portion of those misclassifications
which were classified in the same (incorrect) way by both models.
The shading is in-proportion to that fraction.}
\end{figure*}


The misclassifications of the models can be compared. By selecting
one of the test/train folds from the classification task above, and
comparing the predicted classifications for each test-set sentence,
the similarities of the models were assessed. The heatmaps in \Cref{fig:misclass_agreement}
show the agreement in errors. Here misclassification agreement is
given as an approximation to $P(m_{1}(x)=m_{2}(x)\,|\,m_{1}(x)\ne y\,\wedge\,m_{2}(x)\ne y)$,
for a randomly selected sentence $x$, with ground truth classification
$y$, where the models $m_{1}$ and $m_{2}$ are used to produce classifications.
Only considering the cases where both models were incorrect, rather
than simple agreement, avoids the analysis being entirely dominated
by the agreement of the models with the ground truth. 

The word based models showed significant agreement. Unsurprisingly
MOWE and SOWE have almost complete agreement in both evaluations.
The other models showed less agreement -- while they got many of the
same cases wrong the models produced different misclassifications.
This overall suggests that the various full sentence models are producing
substantially dissimilar maps from meaning to vector space. Thus it
seems reasonable that using a ensemble approach between multiple sentence
models and one word-based model would produce strong results. \citet{Yin2015} found this successful when combining different
word embedding models.


\subsection{Limitations}

This evaluation has some limitations. As with all such empirical evaluations
of machine learning models, a more optimal choice of hyper-parameters
and training data will have an impact on the performance. In particular,
if the model training was on the evaluation data the models would
be expected to be better able to position their embedding. This was
however unfeasible due to the small sizes of the datasets used for
evaluation, and would not reflect real word application of the models
to data not prior seen. Beyond the limitation of the use of the datasets
is their contents.

The paraphrase groups were not selected to be independent of the word
content overlap -- they were simply collected on commonality of meaning
from real world sourced corpora. This is a distinct contrast to the
the work of \citet{RitterPosition} discussed in \Cref{sub:Evaluations-of-Semantic}
where the classes were chosen to not have meaningful word overlap.
However our work is complementary to theirs, and our findings are
well aligned. The key difference in performance is the magnitude of
the performance of the sum of word embeddings (comparable to the mean
of word embeddings evaluated here). In \cite{RitterPosition} the
word embedding model performed similarly to the best of the more complex
models. In the results presented above we find that the word embedding
based model performs significantly beyond the more complex models.
This can be attributed to the word overlap in the paraphrase groups
-- in real-world speech people trying to say the same thing do in-fact
use the same words very often.


\section{Conclusion}

\label{sec:Conclusion}

A method was presented, to evaluate the semantic localization of sentence
embedding models. Semantically equivalent sentences are those which
exhibit bidirectional entailment -- they each imply the truth of the
other. Paraphrases are semantically equivalent. The evaluation method
is a semantic classification task -- to classify sentences as belonging
to a paraphrase group of semantically equivalent sentences. The datasets
used were derived from subsets of existing sources, the MRSP and the
Opinosis corpora. The relative performance of various models was consistent
across the two tasks, though differed on an absolute scale.

The word embedding and bag of word models performed best, followed
by the paragraph vector models, with the URAE trailing in both tests.
The strong performance of the sum and mean of word embeddings (SOWE
and MOWE) compared to the more advanced models aligned with the results
of \citet{RitterPosition}. The difference in performance
presented here for real-word sentences, were more marked than for
the synthetic sentence used by Ritter et. al. This may be attributed
to real-world sentences often having meaning overlap correspondent
to word overlap -- as seen also in the very strong performance of
bag of words. Combining the result of this work with those of Ritter
et. al., it can be concluded that summing word vector representations
is a practical and surprisingly effective method for encoding the
meaning of a sentence. 


\end{document}
