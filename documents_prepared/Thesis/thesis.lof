\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.1}{\ignorespaces Some word embeddings from the FastText project \parencite {bojanowski2016enriching}. They were originally 300 dimensions but have been reduced to 2 using t-SNE \parencite {maaten2008tsne} algorithm. The colors are from 5 manually annotated categories done before this visualisation was produced: \texttt {foods}, \texttt {sports}, \texttt {colors}, \texttt {tools}, \texttt {other objects}, \texttt {other}. Note that many of these words have multiple meanings (see \Cref {sec:word-sense-representations}), and could fit into multiple categories. Also notice that the information captioned by the unsupervised word embeddings is far finer grained than the manual categorisation. Notice, for example, the separation of ball-sports, from words like \texttt {run} and \texttt {walk}. Not also that \texttt {china} and \texttt {turkey} are together; this no doubt represents that they are both also countries.\relax }}{18}{figure.caption.20}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.2}{\ignorespaces The Neural Trigram Language Model\relax }}{20}{figure.caption.22}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.3}{\ignorespaces Neural Trigram Language Model as considered with output embeddings. This is mathematically identical to \Cref {fig:trigram-neural-language-model}\relax }}{22}{figure.caption.24}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.4}{\ignorespaces Neural Probabilistic Language Model\relax }}{24}{figure.caption.27}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.5}{\ignorespaces RNN Language Model. The RU equation shown is the basic RU used in \textcite {mikolov2010recurrent}. It can be substituted for a LSTM RU or an GRU as was done in \textcite {sundermeyer2012lstm,jozefowicz2015empirical}, with appropriate changes. \relax }}{24}{figure.caption.28}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.6}{\ignorespaces CBOW Language Model\relax }}{26}{figure.caption.29}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.7}{\ignorespaces Skip-gram language Language Model. Note that the probability $P(\n w_j \mid \n w_i)$ is optimised during training for every $\n w_j$ in a window around the central word $\n w_i$. Note that the final layer in this diagram is just a softmax layer, written in in output embedding form.\relax }}{27}{figure.caption.30}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.8}{\ignorespaces Example of analogy algebra\relax }}{28}{figure.caption.31}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.9}{\ignorespaces Vectors involved in analogy ranking tasks, this may help to understand the math in \Cref {equ:analogy}\relax }}{28}{figure.caption.32}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.10}{\ignorespaces Tree for 2 words\relax }}{32}{figure.caption.35}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.11}{\ignorespaces Tree for 3 words\relax }}{32}{figure.caption.37}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.1}{\ignorespaces The relationship between terms used to discuss various word sense problems. The lemma is used as the representation for the lexeme, for WordNet's purposes when indexing. For many tasks each the word-use is pre-tagged with its lemma and POS tag, as these can be found with high reliability using standard tools. Note that the arrows in this diagram are \emph {directional}. That is to say, for example, each Synset \emph {has 1} POS, but each POS \emph {has many} Synsets. \relax }}{40}{figure.caption.42}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.2}{\ignorespaces The process used by \textcite {Chen2014} to create word sense embeddings. \relax }}{44}{figure.caption.44}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.1}{\ignorespaces The unrolled structure of an RNN for use in (a) Matched-sequence (b) Encoding, (c) Decoding and (d) Encoding-Decoding (sequence-to-sequence) problems. RU is the recurrent unit -- the neural network which reoccurs at each time step. \relax }}{52}{figure.caption.47}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.2}{\ignorespaces The VAE plus encoder-decoder of \textcite {Bowman2015SmoothGeneration}. Note that during training, $\n \mathaccentV {hat}05E{y}_i=\n w_i$, as it is an autoencoder model. As is normal for encoder-decoders the prompts are the previous output (target during training, predicted during testing): $\n r_i=\n y_{i-1}$, with $\n r_1= \n y_0= \texttt {<EOS>}$ being a pseudo-token marker for the end of the string. The Emb. step represents the embedding table lookup. In the diagrams for \Cref {sec:word-representations} we showed this as as a table but just as a block here for conciseness. \relax }}{53}{figure.caption.48}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.3}{\ignorespaces The skip-thought model \parencite {DBLP:journals/corr/KirosZSZTUF15}. Note that for the next and previous sentences respectively the outputs are $\n \mathaccentV {hat}05E{q}_i$ and $\n \mathaccentV {hat}05E{p}_i$, and the prompts are $\n {q}_{i-1}$ and $\n {p}_{i-1}$. As there is no intent to use the decoders after training, there is no need to worry about providing an evaluation-time prompt, so the prompt is always the previous word. $\n p_0= \n p_{\d m_p} = \n q_0= \n q_{\d m_q} = \texttt {<EOS>}$ being a pseudo-token marker for the end of the string. The input words are $\n w_i$, which come from the current sentence. the Emb. steps represents the look-up of the embedding for the word. \relax }}{55}{figure.caption.49}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.4}{\ignorespaces A constituency parse tree for the sentence: \texttt {This is a simple example of a parse tree}. In this diagram the leaf nodes are the input words, their intimidate parents are their POS tags, and the other nodes with multiple children represent sub-phrases of the sentence, for example NP is a Noun Phrase.\relax }}{56}{figure.caption.50}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.5}{\ignorespaces A dependency parse tree for the sentence \texttt {This is a simple example of a parse tree}, This flattened view may be misleading. \texttt {example} is at the peak of the tree, with direct children being: \texttt {this},\texttt {is},\texttt {a},\texttt {simple}, and \texttt {tree}. \texttt {tree} has direct children being: \texttt {of},\texttt {a}, and \texttt {parse}. \relax }}{56}{figure.caption.51}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {4.6}{\ignorespaces A Matrix Vector recurrent unit\relax }}{61}{figure.caption.53}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.1}{\ignorespaces Process Diagram for the Evaluation of Semantic Consistency via our method\relax }}{69}{figure.caption.58}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.2}{\ignorespaces Break down of how many paraphrases groups are present in the MSRP subcorpus of which sizes.It contains a total of 859 unique sentences, broken up into 273 paraphrase groups.\relax }}{72}{figure.caption.63}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.3}{\ignorespaces Break down of how many paraphrases groups are present in the Opinosis subcorpus of which sizes. It contains a total of 521 unique sentences, broken up into 89 paraphrase groups.\relax }}{73}{figure.caption.64}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.4}{\ignorespaces The misclassification agreement between each of the models for the MSRP (left) and Opinosis (right) subcorpora. Below each model name is the total mistakes made. The denominator of each fraction is the number of test cases incorrectly classified by both models. The numerator is the portion of those misclassifications which were classified in the same (incorrect) way by both models. The shading is in-proportion to that fraction.\relax }}{75}{figure.caption.66}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.1}{\ignorespaces The overall architecture of our systems \relax }}{81}{figure.caption.67}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.2}{\ignorespaces The RNN Input module for the example input \texttt {light greenish blue}. Each dashed box represents 1 time-step. \relax }}{83}{figure.caption.73}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.3}{\ignorespaces The SOWE input module for the example input \texttt {light bluish green}\relax }}{84}{figure.caption.75}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.4}{\ignorespaces The CNN input module for the example input \texttt {light greenish blue}\relax }}{85}{figure.caption.77}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.5}{\ignorespaces The Distribution Output Module \relax }}{85}{figure.caption.79}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.6}{\ignorespaces The Point Estimate Output Module. Here $\mathrm {atan2}$ is the quadrant preserving arctangent, outputting the angle in turns. \relax }}{86}{figure.caption.81}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.7}{\ignorespaces Some examples of the output distribution estimates from the models trained on the full dataset\relax }}{91}{figure.caption.87}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.8}{\ignorespaces Some examples of the output point estimates from the models trained on the full dataset\relax }}{92}{figure.caption.88}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.9}{\ignorespaces Some example distribution estimations for colors names which are completely outside the training data. The terms: \texttt {Brown}, \texttt {gray}, \texttt {Gray}, \texttt {Green}, and \texttt {Purple}, do not occur in any of the color data; however \texttt {brown}, \texttt {grey} \texttt {green}, and \texttt {purple} do occur.\relax }}{96}{figure.caption.100}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.10}{\ignorespaces Some example point estimates for colors names which are completely outside the training data. The terms: \texttt {Brown}, \texttt {gray}, \texttt {Gray}, \texttt {Green}, and \texttt {Purple}, do not occur in any of the color data; however \texttt {brown}, \texttt {grey} \texttt {green}, and \texttt {purple} do occur.\relax }}{97}{figure.caption.101}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {7.1}{\ignorespaces Block diagram for RefittedSim similarity measure\relax }}{107}{figure.caption.106}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {7.2}{\ignorespaces Block diagram for performing WSD using refitting.\relax }}{109}{figure.caption.109}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.1}{\ignorespaces The full NovelPerspective pipeline. Note that step 5 uses the original ebook to subset. \relax }}{114}{figure.caption.111}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.2}{\ignorespaces The general structure of the character classification systems. This repeated for each section of the book during step 3 of the full pipeline shown in \Cref {fig:fullprocess}. \relax }}{115}{figure.caption.112}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {9.1}{\ignorespaces The process for the regenerating BOW from SOWE sentence embeddings.\relax }}{122}{figure.caption.121}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {9.2}{\ignorespaces The mean Jaccard index achieved during the word selection step, shown against the ground truth length of the sentence. Note that the vast majority of sentences are in the far left end of the plot. The diminishing samples are also the cause of the roughness, as the sentence length increases.\relax }}{126}{figure.caption.127}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {10.1}{\ignorespaces The Sel.\nobreakspace {}BOW+Ord.{} process for the regenerating sentences from SOWE-type sentence vectors.\relax }}{132}{figure.caption.129}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {10.2}{\ignorespaces A graph showing the legal transitions between states, when the word-ordering problem is expressed similar to a GA-TSP. Each edge $\delimiter "426830A w_{a},w_{b}\delimiter "526930B \to \delimiter "426830A w_{c},w_{d}\delimiter "526930B $ has cost $-\qopname \relax o{log}(P(w_c\tmspace +\medmuskip {.2222em}|\tmspace +\medmuskip {.2222em}w_aw_b)$. The nodes are grouped into districts (words). Nodes for invalid states are greyed out.\relax }}{135}{figure.caption.130}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {10.3}{\ignorespaces The distribution of the evaluation corpus after preprocessing.\relax }}{138}{figure.caption.135}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {10.4}{\ignorespaces The portion of sentences reconstructed perfectly by the Sel.\nobreakspace {}BOW+Ord.{} process. Shown also is the results on ordering only (Ref.\nobreakspace {}BOW+Ord.{}), which orders the reference BOWS; and the portion of BOWs perfect from the word selection step only (Sel.\nobreakspace {}BOW\nobreakspace {}(only){}) i.e. the input to the ordering step.\relax }}{140}{figure.caption.138}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {11.1}{\ignorespaces The representation space is a computationally manipulate representation of the meaning space. The natural language utterances come from points in the meaning space; though due to ambiguity we can only truly hope to estimate distributions when the interpret them. A single point embedding as an approximation to a distribution with a single tight peak.\relax }}{148}{figure.caption.142}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {13.1}{\ignorespaces The current package ecosystem depending on DataDeps.jl. \relax }}{168}{figure.caption.146}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {13.2}{\ignorespaces The process that is executed when a data dependency is accessed by name. \relax }}{169}{figure.caption.147}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
