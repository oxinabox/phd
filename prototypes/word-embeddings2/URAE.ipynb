{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ii in 1:4\n",
    "    addprocs([\"heathred\"])\n",
    "end\n",
    "\n",
    "for ii in 1:1\n",
    "    addprocs([\"amon\"], dir=\"\")\n",
    "    addprocs([\"zeus\"], dir=\"\")\n",
    "    addprocs([\"jove\"], dir=\"\")\n",
    "    addprocs([\"ares\"], dir=\"\")\n",
    "end\n",
    "\n",
    "for ii in 1:10\n",
    "    #addprocs([\"uggp\"], dir=\"\")\n",
    "end\n",
    "\n",
    "addprocs(2)\n",
    "workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Iterators\n",
    "using Pipe\n",
    "function pz(x :: AbstractArray)\n",
    "    println(typeof(x), \": \", size(x))\n",
    "end\n",
    "macro printval(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\" = \", $ee)))\n",
    "end\n",
    "\n",
    "macro pz(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\"\\t\\t\",typeof($ee), \"\\t\", size($ee))))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@everywhere using RecursiveAutoencoders\n",
    "@everywhere  using UnfoldingRAE\n",
    "\n",
    "using ClusterSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using WordEmbeddings\n",
    "LL,word_indexes, indexed_words =  load_embeddings(\"embeddings-scaled.EMBEDDING_SIZE=50.txt\");\n",
    "size(LL) |> println\n",
    "word_indexes |> typeof |> println\n",
    "indexed_words |> typeof |> println"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_trees = open(\"training_sents.jsz\",\"r\") do fs\n",
    "    deserialize(fs)\n",
    "end;\n",
    "@assert length(@pipe training_trees |> filter(x->typeof(x)===UTF8String,_))==0\n",
    "@pz training_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_training_trees = r_chunk_data(training_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@everywhere function unpack!(rae::RAE, θ::Vector)\n",
    "    W_e_len = length(rae.W_e)\n",
    "    b_e_len = length(rae.b_e)\n",
    "    W_d_len = length(rae.W_d)\n",
    "    b_d_len = length(rae.b_d)\n",
    "    W_e_shape = size(rae.W_e)\n",
    "    W_d_shape = size(rae.W_d)\n",
    "    \n",
    "    rae.W_e = reshape(θ[1: W_e_len],W_e_shape)\n",
    "    rae.b_e = θ[W_e_len+1: W_e_len+b_e_len]\n",
    "    rae.W_d = reshape(θ[W_e_len+b_e_len+1: W_e_len+b_e_len+W_d_len],W_d_shape)\n",
    "    rae.b_d = θ[W_e_len+b_e_len+W_d_len+1: end]\n",
    "    \n",
    "    rae\n",
    "end\n",
    "\n",
    "@everywhere function pack(rae::RAE)\n",
    "    pack(rae.W_e,rae.b_e, rae.W_d,rae.b_d)\n",
    "end\n",
    "\n",
    "@everywhere function pack(∇W_e::Matrix{Float64}, ∇b_e::Vector{Float64}, ∇W_d::Matrix{Float64}, ∇b_d::Vector{Float64})\n",
    "    [∇W_e[:], ∇b_e, ∇W_d[:], ∇b_d] \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "function loss!(θ::Vector)  \n",
    "    #warn(\"loss! not defined\")\n",
    "    grad = similar(θ)\n",
    "    #loss_and_loss_grad!(θ::Vector)\n",
    "    cached_loss_and_loss_grad!(θ, grad)\n",
    "end\n",
    "\n",
    "function loss_grad!(θ::Vector, storage::Vector) \n",
    "    #warn(\"loss_grad not defined\")\n",
    "    cached_loss_and_loss_grad!(θ, grad)\n",
    "end\n",
    "\n",
    "\n",
    "#rae_outer = RAE(LL,word_indexes,indexed_words);\n",
    "#r_rae_outers = put!(workers(), rae_outer)\n",
    "function loss_and_loss_grad!(θ::Vector, grad::Vector)   \n",
    "    grad[:] = 0\n",
    "    @inbounds for r_rae_outer in r_rae_outers\n",
    "        update_remote(r_rae_outer, rae->unpack!(rae, θ) )\n",
    "    end\n",
    "    \n",
    "    function get_remote_loss_grad_function(r_rae_outer::RemoteRef)\n",
    "        @assert r_rae_outer.where == myid()\n",
    "        rae = fetch(r_rae_outer)        \n",
    "        function loss_and_loss_grad(tree::(Any,Any))\n",
    "            Δs, err = UnfoldingRAE.loss_and_loss_grad(rae, tree)\n",
    "            [pack(Δs...), err]\n",
    "        end\n",
    "    end\n",
    "    loss_and_loss_grads = map(r_rae_outers) do r_raeouter\n",
    "        remotecall(r_raeouter.where, get_remote_loss_grad_function, r_raeouter)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    ret = prechunked_mapreduce(r_training_trees, loss_and_loss_grads, (+)) \n",
    "    grad[:] = ret[1:end-1]\n",
    "    err=ret[end]\n",
    "    \n",
    "    grad[:]/=length(training_trees)\n",
    "    err/=length(training_trees)\n",
    "    err\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "loss_and_loss_grad_cache = Dict{Vector{Float64},(Float64, Vector{Float64})}()\n",
    "loss_and_loss_grad_cache_hits = 0\n",
    "loss_and_loss_grad_cache_misses = 0\n",
    "function cached_loss_and_loss_grad!(θ::Vector, grad::Vector)\n",
    "    global loss_and_loss_grad_cache\n",
    "    global loss_and_loss_grad_cache_hits\n",
    "    global loss_and_loss_grad_cache_misses\n",
    "    if haskey(loss_and_loss_grad_cache,θ)\n",
    "        loss_and_loss_grad_cache_hits+=1\n",
    "        err, grad[:] = loss_and_loss_grad_cache[θ]\n",
    "        err\n",
    "    else\n",
    "        loss_and_loss_grad_cache_misses+=1\n",
    "        err = loss_and_loss_grad!(θ, grad)\n",
    "        loss_and_loss_grad_cache[θ] = (err, grad)\n",
    "        err\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using NLopt\n",
    "\n",
    "f_call_count = 0\n",
    "function tracking_loss_and_loss_grad!(θ::Vector, grad::Vector)\n",
    "    global f_call_count\n",
    "    f_call_count+=1\n",
    "    f_val = cached_loss_and_loss_grad!(θ, grad)   \n",
    "    println(f_call_count, '\\t',f_val,'\\t',norm(grad))\n",
    "    f_val\n",
    "end\n",
    "#:LD_MMA, :LD_CCSAQ, :LD_LBFGS, :LD_SLSQP, :LD_VAR2, :LD_VAR1, :LD_TNEWTON_RESTART\n",
    "opt = Opt(:LD_MMA, length(pack(rae_outer)))\n",
    "\n",
    "#ftol_abs!(opt,1e-9)\n",
    "maxtime!(opt, 30)\n",
    "min_objective!(opt, tracking_loss_and_loss_grad!)\n",
    "(optf,optx,ret) = optimize!(opt,pack(rae_outer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#MMA\n",
    "#167\t623.6986598947503\t3.472825423124687e12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Optim #https://github.com/JuliaOpt/Optim.jl\n",
    "f=DifferentiableFunction(loss!,loss_grad!,cached_loss_and_loss_grad!)\n",
    "\n",
    "res = optimize(f, res.minimum, method=:l_bfgs, show_trace = true, store_trace = true, iterations = 30);\n",
    "@printval res.f_calls \n",
    "@printval res.g_calls \n",
    "@printval res.iterations\n",
    "@printval res.f_minimum\n",
    "@printval res.gr_converged\n",
    "@printval res.x_converged \n",
    "\n",
    "@printval res.trace\n",
    "@printval loss_and_loss_grad_cache_hits\n",
    "@printval loss_and_loss_grad_cache_misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "θ=pack(rae_outer)\n",
    "@time loss_and_loss_grad!(θ, zeros(θ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Profile.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.8",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
