{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Int64,1}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ii in 1:4\n",
    "    #addprocs([\"heathred\"], dir=\"~\",)\n",
    "end\n",
    "\n",
    "for ii in 1:3\n",
    "#    addprocs([\"amon\"], dir=\"~\", exename=\"./julia\")\n",
    "#    addprocs([\"zeus\"], dir=\"~\", exename=\"./julia\")\n",
    "#    addprocs([\"jove\"], dir=\"~\", exename=\"./julia\")\n",
    "#    addprocs([\"ares\"], dir=\"~\", exename=\"./julia\")\n",
    "end\n",
    "\n",
    "for ii in 1:10\n",
    "    #addprocs([\"uggp\"], dir=\"\")\n",
    "end\n",
    "\n",
    "addprocs(4)\n",
    "workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Iterators\n",
    "using Pipe\n",
    "\n",
    "macro printval(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\" = \", $ee)))\n",
    "end\n",
    "\n",
    "macro pz(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\"\\t\\t\",typeof($ee), \"\\t\", size($ee))))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@everywhere using RecursiveAutoencoders\n",
    "@everywhere  using UnfoldingRAE\n",
    "@everywhere using Zlib\n",
    "push!(LOAD_PATH, \"../util\")\n",
    "using ClusterSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200"
     ]
    }
   ],
   "source": [
    "@everywhere using WordEmbeddings\n",
    "#LL,word_indexes, indexed_words = load_word2vec_embeddings(\"../../Resources/example_code/word2vec/GoogleNews-vectors-negative300.bin\", 15000)\n",
    "LL,word_indexes, indexed_words = load_embeddings(\"word_emb_data/embeddings-scaled.EMBEDDING_SIZE=200.txt\")\n",
    "LL[:]./=maximum(abs(LL[:]))\n",
    "size(LL) |> println\n",
    "word_indexes |> typeof |> println\n",
    "indexed_words |> typeof |> println"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_trees\t\tArray{Any,1}\t(10314,)\n"
     ]
    }
   ],
   "source": [
    "training_trees1 = open(\"questionbank_sents.jsz\",\"r\") do fs\n",
    "    deserialize(fs)\n",
    "end;\n",
    "training_trees2 = open(\"opinosis_sents.jsz\",\"r\") do fs\n",
    "    deserialize(fs)\n",
    "end\n",
    "training_trees = [training_trees1;training_trees2]\n",
    "training_trees1=nothing\n",
    "training_trees2=nothing\n",
    "@pz training_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: InterruptException:\nwhile loading In[88], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "LoadError: InterruptException:\nwhile loading In[88], in expression starting on line 1",
      "",
      " in typeinf at ./inference.jl:1346",
      " in inlineable at ./inference.jl:2317",
      " in inlining_pass at ./inference.jl:2884",
      " in inlining_pass at ./inference.jl:2816",
      " in inlining_pass at ./inference.jl:2776",
      " in typeinf_uncached at ./inference.jl:1696",
      " in typeinf at ./inference.jl:1334",
      " in typeinf at ./inference.jl:1284",
      " in abstract_call_gf at ./inference.jl:731",
      " in abstract_call at ./inference.jl:844",
      " in abstract_eval_call at ./inference.jl:931",
      " in abstract_eval at ./inference.jl:958",
      " in abstract_interpret at ./inference.jl:1115",
      " in typeinf_uncached at ./inference.jl:1544",
      " in typeinf at ./inference.jl:1334",
      " in typeinf_ext at ./inference.jl:1278",
      " in serialize at serialize.jl:185",
      " in serialize at serialize.jl:119",
      " in serialize at serialize.jl:421",
      " in send_msg_ at multi.jl:160",
      " in remotecall_fetch at multi.jl:657",
      " in remotecall_fetch at multi.jl:663",
      " in call_on_owner at multi.jl:705",
      " in r_chunk_data at /home/wheel/oxinabox/phd/prototypes/util/ClusterSoup.jl:42"
     ]
    }
   ],
   "source": [
    "r_training_trees = r_chunk_data(training_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@everywhere function unpack!(rae::RAE, θ::Vector)\n",
    "    W_e_len = length(rae.W_e)\n",
    "    b_e_len = length(rae.b_e)\n",
    "    W_d_len = length(rae.W_d)\n",
    "    b_d_len = length(rae.b_d)\n",
    "    \n",
    "    rae.W_e[:] = θ[1: W_e_len]\n",
    "    rae.b_e[:] = θ[W_e_len+1: W_e_len+b_e_len]\n",
    "    rae.W_d[:] = θ[W_e_len+b_e_len+1: W_e_len+b_e_len+W_d_len]\n",
    "    rae.b_d[:] = θ[W_e_len+b_e_len+W_d_len+1: end]\n",
    "    \n",
    "    rae\n",
    "end\n",
    "\n",
    "@everywhere function pack(rae::RAE)\n",
    "    pack(rae.W_e,rae.b_e, rae.W_d,rae.b_d)\n",
    "end\n",
    "\n",
    "@everywhere function pack(∇W_e::NumericMatrix, ∇b_e::NumericVector, ∇W_d::NumericMatrix, ∇b_d::NumericVector)\n",
    "    [∇W_e[:], ∇b_e, ∇W_d[:], ∇b_d] \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function loss!(θ::Vector)  \n",
    "    #warn(\"loss! not defined\")\n",
    "    grad = similar(θ)\n",
    "    #loss_and_loss_grad!(θ::Vector)\n",
    "    cached_loss_and_loss_grad!(θ, grad)\n",
    "end\n",
    "\n",
    "function loss_grad!(θ::Vector, storage::Vector) \n",
    "    #warn(\"loss_grad not defined\")\n",
    "    cached_loss_and_loss_grad!(θ, grad)\n",
    "end\n",
    "\n",
    "\n",
    "rae_outer = RAE(LL,word_indexes,indexed_words,0.001);\n",
    "@time r_rae_outers = put!(workers(), rae_outer, 2)\n",
    "function loss_and_loss_grad!(θ::Vector, grad::Vector)   \n",
    "    grad[:] = 0\n",
    "    @inbounds for r_rae_outer in r_rae_outers\n",
    "        update_remote(r_rae_outer, rae->unpack!(rae, θ) )\n",
    "    end\n",
    "    \n",
    "    function get_remote_loss_grad_function(r_rae_outer::RemoteRef)\n",
    "        @assert r_rae_outer.where == myid()\n",
    "        rae = fetch(r_rae_outer)        \n",
    "        function loss_and_loss_grad(tree::(Any,Any))\n",
    "            Δs, err = UnfoldingRAE.loss_and_loss_grad(rae, tree)\n",
    "            [pack(Δs...), err]\n",
    "        end\n",
    "    end\n",
    "    loss_and_loss_grads::Array{RemoteRef,1} = map(r_rae_outers) do r_raeouter\n",
    "        remotecall(r_raeouter.where, get_remote_loss_grad_function, r_raeouter)::RemoteRef\n",
    "    end\n",
    "    \n",
    "    \n",
    "    ret = prechunked_mapreduce(r_training_trees, loss_and_loss_grads, (+)) \n",
    "    grad[:] = ret[1:end-1]\n",
    "    err=ret[end]\n",
    "    \n",
    "    grad[:]/=length(training_trees)\n",
    "    err/=length(training_trees)\n",
    "    err\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_and_loss_grad_cache = Dict{NumericVector,(Number, NumericVector)}()\n",
    "loss_and_loss_grad_cache_hits = 0\n",
    "loss_and_loss_grad_cache_misses = 0\n",
    "function cached_loss_and_loss_grad!(θ::Vector, grad::Vector)\n",
    "    global loss_and_loss_grad_cache\n",
    "    global loss_and_loss_grad_cache_hits\n",
    "    global loss_and_loss_grad_cache_misses\n",
    "    if haskey(loss_and_loss_grad_cache,θ)\n",
    "        loss_and_loss_grad_cache_hits+=1\n",
    "        err, grad[:] = loss_and_loss_grad_cache[θ]\n",
    "        err\n",
    "    else\n",
    "        loss_and_loss_grad_cache_misses+=1\n",
    "        err = loss_and_loss_grad!(θ, grad)\n",
    "        loss_and_loss_grad_cache[θ] = (err, grad)\n",
    "        err\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using NLopt\n",
    "\n",
    "f_call_count = 0\n",
    "function tracking_loss_and_loss_grad!(θ::Vector, grad::Vector)\n",
    "    global f_call_count\n",
    "    f_call_count+=1\n",
    "    f_val = cached_loss_and_loss_grad!(θ, grad)   \n",
    "    println(f_call_count, '\\t',f_val,'\\t',norm(grad))\n",
    "    f_val\n",
    "end\n",
    "#:LD_MMA, :LD_CCSAQ, :LD_LBFGS, :LD_SLSQP, :LD_VAR2, :LD_VAR1, :LD_TNEWTON_RESTART\n",
    "opt = Opt(:LD_TNEWTON_RESTART, length(pack(rae_outer)))\n",
    "\n",
    "#ftol_abs!(opt,1e-9)\n",
    "maxtime!(opt, 10*60)\n",
    "min_objective!(opt, tracking_loss_and_loss_grad!)\n",
    "\n",
    "#θ = pack(rae_outer)\n",
    "θ = optx\n",
    "\n",
    "(optf,optx,ret) = optimize!(opt,θ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Optim #https://github.com/JuliaOpt/Optim.jl\n",
    "f=DifferentiableFunction(loss!,loss_grad!,cached_loss_and_loss_grad!)\n",
    "θ = pack(rae_outer)\n",
    "#θ=res.minimum\n",
    "res = optimize(f, θ, method=:cg, show_trace = true, store_trace = true, iterations = 10);\n",
    "@printval res.f_calls \n",
    "@printval res.g_calls \n",
    "@printval res.iterations\n",
    "@printval res.f_minimum\n",
    "@printval res.gr_converged\n",
    "@printval res.x_converged                       \n",
    "@printval res.f_converged \n",
    "@printval res.trace\n",
    "@printval loss_and_loss_grad_cache_hits\n",
    "@printval loss_and_loss_grad_cache_misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "open(\"rae1062.jsz\",\"w\") do fs\n",
    "    serialize(fs, rae_outer)\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unpack!(rae_outer, res.minimum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importall WordEmbeddings \n",
    "function show_best(embedder,ĉ::Embedding, nbest=20, similarity=WordEmbeddings.cosine_sim )\n",
    "    candidates=WordEmbeddings.neighbour_sims(ĉ,embedder.L, similarity)  \n",
    "    candidates[find(map(isnan, candidates))]= -Inf #If the zero vector was an option, it will be NaN similar so it is banned from winning\n",
    "    best_cands = [ (findfirst(candidates,score), score)\n",
    "                    for score in select(candidates,1:nbest, rev=true)[1:nbest]]\n",
    "    vcat([[embedder.indexed_words[ii] round(score,2)] for (ii,score) in best_cands]...)\n",
    "end\n",
    "\n",
    "function show_bests(embedder,ĉs::Embeddings, nbest=20, similarity=WordEmbeddings.cosine_sim)\n",
    "    hcat([show_best(embedder,ĉs[:,ii],nbest, similarity) for ii in 1:size(ĉs,2)]...)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tr=((\"What\", (\"are\", \"birds\")), \"?\")\n",
    "a = fold(rae_outer,tr)\n",
    "b= unfold(rae_outer, a)\n",
    "ĉs = hcat(map(leave-> leave.ĉ, b)...)\n",
    "bests = show_bests(rae_outer, ĉs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "haskey(rae_outer.word_index,\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "findfirst(candidates,NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "select(candidates,1:nbest, rev=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0-dev",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
