{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV[\"LINES\"] = 30\n",
    "ENV[\"COLUMNS\"] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pz (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Pipe\n",
    "function pz(x :: AbstractArray)\n",
    "    println(typeof(x), \": \", size(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/numpy/core/fromnumeric.py:2499: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  VisibleDeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenize (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyCall\n",
    "@pyimport nltk\n",
    "function tokenize(sentence::String)\n",
    "    convert(Array{String,1},nltk.word_tokenize(sentence))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_embeddings (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"load_embeddings.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,268810)\n",
      "Dict{String,Int64}\n",
      "Array{String,1}\n"
     ]
    }
   ],
   "source": [
    "LL,word_indexes, indexed_words =  load_embeddings(\"embeddings-scaled.EMBEDDING_SIZE=50.txt\");\n",
    "size(LL) |> println\n",
    "word_indexes |> typeof |> println\n",
    "indexed_words |> typeof |> println"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unfold_merges (generic function with 1 method)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typealias Embedding Vector{Float64}\n",
    "typealias Embeddings Matrix{Float64}\n",
    "typealias Words Union(AbstractArray{ASCIIString,1},AbstractArray{String,1})\n",
    "type RAE\n",
    "    L::Matrix{Float64}\n",
    "    word_index::Dict{String,Int}\n",
    "    indexed_words::Vector{String}\n",
    "    \n",
    "    W_e::Matrix{Float64}\n",
    "    b_e::Vector{Float64}\n",
    "    W_d::Matrix{Float64}\n",
    "    b_d::Vector{Float64}\n",
    "   \n",
    "end\n",
    "\n",
    "\n",
    "function RAE(L::Matrix{Float64},word_index::Dict{String,Int}, indexed_words::Vector{String})\n",
    "    emb_width = size(L,1)\n",
    "    \n",
    "    W_e =0.01*randn(emb_width,emb_width*2) \n",
    "    b_e = 0.01*randn(emb_width) \n",
    "    #W_d = 0.01*randn(emb_width*2,emb_width)\n",
    "    W_d = pinv(W_e) #Cheat (Actually why can't I always do this to initialize?);\n",
    "    b_d = 0.01*randn(emb_width*2)\n",
    "    \n",
    "    RAE(L,word_index, indexed_words, W_e, b_e, W_d, b_d)\n",
    "end\n",
    "\n",
    "\n",
    "function get_word_index(rae::RAE, input::String, show_warn=true)\n",
    "    if haskey(rae.word_index, input)\n",
    "        ii = rae.word_index[input]\n",
    "    elseif haskey(rae.word_index, lowercase(input))\n",
    "        ii = rae.word_index[lowercase(input)]\n",
    "    else\n",
    "        ii = rae.word_index[\"*UNKNOWN*\"]\n",
    "        if show_warn\n",
    "            println(\"$input not found. Defaulting.\")\n",
    "        end\n",
    "    end\n",
    "    ii\n",
    "end\n",
    "\n",
    "\n",
    "function eval_word_embedding(rae::RAE, input::String, show_warn=true)\n",
    "    k=get_word_index(rae, input, show_warn)\n",
    "    rae.L[:,k]\n",
    "end\n",
    "\n",
    "function eval_word_embeddings(rae::RAE, inputs::Words, show_warn=false)\n",
    "    ks = @pipe inputs |> map(ii -> get_word_index(rae,ii, show_warn), _)\n",
    "    rae.L[:,ks]\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function eval_merges(rae::RAE, c_is::Embeddings, c_js::Embeddings)\n",
    "    @assert size(c_is)==size(c_js)\n",
    "\n",
    "    tanh(rae.W_e*[c_is;c_js].+rae.b_e)\n",
    "end\n",
    "\n",
    "function reconstruct(rae::RAE, pp::Embedding)\n",
    "    ĉ_ijs = tanh(rae.W_d*pp+rae.b_d)\n",
    "    ĉ_is = ĉ_ijs[1:end/2]\n",
    "    ĉ_js = ĉ_ijs[end/2+1:end]\n",
    "    ĉ_is, ĉ_js\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function unfold_merges(rae::RAE, pps::Embeddings)\n",
    "    ĉ_ijs = tanh(rae.W_d*pps .+ rae.b_d)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eval_scores_gradient (generic function with 1 method)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function eval_scores(rae::RAE, c_is::Embeddings, c_js::Embeddings,\n",
    "                     pps=eval_merges(rae, c_is, c_js))\n",
    "    c_ijs = [c_is;c_js]\n",
    "    ĉ_ijs = unfold_merges(rae,pps)\n",
    "    \n",
    "    1/2*sum((c_ijs-ĉ_ijs).^2,1)\n",
    "end\n",
    "#A better scoring function is given in SorcherRAE (eaquation 6)\n",
    "\n",
    "\n",
    "function eval_scores_gradient(rae::RAE, \n",
    "                              ĉ_ijs::Embeddings,\n",
    "                              pps::Embeddings,\n",
    "                              c_ijs::Embeddings)\n",
    "    #http://neuralnetworksanddeeplearning.com/chap2.html\n",
    "    \n",
    "    da = (c_ijs - ĉ_ijs)\n",
    "    dz_d = (1-ĉ_ijs.^2)\n",
    "    δ_d = da.*dz_d\n",
    "    \n",
    "    #loop below if deep    \n",
    "    dz_e = (1-pps.^2)\n",
    "    δ_e = (rae.W_d'*δ_d).*dz_e \n",
    "    \n",
    "    \n",
    "    ∇W_d = δ_d*pps'\n",
    "    ∇b_d = δ_d\n",
    "    ∇W_e = δ_e*c_ijs'\n",
    "    ∇b_e = δ_e\n",
    "    \n",
    "    \n",
    "    ∇W_e, ∇b_e, ∇W_d, ∇b_d\n",
    "\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "50x100 Array{Float64,2}:\n",
       " -0.358534     -0.828338    -1.6629      -0.266672     -0.285667     2.52346      0.952676     2.293        2.17838      1.23577      1.33082     -0.832697    …   0.141022      0.678446    -1.15244      0.246818     -1.18257      0.0400143    -0.169281     -0.0633738     0.0439598    -1.25544   \n",
       " -0.228286     -0.52742     -1.0588      -0.169795     -0.18189      1.60674      0.606588     1.46         1.38702      0.78684      0.847358    -0.530195        0.0897913     0.43198     -0.73378      0.157154     -0.752967     0.0254779    -0.107785     -0.0403514     0.0279901    -0.799362  \n",
       "  0.00643422    0.0148653    0.0298423    0.00478567    0.00512655  -0.0452858   -0.0170966   -0.0411501   -0.039093    -0.022177    -0.0238827    0.0149435      -0.00253076   -0.0121753    0.0206815   -0.00442937    0.0212223   -0.000718094   0.0030379     0.0011373    -0.0007889     0.02253   \n",
       "  0.0151909     0.0350962    0.0704563    0.0112987     0.0121035   -0.106918    -0.0403643   -0.0971532   -0.0922966   -0.0523589   -0.0563859    0.0352809      -0.005975     -0.0287453    0.0488281   -0.0104575     0.0501049   -0.00169538    0.00717234    0.00268511   -0.00186255    0.0531922 \n",
       " -0.00560435   -0.012948    -0.0259934   -0.00416843   -0.00446534   0.039445     0.0148916    0.0358426    0.0340509    0.0193167    0.0208024   -0.0130161       0.00220435    0.010605    -0.0180141    0.00385809   -0.0184851    0.000625476  -0.00264608   -0.000990615   0.000687149  -0.0196241 \n",
       "  0.0811607     0.18751      0.376429     0.0603661     0.064666    -0.571232    -0.215656    -0.519064    -0.493116    -0.27974     -0.301255     0.188496    …  -0.0319228    -0.153579     0.260875    -0.0558718     0.267697    -0.00905798    0.0383199     0.0143458    -0.00995112    0.284192  \n",
       "  0.000780997   0.00180438   0.00362232   0.000580893   0.00062227  -0.00549687  -0.00207522  -0.00499487  -0.00474518  -0.00269189  -0.00289893   0.00181387     -0.000307188  -0.00147786   0.00251036  -0.000537645   0.00257601  -8.71635e-5    0.000368746   0.000138048  -9.5758e-5     0.00273473\n",
       " -0.0125774    -0.0290583   -0.058335    -0.0093549    -0.0100212    0.0885235    0.0334201    0.080439     0.0764179    0.0433511    0.0466853   -0.0292112       0.00494707    0.0238      -0.0404277    0.00865842   -0.0414849    0.00140371   -0.00593842   -0.00222317    0.00154212   -0.044041  \n",
       "  0.0808613     0.186818     0.37504      0.0601433     0.0644273   -0.569124    -0.21486     -0.517148    -0.491296    -0.278707    -0.300143     0.187801       -0.0318051    -0.153012     0.259913    -0.0556656     0.266709    -0.00902456    0.0381785     0.0142929    -0.0099144     0.283143  \n",
       "  0.30032       0.693843     1.3929       0.223373      0.239284    -2.11373     -0.797992    -1.92069     -1.82468     -1.03512     -1.11474      0.697494       -0.118124     -0.568288     0.965319    -0.206743      0.990561    -0.0335173     0.141795      0.0530839    -0.0368222     1.0516    \n",
       "  0.125113      0.289054     0.58028      0.0930567     0.0996851   -0.880577    -0.332442    -0.800157    -0.760158    -0.43123     -0.464397     0.290575    …  -0.0492103    -0.236748     0.40215     -0.0861286     0.412666    -0.0139632     0.0590717     0.0221147    -0.01534       0.438093  \n",
       "  0.00724021    0.0167274    0.0335806    0.00538515    0.00576874  -0.0509586   -0.0192383   -0.0463048   -0.04399     -0.0249551   -0.0268744    0.0168154      -0.00284778   -0.0137005    0.0232722   -0.00498422    0.0238808   -0.000808047   0.00341845    0.00127977   -0.000887722   0.0253522 \n",
       " -0.101166     -0.233728    -0.469213    -0.0752453    -0.080605     0.712031     0.268811     0.647004     0.614661     0.348691     0.375509    -0.234958        0.0397913     0.191433    -0.325177     0.0696433    -0.33368      0.0112906    -0.0477651    -0.0178818     0.0124039    -0.35424   \n",
       "  ⋮                                                                  ⋮                                                                ⋮                        ⋱   ⋮                                                                  ⋮                                                                 \n",
       "  0.163128      0.376883     0.7566       0.121332      0.129975    -1.14814     -0.433455    -1.04329     -0.991134    -0.56226     -0.605505     0.378867       -0.064163     -0.308684     0.524344    -0.112299      0.538055    -0.018206      0.0770207     0.0288343    -0.0200012     0.571208  \n",
       " -0.144778     -0.334488    -0.67149     -0.107684     -0.115354     1.01899      0.384696     0.925928     0.879642     0.499011     0.537392    -0.336248        0.0569453     0.27396     -0.465361     0.0996665    -0.47753      0.016158     -0.0683567    -0.0255907     0.0177512    -0.506953  \n",
       "  0.152817      0.35306      0.708774     0.113663      0.121759    -1.07557     -0.406056    -0.977339    -0.928483    -0.526718    -0.56723      0.354918    …  -0.0601072    -0.289172     0.491199    -0.1052        0.504044    -0.0170552     0.0721521     0.0270116    -0.0187369     0.535101  \n",
       "  0.248279      0.573612     1.15154      0.184666      0.19782     -1.74746     -0.659713    -1.58787     -1.50849     -0.855752    -0.92157      0.57663        -0.0976553    -0.469813     0.798045    -0.170918      0.818913    -0.0277093     0.117225      0.0438854    -0.0304415     0.869371  \n",
       "  0.121723      0.281224     0.564562     0.0905359     0.0969849   -0.856724    -0.323437    -0.778483    -0.739567    -0.419548    -0.451817     0.282704       -0.0478773    -0.230335     0.391256    -0.0837955     0.401487    -0.013585      0.0574715     0.0215156    -0.0149245     0.426225  \n",
       " -0.0126235    -0.0291647   -0.0585486   -0.00938914   -0.0100579    0.0888476    0.0335424    0.0807335    0.0766977    0.0435098    0.0468562   -0.0293181       0.00496518    0.0238871   -0.0405757    0.00869012   -0.0416368    0.00140885   -0.00596016   -0.00223131    0.00154777   -0.0442023 \n",
       "  0.100222      0.231547     0.464835     0.0745433     0.0798531   -0.705389    -0.266304    -0.640968    -0.608927    -0.345438    -0.372006     0.232766       -0.0394201    -0.189647     0.322143    -0.0689936     0.330567    -0.0111853     0.0473195     0.017715     -0.0122882     0.350935  \n",
       "  0.0727097     0.167985     0.337233     0.0540803     0.0579325   -0.511751    -0.1932      -0.465015    -0.441769    -0.250611    -0.269886     0.168869    …  -0.0285988    -0.137587     0.233711    -0.050054      0.239823    -0.0081148     0.0343298     0.012852     -0.00891494    0.254599  \n",
       " -0.212483     -0.490911    -0.985512    -0.158042     -0.169299     1.49552      0.564599     1.35894      1.291        0.732373     0.788702    -0.493494        0.0835758     0.402077    -0.682986     0.146275     -0.700846     0.0237143    -0.100324     -0.0375582     0.0260526    -0.744029  \n",
       " -0.161454     -0.373015    -0.748834    -0.120087     -0.128641     1.13636      0.429006     1.03258      0.98096      0.556488     0.599289    -0.374978        0.0635044     0.305516    -0.518962     0.111146     -0.532532     0.0180191    -0.0762301    -0.0285383     0.0197959    -0.565345  \n",
       " -0.134916     -0.311703    -0.62575     -0.100348     -0.107496     0.949577     0.358492     0.862856     0.819723     0.46502      0.500786    -0.313344        0.0530664     0.255299    -0.433662     0.0928775    -0.445002     0.0150574    -0.0637004    -0.0238475     0.0165421    -0.472421  \n",
       "  0.0767447     0.177307     0.355947     0.0570815     0.0611474   -0.540151    -0.203922    -0.490821    -0.466285    -0.264519    -0.284864     0.17824        -0.0301859    -0.145222     0.246681    -0.0528318     0.253132    -0.00856513    0.0362349     0.0135653    -0.00940967    0.268729  ,\n",
       "\n",
       "50x1 Array{Float64,2}:\n",
       " -5.06378  \n",
       " -3.22421  \n",
       "  0.0908741\n",
       "  0.214549 \n",
       " -0.0791534\n",
       "  1.14628  \n",
       "  0.0110305\n",
       " -0.177638 \n",
       "  1.14205  \n",
       "  4.24158  \n",
       "  1.76704  \n",
       "  0.102258 \n",
       " -1.42882  \n",
       "  ⋮        \n",
       "  2.30395  \n",
       " -2.04478  \n",
       "  2.15832  \n",
       "  3.50659  \n",
       "  1.71917  \n",
       " -0.178289 \n",
       "  1.41549  \n",
       "  1.02692  \n",
       " -3.00102  \n",
       " -2.2803   \n",
       " -1.9055   \n",
       "  1.08391  ,\n",
       "\n",
       "100x50 Array{Float64,2}:\n",
       "  0.0030163     0.00217407    0.00234853   -0.000638146  -0.000309473   0.00485419    0.00161993   -0.0035739    -0.00362174    0.00196932   -0.000480938  …   0.000958916  -0.00559192   0.00370807    0.00501902   -0.0041593    -0.0018277    -0.00144922   -0.00192125   -0.000788159   0.00215135 \n",
       "  0.00287514    0.00207232    0.00223861   -0.00060828   -0.000294989   0.00462701    0.00154412   -0.00340663   -0.00345223    0.00187715   -0.00045843       0.000914038  -0.00533021   0.00353452    0.00478412   -0.00396464   -0.00174216   -0.0013814    -0.00183134   -0.000751272   0.00205066 \n",
       "  0.00490757    0.00353724    0.00382108   -0.00103827   -0.000503517   0.00789783    0.00263566   -0.00581478   -0.00589261    0.00320411   -0.000782493      0.00156017   -0.00909813   0.00603308    0.00816601   -0.00676724   -0.0029737    -0.0023579    -0.00312591   -0.00128234    0.00350027 \n",
       "  0.000866715   0.000624703   0.000674833  -0.000183367  -8.8925e-5     0.00139482    0.000465477  -0.00102694   -0.00104068    0.000565871  -0.000138194      0.000275538  -0.0016068    0.00106549    0.00144218   -0.00119515   -0.000525178  -0.000416424  -0.000552059  -0.000226472   0.000618175\n",
       "  0.0075607     0.00544954    0.00588684   -0.00159958   -0.000775729   0.0121676     0.00406055   -0.00895838   -0.00907829    0.00493633   -0.00120553       0.00240363   -0.0140168    0.00929469    0.0125807    -0.0104258    -0.00458134   -0.00363264   -0.00481584   -0.00197561    0.00539259 \n",
       " -0.00727022   -0.00524017   -0.00566067    0.00153813    0.000745925  -0.0117001    -0.00390454    0.00861419    0.00872949   -0.00474667    0.00115921   …  -0.00231128    0.0134782   -0.00893758   -0.0120974     0.0100252     0.00440532    0.00349307    0.00463081    0.0018997    -0.0051854  \n",
       " -0.00115349   -0.000831403  -0.00089812    0.000244039   0.000118348  -0.00185633   -0.000619493   0.00136673    0.00138502   -0.000753105   0.00018392      -0.000366707   0.00213845  -0.00141803   -0.00191937    0.00159059    0.000698948   0.000554209   0.000734723   0.000301406  -0.000822715\n",
       " -0.00860387   -0.00620142   -0.00669906    0.00182028    0.000882757  -0.0138463    -0.00462079    0.0101944     0.0103308    -0.0056174     0.00137185      -0.00273527    0.0159507   -0.0105771    -0.0143165     0.0118642     0.00521344    0.00413384    0.00548029    0.00224819   -0.00613662 \n",
       " -0.0010364    -0.000747008  -0.000806952   0.000219267   0.000106335  -0.0016679    -0.000556609   0.00122799    0.00124443   -0.000676658   0.00016525      -0.000329483   0.00192138  -0.00127409   -0.00172453    0.00142913    0.000627998   0.000497952   0.000660142   0.000270811  -0.000739201\n",
       " -0.00560675   -0.00404119   -0.00436547    0.0011862     0.000575253  -0.00902304   -0.00301116    0.00664322    0.00673214   -0.0036606     0.000893975     -0.00178245    0.0103943   -0.00689261   -0.00932942    0.00773137    0.00339736    0.00269384    0.00357125    0.00146504   -0.00399895 \n",
       " -0.000562471  -0.000405413  -0.000437946   0.000119      5.77096e-5   -0.000905194  -0.000302081   0.00066645    0.00067537   -0.000367233   8.96839e-5   …  -0.000178816   0.00104276  -0.000691469  -0.000935931   0.000775614   0.000340825   0.000270247   0.00035827    0.000146973  -0.000401177\n",
       " -0.00249337   -0.00179715   -0.00194136    0.000527511   0.00025582   -0.00401262   -0.00133909    0.00295429    0.00299384   -0.0016279     0.000397558     -0.00079267    0.00462245  -0.0030652    -0.00414887    0.00343821    0.00151084    0.00119797    0.00158817    0.000651516  -0.00177837 \n",
       " -0.0054028    -0.00389418   -0.00420667    0.00114305    0.000554327  -0.00869481   -0.00290162    0.00640156    0.00648725   -0.00352744    0.000861456     -0.00171761    0.0100162   -0.00664188   -0.00899005    0.00745013    0.00327378    0.00259584    0.00344135    0.00141175   -0.00385349 \n",
       "  ⋮                                                                     ⋮                                                                     ⋮            ⋱   ⋮                                                                    ⋮                                                                  \n",
       "  0.000718133   0.00051761    0.000559146  -0.000151932  -7.36805e-5    0.0011557     0.00038568   -0.000850888  -0.000862277   0.000468864  -0.000114504      0.000228303  -0.00133135   0.000882831   0.00119495   -0.000990263  -0.000435147  -0.000345036  -0.00045742   -0.000187648   0.000512201\n",
       "  0.00987886    0.0071204     0.00769178   -0.00209003   -0.00101357    0.0158982     0.00530554   -0.0117051    -0.0118617     0.00644983   -0.00157515       0.0031406    -0.0183144    0.0121445     0.0164381    -0.0136224    -0.00598601   -0.00474643   -0.0062924    -0.00258134    0.00704599 \n",
       "  0.000748958   0.000539828   0.000583147  -0.000158454  -7.68432e-5    0.00120531    0.000402235  -0.000887411  -0.000899289   0.000488989  -0.000119419  …   0.000238102  -0.00138849   0.000920726   0.00124624   -0.00103277   -0.000453825  -0.000359847  -0.000477054  -0.000195702   0.000534186\n",
       " -0.00396946   -0.00286108   -0.00309066    0.000839802   0.000407267  -0.00638812   -0.00213184    0.00470326    0.00476621   -0.00259163    0.000632916     -0.00126194    0.00735898  -0.00487982   -0.00660504    0.00547365    0.00240526    0.00190718    0.00252837    0.00103722   -0.00283118 \n",
       "  0.00539228    0.0038866     0.00419848   -0.00114082   -0.000553248   0.00867788    0.00289597   -0.00638909   -0.00647461    0.00352058   -0.000859778      0.00171426   -0.00999673   0.00662895    0.00897255   -0.00743563   -0.0032674    -0.00259079   -0.00343464   -0.001409      0.00384598 \n",
       "  0.00390717    0.00281617    0.00304216   -0.000826621  -0.000400875   0.00628787    0.00209838   -0.00462944   -0.00469141    0.00255096   -0.000622983      0.00124213   -0.00724348   0.00480324    0.00650138   -0.00538775   -0.00236751   -0.00187725   -0.00248869   -0.00102094    0.00278674 \n",
       "  0.00481922    0.00347355    0.00375229   -0.00101958   -0.000494452   0.00775565    0.00258821   -0.0057101    -0.00578653    0.00314643   -0.000768406      0.00153208   -0.00893433   0.00592446    0.008019     -0.00664541   -0.00292016   -0.00231545   -0.00306963   -0.00125926    0.00343725 \n",
       " -0.00302538   -0.00218061   -0.00235559    0.000640066   0.000310404  -0.00486879   -0.00162481    0.00358465    0.00363263   -0.00197525    0.000482385  …  -0.000961802   0.00560874  -0.00371922   -0.00503412    0.00417182    0.0018332     0.00145358    0.00192703    0.00079053   -0.00215782 \n",
       "  0.00123844    0.000892634   0.000964264  -0.000262012  -0.000127064   0.00199305    0.000665117  -0.00146738   -0.00148702    0.00080857   -0.000197465      0.000393715  -0.00229595   0.00152247    0.00206072   -0.00170774   -0.000750423  -0.000595026  -0.000788834  -0.000323604   0.000883306\n",
       " -0.00611648   -0.00440858   -0.00476235    0.00129404    0.000627551  -0.00984335   -0.00328491    0.00724717    0.00734417   -0.0039934     0.000975249     -0.0019445     0.0113393   -0.00751924   -0.0101776     0.00843426    0.00370623    0.00293874    0.00389593    0.00159823   -0.00436251 \n",
       " -0.00403222   -0.00290631   -0.00313953    0.000853079   0.000413706  -0.00648912   -0.00216554    0.00477762    0.00484157   -0.00263261    0.000642922     -0.00128189    0.00747532  -0.00495697   -0.00670946    0.00556019    0.00244329    0.00193733    0.00256835    0.00105362   -0.00287594 \n",
       " -0.00174161   -0.0012553    -0.00135603    0.000368464   0.000178689  -0.00280279   -0.000935346   0.00206356    0.00209118   -0.00113708    0.000277692     -0.000553676   0.00322876  -0.00214103   -0.00289797    0.00240157    0.00105531    0.000836777   0.00110933    0.000455081  -0.00124218 ,\n",
       "\n",
       "100x1 Array{Float64,2}:\n",
       "  0.125297 \n",
       "  0.119433 \n",
       "  0.203861 \n",
       "  0.0360034\n",
       "  0.314072 \n",
       " -0.302005 \n",
       " -0.0479161\n",
       " -0.357405 \n",
       " -0.0430521\n",
       " -0.232905 \n",
       " -0.0233651\n",
       " -0.103575 \n",
       " -0.224433 \n",
       "  ⋮        \n",
       "  0.0298313\n",
       "  0.410369 \n",
       "  0.0311118\n",
       " -0.164892 \n",
       "  0.223996 \n",
       "  0.162304 \n",
       "  0.200191 \n",
       " -0.125674 \n",
       "  0.051445 \n",
       " -0.254079 \n",
       " -0.167499 \n",
       " -0.0723464)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr = RAE(LL,word_indexes,indexed_words);\n",
    "c_1 = eval_word_embedding(rr,\"killer\")''\n",
    "c_2 = eval_word_embedding(rr, \"cows\")''\n",
    "c_12 = [c_1; c_2]\n",
    "p_12=eval_merges(rr, c_1'', c_2'')\n",
    "ĉ_12 = unfold_merges(rr,p_12)\n",
    "#eval_scores_gradient(rr, ĉ_12, p_12, c_12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eval_to_tree (generic function with 2 methods)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function eval_to_tree(rr::RAE,sentence::String)\n",
    "    eval_to_tree(rr, tokenize(sentence))\n",
    "end\n",
    "\n",
    "function eval_to_tree(rr::RAE, sentence::Words)\n",
    "    tree = tuple(sentence...)\n",
    "    cs = eval_word_embeddings(rr, sentence)\n",
    "    score_total = 0.0\n",
    "    while(size(cs,2)>1)\n",
    "        c_is = cs[:, 1:end-1]\n",
    "        c_js = cs[:, 2:end]\n",
    "        \n",
    "        pps = eval_merges(rr, c_is, c_js)\n",
    "        scores = eval_scores(rr, c_is, c_js, pps)\n",
    "        im = indmax(scores)\n",
    "        \n",
    "        score_total+=scores[im]\n",
    "         \n",
    "        cs = [cs[:,1:im-1] pps[:,im] cs[:,im+2:end]]\n",
    "        tree = tuple(tree[1:im-1]..., (tree[im], tree[im+1]), tree[im+2:end]...)\n",
    "    end\n",
    "    tree = tree[1] #The final step creates a tuple containing one element, as first and last parts are empty\n",
    "    embedding = cs[:]\n",
    "    tree, embedding, score_total\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((((\"the\",\"boy\"),\"destroyed\"),(\"the\",\"house\")),[-0.00703771,0.0261892,0.0165252,0.0143384,-0.0128244,0.0052343,0.00878562,0.00292834,-0.0146886,-0.0065181  …  -0.00387706,-0.000136323,0.0131976,0.00165049,0.0189463,-0.00262567,0.0107307,0.0139779,-0.0053516,0.0120967],8.204136328437572)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr = RAE(LL,word_indexes,indexed_words);\n",
    "\n",
    "sent = \"the boy destroyed the house\"\n",
    "sent_toks = tokenize(sent)\n",
    "\n",
    "tree, pp, score_total = eval_to_tree(rr,sent_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Iterators\n",
    "@pyimport nltk.corpus as nltk_corpus\n",
    "n_training = 5\n",
    "training_sents = @pipe nltk_corpus.brown[:sents]() |> take(_,n_training)  |> collect |> convert(Vector{Vector{String}},_);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using Optim #https://github.com/JuliaOpt/Optim.jl\n",
    "\n",
    "rae_outer = RAE(LL,word_indexes,indexed_words);\n",
    "\n",
    "function unpack!(rae::RAE, θ::Vector)\n",
    "    W_e_len = length(rae.W_e)\n",
    "    b_e_len = length(rae.b_e)\n",
    "    W_d_len = length(rae.W_d)\n",
    "    b_d_len = length(rae.b_d)\n",
    "    W_e_shape = size(rae.W_e)\n",
    "    W_d_shape = size(rae.W_d)\n",
    "    \n",
    "    rae.W_e = reshape(θ[1: W_e_len],W_e_shape)\n",
    "    rae.b_e = θ[W_e_len+1: b_e_len]\n",
    "    rae.W_d = reshape(θ[W_e_len+b_e_len+1: W_e_len+b_e_len+W_d_len],W_d_shape)\n",
    "    rae.b_d = θ[W_e_len+b_e_len+W_d_len+1: end]\n",
    "    \n",
    "    rae\n",
    "end\n",
    "\n",
    "function pack(rae::RAE)\n",
    "    [rae.W_e[:],rae.b_e[:], rae.W_d[:],rae.b_d[:],] \n",
    "end\n",
    "\n",
    "#--------------------------------------------------------\n",
    "\n",
    "function loss(θ::Vector)  \n",
    "    rae = unpack!(rae_outer, θ)\n",
    "    err =@pipe training_sents |> map( ss-> eval_to_tree(rae, ss)[3], _) |> mean\n",
    "    err\n",
    "end\n",
    "\n",
    "function loss_grad(θ::Vector, storage::Vector)   \n",
    "    rae = unpack!(rae_outer, θ)\n",
    "    err =@pipe training_sents |> map( ss-> eval_to_tree(rae, ss)[3], _) |> mean\n",
    "    err\n",
    "end\n",
    "\n",
    "function loss_grad_and_loss_grad(θ::Vector, storage::Vector)   \n",
    "    rae = unpack!(rae_outer, θ)\n",
    "    err =@pipe training_sents |> map( ss-> eval_to_tree(rae, ss)[3], _) |> mean\n",
    "    println(\"**\", err)\n",
    "    err\n",
    "end\n",
    "\n",
    "f=DifferentiableFunction(loss,loss_grad!,loss_and_loss_grad!)\n",
    "#Must provide Graident as finite difference requires ~length(θ) calls to f\n",
    "res = optimize(f, pack(rae_outer), method=:l_bfgs, show_trace = true, store_trace=true, iterations = 10)\n",
    "println(res)\n",
    "rae_outer = unpack!(rae_outer, res.minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/JuliaLang/julia/blob/master/doc/manual/profile.rst Actual instructions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Profile.clear()\n",
    "@profile f(pack(rae_outer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using ProfileView\n",
    "ProfileView.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tree data in tree is not use, other than it's structure.\n",
    "#(((\"the\",\"house\"),(\"destroyed\",(\"the\",\"boy\")))  is equivalent to (((\"\",\"\"),(\"\",(\"\",\"\"))) \n",
    "\n",
    "\n",
    "\n",
    "function unfold(rae::RAE, tree::(String,String), pp::Embedding)\n",
    "    ĉ_is, ĉ_js = reconstruct(rae, pp)\n",
    "    [ĉ_is ĉ_js]\n",
    "end\n",
    "\n",
    "\n",
    "function unfold(rae::RAE, tree::(Any,String), pp::Embedding)\n",
    "    p̂_is, ĉ_js = reconstruct(rae, pp)\n",
    "    ĉ_is = unfold(rae, tree[1], p̂_is)\n",
    "    [ĉ_is ĉ_js]\n",
    "end\n",
    "\n",
    "function unfold(rae::RAE, tree::(String,Any), pp::Embedding)\n",
    "    ĉ_is, p̂_js = reconstruct(rae, pp)\n",
    "    ĉ_js = unfold(rae, tree[2], p̂_js)\n",
    "    [ĉ_is ĉ_js]\n",
    "    \n",
    "end\n",
    "\n",
    "function unfold(rae::RAE, tree::(Any,Any), pp::Embedding)\n",
    "    p̂_is, p̂_js = reconstruct(rae, pp)\n",
    "    ĉ_is = unfold(rae, tree[1], p̂_is)\n",
    "    ĉ_js = unfold(rae, tree[2], p̂_js)\n",
    "    [ĉ_is ĉ_js]\n",
    "end\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree, pp, score_total = eval_to_tree(rr,\"killer cows\")\n",
    "ĉs = unfold(rr,tree,pp)\n",
    "\n",
    "show_bests(rr, ĉs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function cosine_dist(a,b)\n",
    "    (a⋅b)/(norm(a)*norm(b))\n",
    "end\n",
    "\n",
    "function neighbour_dists(cc::Vector{Float64}, globe::Matrix{Float64})\n",
    "    [cosine_dist(cc, globe[:,ii]) for ii in 1:size(globe,2)]\n",
    "end\n",
    "\n",
    "\n",
    "function show_best(rae::RAE,ĉ::Embedding, nbest=20)\n",
    "    candidates=neighbour_dists(ĉ,rae.L)   \n",
    "    best_cands = [ (findfirst(candidates,score), score)\n",
    "                    for score in select(candidates,1:nbest, rev=true)[1:nbest]]\n",
    "    vcat([[rae.indexed_words[ii] score] for (ii,score) in best_cands]...)\n",
    "end\n",
    "\n",
    "function show_bests(rae::RAE,ĉs::Embeddings, nbest=20)\n",
    "    hcat([show_best(rae,ĉs[:,ii],nbest) for ii in 1:size(ĉs,2)]...)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function depth_inc(ele::(Int,String))\n",
    "    (ele[1]+1,ele[2])\n",
    "end\n",
    "\n",
    "function unfold_struct(tree::(Any,Any))\n",
    "    left_tree = unfold_struct(tree[1]) \n",
    "    left = @pipe left_tree |> map(depth_inc,_)\n",
    "    right_tree = unfold_struct(tree[2]) \n",
    "    right = @pipe right_tree |> map(depth_inc,_)\n",
    "    [left, right, (0,\"\")]\n",
    "end\n",
    "\n",
    "function unfold_struct(tree::(Any,String))\n",
    "    left_tree = unfold_struct(tree[1]) \n",
    "    left = @pipe left_tree |> map(depth_inc,_)\n",
    "    [left, (0,tree[2]), (0,\"\")]\n",
    "end\n",
    "function unfold_struct(tree::(String,Any))\n",
    "    right_tree = unfold_struct(tree[2]) \n",
    "    right = @pipe right_tree |> map(depth_inc,_)\n",
    "    [(0,tree[1]),right, (0,\"\")]\n",
    "end\n",
    "function unfold_struct(tree::(String,String))\n",
    "    [(0,tree[1]), (0, tree[2]), (0,\"\")]\n",
    "end\n",
    "\n",
    "function print_tree(tree::(Any,Any))\n",
    "    \n",
    "    for (depth,word ) in unfold_struct(tree)\n",
    "        println(\"\\t\"^depth, word)\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0-dev",
   "language": "julia",
   "name": "julia 0.4"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
