{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Compat\n",
    "using Docile\n",
    "using Iterators\n",
    "using Pipe\n",
    "using Devectorize\n",
    "\n",
    "macro printval(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\" = \", $ee)))\n",
    "end\n",
    "\n",
    "macro pz(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\"\\t\\t\",typeof($ee), \"\\t\", size($ee))))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unzip (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function unzip(xs)\n",
    "    [zip(xs...)...]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11-element Array{Union(ASCIIString,UTF8String),1}:\n",
       " \"/root/buildFromSource/julia/usr/local/share/julia/site/v0.3\"\n",
       " \"/root/buildFromSource/julia/usr/share/julia/site/v0.3\"      \n",
       " \"../Corpus\"                                                  \n",
       " \"../doc2vec\"                                                 \n",
       " \"../Models\"                                                  \n",
       " \"../Optimisation\"                                            \n",
       " \"../recursive_embeddings\"                                    \n",
       " \"../summaristation\"                                          \n",
       " \"../tools\"                                                   \n",
       " \"../util\"                                                    \n",
       " \"../word-embedding3\"                                         "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "push!(LOAD_PATH, map(x->\"../\"*x, filter(fn-> !(contains(fn,\".\")),readdir(\"..\")))...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using WordEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\t\tArray{Array{String,1},1}\t(6097,)\n"
     ]
    }
   ],
   "source": [
    "training = open(\"../Corpus/serialised/opinosis_train_dev_plain.jsz\",\"r\") do fs\n",
    "    deserialize(fs)\n",
    "end\n",
    "@pz training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Array{String,1},1}:\n",
       " String[\"being\",\"able\",\"to\",\"change\",\"the\",\"*UNKNOWN*\",\"sizes\",\"is\",\"awesome\",\"!\"]                                                                     \n",
       " String[\"for\",\"whatever\",\"reason\",\",\",\"*UNKNOWN*\",\"decided\",\"to\",\"make\",\"the\",\"*UNKNOWN*\",\"on\",\"the\",\"home\",\"screen\",\"than\",\"on\",\"the\",\"*UNKNOWN*\",\".\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training=training[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pad (generic function with 2 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function pad{S<:String}(sent::Vector{S}, padded_length, pad_word=\"*STARTPAD*\")\n",
    "    if length(sent) <= padded_length\n",
    "        ret =  fill(pad_word,padded_length)\n",
    "        ret[end-length(sent)+1:end] = sent\n",
    "        ret\n",
    "    else\n",
    "        sent\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "add_all_words! (generic function with 4 methods)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import WordEmbeddings.WE\n",
    "function WE(N::DataType,S::DataType, embedding_width::Int)\n",
    "    L=Array(N,(embedding_width,0))\n",
    "    word_index=Dict{S,Int}()\n",
    "    indexed_words=S[]\n",
    "    WE(L,word_index,indexed_words)\n",
    "end\n",
    "\n",
    "function WE_light{N,S}(we::WE{N,S}, N2=N::DataType)\n",
    "    L=convert(Matrix{N2},we.L)\n",
    "    word_index=Dict{S,Int}()\n",
    "    indexed_words=S[]\n",
    "    WE(L,word_index,indexed_words)\n",
    "end\n",
    "\n",
    "function WE_light{N,S}(we::WE{N,S})\n",
    "    L=we.L\n",
    "    word_index=Dict{S,Int}()\n",
    "    indexed_words=S[]\n",
    "    WE(L,word_index,indexed_words)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "@doc \"Gets the word index, or creates one if it doesn't already exist\" ->\n",
    "function get_word_index!{N,S, S2}(we::WE{N,S}, word::S2, word_varience = 0.01)\n",
    "    if (word in keys(we.word_index))\n",
    "        we.word_index[word]\n",
    "    else\n",
    "        index = length(we.indexed_words)+1\n",
    "        we.word_index[word]=index\n",
    "        push!(we.indexed_words,word)\n",
    "        \n",
    "        embedding = convert(Vector{N},word_varience.*randn(size(we.L,1)))\n",
    "        we.L = hcat(we.L,embedding)\n",
    "        index\n",
    "    end\n",
    "end\n",
    "\n",
    "function add_all_words!{N,S}(we::WE{N,S}, words::Vector{S}, word_varience=0.01)\n",
    "    for word in words\n",
    "        get_word_index!(we, word, word_varience)\n",
    "    end\n",
    "    we\n",
    "end\n",
    "function add_all_words!{N,S}(we::WE{N,S}, paras::Vector{Vector{S}}, word_varience=0.01)\n",
    "    for para in paras\n",
    "        add_all_words!(we, para, word_varience)\n",
    "    end\n",
    "    we\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PVDM_light (generic function with 2 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type PVDM{N<:Number, S<:AbstractString}\n",
    "    we::WE\n",
    "    pe::WE #use a word embedder for Paragraphs too\n",
    "    \n",
    "    W::AbstractMatrix{N}\n",
    "    b::AbstractVector{N}\n",
    "\n",
    "    window_length::Int\n",
    "    varience::N\n",
    "end\n",
    "\n",
    "function PVDM{N,S}(we::WE{N,S}, window_length::Int, varience=0.001)\n",
    "    \n",
    "    emb_width,n_words = size(we.L)\n",
    "    concat_layer_width = emb_width*(window_length+1)\n",
    "    const W = convert(Matrix{N}, varience*randn(n_words,concat_layer_width))\n",
    "    const b = convert(Vector{N}, varience*randn(n_words))\n",
    "    \n",
    "    pe = WE(N,Vector{S},emb_width)\n",
    "    \n",
    "    PVDM{N,S}( we, pe, W,b, window_length, varience)\n",
    "end\n",
    "\n",
    "@doc \"Lightwieght version, that does not have support for lookups or additions\" ->\n",
    "function PVDM_light{N,S}(pvdm::PVDM{N,S})\n",
    "    we = WE_light(pvdm.we)\n",
    "    pe = WE_light(pvdm.pe)\n",
    "    W = pvdm.W\n",
    "    b = pvdm.b\n",
    "    window_length = pvdm.window_length\n",
    "    varience = nan(N)\n",
    "    PVDM{N,S}( we, pe, W,b, window_length, varience)\n",
    "end\n",
    "\n",
    "@doc \"Lightwieght version, that does not have support for lookups or additions, and has N converted type N2\" ->\n",
    "function PVDM_light{N,S}(pvdm::PVDM{N,S}, N2::DataType)\n",
    "    we = WE_light(pvdm.we,N2)\n",
    "    pe = WE_light(pvdm.pe, N2)\n",
    "    W = convert(Matrix{N2},pvdm.W)\n",
    "    b = convert(Vector{N2},pvdm.b)\n",
    "    window_length = pvdm.window_length\n",
    "    varience = convert(N2,NaN)\n",
    "    PVDM{N2,S}( we, pe, W,b, window_length, varience)\n",
    "end\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_para_training_cases! (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc \"\"\"gets the training cases as vector of (paraIndex, [word_indexes], label_word_index),\n",
    "cycling by the window length.\n",
    "Adds the paragraph if it does not already have an index\n",
    "\"\"\" ->\n",
    "function get_para_training_cases!{S<:String}(pvdm::PVDM, para::Vector{S})\n",
    "    para_ind = get_word_index!(pvdm.pe, para)\n",
    "    \n",
    "    Task() do \n",
    "        @assert length(para)>=pvdm.window_length+1\n",
    "        for offset in 0:length(para)-(pvdm.window_length+1)\n",
    "            window_iis = [1:pvdm.window_length;]+offset\n",
    "            label_ii = pvdm.window_length+1+offset\n",
    "            \n",
    "            window_words = para[window_iis]\n",
    "            label_word = para[label_ii]\n",
    "                        \n",
    "            windows_indexes = map(word->get_word_index(pvdm.we, word), window_words)\n",
    "            label_index = get_word_index(pvdm.we, label_word)\n",
    "            \n",
    "            produce(Int64[para_ind, windows_indexes..., label_index])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_input_layers (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_input_layer(pvdm::PVDM, para_index::Int, window_indexes::Vector{Int})\n",
    "    @inbounds [pvdm.pe.L[:,para_index], vec(pvdm.we.L[:,window_indexes])]\n",
    "end \n",
    "\n",
    "function get_input_layers{N,S, I<:Int}(pvdm::PVDM{N,S}, para_indexes::Vector{I}, window_indexeses::Matrix{I})\n",
    "    const emb_width = size(pvdm.we.L,1)\n",
    "    const n_training = length(para_indexes)\n",
    "    \n",
    "    xs = Array(N,(emb_width * (pvdm.window_length+1),n_training))\n",
    "    @inbounds xs[1:emb_width,:] = pvdm.pe.L[:,para_indexes]\n",
    "    for training_case in 1:n_training\n",
    "        @inbounds const window_indexes = window_indexeses[:,training_case]\n",
    "        @inbounds xs[emb_width+1:end,training_case] = vec(pvdm.we.L[:,window_indexes])\n",
    "    end\n",
    "    xs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feedforward (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax(zs)\n",
    "    (1./sum(exp(zs),1)).*exp(zs)\n",
    "end\n",
    "\n",
    "function feedforward{N,S, I<:Int}(pvdm::PVDM{N,S}, para_indexes::Vector{I}, window_indexeses::Matrix{I})\n",
    "    xs = get_input_layers(pvdm, para_indexes, window_indexeses)\n",
    "    \n",
    "    \n",
    "    #Speed optimised version of `zs = pvdm.W*xs .+ pvdm.b`\n",
    "    zs = pvdm.W*xs \n",
    "    const n_training = length(para_indexes)\n",
    "    for ii in 1:n_training\n",
    "        @inbounds zs[:,ii]+= pvdm.b\n",
    "    end\n",
    "    ŷs = softmax(zs)\n",
    "    ŷs, xs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss{I,N}(y_indexes::Vector{I}, ŷs::Matrix{N})\n",
    "    #C=−∑j yj*log ŷj,\n",
    "    c = zero(N)\n",
    "    for tc in 1:length(y_indexes)\n",
    "        @inbounds c-=log(ŷs[y_indexes[tc],tc])\n",
    "    end\n",
    "    return c\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backprop{N,S,I}(pvdm::PVDM{N,S}, y_indexes::Vector{I}, ŷs::Matrix{N}, xs::Matrix{N} , para_indexes::Vector{I}, window_indexeses::Matrix{I} )\n",
    "    const emb_width = size(pvdm.we.L,1)\n",
    "    const n_training = length(para_indexes)\n",
    "    const window_len = pvdm.window_length \n",
    "    #Δb = zeros(pvdm.b)\n",
    "    #ΔW = zeros(pvdm.W)\n",
    "    ΔL = zeros(pvdm.we.L) #Word Vector Changes\n",
    "    ΔD = zeros(pvdm.pe.L) #Paragraph Vector Changes\n",
    "     \n",
    "        \n",
    "    #speed optimistation of `δ_top_s = ŷs.-ys`\n",
    "    δ_top_s = copy(ŷs)\n",
    "    for tc in 1:length(y_indexes)\n",
    "        @inbounds δ_top_s[y_indexes[tc],tc]-=one(typeof(ŷs[1]))\n",
    "    end\n",
    "\n",
    "    Δb = sum(δ_top_s,2) |> vec\n",
    "    ΔW = (δ_top_s * xs')\n",
    "    δ_input_s= (pvdm.W'*δ_top_s) #the activation function of the layer below dxs=d(1*D[ii];L[iis]) =1\n",
    "    \n",
    "    #Paragraph vector Error\n",
    "    for ii in 1:n_training #Add sequentially, reather than via in a += as that would only allow one add for repreased index\n",
    "        @inbounds ΔD[:,para_indexes[ii]] += δ_input_s[1:emb_width,ii]\n",
    "    end\n",
    "    \n",
    "    #word vectors\n",
    "    for ii in 1:n_training\n",
    "        for ww in 1:window_len\n",
    "            const offset=ww*emb_width\n",
    "            @inbounds ΔL[:,window_indexeses[ww,ii]]+=δ_input_s[offset+1:offset+emb_width, ii]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "   \n",
    "    ΔL./n_training, ΔD./n_training, ΔW./n_training, Δb./n_training\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "200x21 Array{Float32,2}:\n",
       " -5.31151e-5    1.14947e-5   -0.000253849  …  -9.74821e-5    0.00013128   0.0\n",
       " -1.81692e-6    9.70491e-6    0.000441142     -0.000233459   0.000103386  0.0\n",
       " -2.04389e-5    0.000112936   6.72476e-5       0.000111273  -0.000144194  0.0\n",
       "  3.26516e-5   -2.14918e-5    8.41979e-5      -8.72675e-5   -0.000267858  0.0\n",
       " -2.2649e-5    -3.30211e-5    0.000165276     -7.8168e-5    -0.000144109  0.0\n",
       " -3.01287e-5    3.59327e-5   -0.000257088  …   0.000146849  -1.05515e-5   0.0\n",
       "  1.96402e-5   -8.70343e-5    0.000150983     -1.45888e-5   -0.000149753  0.0\n",
       "  5.33495e-5    0.000251187   0.000215159     -4.65493e-5    4.25495e-5   0.0\n",
       "  3.21489e-5    0.000102656   0.000205646     -3.72566e-5   -7.50417e-5   0.0\n",
       "  6.23096e-5   -7.15884e-5   -0.000168245     -0.000206094  -0.000221802  0.0\n",
       "  2.22059e-5   -4.63102e-6   -6.6539e-5    …  -0.00022765    0.00011634   0.0\n",
       " -4.93092e-5   -0.000137815  -0.000265181     -4.16604e-5   -2.75067e-5   0.0\n",
       " -5.34164e-5    5.94748e-5    0.000114689      0.00010779    0.000107494  0.0\n",
       "  ⋮                                        ⋱                              ⋮  \n",
       "  2.91895e-5    3.58467e-5    7.41651e-5      -0.000192493  -4.50771e-6   0.0\n",
       " -8.6757e-5     0.000109267  -0.000175846     -0.00012436   -0.000335226  0.0\n",
       " -9.43055e-5   -7.96284e-5    0.000342957  …   0.000276739   0.000202501  0.0\n",
       " -2.73386e-5   -4.3058e-5    -0.000415243     -7.8746e-5    -0.000182643  0.0\n",
       "  2.2645e-5     6.23607e-6    0.000105054      4.41647e-5   -2.11071e-5   0.0\n",
       "  3.63906e-5    0.000341096  -0.000316338     -3.82655e-5   -5.77809e-5   0.0\n",
       "  5.51715e-5   -0.000168824   0.000458944      2.59515e-5    0.00020274   0.0\n",
       " -5.16264e-5    2.10437e-5   -0.000303316  …   7.66891e-5   -0.000249368  0.0\n",
       " -0.000115873   0.000202359   0.000157952     -0.000112374   9.03893e-5   0.0\n",
       " -8.82695e-5   -0.000117615  -0.000108816      7.64217e-5    2.84419e-5   0.0\n",
       "  6.98407e-5   -6.68435e-6    0.000466628     -0.000170635   0.000306918  0.0\n",
       " -5.3392e-5    -1.23321e-6    0.00012059       7.80243e-5    3.12006e-5   0.0,\n",
       "\n",
       "200x2 Array{Float32,2}:\n",
       "  8.71753e-5   -0.000250921\n",
       " -1.40049e-6    9.3588e-5  \n",
       " -0.000101173  -0.000246265\n",
       " -0.000119398   0.000306929\n",
       "  5.96939e-5    0.000375854\n",
       " -2.95108e-5    3.91951e-5 \n",
       "  0.000132608  -8.94693e-5 \n",
       " -7.61915e-5   -0.000330046\n",
       " -7.19739e-5    0.000363519\n",
       "  8.24738e-5    0.000222188\n",
       "  3.55941e-5   -0.000264835\n",
       " -1.67416e-5    5.14219e-5 \n",
       " -0.000116975   0.000304717\n",
       "  ⋮                        \n",
       "  0.000112889  -0.000504654\n",
       " -1.92036e-5    0.000698116\n",
       " -1.83941e-7    0.000110186\n",
       " -3.72559e-6    0.000786792\n",
       "  0.000136766  -0.000757457\n",
       " -3.648e-5      0.000417244\n",
       " -9.22582e-5   -0.000132245\n",
       " -2.39983e-5    0.000569811\n",
       "  3.72481e-5   -5.67487e-5 \n",
       " -3.17137e-5   -0.000304237\n",
       "  9.42312e-5   -0.000140353\n",
       "  5.2595e-5     0.000263857,\n",
       "\n",
       "21x1800 Array{Float32,2}:\n",
       " -0.000540039   2.02778e-5    0.000611038  …  -0.000103693   0.000263438\n",
       " -0.000539736   2.02523e-5    0.000610658     -0.000103611   0.00026319 \n",
       " -0.000540498   2.02886e-5    0.00061154      -0.000103839   0.000263606\n",
       " -0.000539869   2.02487e-5    0.000610786     -0.000103672   0.000263269\n",
       "  0.00294987    9.80466e-5   -0.00279863       0.00116034   -0.00316742 \n",
       "  0.00178607    7.21328e-5   -0.00166153   …  -0.000902338  -0.00114552 \n",
       " -0.000540204   2.02611e-5    0.000611164     -0.00010366    0.000263532\n",
       " -0.00053999    2.02753e-5    0.00061098      -0.000103711   0.000263483\n",
       " -0.00126832   -0.000335122   0.000446974      0.001455     -0.00014095 \n",
       " -0.0012667    -0.000335174   0.000445164      5.57779e-5    0.000404473\n",
       " -0.000539771   2.02409e-5    0.000610665  …  -0.000103663   0.000263238\n",
       " -0.000539282   2.02504e-5    0.000610183     -0.000103613   0.000263005\n",
       " -0.000538889   2.02264e-5    0.000609715     -0.000103444   0.000262931\n",
       " -0.000540509   2.02925e-5    0.000611561     -0.0001038     0.000263679\n",
       " -0.000539809   2.02719e-5    0.000610784     -0.000103613   0.000263402\n",
       " -0.000540451   2.02716e-5    0.000611448  …  -0.000103659   0.000263632\n",
       "  0.00178589    7.21459e-5   -0.00166131      -0.000300145   3.11527e-6 \n",
       "  0.000622892   4.62099e-5   -0.000525108     -0.000503081  -0.000441005\n",
       "  0.000623895   4.61855e-5   -0.000526208     -0.000531462  -0.000260921\n",
       "  0.000622698   4.62088e-5   -0.00052491       0.000180963   0.000410226\n",
       "  0.000622748   4.62087e-5   -0.000524962  …   0.000628922   0.0011776  ,\n",
       "\n",
       "Float32[0.0476144,0.0475825,0.0476525,0.047591,-0.183141,-0.106182,0.0476205,0.0476099,-0.0292133,-0.029353  …  0.047548,0.04751,0.0476548,0.0475951,0.0476428,-0.106164,-0.0292728,-0.0293563,-0.0292588,-0.0292625])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const WINDOW_LEN = 8 \n",
    "training = Vector{String}[pad(para, WINDOW_LEN+1) for para in training]\n",
    "\n",
    "we_outer = WE(Float32,String, 200)\n",
    "add_all_words!(we_outer, training)\n",
    "pvdm_outer = PVDM(we_outer, WINDOW_LEN);\n",
    "we_outer=0\n",
    "\n",
    "\n",
    "training_indexes = @pipe chain(map(para -> get_para_training_cases!(pvdm_outer, para), training)...) |> hcat(_...)\n",
    "para_indexes_o = training_indexes[1,:] |> vec\n",
    "window_indexes_o = training_indexes[2:end-1,:] \n",
    "label_indexes_o = training_indexes[end,:] |> vec;\n",
    "\n",
    "ŷs,xs = feedforward(pvdm_outer, para_indexes_o, window_indexes_o)\n",
    "ΔW,Δb, ΔL, ΔD = backprop(pvdm_outer, label_indexes_o, ŷs,xs, para_indexes_o, window_indexes_o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pack (generic function with 2 methods)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc \"This assumes the number of works and paragraphs known remains constant\" ->\n",
    "function unpack!(pvdm::PVDM, θ::Vector)\n",
    "    start=0\n",
    "    item=pvdm.we.L\n",
    "    len_total=length(item)\n",
    "    @inbounds pvdm.we.L = @pipe θ[1+start:len_total]|>reshape(_,size(item)...)\n",
    "    \n",
    "    start+=length(item)\n",
    "    item=pvdm.pe.L\n",
    "    len_total+=length(item)\n",
    "    @inbounds pvdm.pe.L = @pipe θ[1+start:len_total]|>reshape(_,size(item)...)\n",
    "    \n",
    "    start+=length(item)\n",
    "    item=pvdm.W \n",
    "    len_total+=length(item)\n",
    "    @inbounds pvdm.W = @pipe θ[1+start:len_total]|>reshape(_,size(item)...)\n",
    "    \n",
    "    start+=length(item)\n",
    "    item=pvdm.b \n",
    "    len_total+=length(item)\n",
    "    @inbounds pvdm.b = @pipe θ[1+start:len_total]\n",
    "    \n",
    "    pvdm\n",
    "end\n",
    "\n",
    "\n",
    "@doc \"This assumes the number of works and paragraphs known remains constant\" ->\n",
    "function pack{N}(L::AbstractMatrix{N}, D::AbstractMatrix{N}, W::AbstractMatrix{N},b::AbstractVector{N})\n",
    "    vcat(vec(L),vec(D), vec(W),b)\n",
    "end\n",
    "\n",
    "@doc \"This assumes the number of works and paragraphs known remains constant\" ->\n",
    "function pack(pvdm::PVDM)\n",
    "    pack(pvdm.we.L, pvdm.pe.L, pvdm.W, pvdm.b)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using DualNumbers\n",
    "pvdm_outer_dual = PVDM_light(pvdm_outer,Dual{Float32});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "g (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ForwardDiff\n",
    "\n",
    "function f(θ)\n",
    "    pvdm_inner = PVDM_light(pvdm_outer_dual)\n",
    "    unpack!(pvdm_inner,θ)\n",
    "    ŷs = feedforward(pvdm_inner, para_indexes_o, window_indexes_o)[1]\n",
    "    loss(label_indexes_o, ŷs)\n",
    "end\n",
    "\n",
    "# Using forwarddiff_jacobian\n",
    "g = forwarddiff_gradient(f, Float32, fadtype=:dual, n=length(pack(pvdm_outer)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "θ = pack(pvdm_outer)\n",
    "dg = g(θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.9-pre",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
