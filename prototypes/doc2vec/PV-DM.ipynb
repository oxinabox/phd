{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Compat\n",
    "using Iterators\n",
    "using Pipe\n",
    "using DataStructures\n",
    "\n",
    "macro printval(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\" = \", $ee)))\n",
    "end\n",
    "\n",
    "macro pz(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\"\\t\\t\",typeof($ee), \"\\t\", size($ee))))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11-element Array{Union{UTF8String,ASCIIString},1}:\n",
       " \"/root/buildFromSource/julia0.4/usr/local/share/julia/site/v0.4\"\n",
       " \"/root/buildFromSource/julia0.4/usr/share/julia/site/v0.4\"      \n",
       " \"../Corpus\"                                                     \n",
       " \"../doc2vec\"                                                    \n",
       " \"../Models\"                                                     \n",
       " \"../Optimisation\"                                               \n",
       " \"../recursive_embeddings\"                                       \n",
       " \"../summaristation\"                                             \n",
       " \"../tools\"                                                      \n",
       " \"../util\"                                                       \n",
       " \"../word-embedding3\"                                            "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "push!(LOAD_PATH, map(x->\"../\"*x, filter(fn-> !(contains(fn,\".\")),readdir(\"..\")))...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hcat_no_splatting (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function hcat_no_splatting(xss::Vector{Any}) #Not actually for vector Any, only really for Vector{Vector{Any}}\n",
    "    ncols = length(xss)\n",
    "    nrows = length(first(xss))\n",
    "    S = xss|>first|>eltype\n",
    "    ret = Array(S,(nrows,ncols))\n",
    "    for ii in eachindex(xss)\n",
    "        @inbounds col = xss[ii]\n",
    "        #@assert(length(col)==nrows)\n",
    "        @inbounds ret[:,ii] = col\n",
    "    end\n",
    "    ret\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using WordEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\t\t"
     ]
    }
   ],
   "source": [
    "training = open(\"../Corpus/serialised/opinosis_train_dev_plain.jsz\",\"r\") do fs\n",
    "    deserialize(fs)\n",
    "end\n",
    "@pz training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pad_advanced (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const START_PAD_WORD = \"*START*\"\n",
    "const END_WORD = \"*END*\"\n",
    "\n",
    "function pad{S<:String}(sent::Vector{S}, padded_length)\n",
    "    if length(sent) <= padded_length\n",
    "        ret =  fill(START_PAD_WORD,padded_length)\n",
    "        ret[end-length(sent)+1:end] = sent\n",
    "        ret\n",
    "    else\n",
    "        sent\n",
    "    end\n",
    "end\n",
    "\n",
    "function pad_advanced{S<:String}(sent::Vector{S}, window_length::Int)\n",
    "    padded_length = 2 + max(window_length+1, length(sent)) # ALways have at least 1 start and end padding, so plus 2 elements\n",
    "    ret =  fill(START_PAD_WORD,padded_length)\n",
    "    ret[end]=END_WORD\n",
    "    ret[end-length(sent)-1+1: end-1]=sent\n",
    "    ret\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type PVDM{N<:Number, S<:AbstractString}\n",
    "    we::WE\n",
    "    pe::WE #use a word embedder for Paragraphs too\n",
    "    \n",
    "    W::AbstractMatrix{N}\n",
    "    b::AbstractVector{N}\n",
    "\n",
    "    window_length::Int\n",
    "    varience::N\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PVDM_light (generic function with 2 methods)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array{Array{UTF8String,1},1}\t(6097,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "function PVDM{N,S}(we::WE{N,S}, window_length::Int, varience=0.001, output_width::Int=size(we.L,2))\n",
    "    emb_width = size(we.L,1)\n",
    "    concat_layer_width = emb_width*(window_length+1)\n",
    "    const W = convert(Matrix{N}, varience*randn(output_width,concat_layer_width))\n",
    "    const b = convert(Vector{N}, varience*randn(output_width))\n",
    "    pe = WE(N,Vector{S},emb_width)\n",
    "    \n",
    "    PVDM{N,S}( we, pe, W,b, window_length, varience)\n",
    "end\n",
    "\n",
    "@doc \"Lightwieght version, that does not have support for lookups or additions\" ->\n",
    "function PVDM_light{N,S}(pvdm::PVDM{N,S})\n",
    "    we = WE_light(pvdm.we)\n",
    "    pe = WE_light(pvdm.pe)\n",
    "    W = pvdm.W\n",
    "    b = pvdm.b\n",
    "    window_length = pvdm.window_length\n",
    "    varience = nan(N)\n",
    "    PVDM{N,S}( we, pe, W,b, window_length, varience)\n",
    "end\n",
    "\n",
    "@doc \"Lightwieght version, that does not have support for lookups or additions, and has N converted type N2\" ->\n",
    "function PVDM_light{N,S}(pvdm::PVDM{N,S}, N2::DataType)\n",
    "    we = WE_light(pvdm.we,N2)\n",
    "    pe = WE_light(pvdm.pe, N2)\n",
    "    W = convert(Matrix{N2},pvdm.W)\n",
    "    b = convert(Vector{N2},pvdm.b)\n",
    "    window_length = pvdm.window_length\n",
    "    varience = convert(N2,NaN)\n",
    "    PVDM{N2,S}( we, pe, W,b, window_length, varience)\n",
    "end\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_para_training_cases! (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc \"\"\"gets the training cases as vector of (paraIndex, [word_indexes], label_word_index),\n",
    "cycling by the window length.\n",
    "Adds the paragraph if it does not already have an index\n",
    "\"\"\" ->\n",
    "function get_para_training_cases!{S<:String}(pvdm::PVDM, para::Vector{S})\n",
    "    para_ind = get_word_index!(pvdm.pe, para, pvdm.varience)\n",
    "    \n",
    "    Task() do \n",
    "        @assert length(para)>=pvdm.window_length+1\n",
    "        for offset in 0:length(para)-(pvdm.window_length+1)\n",
    "            window_iis = [1:pvdm.window_length;]+offset\n",
    "            label_ii = pvdm.window_length+1+offset\n",
    "            \n",
    "            window_words = para[window_iis]\n",
    "            label_word = para[label_ii]\n",
    "                        \n",
    "            windows_indexes = map(word->get_word_index(pvdm.we, word), window_words)\n",
    "            label_index = get_word_index(pvdm.we, label_word)\n",
    "            \n",
    "            produce(Int64[para_ind, windows_indexes..., label_index])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_input_layers (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_input_layer(pvdm::PVDM, para_index::Int, window_indexes::Vector{Int})\n",
    "    @inbounds [pvdm.pe.L[:,para_index], vec(pvdm.we.L[:,window_indexes])]\n",
    "end \n",
    "\n",
    "function get_input_layers{N,S, I<:Int}(pvdm::PVDM{N,S}, para_indexes::Vector{I}, window_indexeses::Matrix{I})\n",
    "    const emb_width = size(pvdm.we.L,1)\n",
    "    const n_training = length(para_indexes)\n",
    "    \n",
    "    xs = Array(N,(emb_width * (pvdm.window_length+1),n_training))\n",
    "    @inbounds xs[1:emb_width,:] = pvdm.pe.L[:,para_indexes]\n",
    "    for training_case in 1:n_training\n",
    "        @inbounds const window_indexes = window_indexeses[:,training_case]\n",
    "        @inbounds xs[emb_width+1:end,training_case] = vec(pvdm.we.L[:,window_indexes])\n",
    "    end\n",
    "    xs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feedforward (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function feedforward{N,S, I<:Int}(pvdm::PVDM{N,S}, para_indexes::Vector{I}, window_indexeses::Matrix{I}, output_activation::Function)\n",
    "    xs = get_input_layers(pvdm, para_indexes, window_indexeses)\n",
    "    \n",
    "    #Speed optimised version of `zs = pvdm.W*xs .+ pvdm.b`\n",
    "    zs = pvdm.W*xs \n",
    "    const n_training = length(para_indexes)\n",
    "    for ii in 1:n_training\n",
    "        @inbounds zs[:,ii]+= pvdm.b\n",
    "    end\n",
    "    ŷs = output_activation(zs)\n",
    "    ŷs, xs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop_softmax (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax(zs)\n",
    "    (1./sum(exp(zs),1)).*exp(zs)\n",
    "end\n",
    "\n",
    "function loss_softmax{I,N}(y_indexes::Vector{I}, ŷs::Matrix{N})\n",
    "    @assert length(y_indexes)>0\n",
    "    #C=−∑j yj*log ŷj,\n",
    "    c = zero(N)\n",
    "    for tc in 1:length(y_indexes)\n",
    "        @inbounds c-=log(ŷs[y_indexes[tc],tc])\n",
    "    end\n",
    "    c/length(y_indexes)\n",
    "end\n",
    "\n",
    "function feedforward_softmax{N,S, I<:Int}(pvdm::PVDM{N,S}, para_indexes::Vector{I}, window_indexeses::Matrix{I})\n",
    "    feedforward(pvdm, para_indexes, window_indexeses, softmax)\n",
    "end\n",
    "\n",
    "function backprop_softmax{N,S,I}(pvdm::PVDM{N,S}, y_indexes::Vector{I}, ŷs::Matrix{N}, xs::Matrix{N} , para_indexes::Vector{I}, window_indexeses::Matrix{I})\n",
    "    #speed optimistation of `δ_top_s = ŷs.-ys`\n",
    "    δ_top_s = copy(ŷs)\n",
    "    for tc in 1:length(y_indexes)\n",
    "        @inbounds δ_top_s[y_indexes[tc],tc]-=one(typeof(ŷs[1]))\n",
    "    end\n",
    "\n",
    "    backprop(pvdm, δ_top_s, ŷs, xs, para_indexes, window_indexeses)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop_hierarchical_softmax (generic function with 1 method)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@fastmath function sigmoid(zs) \n",
    "    #1./(1.0+e.^-zs)\n",
    "    #Speed op of 1./(1+e.^-zs) \n",
    "    ret = similar(zs)\n",
    "    @simd for ii in eachindex(ret)\n",
    "        @inbounds ret[ii]=1./exp(-zs[ii]+1.0) #\n",
    "    end\n",
    "    ret\n",
    "end\n",
    "\n",
    "function loss_hierarchical_softmax{N}(y_codes::Vector{BitVector}, ŷs::Matrix{N})\n",
    "    c = zero(N)\n",
    "    for tc in 1:size(ŷs,2)\n",
    "        const y_code= y_codes[tc]\n",
    "        for d in eachindex(y_code)\n",
    "            const y = y_code[d] ? zero(N) : one(N)\n",
    "            c += y - ŷs[d,tc]\n",
    "            #if y_code[d]\n",
    "            #    c+=ln(ŷs[d,tc])\n",
    "            #end\n",
    "        end\n",
    "    end\n",
    "    c/size(ŷs,2)\n",
    "end\n",
    "\n",
    "\n",
    "function feedforward_hierarchical_softmax{N,S, I<:Int}(pvdm::PVDM{N,S}, para_indexes::Vector{I}, window_indexeses::Matrix{I})\n",
    "    feedforward(pvdm, para_indexes, window_indexeses, sigmoid)\n",
    "end\n",
    "\n",
    "function backprop_hierarchical_softmax{N,S,I}(pvdm::PVDM{N,S}, y_codes::Vector{BitVector}, ŷs::Matrix{N}, xs::Matrix{N} , para_indexes::Vector{I}, window_indexeses::Matrix{I})\n",
    "    #speed optimistation of `δ_top_s = ŷs.-ys`\n",
    "    δ_top_s = similar(ŷs)\n",
    "    for tc in size(ŷs,2)\n",
    "        const y_code= y_codes[tc]\n",
    "        for d in length(y_code)\n",
    "            const y = y_code[d] ? zero(N) : one(N)\n",
    "            δ_top_s[d,tc] = y - ŷs[d,tc]\n",
    "            #δ_top_s[d,tc] =  y_code[d] ? one(N)-ŷs[d,tc] : zero(N)\n",
    "        end\n",
    "        for d in length(y_code):size(ŷs,1)\n",
    "            δ_top_s[d,tc]=zero(N)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    backprop(pvdm, δ_top_s, ŷs, xs, para_indexes, window_indexeses)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "200x120 Array{Float32,2}:\n",
       "  2.1606e-16    2.61834e-17  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " -3.3324e-16   -7.96584e-17     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "  1.56624e-16   5.35046e-18     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "  8.14283e-16  -2.94394e-17     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " -1.43992e-16   1.43461e-18     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "  1.44957e-16   1.40824e-17  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " -8.45804e-17   5.58556e-20     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " -5.30562e-17  -3.11619e-17     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " -2.24973e-16  -2.78575e-17     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " -6.05449e-16   3.64114e-17     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " -3.11204e-16   2.36465e-17  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " -3.5121e-16   -2.92883e-18     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "  1.25391e-16  -3.45746e-17     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "  ⋮                          ⋱            ⋮                      \n",
       "  5.69366e-16   4.03416e-18     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " -1.69861e-16   5.96627e-18     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "  1.55516e-16   5.57155e-17  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "  1.87936e-16   4.61067e-18     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " -2.11905e-16  -5.45058e-17     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "  5.54556e-17  -2.02426e-17     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " -1.33023e-17   9.63859e-18     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "  3.32079e-17   4.74111e-18  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " -7.94666e-17   2.20944e-17     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "  1.32964e-16  -3.42145e-17     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "  3.90015e-16   4.36891e-18     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "  1.44529e-16  -7.40999e-18     0.0  0.0  0.0  0.0  0.0  0.0  0.0,\n",
       "\n",
       "200x10 Array{Float32,2}:\n",
       "  2.01161e-17   2.39425e-17   1.71098e-17  …  -1.17378e-18  0.0  0.0\n",
       " -1.47759e-17  -1.80936e-17  -1.48162e-17     -7.62725e-19  0.0  0.0\n",
       " -6.55626e-17  -1.14949e-16  -1.37946e-16     -3.20159e-17  0.0  0.0\n",
       "  1.22979e-16   1.73691e-16   1.90984e-16      3.42102e-17  0.0  0.0\n",
       "  6.17655e-17   1.40825e-16   1.41866e-16      2.68394e-17  0.0  0.0\n",
       " -1.43982e-16  -2.05715e-16  -2.14058e-16  …  -3.16383e-17  0.0  0.0\n",
       "  3.42552e-17   1.36627e-17   7.74947e-18     -7.59628e-18  0.0  0.0\n",
       " -2.15068e-17  -4.28021e-17  -4.32701e-17     -9.17878e-18  0.0  0.0\n",
       "  4.33331e-17   8.82033e-17   9.00609e-17      1.79101e-17  0.0  0.0\n",
       " -1.48371e-16  -2.41808e-16  -2.61457e-16     -4.919e-17    0.0  0.0\n",
       " -8.2231e-17   -1.4115e-16   -1.46876e-16  …  -2.69826e-17  0.0  0.0\n",
       "  2.72548e-17   2.30057e-17   2.2114e-17      -1.83338e-20  0.0  0.0\n",
       "  5.97436e-18  -2.52054e-17  -2.62489e-17     -9.445e-18    0.0  0.0\n",
       "  ⋮                                        ⋱                        \n",
       "  3.34615e-17   4.85721e-17   4.86394e-17      6.85606e-18  0.0  0.0\n",
       " -2.30657e-17  -5.19085e-17  -5.66153e-17     -1.28468e-17  0.0  0.0\n",
       " -5.09135e-17  -8.59181e-17  -9.6069e-17   …  -2.03901e-17  0.0  0.0\n",
       " -2.1857e-17   -5.63353e-17  -5.14949e-17     -8.65518e-18  0.0  0.0\n",
       "  4.39207e-17   1.05749e-16   1.1602e-16       2.57922e-17  0.0  0.0\n",
       "  1.36639e-17   2.05017e-17   5.63467e-18     -5.34985e-18  0.0  0.0\n",
       " -8.51745e-17  -8.24065e-17  -1.00445e-16     -1.43842e-17  0.0  0.0\n",
       "  4.82402e-17   7.9639e-17    9.03744e-17  …   1.93756e-17  0.0  0.0\n",
       " -6.74593e-17  -8.2911e-17   -6.1826e-17       3.96869e-18  0.0  0.0\n",
       " -6.47405e-19  -1.2344e-17   -2.28527e-17     -1.05367e-17  0.0  0.0\n",
       "  8.0171e-17    1.01354e-16   1.08255e-16      1.4881e-17   0.0  0.0\n",
       "  1.55012e-17   1.3993e-17    2.43956e-19     -7.47487e-18  0.0  0.0,\n",
       "\n",
       "8x1800 Array{Float32,2}:\n",
       " -4.72435e-16  -3.30328e-16  -1.08615e-16  …  -6.59106e-17   5.72071e-17\n",
       " -1.4013e-45   -1.4013e-45   -0.0             -0.0           1.4013e-45 \n",
       " -2.35574e-16  -1.52647e-16  -4.91006e-17      5.41188e-18  -3.27214e-17\n",
       "  0.0           0.0           0.0              0.0           0.0        \n",
       " -3.00869e-16  -1.86992e-16  -1.51404e-17      5.0705e-17   -1.39776e-18\n",
       "  0.0           0.0           0.0          …   0.0           0.0        \n",
       " -3.80766e-16  -2.54444e-16  -5.84115e-17      2.64863e-17   5.23061e-18\n",
       "  0.0           0.0           0.0              0.0           0.0        ,\n",
       "\n",
       "Float32[5.487803f-14,1.47f-43,2.927107f-14,0.0f0,3.5783357f-14,1.0f-45,4.5084935f-14,0.0f0])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ŷs,xs=feedforward_hierarchical_softmax(pvdm_outer, para_indexes_o, window_indexes_o)\n",
    "backprop_hierarchical_softmax(pvdm_outer, label_codes_o, ŷs,xs, para_indexes_o, window_indexes_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop (generic function with 1 method)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backprop{N,S,I}(pvdm::PVDM{N,S}, δ_top_s::Matrix{N}, ŷs::Matrix{N}, xs::Matrix{N} , para_indexes::Vector{I}, window_indexeses::Matrix{I})\n",
    "    const emb_width = size(pvdm.we.L,1)\n",
    "    const n_training = length(para_indexes)\n",
    "    const window_len = pvdm.window_length \n",
    "\n",
    "    @assert length(n_training)>0\n",
    "    \n",
    "    ΔL = zeros(pvdm.we.L) #Word Vector Changes\n",
    "    ΔD = zeros(pvdm.pe.L) #Paragraph Vector Changes\n",
    "    \n",
    "    Δb = sum(δ_top_s,2) |> vec\n",
    "    ΔW = (δ_top_s * xs')\n",
    "    δ_input_s= (pvdm.W'*δ_top_s) #the activation function of the layer below dxs=d(1*D[ii];L[iis]) =1\n",
    "    \n",
    "    #Paragraph vector Error\n",
    "    for ii in 1:n_training #Add sequentially, reather than via in a += as that would only allow one add for repreased index\n",
    "        @inbounds ΔD[:,para_indexes[ii]] += δ_input_s[1:emb_width,ii]\n",
    "    end\n",
    "    \n",
    "    #word vectors\n",
    "    for ii in 1:n_training\n",
    "        for ww in 1:window_len\n",
    "            const offset=ww*emb_width\n",
    "            @inbounds ΔL[:,window_indexeses[ww,ii]]+=δ_input_s[offset+1:offset+emb_width, ii]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    ΔL./n_training, ΔD./n_training, ΔW./n_training, Δb./n_training\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pack (generic function with 2 methods)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc \"This assumes the number of works and paragraphs known remains constant\" ->\n",
    "function unpack!(pvdm::PVDM, θ::Vector)\n",
    "    start=0\n",
    "    item=pvdm.we.L\n",
    "    len_total=length(item)\n",
    "    @inbounds pvdm.we.L = @pipe θ[1+start:len_total]|>reshape(_,size(item)...)\n",
    "    \n",
    "    start+=length(item)\n",
    "    item=pvdm.pe.L\n",
    "    len_total+=length(item)\n",
    "    @inbounds pvdm.pe.L = @pipe θ[1+start:len_total]|>reshape(_,size(item)...)\n",
    "    \n",
    "    start+=length(item)\n",
    "    item=pvdm.W \n",
    "    len_total+=length(item)\n",
    "    @inbounds pvdm.W = @pipe θ[1+start:len_total]|>reshape(_,size(item)...)\n",
    "    \n",
    "    start+=length(item)\n",
    "    item=pvdm.b \n",
    "    len_total+=length(item)\n",
    "    @inbounds pvdm.b = @pipe θ[1+start:len_total]\n",
    "    \n",
    "    pvdm\n",
    "end\n",
    "\n",
    "\n",
    "@doc \"This assumes the number of works and paragraphs known remains constant\" ->\n",
    "function pack{N}(L::AbstractMatrix{N}, D::AbstractMatrix{N}, W::AbstractMatrix{N},b::AbstractVector{N})\n",
    "    vcat(vec(L),vec(D), vec(W),b)\n",
    "end\n",
    "\n",
    "@doc \"This assumes the number of works and paragraphs known remains constant\" ->\n",
    "function pack(pvdm::PVDM)\n",
    "    pack(pvdm.we.L, pvdm.pe.L, pvdm.W, pvdm.b)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Huffman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 116"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{AbstractString,BitArray{1}} with 120 entries:\n",
       "  \"home\"      => Bool[true,true,false,true,true,true,true,true]\n",
       "  \"!\"         => Bool[true,false,false,true,false,true,true,true]\n",
       "  \"reading\"   => Bool[true,true,true,true,true,true,true]\n",
       "  \"was\"       => Bool[false,true,true,true,true,false,true,true]\n",
       "  \"would\"     => Bool[true,false,false,false,false,true,false]\n",
       "  \"angle\"     => Bool[true,false,false,false,true,false,false,false]\n",
       "  \"again\"     => Bool[true,false,false,true,true,false,true,false]\n",
       "  \"more\"      => Bool[true,true,true,true,false,false,false]\n",
       "  \"that\"      => Bool[true,false,true,false,false,true,true,true]\n",
       "  \"'\"         => Bool[true,true,false,false,true,false,false,true]\n",
       "  \"readers\"   => Bool[true,true,true,false,false,true,true,true]\n",
       "  \"'s\"        => Bool[true,false,false,true,true,true,false]\n",
       "  \".\"         => Bool[false,true,true,true,false]\n",
       "  \"best\"      => Bool[true,false,false,false,true,false,false,true]\n",
       "  \"screen\"    => Bool[true,false,true,false,true,true,true,false]\n",
       "  \"an\"        => Bool[true,true,false,true,true,true,true,false]\n",
       "  \"choose\"    => Bool[true,false,false,true,false,true,false,false]\n",
       "  \"vision\"    => Bool[false,true,true,true,true,true,false,false]\n",
       "  \"make\"      => Bool[true,true,false,false,true,true,true,false]\n",
       "  \"decided\"   => Bool[true,true,false,false,true,false,true,true]\n",
       "  \"radically\" => Bool[true,true,false,true,false,true,false,true]\n",
       "  \"fine\"      => Bool[true,true,false,true,true,false,true,true]\n",
       "  \"myself\"    => Bool[true,true,true,true,true,false,false,true]\n",
       "  \"provides\"  => Bool[true,false,true,true,true,true,true,true]\n",
       "  \"down\"      => Bool[true,true,false,false,true,false,true,false]\n",
       "  ⋮           => ⋮"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".770 microseconds (57 allocations: 8592 bytes)\n",
      "  34.171 milliseconds (60229 allocations: 2110 KB)\n"
     ]
    }
   ],
   "source": [
    "const WINDOW_LEN = 8 \n",
    "#############USE 10 training case for now\n",
    "@time training = Vector{String}[pad_advanced(para, WINDOW_LEN) for para in training[1:10]]\n",
    "\n",
    "@time word2code_o = get_huffman_codes(training) # Unexpectedly this runs 4 times as fast indexed with strings as with ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: redefining constant pvdm_outer\n",
      "Warning: redefining constant training_indexes_o\n",
      "Warning: redefining constant para_indexes_o\n",
      "Warning: redefining constant window_indexes_o\n",
      "Warning: redefining constant label_indexes_o\n",
      "Warning: redefining constant label_codes_o\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "253-element Array{BitArray{1},1}:\n",
       " Bool[false,true,false,false,true,false,true]      \n",
       " Bool[true,false,true,true,false,true]             \n",
       " Bool[false,true,false,false,false]                \n",
       " Bool[false,true,true,false]                       \n",
       " Bool[true,true,true,false,true,false,true]        \n",
       " Bool[true,false,true,true,false,false]            \n",
       " Bool[true,false,true,false,true,true,true,true]   \n",
       " Bool[true,false,false,true,false,true,true,true]  \n",
       " Bool[false,false,false]                           \n",
       " Bool[false,false,false]                           \n",
       " Bool[false,false,false]                           \n",
       " Bool[false,false,false]                           \n",
       " Bool[false,false,false]                           \n",
       " ⋮                                                 \n",
       " Bool[true,true,false,true,false,false,false,false]\n",
       " Bool[true,true,false,true,false,false,false,true] \n",
       " Bool[false,true,false,false,false]                \n",
       " Bool[true,false,true,true,true,true,false,false]  \n",
       " Bool[false,true,true,false]                       \n",
       " Bool[false,true,true,true,false]                  \n",
       " Bool[false,false,false]                           \n",
       " Bool[false,false,false]                           \n",
       " Bool[false,false,false]                           \n",
       " Bool[false,false,false]                           \n",
       " Bool[false,false,false]                           \n",
       " Bool[false,false,false]                           "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.316 microseconds (14 allocations: 2624 bytes)\n"
     ]
    }
   ],
   "source": [
    "@time const max_code_length = @pipe word2code_o |> values |> map(length,_) |> maximum(_)\n",
    "\n",
    "we_outer = WE(Float32,String, 200)\n",
    "add_all_words!(we_outer, training, 0.01)\n",
    "const pvdm_outer = PVDM(we_outer, WINDOW_LEN, 0.01, max_code_length);\n",
    "we_outer=0\n",
    "\n",
    "index2code_o(ii) = word2code_o[pvdm_outer.we.indexed_words[ii]]\n",
    "const training_indexes_o = @pipe ( training\n",
    "                                    |> map(para -> get_para_training_cases!(pvdm_outer, para),_) \n",
    "                                    |> chain(_...)\n",
    "                                    |> collect\n",
    "                                    |> hcat_no_splatting)\n",
    "const para_indexes_o = training_indexes_o[1,:] |> vec\n",
    "const window_indexes_o = training_indexes_o[2:end-1,:] \n",
    "const label_indexes_o = training_indexes_o[end,:] |> vec;\n",
    "\n",
    "\n",
    "const label_codes_o = BitVector[index2code_o(ii) for ii in label_indexes_o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_and_loss_grad! (generic function with 1 method)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss!(θ::Vector)  \n",
    "    #warn(\"loss! not defined\")\n",
    "    grad = similar(θ)\n",
    "    cached_loss_and_loss_grad!(θ, grad)\n",
    "end\n",
    "\n",
    "function loss_grad!(θ::Vector, storage::Vector) \n",
    "    #warn(\"loss_grad not defined\")\n",
    "    cached_loss_and_loss_grad!(θ, grad)\n",
    "end\n",
    "\n",
    "\n",
    "function loss_and_loss_grad!(θ::Vector, grad::Vector)   \n",
    "    unpack!(pvdm_outer, θ)\n",
    "    \n",
    "    ŷs,xs = feedforward_hierarchical_softmax(pvdm_outer, para_indexes_o, window_indexes_o)\n",
    "    Δs  = backprop_hierarchical_softmax(pvdm_outer, label_codes_o, ŷs,xs, para_indexes_o, window_indexes_o)\n",
    "    grad[:] = pack(Δs...)\n",
    "    loss_hierarchical_softmax(label_codes_o, ŷs)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cached_loss_and_loss_grad! (generic function with 1 method)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = typeof(pvdm_outer.varience)\n",
    "loss_and_loss_grad_cache = Dict{Vector{N}, Tuple{N,AbstractVector{N}}}()\n",
    "loss_and_loss_grad_cache_hits = 0\n",
    "loss_and_loss_grad_cache_misses = 0\n",
    "function cached_loss_and_loss_grad!(θ::Vector, grad::Vector)\n",
    "    global loss_and_loss_grad_cache\n",
    "    global loss_and_loss_grad_cache_hits\n",
    "    global loss_and_loss_grad_cache_misses\n",
    "    if haskey(loss_and_loss_grad_cache,θ)\n",
    "        loss_and_loss_grad_cache_hits+=1\n",
    "        err, grad[:] = loss_and_loss_grad_cache[θ]\n",
    "        err\n",
    "    else\n",
    "        loss_and_loss_grad_cache_misses+=1\n",
    "        err = loss_and_loss_grad!(θ, grad)\n",
    "        loss_and_loss_grad_cache[θ] = (err, grad)\n",
    "        err\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     Function value   Gradient norm \n",
      " "
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: Error not defined\nwhile loading In[121], in expression starting on line 172",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: Error not defined\nwhile loading In[121], in expression starting on line 172",
      "",
      " in secant2! at /home/wheel/oxinabox/.julia/v0.4/Optim/src/linesearch/hz_linesearch.jl:145",
      " in hz_linesearch! at /home/wheel/oxinabox/.julia/v0.4/Optim/src/linesearch/hz_linesearch.jl:333",
      " in hz_linesearch! at /home/wheel/oxinabox/.julia/v0.4/Optim/src/linesearch/hz_linesearch.jl:188 (repeats 10 times)",
      " in cg at /home/wheel/oxinabox/.julia/v0.4/Optim/src/cg.jl:217",
      " in optimize at /home/wheel/oxinabox/.julia/v0.4/Optim/src/optimize.jl:113"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0     9.308112e-01     0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "using Optim #https://github.com/JuliaOpt/Optim.jl\n",
    "f=DifferentiableFunction(loss!,loss_grad!,cached_loss_and_loss_grad!)\n",
    "θ = pack(pvdm_outer)\n",
    "#θ=res.minimum\n",
    "#θ=optx\n",
    "@time res = optimize(f, θ, method=:cg, show_trace = true, store_trace = true, iterations = 2);\n",
    "@printval res.f_calls \n",
    "@printval res.g_calls \n",
    "@printval res.iterations\n",
    "@printval res.f_minimum\n",
    "@printval res.f_converged \n",
    "@printval res.trace\n",
    "@printval loss_and_loss_grad_cache_hits\n",
    "@printval loss_and_loss_grad_cache_misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ")elapsed time: 902.511280671 seconds (173774005532 bytes allocated, 10.39% gc time)\n",
    "res.f_calls = 18\n",
    "res.g_calls = 16\n",
    "res.iterations = 2\n",
    "res.f_minimum = 6.521084785461426\n",
    "res.f_converged = false\n",
    "res.trace = Iter     Function value   Gradient norm \n",
    "------   --------------   --------------\n",
    "     0     8.649927e+00     1.244972e-01\n",
    "     1     6.932991e+00     7.786864e-02\n",
    "     2     6.521085e+00     1.384170e-01\n",
    "\n",
    "loss_and_loss_grad_cache_hits = 9\n",
    "loss_and_loss_grad_cache_misses = 9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0-dev",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
