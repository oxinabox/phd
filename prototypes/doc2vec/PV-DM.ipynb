{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Compat\n",
    "using Docile\n",
    "using Iterators\n",
    "using Pipe\n",
    "using Devectorize\n",
    "\n",
    "macro printval(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\" = \", $ee)))\n",
    "end\n",
    "\n",
    "macro pz(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\"\\t\\t\",typeof($ee), \"\\t\", size($ee))))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unzip (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function unzip(xs)\n",
    "    [zip(xs...)...]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11-element Array{Union(ASCIIString,UTF8String),1}:\n",
       " \"/root/buildFromSource/julia/usr/local/share/julia/site/v0.3\"\n",
       " \"/root/buildFromSource/julia/usr/share/julia/site/v0.3\"      \n",
       " \"../Corpus\"                                                  \n",
       " \"../doc2vec\"                                                 \n",
       " \"../Models\"                                                  \n",
       " \"../Optimisation\"                                            \n",
       " \"../recursive_embeddings\"                                    \n",
       " \"../summaristation\"                                          \n",
       " \"../tools\"                                                   \n",
       " \"../util\"                                                    \n",
       " \"../word-embedding3\"                                         "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "push!(LOAD_PATH, map(x->\"../\"*x, filter(fn-> !(contains(fn,\".\")),readdir(\"..\")))...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using WordEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\t\tArray{Array{String,1},1}\t(6097,)\n"
     ]
    }
   ],
   "source": [
    "training = open(\"../Corpus/serialised/opinosis_train_dev_plain.jsz\",\"r\") do fs\n",
    "    deserialize(fs)\n",
    "end\n",
    "@pz training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Array{String,1},1}:\n",
       " String[\"being\",\"able\",\"to\",\"change\",\"the\",\"*UNKNOWN*\",\"sizes\",\"is\",\"awesome\",\"!\"]                                                                     \n",
       " String[\"for\",\"whatever\",\"reason\",\",\",\"*UNKNOWN*\",\"decided\",\"to\",\"make\",\"the\",\"*UNKNOWN*\",\"on\",\"the\",\"home\",\"screen\",\"than\",\"on\",\"the\",\"*UNKNOWN*\",\".\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training=training[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pad (generic function with 2 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function pad{S<:String}(sent::Vector{S}, padded_length, pad_word=\"*STARTPAD*\")\n",
    "    if length(sent) <= padded_length\n",
    "        ret =  fill(pad_word,padded_length)\n",
    "        ret[end-length(sent)+1:end] = sent\n",
    "        ret\n",
    "    else\n",
    "        sent\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "add_all_words! (generic function with 4 methods)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import WordEmbeddings.WE\n",
    "function WE(N::DataType,S::DataType, embedding_width::Int)\n",
    "    L=Array(N,(embedding_width,0))\n",
    "    word_index=Dict{S,Int}()\n",
    "    indexed_words=S[]\n",
    "    WE(L,word_index,indexed_words)\n",
    "end\n",
    "\n",
    "function WE_light{N,S}(we::WE{N,S}, N2=N::DataType)\n",
    "    L=convert(Matrix{N2},we.L)\n",
    "    word_index=Dict{S,Int}()\n",
    "    indexed_words=S[]\n",
    "    WE(L,word_index,indexed_words)\n",
    "end\n",
    "\n",
    "function WE_light{N,S}(we::WE{N,S})\n",
    "    L=we.L\n",
    "    word_index=Dict{S,Int}()\n",
    "    indexed_words=S[]\n",
    "    WE(L,word_index,indexed_words)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "@doc \"Gets the word index, or creates one if it doesn't already exist\" ->\n",
    "function get_word_index!{N,S, S2}(we::WE{N,S}, word::S2, word_varience = 0.01)\n",
    "    if (word in keys(we.word_index))\n",
    "        we.word_index[word]\n",
    "    else\n",
    "        index = length(we.indexed_words)+1\n",
    "        we.word_index[word]=index\n",
    "        push!(we.indexed_words,word)\n",
    "        \n",
    "        embedding = convert(Vector{N},word_varience.*randn(size(we.L,1)))\n",
    "        we.L = hcat(we.L,embedding)\n",
    "        index\n",
    "    end\n",
    "end\n",
    "\n",
    "function add_all_words!{N,S}(we::WE{N,S}, words::Vector{S}, word_varience=0.01)\n",
    "    for word in words\n",
    "        get_word_index!(we, word, word_varience)\n",
    "    end\n",
    "    we\n",
    "end\n",
    "function add_all_words!{N,S}(we::WE{N,S}, paras::Vector{Vector{S}}, word_varience=0.01)\n",
    "    for para in paras\n",
    "        add_all_words!(we, para, word_varience)\n",
    "    end\n",
    "    we\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PVDM_light (generic function with 2 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type PVDM{N<:Number, S<:AbstractString}\n",
    "    we::WE\n",
    "    pe::WE #use a word embedder for Paragraphs too\n",
    "    \n",
    "    W::AbstractMatrix{N}\n",
    "    b::AbstractVector{N}\n",
    "\n",
    "    window_length::Int\n",
    "    varience::N\n",
    "end\n",
    "\n",
    "function PVDM{N,S}(we::WE{N,S}, window_length::Int, varience=0.001)\n",
    "    \n",
    "    emb_width,n_words = size(we.L)\n",
    "    concat_layer_width = emb_width*(window_length+1)\n",
    "    const W = convert(Matrix{N}, varience*randn(n_words,concat_layer_width))\n",
    "    const b = convert(Vector{N}, varience*randn(n_words))\n",
    "    \n",
    "    pe = WE(N,Vector{S},emb_width)\n",
    "    \n",
    "    PVDM{N,S}( we, pe, W,b, window_length, varience)\n",
    "end\n",
    "\n",
    "@doc \"Lightwieght version, that does not have support for lookups or additions\" ->\n",
    "function PVDM_light{N,S}(pvdm::PVDM{N,S})\n",
    "    we = WE_light(pvdm.we)\n",
    "    pe = WE_light(pvdm.pe)\n",
    "    W = pvdm.W\n",
    "    b = pvdm.b\n",
    "    window_length = pvdm.window_length\n",
    "    varience = nan(N)\n",
    "    PVDM{N,S}( we, pe, W,b, window_length, varience)\n",
    "end\n",
    "\n",
    "@doc \"Lightwieght version, that does not have support for lookups or additions, and has N converted type N2\" ->\n",
    "function PVDM_light{N,S}(pvdm::PVDM{N,S}, N2::DataType)\n",
    "    we = WE_light(pvdm.we,N2)\n",
    "    pe = WE_light(pvdm.pe, N2)\n",
    "    W = convert(Matrix{N2},pvdm.W)\n",
    "    b = convert(Vector{N2},pvdm.b)\n",
    "    window_length = pvdm.window_length\n",
    "    varience = convert(N2,NaN)\n",
    "    PVDM{N2,S}( we, pe, W,b, window_length, varience)\n",
    "end\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_para_training_cases! (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc \"\"\"gets the training cases as vector of (paraIndex, [word_indexes], label_word_index),\n",
    "cycling by the window length.\n",
    "Adds the paragraph if it does not already have an index\n",
    "\"\"\" ->\n",
    "function get_para_training_cases!{S<:String}(pvdm::PVDM, para::Vector{S})\n",
    "    para_ind = get_word_index!(pvdm.pe, para)\n",
    "    \n",
    "    Task() do \n",
    "        @assert length(para)>=pvdm.window_length+1\n",
    "        for offset in 0:length(para)-(pvdm.window_length+1)\n",
    "            window_iis = [1:pvdm.window_length;]+offset\n",
    "            label_ii = pvdm.window_length+1+offset\n",
    "            \n",
    "            window_words = para[window_iis]\n",
    "            label_word = para[label_ii]\n",
    "                        \n",
    "            windows_indexes = map(word->get_word_index(pvdm.we, word), window_words)\n",
    "            label_index = get_word_index(pvdm.we, label_word)\n",
    "            \n",
    "            produce(Int64[para_ind, windows_indexes..., label_index])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_input_layers (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_input_layer(pvdm::PVDM, para_index::Int, window_indexes::Vector{Int})\n",
    "    @inbounds [pvdm.pe.L[:,para_index], vec(pvdm.we.L[:,window_indexes])]\n",
    "end \n",
    "\n",
    "function get_input_layers{N,S, I<:Int}(pvdm::PVDM{N,S}, para_indexes::Vector{I}, window_indexeses::Matrix{I})\n",
    "    const emb_width = size(pvdm.we.L,1)\n",
    "    const n_training = length(para_indexes)\n",
    "    \n",
    "    xs = Array(N,(emb_width * (pvdm.window_length+1),n_training))\n",
    "    @inbounds xs[1:emb_width,:] = pvdm.pe.L[:,para_indexes]\n",
    "    for training_case in 1:n_training\n",
    "        @inbounds const window_indexes = window_indexeses[:,training_case]\n",
    "        @inbounds xs[emb_width+1:end,training_case] = vec(pvdm.we.L[:,window_indexes])\n",
    "    end\n",
    "    xs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feedforward (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax(zs)\n",
    "    (1./sum(exp(zs),1)).*exp(zs)\n",
    "end\n",
    "\n",
    "function feedforward{N,S, I<:Int}(pvdm::PVDM{N,S}, para_indexes::Vector{I}, window_indexeses::Matrix{I})\n",
    "    xs = get_input_layers(pvdm, para_indexes, window_indexeses)\n",
    "    \n",
    "    \n",
    "    #Speed optimised version of `zs = pvdm.W*xs .+ pvdm.b`\n",
    "    zs = pvdm.W*xs \n",
    "    const n_training = length(para_indexes)\n",
    "    for ii in 1:n_training\n",
    "        @inbounds zs[:,ii]+= pvdm.b\n",
    "    end\n",
    "    ŷs = softmax(zs)\n",
    "    ŷs, xs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss{I,N}(y_indexes::Vector{I}, ŷs::Matrix{N})\n",
    "    #C=−∑j yj*log ŷj,\n",
    "    c = zero(N)\n",
    "    for tc in 1:length(y_indexes)\n",
    "        @inbounds c-=log(ŷs[y_indexes[tc],tc])\n",
    "    end\n",
    "    c/length(y_indexes)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop (generic function with 1 method)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backprop{N,S,I}(pvdm::PVDM{N,S}, y_indexes::Vector{I}, ŷs::Matrix{N}, xs::Matrix{N} , para_indexes::Vector{I}, window_indexeses::Matrix{I} )\n",
    "    const emb_width = size(pvdm.we.L,1)\n",
    "    const n_training = length(para_indexes)\n",
    "    const window_len = pvdm.window_length \n",
    "    #Δb = zeros(pvdm.b)\n",
    "    #ΔW = zeros(pvdm.W)\n",
    "    ΔL = zeros(pvdm.we.L) #Word Vector Changes\n",
    "    ΔD = zeros(pvdm.pe.L) #Paragraph Vector Changes\n",
    "    \n",
    "        \n",
    "    #speed optimistation of `δ_top_s = ŷs.-ys`\n",
    "    δ_top_s = copy(ŷs)\n",
    "    for tc in 1:length(y_indexes)\n",
    "        @inbounds δ_top_s[y_indexes[tc],tc]-=one(typeof(ŷs[1]))\n",
    "    end\n",
    "\n",
    "    Δb = sum(δ_top_s,2) |> vec\n",
    "    ΔW = (δ_top_s * xs')\n",
    "    δ_input_s= (pvdm.W'*δ_top_s) #the activation function of the layer below dxs=d(1*D[ii];L[iis]) =1\n",
    "    \n",
    "    #Paragraph vector Error\n",
    "    for ii in 1:n_training #Add sequentially, reather than via in a += as that would only allow one add for repreased index\n",
    "        @inbounds ΔD[:,para_indexes[ii]] += δ_input_s[1:emb_width,ii]\n",
    "    end\n",
    "    \n",
    "    #word vectors\n",
    "    for ii in 1:n_training\n",
    "        for ww in 1:window_len\n",
    "            const offset=ww*emb_width\n",
    "            @inbounds ΔL[:,window_indexeses[ww,ii]]+=δ_input_s[offset+1:offset+emb_width, ii]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "   \n",
    "    ΔL./n_training, ΔD./n_training, ΔW./n_training, Δb./n_training\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pack (generic function with 2 methods)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc \"This assumes the number of works and paragraphs known remains constant\" ->\n",
    "function unpack!(pvdm::PVDM, θ::Vector)\n",
    "    start=0\n",
    "    item=pvdm.we.L\n",
    "    len_total=length(item)\n",
    "    @inbounds pvdm.we.L = @pipe θ[1+start:len_total]|>reshape(_,size(item)...)\n",
    "    \n",
    "    start+=length(item)\n",
    "    item=pvdm.pe.L\n",
    "    len_total+=length(item)\n",
    "    @inbounds pvdm.pe.L = @pipe θ[1+start:len_total]|>reshape(_,size(item)...)\n",
    "    \n",
    "    start+=length(item)\n",
    "    item=pvdm.W \n",
    "    len_total+=length(item)\n",
    "    @inbounds pvdm.W = @pipe θ[1+start:len_total]|>reshape(_,size(item)...)\n",
    "    \n",
    "    start+=length(item)\n",
    "    item=pvdm.b \n",
    "    len_total+=length(item)\n",
    "    @inbounds pvdm.b = @pipe θ[1+start:len_total]\n",
    "    \n",
    "    pvdm\n",
    "end\n",
    "\n",
    "\n",
    "@doc \"This assumes the number of works and paragraphs known remains constant\" ->\n",
    "function pack{N}(L::AbstractMatrix{N}, D::AbstractMatrix{N}, W::AbstractMatrix{N},b::AbstractVector{N})\n",
    "    vcat(vec(L),vec(D), vec(W),b)\n",
    "end\n",
    "\n",
    "@doc \"This assumes the number of works and paragraphs known remains constant\" ->\n",
    "function pack(pvdm::PVDM)\n",
    "    pack(pvdm.we.L, pvdm.pe.L, pvdm.W, pvdm.b)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42421-element Array{Float32,1}:\n",
       " -0.000109332\n",
       " -4.84561e-5 \n",
       " -9.56933e-6 \n",
       "  5.8016e-5  \n",
       "  1.8734e-5  \n",
       " -7.14103e-6 \n",
       "  5.63339e-5 \n",
       "  4.13632e-7 \n",
       " -4.80776e-5 \n",
       "  6.11413e-5 \n",
       " -5.46602e-5 \n",
       " -3.38046e-5 \n",
       "  9.00398e-5 \n",
       "  ⋮          \n",
       " -0.0293528  \n",
       "  0.0476082  \n",
       "  0.0475605  \n",
       "  0.0476708  \n",
       "  0.0475885  \n",
       "  0.0476158  \n",
       "  0.0475833  \n",
       " -0.106188   \n",
       " -0.0293897  \n",
       " -0.0292676  \n",
       " -0.0292313  \n",
       " -0.0293492  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const WINDOW_LEN = 8 \n",
    "training = Vector{String}[pad(para, WINDOW_LEN+1) for para in training]\n",
    "\n",
    "we_outer = WE(Float32,String, 200)\n",
    "add_all_words!(we_outer, training)\n",
    "pvdm_outer = PVDM(we_outer, WINDOW_LEN);\n",
    "we_outer=0\n",
    "\n",
    "\n",
    "training_indexes = @pipe chain(map(para -> get_para_training_cases!(pvdm_outer, para), training)...) |> hcat(_...)\n",
    "para_indexes_o = training_indexes[1,:] |> vec\n",
    "window_indexes_o = training_indexes[2:end-1,:] \n",
    "label_indexes_o = training_indexes[end,:] |> vec;\n",
    "\n",
    "ŷs,xs = feedforward(pvdm_outer, para_indexes_o, window_indexes_o)\n",
    "ΔL, ΔD, ΔW, Δb  = backprop(pvdm_outer, label_indexes_o, ŷs,xs, para_indexes_o, window_indexes_o)\n",
    "\n",
    "ag = pack(ΔL, ΔD, ΔW, Δb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "g (generic function with 1 method)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DualNumbers\n",
    "using ForwardDiff\n",
    "\n",
    "function f(θ)\n",
    "    pvdm_inner = PVDM_light(pvdm_outer,Dual{typeof(pvdm_outer.varience)})\n",
    "    unpack!(pvdm_inner,θ)\n",
    "    ŷs = feedforward(pvdm_inner, para_indexes_o, window_indexes_o)[1]\n",
    "    loss(label_indexes_o, ŷs)\n",
    "end\n",
    "\n",
    "# Using forwarddiff_jacobian\n",
    "g = forwarddiff_gradient(f, typeof(pvdm_outer.varience), fadtype=:dual, n=length(pack(pvdm_outer)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42421-element Array{Float32,1}:\n",
       " -0.000109332\n",
       " -4.84561e-5 \n",
       " -9.56933e-6 \n",
       "  5.8016e-5  \n",
       "  1.8734e-5  \n",
       " -7.14103e-6 \n",
       "  5.63339e-5 \n",
       "  4.13633e-7 \n",
       " -4.80776e-5 \n",
       "  6.11413e-5 \n",
       " -5.46602e-5 \n",
       " -3.38046e-5 \n",
       "  9.00399e-5 \n",
       "  ⋮          \n",
       " -0.0293527  \n",
       "  0.0476082  \n",
       "  0.0475605  \n",
       "  0.0476708  \n",
       "  0.0475885  \n",
       "  0.0476158  \n",
       "  0.0475833  \n",
       " -0.106188   \n",
       " -0.0293897  \n",
       " -0.0292676  \n",
       " -0.0292313  \n",
       " -0.0293492  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "θ = pack(pvdm_outer)\n",
    "dg = g(θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42421x4 Array{Float32,2}:\n",
       " -0.000109332  -0.000109332  2.18279e-11  1.0\n",
       " -4.84561e-5   -4.84561e-5   3.63798e-12  1.0\n",
       " -9.56933e-6   -9.56933e-6   1.81899e-12  1.0\n",
       "  5.8016e-5     5.8016e-5    1.09139e-11  1.0\n",
       "  1.8734e-5     1.8734e-5    5.45697e-12  1.0\n",
       " -7.14103e-6   -7.14103e-6   1.36424e-12  1.0\n",
       "  5.63339e-5    5.63339e-5   7.27596e-12  1.0\n",
       "  4.13633e-7    4.13632e-7   5.40012e-13  1.0\n",
       " -4.80776e-5   -4.80776e-5   3.63798e-12  1.0\n",
       "  6.11413e-5    6.11413e-5   0.0          1.0\n",
       " -5.46602e-5   -5.46602e-5   7.27596e-12  1.0\n",
       " -3.38046e-5   -3.38046e-5   0.0          1.0\n",
       "  9.00399e-5    9.00398e-5   1.45519e-11  1.0\n",
       "  ⋮                                          \n",
       " -0.0293527    -0.0293528    5.58794e-9   1.0\n",
       "  0.0476082     0.0476082    7.45058e-9   1.0\n",
       "  0.0475605     0.0475605    0.0          1.0\n",
       "  0.0476708     0.0476708    0.0          1.0\n",
       "  0.0475885     0.0475885    0.0          1.0\n",
       "  0.0476158     0.0476158    3.72529e-9   1.0\n",
       "  0.0475833     0.0475833    0.0          1.0\n",
       " -0.106188     -0.106188     2.23517e-8   1.0\n",
       " -0.0293897    -0.0293897    7.45058e-9   1.0\n",
       " -0.0292676    -0.0292676    5.58794e-9   1.0\n",
       " -0.0292313    -0.0292313    0.0          1.0\n",
       " -0.0293492    -0.0293492    9.31323e-9   1.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dg ag abs(dg-ag) dg./ag] #[1:length(pvdm_outer.we.L), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countnz(dg./ag.==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length(pvdm_outer.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.9-pre",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
