{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Compat\n",
    "using Docile\n",
    "using Iterators\n",
    "using Pipe\n",
    "using DataStructures\n",
    "\n",
    "macro printval(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\" = \", $ee)))\n",
    "end\n",
    "\n",
    "macro pz(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\"\\t\\t\",typeof($ee), \"\\t\", size($ee))))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20-element Array{Union(ASCIIString,UTF8String),1}:\n",
       " \"/root/buildFromSource/julia/usr/local/share/julia/site/v0.3\"\n",
       " \"/root/buildFromSource/julia/usr/share/julia/site/v0.3\"      \n",
       " \"../Corpus\"                                                  \n",
       " \"../doc2vec\"                                                 \n",
       " \"../Models\"                                                  \n",
       " \"../Optimisation\"                                            \n",
       " \"../recursive_embeddings\"                                    \n",
       " \"../summaristation\"                                          \n",
       " \"../tools\"                                                   \n",
       " \"../util\"                                                    \n",
       " \"../word-embedding3\"                                         \n",
       " \"../Corpus\"                                                  \n",
       " \"../doc2vec\"                                                 \n",
       " \"../Models\"                                                  \n",
       " \"../Optimisation\"                                            \n",
       " \"../recursive_embeddings\"                                    \n",
       " \"../summaristation\"                                          \n",
       " \"../tools\"                                                   \n",
       " \"../util\"                                                    \n",
       " \"../word-embedding3\"                                         "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "push!(LOAD_PATH, map(x->\"../\"*x, filter(fn-> !(contains(fn,\".\")),readdir(\"..\")))...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using WordEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\t\tArray{Array{String,1},1}\t(6097,)\n"
     ]
    }
   ],
   "source": [
    "training = open(\"../Corpus/serialised/opinosis_train_dev_plain.jsz\",\"r\") do fs\n",
    "    deserialize(fs)\n",
    "end\n",
    "@pz training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: redefining constant START_PAD_WORD\n",
      "Warning: redefining constant END_WORD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pad_advanced (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const START_PAD_WORD = \"*START*\"\n",
    "const END_WORD = \"*END*\"\n",
    "\n",
    "function pad{S<:String}(sent::Vector{S}, padded_length)\n",
    "    if length(sent) <= padded_length\n",
    "        ret =  fill(START_PAD_WORD,padded_length)\n",
    "        ret[end-length(sent)+1:end] = sent\n",
    "        ret\n",
    "    else\n",
    "        sent\n",
    "    end\n",
    "end\n",
    "\n",
    "function pad_advanced{S<:String}(sent::Vector{S}, window_length::Int)\n",
    "    padded_length = 2 + max(window_length+1, length(sent)) # ALways have at least 1 start and end padding, so plus 2 elements\n",
    "    ret =  fill(START_PAD_WORD,padded_length)\n",
    "    ret[end]=END_WORD\n",
    "    ret[end-length(sent)-1+1: end-1]=sent\n",
    "    ret\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "add_all_words! (generic function with 4 methods)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "invalid redefinition of constant PVDM\nwhile loading In[15], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "invalid redefinition of constant PVDM\nwhile loading In[15], in expression starting on line 1",
      ""
     ]
    }
   ],
   "source": [
    "type PVDM{N<:Number, S<:AbstractString}\n",
    "    we::WE\n",
    "    pe::WE #use a word embedder for Paragraphs too\n",
    "    \n",
    "    W::AbstractMatrix{N}\n",
    "    b::AbstractVector{N}\n",
    "\n",
    "    window_length::Int\n",
    "    varience::N\n",
    "end\n",
    "\n",
    "function PVDM{N,S}(we::WE{N,S}, window_length::Int, varience=0.001)\n",
    "    \n",
    "    emb_width,n_words = size(we.L)\n",
    "    concat_layer_width = emb_width*(window_length+1)\n",
    "    const W = convert(Matrix{N}, varience*randn(n_words,concat_layer_width))\n",
    "    const b = convert(Vector{N}, varience*randn(n_words))\n",
    "    \n",
    "    pe = WE(N,Vector{S},emb_width)\n",
    "    \n",
    "    PVDM{N,S}( we, pe, W,b, window_length, varience)\n",
    "end\n",
    "\n",
    "@doc \"Lightwieght version, that does not have support for lookups or additions\" ->\n",
    "function PVDM_light{N,S}(pvdm::PVDM{N,S})\n",
    "    we = WE_light(pvdm.we)\n",
    "    pe = WE_light(pvdm.pe)\n",
    "    W = pvdm.W\n",
    "    b = pvdm.b\n",
    "    window_length = pvdm.window_length\n",
    "    varience = nan(N)\n",
    "    PVDM{N,S}( we, pe, W,b, window_length, varience)\n",
    "end\n",
    "\n",
    "@doc \"Lightwieght version, that does not have support for lookups or additions, and has N converted type N2\" ->\n",
    "function PVDM_light{N,S}(pvdm::PVDM{N,S}, N2::DataType)\n",
    "    we = WE_light(pvdm.we,N2)\n",
    "    pe = WE_light(pvdm.pe, N2)\n",
    "    W = convert(Matrix{N2},pvdm.W)\n",
    "    b = convert(Vector{N2},pvdm.b)\n",
    "    window_length = pvdm.window_length\n",
    "    varience = convert(N2,NaN)\n",
    "    PVDM{N2,S}( we, pe, W,b, window_length, varience)\n",
    "end\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_para_training_cases! (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc \"\"\"gets the training cases as vector of (paraIndex, [word_indexes], label_word_index),\n",
    "cycling by the window length.\n",
    "Adds the paragraph if it does not already have an index\n",
    "\"\"\" ->\n",
    "function get_para_training_cases!{S<:String}(pvdm::PVDM, para::Vector{S})\n",
    "    para_ind = get_word_index!(pvdm.pe, para, pvdm.varience)\n",
    "    \n",
    "    Task() do \n",
    "        @assert length(para)>=pvdm.window_length+1\n",
    "        for offset in 0:length(para)-(pvdm.window_length+1)\n",
    "            window_iis = [1:pvdm.window_length;]+offset\n",
    "            label_ii = pvdm.window_length+1+offset\n",
    "            \n",
    "            window_words = para[window_iis]\n",
    "            label_word = para[label_ii]\n",
    "                        \n",
    "            windows_indexes = map(word->get_word_index(pvdm.we, word), window_words)\n",
    "            label_index = get_word_index(pvdm.we, label_word)\n",
    "            \n",
    "            produce(Int64[para_ind, windows_indexes..., label_index])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_input_layers (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_input_layer(pvdm::PVDM, para_index::Int, window_indexes::Vector{Int})\n",
    "    @inbounds [pvdm.pe.L[:,para_index], vec(pvdm.we.L[:,window_indexes])]\n",
    "end \n",
    "\n",
    "function get_input_layers{N,S, I<:Int}(pvdm::PVDM{N,S}, para_indexes::Vector{I}, window_indexeses::Matrix{I})\n",
    "    const emb_width = size(pvdm.we.L,1)\n",
    "    const n_training = length(para_indexes)\n",
    "    \n",
    "    xs = Array(N,(emb_width * (pvdm.window_length+1),n_training))\n",
    "    @inbounds xs[1:emb_width,:] = pvdm.pe.L[:,para_indexes]\n",
    "    for training_case in 1:n_training\n",
    "        @inbounds const window_indexes = window_indexeses[:,training_case]\n",
    "        @inbounds xs[emb_width+1:end,training_case] = vec(pvdm.we.L[:,window_indexes])\n",
    "    end\n",
    "    xs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feedforward (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax(zs)\n",
    "    (1./sum(exp(zs),1)).*exp(zs)\n",
    "end\n",
    "\n",
    "function feedforward{N,S, I<:Int}(pvdm::PVDM{N,S}, para_indexes::Vector{I}, window_indexeses::Matrix{I})\n",
    "    xs = get_input_layers(pvdm, para_indexes, window_indexeses)\n",
    "    \n",
    "    #Speed optimised version of `zs = pvdm.W*xs .+ pvdm.b`\n",
    "    zs = pvdm.W*xs \n",
    "    const n_training = length(para_indexes)\n",
    "    for ii in 1:n_training\n",
    "        @inbounds zs[:,ii]+= pvdm.b\n",
    "    end\n",
    "    ŷs = softmax(zs)\n",
    "    ŷs, xs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss{I,N}(y_indexes::Vector{I}, ŷs::Matrix{N})\n",
    "    @assert length(y_indexes)>0\n",
    "    #C=−∑j yj*log ŷj,\n",
    "    c = zero(N)\n",
    "    for tc in 1:length(y_indexes)\n",
    "        @inbounds c-=log(ŷs[y_indexes[tc],tc])\n",
    "    end\n",
    "    c/length(y_indexes)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backprop{N,S,I}(pvdm::PVDM{N,S}, y_indexes::Vector{I}, ŷs::Matrix{N}, xs::Matrix{N} , para_indexes::Vector{I}, window_indexeses::Matrix{I} )\n",
    "    const emb_width = size(pvdm.we.L,1)\n",
    "    const n_training = length(para_indexes)\n",
    "    const window_len = pvdm.window_length \n",
    "    \n",
    "    @assert length(n_training)>0\n",
    "    \n",
    "    #Δb = zeros(pvdm.b)\n",
    "    #ΔW = zeros(pvdm.W)\n",
    "    ΔL = zeros(pvdm.we.L) #Word Vector Changes\n",
    "    ΔD = zeros(pvdm.pe.L) #Paragraph Vector Changes\n",
    "    \n",
    "        \n",
    "    #speed optimistation of `δ_top_s = ŷs.-ys`\n",
    "    δ_top_s = copy(ŷs)\n",
    "    for tc in 1:length(y_indexes)\n",
    "        @inbounds δ_top_s[y_indexes[tc],tc]-=one(typeof(ŷs[1]))\n",
    "    end\n",
    "\n",
    "    Δb = sum(δ_top_s,2) |> vec\n",
    "    ΔW = (δ_top_s * xs')\n",
    "    δ_input_s= (pvdm.W'*δ_top_s) #the activation function of the layer below dxs=d(1*D[ii];L[iis]) =1\n",
    "    \n",
    "    #Paragraph vector Error\n",
    "    for ii in 1:n_training #Add sequentially, reather than via in a += as that would only allow one add for repreased index\n",
    "        @inbounds ΔD[:,para_indexes[ii]] += δ_input_s[1:emb_width,ii]\n",
    "    end\n",
    "    \n",
    "    #word vectors\n",
    "    for ii in 1:n_training\n",
    "        for ww in 1:window_len\n",
    "            const offset=ww*emb_width\n",
    "            @inbounds ΔL[:,window_indexeses[ww,ii]]+=δ_input_s[offset+1:offset+emb_width, ii]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "   \n",
    "    ΔL./n_training, ΔD./n_training, ΔW./n_training, Δb./n_training\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pack (generic function with 2 methods)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc \"This assumes the number of works and paragraphs known remains constant\" ->\n",
    "function unpack!(pvdm::PVDM, θ::Vector)\n",
    "    start=0\n",
    "    item=pvdm.we.L\n",
    "    len_total=length(item)\n",
    "    @inbounds pvdm.we.L = @pipe θ[1+start:len_total]|>reshape(_,size(item)...)\n",
    "    \n",
    "    start+=length(item)\n",
    "    item=pvdm.pe.L\n",
    "    len_total+=length(item)\n",
    "    @inbounds pvdm.pe.L = @pipe θ[1+start:len_total]|>reshape(_,size(item)...)\n",
    "    \n",
    "    start+=length(item)\n",
    "    item=pvdm.W \n",
    "    len_total+=length(item)\n",
    "    @inbounds pvdm.W = @pipe θ[1+start:len_total]|>reshape(_,size(item)...)\n",
    "    \n",
    "    start+=length(item)\n",
    "    item=pvdm.b \n",
    "    len_total+=length(item)\n",
    "    @inbounds pvdm.b = @pipe θ[1+start:len_total]\n",
    "    \n",
    "    pvdm\n",
    "end\n",
    "\n",
    "\n",
    "@doc \"This assumes the number of works and paragraphs known remains constant\" ->\n",
    "function pack{N}(L::AbstractMatrix{N}, D::AbstractMatrix{N}, W::AbstractMatrix{N},b::AbstractVector{N})\n",
    "    vcat(vec(L),vec(D), vec(W),b)\n",
    "end\n",
    "\n",
    "@doc \"This assumes the number of works and paragraphs known remains constant\" ->\n",
    "function pack(pvdm::PVDM)\n",
    "    pack(pvdm.we.L, pvdm.pe.L, pvdm.W, pvdm.b)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "const WINDOW_LEN = 8 \n",
    "training = Vector{String}[pad_advanced(para, WINDOW_LEN) for para in training]\n",
    "\n",
    "we_outer = WE(Float32,String, 200)\n",
    "add_all_words!(we_outer, training, 0.1)\n",
    "const pvdm_outer = PVDM(we_outer, WINDOW_LEN, 0.1);\n",
    "we_outer=0\n",
    "\n",
    "\n",
    "const training_indexes = @pipe chain(map(para -> get_para_training_cases!(pvdm_outer, para), training)...) |> hcat(_...)\n",
    "const para_indexes_o = training_indexes[1,:] |> vec\n",
    "const window_indexes_o = training_indexes[2:end-1,:] \n",
    "const label_indexes_o = training_indexes[end,:] |> vec;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_and_loss_grad! (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss!(θ::Vector)  \n",
    "    #warn(\"loss! not defined\")\n",
    "    grad = similar(θ)\n",
    "    cached_loss_and_loss_grad!(θ, grad)\n",
    "end\n",
    "\n",
    "function loss_grad!(θ::Vector, storage::Vector) \n",
    "    #warn(\"loss_grad not defined\")\n",
    "    cached_loss_and_loss_grad!(θ, grad)\n",
    "end\n",
    "\n",
    "\n",
    "function loss_and_loss_grad!(θ::Vector, grad::Vector)   \n",
    "    grad[:] = 0\n",
    "    unpack!(pvdm_outer, θ)\n",
    "    \n",
    "    ŷs,xs = feedforward(pvdm_outer, para_indexes_o, window_indexes_o)\n",
    "    Δs  = backprop(pvdm_outer, label_indexes_o, ŷs,xs, para_indexes_o, window_indexes_o)\n",
    "    grad[:] = pack(Δs...)\n",
    "    loss(label_indexes_o, ŷs)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cached_loss_and_loss_grad! (generic function with 1 method)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_and_loss_grad_cache = Dict{NumericVector,(Number, NumericVector)}()\n",
    "loss_and_loss_grad_cache_hits = 0\n",
    "loss_and_loss_grad_cache_misses = 0\n",
    "function cached_loss_and_loss_grad!(θ::Vector, grad::Vector)\n",
    "    global loss_and_loss_grad_cache\n",
    "    global loss_and_loss_grad_cache_hits\n",
    "    global loss_and_loss_grad_cache_misses\n",
    "    if haskey(loss_and_loss_grad_cache,θ)\n",
    "        loss_and_loss_grad_cache_hits+=1\n",
    "        err, grad[:] = loss_and_loss_grad_cache[θ]\n",
    "        err\n",
    "    else\n",
    "        loss_and_loss_grad_cache_misses+=1\n",
    "        err = loss_and_loss_grad!(θ, grad)\n",
    "        loss_and_loss_grad_cache[θ] = (err, grad)\n",
    "        err\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     Function value   Gradient norm \n",
      "     0     5.372543e+00     2.525859e-02\n",
      "     1     5.348296e+00     1.087051e-02\n",
      "     2     5.327250e+00     2.117648e-02\n",
      "     3     5.301131e+00     2.745742e-02\n",
      "     4     5.236059e+00     2.439390e-02\n",
      "     5     5.152967e+00     2.285820e-02\n",
      "     6     5.111018e+00     3.013088e-02\n",
      "     7     5.039601e+00     6.645359e-02\n",
      "     8     4.964079e+00     1.482462e-02\n",
      "     9     4.930750e+00     5.614407e-02\n",
      "    10     4.851865e+00     1.417908e-02\n",
      "    11     4.816745e+00     3.000415e-02\n",
      "    12     4.745643e+00     2.255416e-02\n",
      "    13     4.699772e+00     1.087656e-02\n",
      "    14     4.663737e+00     2.686708e-02\n",
      "    15     4.586313e+00     3.745591e-02\n",
      "    16     4.501931e+00     2.965717e-02\n",
      "    17     4.434673e+00     2.103150e-02\n",
      "    18     4.377546e+00     1.942718e-02\n",
      "    19     4.303544e+00     4.502885e-02\n",
      "    20     4.217514e+00     2.911804e-02\n",
      "elapsed time: 3725.801402937 seconds (691079721816 bytes allocated, 8.92% gc time)\n",
      "res.f_calls = 61\n",
      "res.g_calls = 41\n",
      "res.iterations = 20\n",
      "res.f_minimum = 4.217513561248779\n",
      "res.f_converged = false\n",
      "res.trace = Iter     Function value   Gradient norm \n",
      "------   --------------   --------------\n",
      "     0     5.372543e+00     2.525859e-02\n",
      "     1     5.348296e+00     1.087051e-02\n",
      "     2     5.327250e+00     2.117648e-02\n",
      "     3     5.301131e+00     2.745742e-02\n",
      "     4     5.236059e+00     2.439390e-02\n",
      "     5     5.152967e+00     2.285820e-02\n",
      "     6     5.111018e+00     3.013088e-02\n",
      "     7     5.039601e+00     6.645359e-02\n",
      "     8     4.964079e+00     1.482462e-02\n",
      "     9     4.930750e+00     5.614407e-02\n",
      "    10     4.851865e+00     1.417908e-02\n",
      "    11     4.816745e+00     3.000415e-02\n",
      "    12     4.745643e+00     2.255416e-02\n",
      "    13     4.699772e+00     1.087656e-02\n",
      "    14     4.663737e+00     2.686708e-02\n",
      "    15     4.586313e+00     3.745591e-02\n",
      "    16     4.501931e+00     2.965717e-02\n",
      "    17     4.434673e+00     2.103150e-02\n",
      "    18     4.377546e+00     1.942718e-02\n",
      "    19     4.303544e+00     4.502885e-02\n",
      "    20     4.217514e+00     2.911804e-02\n",
      "\n",
      "loss_and_loss_grad_cache_hits = 81\n",
      "loss_and_loss_grad_cache_misses = 164\n"
     ]
    }
   ],
   "source": [
    "using Optim #https://github.com/JuliaOpt/Optim.jl\n",
    "f=DifferentiableFunction(loss!,loss_grad!,cached_loss_and_loss_grad!)\n",
    "#θ = pack(pvdm_outer)\n",
    "θ=res.minimum\n",
    "#θ=optx\n",
    "@time res = optimize(f, θ, method=:cg, show_trace = true, store_trace = true, iterations = 20);\n",
    "@printval res.f_calls \n",
    "@printval res.g_calls \n",
    "@printval res.iterations\n",
    "@printval res.f_minimum\n",
    "@printval res.f_converged \n",
    "@printval res.trace\n",
    "@printval loss_and_loss_grad_cache_hits\n",
    "@printval loss_and_loss_grad_cache_misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res.f_calls = 16\n",
      "res.g_calls = 11\n",
      "res.iterations = 5\n",
      "res.f_minimum = 5.504724979400635\n",
      "res.gr_converged = false\n",
      "res.x_converged = false\n",
      "res.f_converged = false\n",
      "res.trace = Iter     Function value   Gradient norm \n",
      "------   --------------   --------------\n",
      "     0     5.990041e+00     1.915355e-02\n",
      "     1     5.914227e+00     1.475701e-02\n",
      "     2     5.803874e+00     5.204834e-02\n",
      "     3     5.728202e+00     1.351991e-02\n",
      "     4     5.631781e+00     2.845515e-02\n",
      "     5     5.504725e+00     3.959038e-02\n",
      "\n",
      "loss_and_loss_grad_cache_hits = 56\n",
      "loss_and_loss_grad_cache_misses = 119\n"
     ]
    }
   ],
   "source": [
    "@printval res.f_calls \n",
    "@printval res.g_calls \n",
    "@printval res.iterations\n",
    "@printval res.f_minimum\n",
    "@printval res.gr_converged\n",
    "@printval res.x_converged                       \n",
    "@printval res.f_converged \n",
    "@printval res.trace\n",
    "@printval loss_and_loss_grad_cache_hits\n",
    "@printval loss_and_loss_grad_cache_misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.9-pre",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
