{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Compat\n",
    "using Iterators\n",
    "using Pipe\n",
    "using DataStructures\n",
    "\n",
    "macro printval(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\" = \", $ee)))\n",
    "end\n",
    "\n",
    "macro pz(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\"\\t\\t\",typeof($ee), \"\\t\", size($ee))))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11-element Array{Union{UTF8String,ASCIIString},1}:\n",
       " \"/root/buildFromSource/julia0.4/usr/local/share/julia/site/v0.4\"\n",
       " \"/root/buildFromSource/julia0.4/usr/share/julia/site/v0.4\"      \n",
       " \"../Corpus\"                                                     \n",
       " \"../doc2vec\"                                                    \n",
       " \"../Models\"                                                     \n",
       " \"../Optimisation\"                                               \n",
       " \"../recursive_embeddings\"                                       \n",
       " \"../summaristation\"                                             \n",
       " \"../tools\"                                                      \n",
       " \"../util\"                                                       \n",
       " \"../word-embedding3\"                                            "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "push!(LOAD_PATH, map(x->\"../\"*x, filter(fn-> !(contains(fn,\".\")),readdir(\"..\")))...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hcat_no_splatting (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function hcat_no_splatting(xss::Vector{Any}) #Not actually for vector Any, only really for Vector{Vector{Any}}\n",
    "    ncols = length(xss)\n",
    "    nrows = length(first(xss))\n",
    "    S = xss|>first|>eltype\n",
    "    ret = Array(S,(nrows,ncols))\n",
    "    for ii in eachindex(xss)\n",
    "        @inbounds col = xss[ii]\n",
    "        #@assert(length(col)==nrows)\n",
    "        @inbounds ret[:,ii] = col\n",
    "    end\n",
    "    ret\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using WordEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\t\t"
     ]
    }
   ],
   "source": [
    "training = open(\"../Corpus/serialised/opinosis_train_dev_plain.jsz\",\"r\") do fs\n",
    "    deserialize(fs)\n",
    "end\n",
    "@pz training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pad_advanced (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const START_PAD_WORD = \"*START*\"\n",
    "const END_WORD = \"*END*\"\n",
    "\n",
    "function pad{S<:String}(sent::Vector{S}, padded_length)\n",
    "    if length(sent) <= padded_length\n",
    "        ret =  fill(START_PAD_WORD,padded_length)\n",
    "        ret[end-length(sent)+1:end] = sent\n",
    "        ret\n",
    "    else\n",
    "        sent\n",
    "    end\n",
    "end\n",
    "\n",
    "function pad_advanced{S<:String}(sent::Vector{S}, window_length::Int)\n",
    "    padded_length = 2 + max(window_length+1, length(sent)) # ALways have at least 1 start and end padding, so plus 2 elements\n",
    "    ret =  fill(START_PAD_WORD,padded_length)\n",
    "    ret[end]=END_WORD\n",
    "    ret[end-length(sent)-1+1: end-1]=sent\n",
    "    ret\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array{Array{UTF8String,1},1}\t(6097,)\n"
     ]
    }
   ],
   "source": [
    "type PVDM{N<:Number, S<:AbstractString}\n",
    "    we::WE\n",
    "    pe::WE #use a word embedder for Paragraphs too\n",
    "    \n",
    "    W::AbstractMatrix{N}\n",
    "    b::AbstractVector{N}\n",
    "\n",
    "    window_length::Int\n",
    "    varience::N\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PVDM_light (generic function with 2 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "function PVDM{N,S}(we::WE{N,S}, window_length::Int, varience=0.001, output_width::Int=size(we.L,2))\n",
    "    emb_width = size(we.L,1)\n",
    "    concat_layer_width = emb_width*(window_length+1)\n",
    "    const W = convert(Matrix{N}, varience*randn(output_width,concat_layer_width))\n",
    "    const b = convert(Vector{N}, varience*randn(output_width))\n",
    "    pe = WE(N,Vector{S},emb_width)\n",
    "    \n",
    "    PVDM{N,S}( we, pe, W,b, window_length, varience)\n",
    "end\n",
    "\n",
    "@doc \"Lightwieght version, that does not have support for lookups or additions\" ->\n",
    "function PVDM_light{N,S}(pvdm::PVDM{N,S})\n",
    "    we = WE_light(pvdm.we)\n",
    "    pe = WE_light(pvdm.pe)\n",
    "    W = pvdm.W\n",
    "    b = pvdm.b\n",
    "    window_length = pvdm.window_length\n",
    "    varience = nan(N)\n",
    "    PVDM{N,S}( we, pe, W,b, window_length, varience)\n",
    "end\n",
    "\n",
    "@doc \"Lightwieght version, that does not have support for lookups or additions, and has N converted type N2\" ->\n",
    "function PVDM_light{N,S}(pvdm::PVDM{N,S}, N2::DataType)\n",
    "    we = WE_light(pvdm.we,N2)\n",
    "    pe = WE_light(pvdm.pe, N2)\n",
    "    W = convert(Matrix{N2},pvdm.W)\n",
    "    b = convert(Vector{N2},pvdm.b)\n",
    "    window_length = pvdm.window_length\n",
    "    varience = convert(N2,NaN)\n",
    "    PVDM{N2,S}( we, pe, W,b, window_length, varience)\n",
    "end\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_para_training_cases! (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc \"\"\"gets the training cases as vector of (paraIndex, [word_indexes], label_word_index),\n",
    "cycling by the window length.\n",
    "Adds the paragraph if it does not already have an index\n",
    "\"\"\" ->\n",
    "function get_para_training_cases!{S<:String}(pvdm::PVDM, para::Vector{S})\n",
    "    para_ind = get_word_index!(pvdm.pe, para, pvdm.varience)\n",
    "    \n",
    "    Task() do \n",
    "        @assert length(para)>=pvdm.window_length+1\n",
    "        for offset in 0:length(para)-(pvdm.window_length+1)\n",
    "            window_iis = [1:pvdm.window_length;]+offset\n",
    "            label_ii = pvdm.window_length+1+offset\n",
    "            \n",
    "            window_words = para[window_iis]\n",
    "            label_word = para[label_ii]\n",
    "                        \n",
    "            windows_indexes = map(word->get_word_index(pvdm.we, word), window_words)\n",
    "            label_index = get_word_index(pvdm.we, label_word)\n",
    "            \n",
    "            produce(Int64[para_ind, windows_indexes..., label_index])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_input_layers (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_input_layer(pvdm::PVDM, para_index::Int, window_indexes::Vector{Int})\n",
    "    @inbounds [pvdm.pe.L[:,para_index], vec(pvdm.we.L[:,window_indexes])]\n",
    "end \n",
    "\n",
    "function get_input_layers{N,S, I<:Int}(pvdm::PVDM{N,S}, para_indexes::Vector{I}, window_indexeses::Matrix{I})\n",
    "    const emb_width = size(pvdm.we.L,1)\n",
    "    const n_training = length(para_indexes)\n",
    "    \n",
    "    xs = Array(N,(emb_width * (pvdm.window_length+1),n_training))\n",
    "    @inbounds xs[1:emb_width,:] = pvdm.pe.L[:,para_indexes]\n",
    "    for training_case in 1:n_training\n",
    "        @inbounds const window_indexes = window_indexeses[:,training_case]\n",
    "        @inbounds xs[emb_width+1:end,training_case] = vec(pvdm.we.L[:,window_indexes])\n",
    "    end\n",
    "    xs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feedforward (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function feedforward{N,S, I<:Int}(pvdm::PVDM{N,S}, para_indexes::Vector{I}, window_indexeses::Matrix{I}, output_activation::Function)\n",
    "    xs = get_input_layers(pvdm, para_indexes, window_indexeses)\n",
    "    \n",
    "    #Speed optimised version of `zs = pvdm.W*xs .+ pvdm.b`\n",
    "    zs = pvdm.W*xs \n",
    "    const n_training = length(para_indexes)\n",
    "    for ii in 1:n_training\n",
    "        @inbounds zs[:,ii]+= pvdm.b\n",
    "    end\n",
    "    ŷs = output_activation(zs)\n",
    "    ŷs, xs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop_softmax (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax(zs)\n",
    "    (1./sum(exp(zs),1)).*exp(zs)\n",
    "end\n",
    "\n",
    "function loss_softmax{I,N}(y_indexes::Vector{I}, ŷs::Matrix{N})\n",
    "    @assert length(y_indexes)>0\n",
    "    #C=−∑j yj*log ŷj,\n",
    "    c = zero(N)\n",
    "    for tc in 1:length(y_indexes)\n",
    "        @inbounds c-=log(ŷs[y_indexes[tc],tc])\n",
    "    end\n",
    "    c/length(y_indexes)\n",
    "end\n",
    "\n",
    "function feedforward_softmax{N,S, I<:Int}(pvdm::PVDM{N,S}, para_indexes::Vector{I}, window_indexeses::Matrix{I})\n",
    "    feedforward(pvdm, para_indexes, window_indexeses, softmax)\n",
    "end\n",
    "\n",
    "function backprop_softmax{N,S,I}(pvdm::PVDM{N,S}, y_indexes::Vector{I}, ŷs::Matrix{N}, xs::Matrix{N} , para_indexes::Vector{I}, window_indexeses::Matrix{I})\n",
    "    #speed optimistation of `δ_top_s = ŷs.-ys`\n",
    "    δ_top_s = copy(ŷs)\n",
    "    for tc in 1:length(y_indexes)\n",
    "        @inbounds δ_top_s[y_indexes[tc],tc]-=one(typeof(ŷs[1]))\n",
    "    end\n",
    "\n",
    "    backprop(pvdm, δ_top_s, ŷs, xs, para_indexes, window_indexeses)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop_hierarchical_softmax (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@fastmath function sigmoid(zs) \n",
    "    #1./(1.0+e.^-zs)\n",
    "    #Speed op of 1./(1+e.^-zs) \n",
    "    ret = similar(zs)\n",
    "    @simd for ii in eachindex(ret)\n",
    "        @inbounds ret[ii]=1./exp(-zs[ii]+1.0) #\n",
    "    end\n",
    "    ret\n",
    "end\n",
    "\n",
    "function loss_hierarchical_softmax{N}(y_codes::Vector{BitVector}, ŷs::Matrix{N})\n",
    "    c = zero(N)\n",
    "    for tc in 1:size(ŷs,2)\n",
    "        const y_code= y_codes[tc]\n",
    "        for d in eachindex(y_code)\n",
    "            const y = y_code[d] ? zero(N) : one(N)\n",
    "            c += y - ŷs[d,tc]\n",
    "            #if y_code[d]\n",
    "            #    c+=ln(ŷs[d,tc])\n",
    "            #end\n",
    "        end\n",
    "    end\n",
    "    c/size(ŷs,2)\n",
    "end\n",
    "\n",
    "\n",
    "function feedforward_hierarchical_softmax{N,S, I<:Int}(pvdm::PVDM{N,S}, para_indexes::Vector{I}, window_indexeses::Matrix{I})\n",
    "    feedforward(pvdm, para_indexes, window_indexeses, sigmoid)\n",
    "end\n",
    "\n",
    "function backprop_hierarchical_softmax{N,S,I}(pvdm::PVDM{N,S}, y_codes::Vector{BitVector}, ŷs::Matrix{N}, xs::Matrix{N} , para_indexes::Vector{I}, window_indexeses::Matrix{I})\n",
    "    #speed optimistation of `δ_top_s = ŷs.-ys`\n",
    "    δ_top_s = similar(ŷs)\n",
    "    for tc in size(ŷs,2)\n",
    "        const y_code= y_codes[tc]\n",
    "        for d in length(y_code)\n",
    "            const y = y_code[d] ? zero(N) : one(N)\n",
    "            δ_top_s[d,tc] = y - ŷs[d,tc]\n",
    "            #δ_top_s[d,tc] =  y_code[d] ? one(N)-ŷs[d,tc] : zero(N)\n",
    "        end\n",
    "        for d in length(y_code):size(ŷs,1)\n",
    "            δ_top_s[d,tc]=zero(N)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    backprop(pvdm, δ_top_s, ŷs, xs, para_indexes, window_indexeses)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: pvdm_outer not defined\nwhile loading In[14], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: pvdm_outer not defined\nwhile loading In[14], in expression starting on line 1",
      ""
     ]
    }
   ],
   "source": [
    "ŷs,xs=feedforward_hierarchical_softmax(pvdm_outer, para_indexes_o, window_indexes_o)\n",
    "backprop_hierarchical_softmax(pvdm_outer, label_codes_o, ŷs,xs, para_indexes_o, window_indexes_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function backprop{N,S,I}(pvdm::PVDM{N,S}, δ_top_s::Matrix{N}, ŷs::Matrix{N}, xs::Matrix{N} , para_indexes::Vector{I}, window_indexeses::Matrix{I})\n",
    "    const emb_width = size(pvdm.we.L,1)\n",
    "    const n_training = length(para_indexes)\n",
    "    const window_len = pvdm.window_length \n",
    "\n",
    "    @assert length(n_training)>0\n",
    "    \n",
    "    ΔL = zeros(pvdm.we.L) #Word Vector Changes\n",
    "    ΔD = zeros(pvdm.pe.L) #Paragraph Vector Changes\n",
    "    \n",
    "    Δb = sum(δ_top_s,2) |> vec\n",
    "    ΔW = (δ_top_s * xs')\n",
    "    δ_input_s= (pvdm.W'*δ_top_s) #the activation function of the layer below dxs=d(1*D[ii];L[iis]) =1\n",
    "    \n",
    "    #Paragraph vector Error\n",
    "    for ii in 1:n_training #Add sequentially, reather than via in a += as that would only allow one add for repreased index\n",
    "        @inbounds ΔD[:,para_indexes[ii]] += δ_input_s[1:emb_width,ii]\n",
    "    end\n",
    "    \n",
    "    #word vectors\n",
    "    for ii in 1:n_training\n",
    "        for ww in 1:window_len\n",
    "            const offset=ww*emb_width\n",
    "            @inbounds ΔL[:,window_indexeses[ww,ii]]+=δ_input_s[offset+1:offset+emb_width, ii]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    ΔL./n_training, ΔD./n_training, ΔW./n_training, Δb./n_training\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pack (generic function with 2 methods)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc \"This assumes the number of works and paragraphs known remains constant\" ->\n",
    "function unpack!(pvdm::PVDM, θ::Vector)\n",
    "    start=0\n",
    "    item=pvdm.we.L\n",
    "    len_total=length(item)\n",
    "    @inbounds pvdm.we.L = @pipe θ[1+start:len_total]|>reshape(_,size(item)...)\n",
    "    \n",
    "    start+=length(item)\n",
    "    item=pvdm.pe.L\n",
    "    len_total+=length(item)\n",
    "    @inbounds pvdm.pe.L = @pipe θ[1+start:len_total]|>reshape(_,size(item)...)\n",
    "    \n",
    "    start+=length(item)\n",
    "    item=pvdm.W \n",
    "    len_total+=length(item)\n",
    "    @inbounds pvdm.W = @pipe θ[1+start:len_total]|>reshape(_,size(item)...)\n",
    "    \n",
    "    start+=length(item)\n",
    "    item=pvdm.b \n",
    "    len_total+=length(item)\n",
    "    @inbounds pvdm.b = @pipe θ[1+start:len_total]\n",
    "    \n",
    "    pvdm\n",
    "end\n",
    "\n",
    "\n",
    "@doc \"This assumes the number of works and paragraphs known remains constant\" ->\n",
    "function pack{N}(L::AbstractMatrix{N}, D::AbstractMatrix{N}, W::AbstractMatrix{N},b::AbstractVector{N})\n",
    "    vcat(vec(L),vec(D), vec(W),b)\n",
    "end\n",
    "\n",
    "@doc \"This assumes the number of works and paragraphs known remains constant\" ->\n",
    "function pack(pvdm::PVDM)\n",
    "    pack(pvdm.we.L, pvdm.pe.L, pvdm.W, pvdm.b)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Huffman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "text/plain": [
       "6097-element Array{Array{AbstractString,1},1}:\n",
       " AbstractString[\"*START*\",\"being\",\"able\",\"to\",\"change\",\"the\",\"*UNKNOWN*\",\"sizes\",\"is\",\"awesome\",\"!\",\"*END*\"]                                                                            \n",
       " AbstractString[\"*START*\",\"for\",\"whatever\",\"reason\",\",\",\"*UNKNOWN*\",\"decided\",\"to\",\"make\",\"the\"  …  \"on\",\"the\",\"home\",\"screen\",\"than\",\"on\",\"the\",\"*UNKNOWN*\",\".\",\"*END*\"]               \n",
       " AbstractString[\"*START*\",\"i\",\"found\",\"myself\",\"constantly\",\"changing\",\"the\",\"angle\",\"of\",\"the\"  …  \"and\",\"down\",\"and\",\"the\",\"distance\",\"away\",\"from\",\"me\",\".\",\"*END*\"]                 \n",
       " AbstractString[\"*START*\",\"i\",\"was\",\"an\",\"avid\",\"reader\",\"but\",\"increasing\",\"age\",\"has\"  …  \"very\",\"light\",\"weight\",\"has\",\"made\",\"reading\",\"fun\",\"again\",\".\",\"*END*\"]                   \n",
       " AbstractString[\"*START*\",\"what\",\"'s\",\"more\",\",\",\"it\",\"'s\",\"easy\",\"to\",\"change\",\"*UNKNOWN*\",\"size\",\".\",\"*END*\"]                                                                         \n",
       " AbstractString[\"*START*\",\"the\",\"*UNKNOWN*\",\"does\",\"not\",\"recognize\",\"page\",\"numbers\",\",\",\"since\"  …  \"the\",\"notion\",\"of\",\"location\",\"which\",\"is\",\"display\",\"independent\",\".\",\"*END*\"]  \n",
       " AbstractString[\"*START*\",\"i\",\"just\",\"crank\",\"the\",\"*UNKNOWN*\",\"up\",\"a\",\"notch\",\"or\",\"two\",\"and\",\"leave\",\"the\",\"readers\",\"behind\",\".\",\"*END*\"]                                          \n",
       " AbstractString[\"*START*\",\"my\",\"*UNKNOWN*\",\"is\",\"fine\",\",\",\"but\",\"i\",\"can\",\"choose\",\"the\",\"*UNKNOWN*\",\"size\",\"that\",\"suits\",\"me\",\"best\",\".\",\"*END*\"]                                    \n",
       " AbstractString[\"*START*\",\"*UNKNOWN*\",\"easy\",\",\",\"on\",\",\",\"the\",\",\",\"eyes\",\"'\"  …  \"not\",\"big\",\"enough\",\"for\",\"people\",\"with\",\"vision\",\"problems\",\".\",\"*END*\"]                          \n",
       " AbstractString[\"*START*\",\"one\",\"or\",\"two\",\"larger\",\"*UNKNOWN*\",\"sizes\",\"would\",\"have\",\"benefited\",\"the\",\"visually\",\"*UNKNOWN*\",\".\",\"*END*\"]                                            \n",
       " AbstractString[\"*START*\",\"downloading\",\"books\",\"is\",\"easy\",\",\",\"the\",\"screen\",\"is\",\"not\"  …  \"and\",\"the\",\"*UNKNOWN*\",\"customer\",\"service\",\"support\",\"is\",\"terrific\",\".\",\"*END*\"]       \n",
       " AbstractString[\"*START*\",\"the\",\"text\",\"appears\",\"slightly\",\"*UNKNOWN*\",\"and\",\"i\",\"have\",\"to\",\"increase\",\"the\",\"*UNKNOWN*\",\"size\",\"to\",\"read\",\"it\",\"comfortably\",\".\",\"*END*\"]           \n",
       " AbstractString[\"*START*\",\"and\",\"with\",\"the\",\"*UNKNOWN*\",\",\",\"i\",\"can\",\"adjust\",\"the\"  …  \"to\",\"those\",\"with\",\"poor\",\"vision\",\"or\",\"tired\",\"eyes\",\".\",\"*END*\"]                          \n",
       " ⋮                                                                                                                                                                                      \n",
       " AbstractString[\"*START*\",\"obviously\",\"compared\",\"to\",\"*UNKNOWN*\",\"you\",\"know\",\"it\",\"will\",\"be\"  …  \"i\",\"am\",\"very\",\"happy\",\"with\",\"the\",\"operating\",\"system\",\".\",\"*END*\"]              \n",
       " AbstractString[\"*START*\",\"it\",\"'s\",\"fast\",\",\",\"it\",\"does\",\"n't\",\"crash\",\",\"  …  \",\",\"like\",\"than\",\"any\",\"previous\",\"*UNKNOWN*\",\"from\",\"*UNKNOWN*\",\".\",\"*END*\"]                         \n",
       " AbstractString[\"*START*\",\"i\",\"own\",\"*UNKNOWN*\",\"professional\",\"and\",\"wanted\",\"a\",\"nice\",\",\"  …  \"*UNKNOWN*\",\"the\",\"*UNKNOWN*\",\"before\",\"doing\",\"the\",\"*UNKNOWN*\",\"install\",\".\",\"*END*\"]\n",
       " AbstractString[\"*START*\",\"it\",\"is\",\"very\",\"fast\",\"and\",\"very\",\"user\",\"friendly\",\".\",\"*END*\"]                                                                                           \n",
       " AbstractString[\"*START*\",\"although\",\"most\",\"are\",\"saying\",\"that\",\"the\",\"start\",\"up\",\"is\"  …  \"disagree\",\",\",\"i\",\"feel\",\"its\",\"about\",\"the\",\"same\",\".\",\"*END*\"]                         \n",
       " AbstractString[\"*START*\",\"it\",\"takes\",\"a\",\"little\",\"work\",\"to\",\"setup\",\"and\",\"does\"  …  \"you\",\"application\",\"as\",\"fast\",\"as\",\"they\",\"would\",\"*UNKNOWN*\",\".\",\"*END*\"]                   \n",
       " AbstractString[\"*START*\",\"you\",\"need\",\"a\",\"faster\",\"*UNKNOWN*\",\"and\",\"extra\",\"memory\",\"if\",\"you\",\"plan\",\"on\",\"using\",\"this\",\"a\",\"lot\",\".\",\"*END*\"]                                     \n",
       " AbstractString[\"*START*\",\"i\",\"am\",\"very\",\"happy\",\"with\",\"the\",\"program\",\",\",\"it\"  …  \"install\",\"for\",\"my\",\"*UNKNOWN*\",\"wireless\",\"printer\",\"to\",\"work\",\".\",\"*END*\"]                    \n",
       " AbstractString[\"*START*\",\"0\",\"out\",\"of\",\"5\",\"stars\",\"windows\",\"7\",\"runs\",\"slower\"  …  \"runs\",\"much\",\"slower\",\"than\",\"it\",\"did\",\"with\",\"*UNKNOWN*\",\".\",\"*END*\"]                         \n",
       " AbstractString[\"*START*\",\"so\",\"i\",\"just\",\"got\",\"the\",\"windows\",\"7\",\"upgrade\",\"installed\"  …  \"amazed\",\"at\",\"how\",\"fast\",\"and\",\"sleek\",\"everything\",\"looked\",\".\",\"*END*\"]               \n",
       " AbstractString[\"*START*\",\"simply\",\"put\",\",\",\"windows\",\"7\",\"looks\",\"amazing\",\",\",\"and\",\"is\",\"fast\",\"and\",\"*UNKNOWN*\",\".\",\"*END*\"]                                                       \n",
       " AbstractString[\"*START*\",\"i\",\"started\",\"to\",\"upload\",\"a\",\"*UNKNOWN*\",\"video\",\",\",\"went\"  …  \"came\",\"back\",\"to\",\"a\",\"blue\",\"screen\",\"of\",\"death\",\".\\n\",\"*END*\"]                         "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.931 milliseconds (188 k allocations: 6608 KB, 26.49% gc time)\n"
     ]
    }
   ],
   "source": [
    "const WINDOW_LEN = 8 \n",
    "\n",
    "@time training = Vector{String}[pad_advanced(para, WINDOW_LEN) for para in training]\n",
    "\n",
    "#@time word2code_o = get_huffman_codes(training) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{AbstractString,BitArray{1}} with 5118 entries:\n",
       "  \"squat\"          => Bool[false,true,true,false,true,true,false,false,true,tru…\n",
       "  \"kms\"            => Bool[false,true,true,false,true,true,false,true,true,fals…\n",
       "  \"crisp\"          => Bool[false,false,false,true,true,false,true,true,false,tr…\n",
       "  \"enjoy\"          => Bool[false,true,false,false,false,true,false,false,false,…\n",
       "  \"whoever\"        => Bool[false,true,true,true,false,false,false,false,false,t…\n",
       "  \"airbags\"        => Bool[false,true,true,true,false,false,true,true,true,true…\n",
       "  \"advertisements\" => Bool[false,true,true,false,true,false,true,true,true,fals…\n",
       "  \"chocolate\"      => Bool[false,false,true,false,true,true,true,false,true,tru…\n",
       "  \"mangled\"        => Bool[false,true,true,true,false,false,true,false,false,tr…\n",
       "  \"everywhere\"     => Bool[true,true,true,true,true,false,true,false,false,true…\n",
       "  \"regular\"        => Bool[false,false,false,false,true,false,true,true,true,tr…\n",
       "  \"favorites\"      => Bool[false,true,true,true,false,true,true,true,false,true…\n",
       "  \"battery\\n0\"     => Bool[false,true,true,true,false,false,true,true,false,fal…\n",
       "  \"mileage\\nin\"    => Bool[false,true,true,false,true,true,false,true,true,fals…\n",
       "  \"outdoors\"       => Bool[false,true,true,true,false,true,false,false,true,fal…\n",
       "  \"helping\"        => Bool[false,true,true,false,true,true,false,true,false,fal…\n",
       "  \"during\"         => Bool[false,true,false,false,false,false,true,false,false,…\n",
       "  \"whose\"          => Bool[false,true,true,true,false,false,false,false,true,fa…\n",
       "  \"loads\"          => Bool[false,true,true,false,true,true,true,true,false,true…\n",
       "  \"favor\"          => Bool[false,true,true,true,false,false,true,false,false,fa…\n",
       "  \"readers\"        => Bool[false,true,true,false,true,true,false,false,true,fal…\n",
       "  \"sometimes\"      => Bool[true,true,false,true,false,true,true,false,false,fal…\n",
       "  \"borders\"        => Bool[false,true,true,true,false,true,true,false,true,true…\n",
       "  \"190\"            => Bool[false,true,true,true,false,true,true,false,true,true…\n",
       "  \"'m\"             => Bool[false,false,true,true,true,false,false,true,false,tr…\n",
       "  ⋮                => ⋮"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using HDF5, JLD\n",
    "#save(\"huffmancodes.jld\", \"word2code\", word2code_o)\n",
    "word2code_o = load(\"huffmancodes.jld\", \"word2code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "text/plain": [
       "85666-element Array{BitArray{1},1}:\n",
       " Bool[false,true,false,true,true,true]                                                  \n",
       " Bool[false,false,true,false,false,false,false,false,false,false,true,false]            \n",
       " Bool[false,true,false,false,false,true,true,true,false]                                \n",
       " Bool[false,false,false,false,false]                                                    \n",
       " Bool[false,false,false,false,true,false,false,false,false,false,false]                 \n",
       " Bool[true,false,false,false]                                                           \n",
       " Bool[false,false,true,true,false]                                                      \n",
       " Bool[true,true,false,false,false,false,false]                                          \n",
       " Bool[true,false,false,false]                                                           \n",
       " Bool[false,true,false,false,true,true,true,true,true,false,false,true,false]           \n",
       " Bool[true,true,false,false,false,false,true,false]                                     \n",
       " Bool[false,true,false,true,false,true,true,true,true]                                  \n",
       " Bool[true,true,false,false,false,false,false]                                          \n",
       " ⋮                                                                                      \n",
       " Bool[false,false,true,false,false,false,false,false,false,true,false]                  \n",
       " Bool[false,true,true,false,false]                                                      \n",
       " Bool[false,false,true,false,true,true,true,false,true,true,true,true,true]             \n",
       " Bool[true,true,false,false,true,true,true,false,true,true]                             \n",
       " Bool[false,false,true,true,true,true]                                                  \n",
       " Bool[false,false,true,false,false,true]                                                \n",
       " Bool[false,false,false,true,true,true,true,false,true,true,false,false,false]          \n",
       " Bool[true,true,false,false,false,false,true,false]                                     \n",
       " Bool[true,true,true,true,false,false]                                                  \n",
       " Bool[false,false,true,false,true,true,true,false,false,true,true,true,false,false,true]\n",
       " Bool[false,true,false,false,false,false,true,true,true,false,true,false]               \n",
       " Bool[false,false,false,false,false]                                                    "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.749 milliseconds (46935 allocations: 2161 KB)\n",
      "max_code_length = 17\n"
     ]
    }
   ],
   "source": [
    "@time const max_code_length = @pipe word2code_o |> values |> map(length,_) |> maximum(_)\n",
    "@printval max_code_length\n",
    "\n",
    "we_outer = WE(Float32,String, 200)\n",
    "add_all_words!(we_outer, training, 0.01)\n",
    "const pvdm_outer = PVDM(we_outer, WINDOW_LEN, 0.01, max_code_length);\n",
    "we_outer=0\n",
    "\n",
    "index2code_o(ii) = word2code_o[pvdm_outer.we.indexed_words[ii]]\n",
    "const training_indexes_o = @pipe ( training\n",
    "                                    |> map(para -> get_para_training_cases!(pvdm_outer, para),_) \n",
    "                                    |> chain(_...)\n",
    "                                    |> collect\n",
    "                                    |> hcat_no_splatting)\n",
    "const para_indexes_o = training_indexes_o[1,:] |> vec\n",
    "const window_indexes_o = training_indexes_o[2:end-1,:] \n",
    "const label_indexes_o = training_indexes_o[end,:] |> vec;\n",
    "\n",
    "\n",
    "const label_codes_o = BitVector[index2code_o(ii) for ii in label_indexes_o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_and_loss_grad! (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss!(θ::Vector)  \n",
    "    #warn(\"loss! not defined\")\n",
    "    grad = similar(θ)\n",
    "    cached_loss_and_loss_grad!(θ, grad)\n",
    "end\n",
    "\n",
    "function loss_grad!(θ::Vector, storage::Vector) \n",
    "    #warn(\"loss_grad not defined\")\n",
    "    cached_loss_and_loss_grad!(θ, grad)\n",
    "end\n",
    "\n",
    "\n",
    "function loss_and_loss_grad!(θ::Vector, grad::Vector)   \n",
    "    unpack!(pvdm_outer, θ)\n",
    "    \n",
    "    ŷs,xs = feedforward_hierarchical_softmax(pvdm_outer, para_indexes_o, window_indexes_o)\n",
    "    Δs  = backprop_hierarchical_softmax(pvdm_outer, label_codes_o, ŷs,xs, para_indexes_o, window_indexes_o)\n",
    "    grad[:] = pack(Δs...)\n",
    "    loss_hierarchical_softmax(label_codes_o, ŷs)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cached_loss_and_loss_grad! (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = typeof(pvdm_outer.varience)\n",
    "loss_and_loss_grad_cache = Dict{Vector{N}, Tuple{N,AbstractVector{N}}}()\n",
    "loss_and_loss_grad_cache_hits = 0\n",
    "loss_and_loss_grad_cache_misses = 0\n",
    "function cached_loss_and_loss_grad!(θ::Vector, grad::Vector)\n",
    "    global loss_and_loss_grad_cache\n",
    "    global loss_and_loss_grad_cache_hits\n",
    "    global loss_and_loss_grad_cache_misses\n",
    "    if haskey(loss_and_loss_grad_cache,θ)\n",
    "        loss_and_loss_grad_cache_hits+=1\n",
    "        err, grad[:] = loss_and_loss_grad_cache[θ]\n",
    "        err\n",
    "    else\n",
    "        loss_and_loss_grad_cache_misses+=1\n",
    "        err = loss_and_loss_grad!(θ, grad)\n",
    "        loss_and_loss_grad_cache[θ] = (err, grad)\n",
    "        err\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Optim #https://github.com/JuliaOpt/Optim.jl\n",
    "f=DifferentiableFunction(loss!,loss_grad!,cached_loss_and_loss_grad!)\n",
    "θ = pack(pvdm_outer)\n",
    "#θ=res.minimum\n",
    "#θ=optx\n",
    "@time res = optimize(f, θ, method=:cg, show_trace = true, store_trace = true, iterations = 2);\n",
    "@printval res.f_calls \n",
    "@printval res.g_calls \n",
    "@printval res.iterations\n",
    "@printval res.f_minimum\n",
    "@printval res.f_converged \n",
    "@printval res.trace\n",
    "@printval loss_and_loss_grad_cache_hits\n",
    "@printval loss_and_loss_grad_cache_misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ")elapsed time: 902.511280671 seconds (173774005532 bytes allocated, 10.39% gc time)\n",
    "res.f_calls = 18\n",
    "res.g_calls = 16\n",
    "res.iterations = 2\n",
    "res.f_minimum = 6.521084785461426\n",
    "res.f_converged = false\n",
    "res.trace = Iter     Function value   Gradient norm \n",
    "------   --------------   --------------\n",
    "     0     8.649927e+00     1.244972e-01\n",
    "     1     6.932991e+00     7.786864e-02\n",
    "     2     6.521085e+00     1.384170e-01\n",
    "\n",
    "loss_and_loss_grad_cache_hits = 9\n",
    "loss_and_loss_grad_cache_misses = 9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0-dev",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
