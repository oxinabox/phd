{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using SoftmaxClassifier\n",
    "using Training\n",
    "using AbstractTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Float64} with 5 entries:\n",
       "  \"B\" => 0.2\n",
       "  \"A\" => 0.4\n",
       "  \"C\" => 0.2\n",
       "  \"D\" => 0.1\n",
       "  \"E\" => 0.1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution = Dict(\"A\"=>0.4, \"B\" => 0.2, \"C\" => 0.2, \"D\"=> 0.1, \"E\"=>0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Trees.BranchNode with 2 children. data = SoftmaxClassifier.LinearClassifier{2}(5,Float32[0.63829 0.548069; -0.372887 -0.169173; 0.241831 -0.692348; -0.676522 0.988183; 0.598434 0.407975]),Dict(\"B\"=>[2,1],\"A\"=>[1],\"C\"=>[2,2,1],\"D\"=>[2,2,2,1],\"E\"=>[2,2,2,2]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_tree,codebook = Training.initialize_network(distribution, 5, WordEmbeddings.huffman_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trees.BranchNode with 2 children. data = SoftmaxClassifier.LinearClassifier{2}(5,Float32[0.63829 0.548069; -0.372887 -0.169173; 0.241831 -0.692348; -0.676522 0.988183; 0.598434 0.407975])\n",
      "├─ Trees.BranchNode with 0 children. data = \"A\"\n",
      "└─ Trees.BranchNode with 2 children. data = SoftmaxClassifier.LinearClassifier{2}(5,Float32[0.862341 -0.506161; 0.248016 0.163408; 0.522552 -0.618117; 0.0447176 0.81089; -0.903935 -0.330532])\n",
      "   ├─ Trees.BranchNode with 0 children. data = \"B\"\n",
      "   └─ Trees.BranchNode with 2 children. data = SoftmaxClassifier.LinearClassifier{2}(5,Float32[0.373455 -0.431678; 0.550001 0.548255; 0.701398 0.446863; -0.741222 -0.671272; 0.807268 -0.411665])\n",
      "      ├─ Trees.BranchNode with 0 children. data = \"C\"\n",
      "      └─ Trees.BranchNode with 2 children. data = SoftmaxClassifier.LinearClassifier{2}(5,Float32[0.770272 -0.965899; -0.680085 -0.13024; -0.252352 -0.26909; 0.862566 0.463776; 0.24174 -0.00312933])\n",
      "         ├─ Trees.BranchNode with 0 children. data = \"D\"\n",
      "         └─ Trees.BranchNode with 0 children. data = \"E\"\n"
     ]
    }
   ],
   "source": [
    "print_tree(STDOUT, classifier_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f_A (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_A(x) = collect(predict(classifier_tree.data, x))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: x not defined\nwhile loading In[6], in expression starting on line 200",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: x not defined\nwhile loading In[6], in expression starting on line 200",
      "",
      " in include_string(::String, ::String) at ./loading.jl:380",
      " in eventloop(::ZMQ.Socket) at /home/ubuntu/.julia/v0.5/IJulia/src/IJulia.jl:143",
      " in (::IJulia.##26#32)() at ./task.jl:309"
     ]
    }
   ],
   "source": [
    "@show predict(classifier_tree.data, x)\n",
    "f_A([0.5f0, 0.5f0, 0.5f0, 0.5f0, 0.5f0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = f_A(x) = 0.4190243f0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5-element Array{Float32,1}:\n",
       "  0.0219637\n",
       " -0.0495929\n",
       "  0.227419 \n",
       " -0.405261 \n",
       "  0.046366 "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [0.5f0, 0.5f0, 0.5f0, 0.5f0, 0.5f0]\n",
    "\n",
    "@show y=f_A(x)\n",
    "\n",
    "\n",
    "ForwardDiff.gradient(f_A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.048719 seconds (11.48 k allocations: 527.631 KB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5-element Array{Float32,1}:\n",
       "  0.0524163\n",
       " -0.118353 \n",
       "  0.542735 \n",
       " -0.967153 \n",
       "  0.110652 "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_A(x) = log(collect(predict(classifier_tree.data, x))[1])\n",
    "@time ForwardDiff.gradient(l_A, x; )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float32,1}:\n",
       " -0.0378048\n",
       "  0.0853613\n",
       " -0.391444 \n",
       "  0.697552 \n",
       " -0.0798071"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_B(x) = log(collect(predict(classifier_tree.data, x))[2])\n",
    "ForwardDiff.gradient(l_B, x; )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculate_input_gradient! (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function calculate_input_gradient!{F1<:Number, F2<:Number,K}(\n",
    "                    c::LinearClassifier{K},\n",
    "                    x::AbstractVector{F1},\n",
    "                    y::Int64,\n",
    "                    input_gradient::AbstractVector{F2})\n",
    "    #input_gradient += -ForwardDiff.gradient(log(collect(predict(c, x))[y]), x)\n",
    "    outputs = collect(predict(c, x))\n",
    "    outputs[y] -= 1\n",
    "\n",
    "    for i in 1:K\n",
    "        m = outputs[i]\n",
    "        for j in 1:c.n\n",
    "            input_gradient[j] -= outputs[i] * c.weights[j, i]\n",
    "        end\n",
    "    end\n",
    "    input_gradient\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using WordEmbeddings\n",
    "using Query\n",
    "using JLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word embedding(dimension = 50)of 88262 words\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ee = load(\"../eval/models/plain/tokenised_lowercase_WestburyLab.wikicorp.201004_50__i1.jld\",\"ee\");\n",
    "#ee = restore(\"../eval/models/ss/keep/tokenised_lowercase_WestburyLab.wikicorp.201004_100_i1.model\")\n",
    "#dtree,labels = nn_using AbstractTreestree(ee)\n",
    "#\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(\"Query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_optimisable (generic function with 1 method)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_optimisable(context, embed)\n",
    "    function loss{F}(x::Vector{F})\n",
    "        total_lp = zero(F)\n",
    "        outputs = Vector{F}(2)\n",
    "        input_gradient = similar(x)\n",
    "        for target_word in context\n",
    "            !haskey(embed.codebook, target_word) && continue          \n",
    "            word_lp  = zero(F)\n",
    "            node = embed.classification_tree\n",
    "            for y in embed.codebook[target_word]\n",
    "                c = node.data\n",
    "                outputs[1],outputs[2] = predict(c, x)\n",
    "                lp = log(outputs[y])\n",
    "                @inbounds node = node.children[y]               \n",
    "                word_lp += lp\n",
    "            end\n",
    "            total_lp += word_lp\n",
    "        end\n",
    "        total_lp  \n",
    "    end\n",
    "    \n",
    "    @assert(size(ee.classification_tree.data.weights,2) == 2) #Only dealing with binary trees\n",
    "    function loss_and_grad!{F}(x::Vector{F}, total_input_gradient)\n",
    "        #Wrong\n",
    "        total_lp = zero(F)\n",
    "        outputs = Vector{F}(2)\n",
    "        input_gradient = similar(x)\n",
    "        for target_word in context\n",
    "            !haskey(embed.codebook, target_word) && continue\n",
    "            \n",
    "            word_lp  = zero(F)\n",
    "            fill!(input_gradient,0.0)\n",
    "            \n",
    "            node = embed.classification_tree\n",
    "            for y in embed.codebook[target_word]\n",
    "                c = node.data\n",
    "                outputs[1],outputs[2] = predict(c, x)\n",
    "                lp = log(outputs[y])\n",
    "                \n",
    "                outputs[y] -= 1                \n",
    "                for j in 1:c.n\n",
    "                    input_gradient[1] -= outputs[1] * c.weights[j, 1]/exp(lp)\n",
    "                    input_gradient[2] -= outputs[2] * c.weights[j, 2]/exp(lp)\n",
    "                end\n",
    "                @inbounds node = node.children[y]               \n",
    "                word_lp += lp\n",
    "            end\n",
    "            total_lp += word_lp\n",
    "            total_input_gradient += input_gradient./exp(word_lp)\n",
    "        end\n",
    "        total_lp  \n",
    "    end\n",
    "    \n",
    "    DifferentiableFunction(loss)#, loss_and_grad!, loss_and_grad!)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Optim.DifferentiableFunction(loss,Optim.g!,Optim.fg!)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"ordinary\"\n",
    "context = split(\"The weighted arithmetic mean is similar to an arithmetic mean\")\n",
    "x_outer = ee.embedding[word]\n",
    "diff_fun = get_optimisable(context, ee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-76.35538f0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_fun.f(x_outer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50×2 Array{Float32,2}:\n",
       " -4.18088    -4.18122  \n",
       "  0.820054    0.819721 \n",
       " -3.25231    -3.25221  \n",
       "  2.87019     2.87096  \n",
       "  0.94562     0.945873 \n",
       "  1.31922     1.31999  \n",
       "  0.397625    0.398699 \n",
       " -3.47011    -3.47057  \n",
       "  3.76078     3.76119  \n",
       " -1.7587     -1.75965  \n",
       "  0.994451    0.995194 \n",
       "  1.5533      1.55326  \n",
       " -0.499938   -0.49923  \n",
       " -2.72602    -2.72729  \n",
       "  2.10207     2.10129  \n",
       " -4.49479    -4.49323  \n",
       "  0.0333292   0.0331022\n",
       " -1.73855    -1.73951  \n",
       " -5.91787    -5.91708  \n",
       " -2.52837    -2.52777  \n",
       " -3.35773    -3.35704  \n",
       " -1.768      -1.7687   \n",
       "  4.88234     4.88233  \n",
       " -1.81605    -1.81622  \n",
       "  2.63843     2.63862  \n",
       " -0.51544    -0.515799 \n",
       " -6.02561    -6.02651  \n",
       "  2.30049     2.30092  \n",
       " -1.65871    -1.65848  \n",
       "  3.57165     3.57141  \n",
       " -4.07934    -4.08149  \n",
       "  0.31779     0.317361 \n",
       " -1.93077    -1.93033  \n",
       "  1.0208      1.02157  \n",
       " -3.55693    -3.5564   \n",
       "  0.132542    0.13192  \n",
       " -0.230204   -0.22957  \n",
       " -0.274385   -0.275123 \n",
       "  1.1673      1.1683   \n",
       " -0.577448   -0.576653 \n",
       " -1.3944     -1.39527  \n",
       "  1.73079     1.7316   \n",
       " -3.48174    -3.48296  \n",
       " -4.64516    -4.64512  \n",
       "  3.03218     3.03215  \n",
       " -1.75327    -1.75512  \n",
       " -0.536368   -0.535534 \n",
       " -5.13115    -5.13017  \n",
       "  2.51984     2.52078  \n",
       "  1.03088     1.03141  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = similar(x_outer)\n",
    "diff_fun.g!(x_outer, grad)\n",
    "[grad ForwardDiff.gradient(diff_fun.f, x_outer)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.824189 seconds (5.08 M allocations: 102.147 MB, 10.81% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: L-BFGS\n",
       " * Starting Point: [0.26443565,-0.5347172, ...]\n",
       " * Minimizer: [0.95223916,-0.66962564, ...]\n",
       " * Minimum: -1.839834e+02\n",
       " * Iterations: 2\n",
       " * Convergence: true\n",
       "   * |x - x'| < 1.0e-32: false\n",
       "   * |f(x) - f(x')| / |f(x)| < 1.0e-32: true\n",
       "   * |g(x)| < 1.0e-08: false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Function Calls: 64\n",
       " * Gradient Calls: 64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time optimize(diff_fun, x_outer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using NBInclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0-dev",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
