{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using SoftmaxClassifier\n",
    "using Training\n",
    "using AbstractTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Float64} with 5 entries:\n",
       "  \"B\" => 0.2\n",
       "  \"A\" => 0.4\n",
       "  \"C\" => 0.2\n",
       "  \"D\" => 0.1\n",
       "  \"E\" => 0.1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution = Dict(\"A\"=>0.4, \"B\" => 0.2, \"C\" => 0.2, \"D\"=> 0.1, \"E\"=>0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Trees.BranchNode with 2 children. data = SoftmaxClassifier.LinearClassifier{2}(5,Float32[-0.0169859 -0.861598; -0.801654 0.604611; -0.473071 -0.406443; 0.0102877 0.877782; 0.128401 -0.114319]),Dict(\"B\"=>[2,1],\"A\"=>[1],\"C\"=>[2,2,1],\"D\"=>[2,2,2,1],\"E\"=>[2,2,2,2]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_tree,codebook = Training.initialize_network(distribution, 5, WordEmbeddings.huffman_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_tree(STDOUT, classifier_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_A(x) = collect(predict(classifier_tree.data, x))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: x not defined\nwhile loading In[6], in expression starting on line 200",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: x not defined\nwhile loading In[6], in expression starting on line 200",
      "",
      " in include_string(::String, ::String) at ./loading.jl:380",
      " in eventloop(::ZMQ.Socket) at /home/ubuntu/.julia/v0.5/IJulia/src/IJulia.jl:143",
      " in (::IJulia.##26#32)() at ./task.jl:309"
     ]
    }
   ],
   "source": [
    "@show predict(classifier_tree.data, x)\n",
    "f_A([0.5f0, 0.5f0, 0.5f0, 0.5f0, 0.5f0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = f_A(x) = 0.34829837f0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5-element Array{Float32,1}:\n",
       "  0.191716 \n",
       " -0.319203 \n",
       " -0.0151236\n",
       " -0.19691  \n",
       "  0.0550944"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [0.5f0, 0.5f0, 0.5f0, 0.5f0, 0.5f0]\n",
    "\n",
    "@show y=f_A(x)\n",
    "\n",
    "\n",
    "ForwardDiff.gradient(f_A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.033970 seconds (11.48 k allocations: 527.631 KB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5-element Array{Float32,1}:\n",
       "  0.550435 \n",
       " -0.916465 \n",
       " -0.0434214\n",
       " -0.565348 \n",
       "  0.158182 "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_A(x) = log(collect(predict(classifier_tree.data, x))[1])\n",
    "@time ForwardDiff.gradient(l_A, x; )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float32,1}:\n",
       " -0.294177 \n",
       "  0.4898   \n",
       "  0.0232063\n",
       "  0.302147 \n",
       " -0.0845393"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_B(x) = log(collect(predict(classifier_tree.data, x))[2])\n",
    "ForwardDiff.gradient(l_B, x; )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "calculate_input_gradient! (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function calculate_input_gradient!{F1<:Number, F2<:Number,K}(\n",
    "                    c::LinearClassifier{K},\n",
    "                    x::AbstractVector{F1},\n",
    "                    y::Int64,\n",
    "                    input_gradient::AbstractVector{F2})\n",
    "    #input_gradient += -ForwardDiff.gradient(log(collect(predict(c, x))[y]), x)\n",
    "    outputs = collect(predict(c, x))\n",
    "    outputs[y] -= 1\n",
    "\n",
    "    for i in 1:K\n",
    "        m = outputs[i]\n",
    "        for j in 1:c.n\n",
    "            input_gradient[j] -= outputs[i] * c.weights[j, i]\n",
    "        end\n",
    "    end\n",
    "    input_gradient\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using WordEmbeddings\n",
    "using Query\n",
    "using JLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word embedding(dimension = 50)of 88262 words\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ee = load(\"../eval/models/plain/tokenised_lowercase_WestburyLab.wikicorp.201004_50__i1.jld\",\"ee\");\n",
    "#ee = restore(\"../eval/models/ss/keep/tokenised_lowercase_WestburyLab.wikicorp.201004_100_i1.model\")\n",
    "#dtree,labels = nn_using AbstractTreestree(ee)\n",
    "#\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(\"Query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_simp_optimisable (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_simp_optimisable(target_word, embed)\n",
    "    function loss{F}(x::Vector{F})\n",
    "        total_lp = zero(F)\n",
    "        outputs = Vector{F}(2)\n",
    "        word_lp  = zero(F)\n",
    "        node = embed.classification_tree\n",
    "        for y in embed.codebook[target_word]\n",
    "            c = node.data\n",
    "            outputs[1],outputs[2] = predict(c, x)\n",
    "            lp = log(outputs[y])\n",
    "            @inbounds node = node.children[y]               \n",
    "            word_lp += lp\n",
    "        end\n",
    "        total_lp += word_lp\n",
    "        total_lp  \n",
    "    end\n",
    "    \n",
    "    @assert(size(ee.classification_tree.data.weights,2) == 2) #Only dealing with binary trees\n",
    "    function loss_and_grad!{F}(x::Vector{F}, total_input_gradient)\n",
    "        #Wrong\n",
    "        total_lp = zero(F)\n",
    "        outputs = Vector{F}(2)\n",
    "        input_gradient = similar(x)\n",
    "        \n",
    "        word_lp  = zero(F)\n",
    "        fill!(input_gradient,0.0)\n",
    "\n",
    "        node = embed.classification_tree\n",
    "        for y in embed.codebook[target_word]\n",
    "            @show y\n",
    "            c = node.data\n",
    "            outputs[1],outputs[2] = predict(c, x)\n",
    "            lp = log(outputs[y])\n",
    "\n",
    "            outputs[y] -= 1  \n",
    "            \n",
    "            for j in 1:c.n\n",
    "                input_gradient[1] -= outputs[1] * c.weights[j, 1]#/exp(lp)\n",
    "                input_gradient[2] -= outputs[2] * c.weights[j, 2]#/exp(lp)\n",
    "            end\n",
    "            @inbounds node = node.children[y]               \n",
    "            word_lp += lp\n",
    "        end\n",
    "        total_lp += word_lp\n",
    "        total_input_gradient += input_gradient#./exp(word_lp)\n",
    "\n",
    "        total_lp  \n",
    "    end\n",
    "    \n",
    "    DifferentiableFunction(loss, loss_and_grad!, loss_and_grad!)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: K not defined\nwhile loading In[17], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: K not defined\nwhile loading In[17], in expression starting on line 1",
      "",
      " in anonymous at ./<missing>:?",
      " in include_string(::String, ::String) at ./loading.jl:380",
      " in eventloop(::ZMQ.Socket) at /home/ubuntu/.julia/v0.5/IJulia/src/IJulia.jl:143",
      " in (::IJulia.##26#32)() at ./task.jl:309"
     ]
    }
   ],
   "source": [
    "for i in 1:K\n",
    "    for j in 1:c.n\n",
    "        input_gradient[j] -= outputs[i] * c.weights[j, i]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_optimisable (generic function with 1 method)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_optimisable(context, embed)\n",
    "    function f{F}(x::Vector{F})\n",
    "        total_lp = zero(F)\n",
    "        outputs = Vector{F}(2)\n",
    "        input_gradient = similar(x)\n",
    "        for target_word in context\n",
    "            !haskey(embed.codebook, target_word) && continue          \n",
    "            word_lp  = zero(F)\n",
    "            node = embed.classification_tree\n",
    "            for y in embed.codebook[target_word]\n",
    "                c = node.data\n",
    "                outputs[1],outputs[2] = predict(c, x)\n",
    "                lp = log(outputs[y])\n",
    "                @inbounds node = node.children[y]               \n",
    "                word_lp += lp\n",
    "            end\n",
    "            total_lp += word_lp\n",
    "        end\n",
    "        -total_lp  \n",
    "    end\n",
    "    \n",
    "    @assert(size(ee.classification_tree.data.weights,2) == 2) #Only dealing with binary trees\n",
    "    function fg!{F}(x::Vector{F}, input_gradient)\n",
    "        fill!(input_gradient, zero(F))\n",
    "        total_lp = zero(F)\n",
    "        outputs = Vector{F}(2)\n",
    "        for target_word in context\n",
    "            !haskey(embed.codebook, target_word) && continue\n",
    "            \n",
    "            word_lp  = zero(F)\n",
    "            \n",
    "            node = embed.classification_tree\n",
    "            for y in embed.codebook[target_word]\n",
    "                c = node.data\n",
    "                outputs[1],outputs[2] = predict(c, x)\n",
    "                lp = log(outputs[y])\n",
    "                \n",
    "                outputs[y] -= 1                \n",
    "                for j in 1:c.n\n",
    "                    input_gradient[j] += outputs[1] * c.weights[j, 1]\n",
    "                    input_gradient[j] += outputs[2] * c.weights[j, 2]\n",
    "                end\n",
    "                @inbounds node = node.children[y]               \n",
    "                word_lp += lp\n",
    "            end\n",
    "            total_lp += word_lp\n",
    "        end\n",
    "        -total_lp  \n",
    "    end\n",
    "    \n",
    "    DifferentiableFunction(f, fg!, fg!)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff_fun.f(x_outer) = 82.16156f0\n"
     ]
    }
   ],
   "source": [
    "word = \"ordinary\"\n",
    "context = split(\"the weighted arithmetic mean is similar to an arithmetic mean\")\n",
    "x_outer = ee.embedding[word]\n",
    "diff_fun = get_optimisable(context, ee)\n",
    "\n",
    "@show diff_fun.f(x_outer)\n",
    "\n",
    "grad = similar(x_outer)\n",
    "diff_fun.g!(x_outer, grad)\n",
    "grad_error =abs(grad .- ForwardDiff.gradient(diff_fun.f, x_outer)) \n",
    "@assert(all(grad_error .< 1e-5), grad_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.092581 seconds (9.94 M allocations: 173.018 MB, 12.88% gc time)\n",
      "\n",
      "Prob = 1.5999949229541361e-9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: L-BFGS\n",
       " * Starting Point: [0.26443565,-0.5347172, ...]\n",
       " * Minimizer: [-463.1336,-316.9553, ...]\n",
       " * Minimum: 2.025327e+01\n",
       " * Iterations: 42\n",
       " * Convergence: true\n",
       "   * |x - x'| < 1.0e-32: false\n",
       "   * |f(x) - f(x')| / |f(x)| < 1.0e-32: true\n",
       "   * |g(x)| < 1.0e-08: false\n",
       "   * Reached Maximum Number of Iterations: false\n",
       " * Objective Function Calls: 150\n",
       " * Gradient Calls: 150"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time res = optimize(diff_fun, x_outer)\n",
    "println(\"\\nProb = \", exp(-1*res.f_minimum))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using NBInclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbinclude(\"../eval/SimTests.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "find_optimal_wordvector (generic function with 2 methods)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function find_optimal_wordvector(ee::GenWordEmbedding, context, word)\n",
    "    diff_fun = get_optimisable(context, ee)\n",
    "    x = ee.embedding[word]\n",
    "    res_status = optimize(diff_fun, x)\n",
    "    x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordpairs,contexts,groundsim = load_SCWS(\"../eval/data/corpora/SCWS/ratings.txt\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair 1 / 2003\n",
      "pair 2 / 2003\n",
      "pair 3 / 2003\n",
      "2016-08-02T14:37:54.73 - warn: KeyError: key \"aglow\" not found prefix: \"error: \"\n",
      "pair 4 / 2003\n",
      "pair 5 / 2003\n",
      "pair 6 / 2003\n",
      "pair 7 / 2003\n",
      "pair 8 / 2003\n",
      "pair 9 / 2003\n",
      "pair 10 / 2003\n",
      "pair 11 / 2003\n",
      "pair 12 / 2003\n"
     ]
    }
   ],
   "source": [
    "local_sims = get_sims(ee, wordpairs, contexts, find_optimal_wordvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5086699404515551"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corspearman(groundsim,local_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0-dev",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
