{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition anynull(AbstractArray) in module NullableArrays at /home/ubuntu/.julia/v0.5/NullableArrays/src/primitives.jl:175 overwritten in module PooledElements at /home/ubuntu/.julia/v0.5/PooledElements/src/pooledstringarray.jl:243.\n",
      "WARNING: Method definition warn(Exception) in module Base at util.jl:373 overwritten in module Lumberjack at /home/ubuntu/.julia/v0.5/Lumberjack/src/lumbermill.jl:99.\n",
      "WARNING: Method definition #warn(Array, Base.#warn, Exception) in module Base overwritten in module Lumberjack.\n",
      "WARNING: Method definition error(AbstractString) in module Base at error.jl:21 overwritten in module Lumberjack at /home/ubuntu/.julia/v0.5/Lumberjack/src/lumbermill.jl:138.\n",
      "WARNING: Method definition error(Any...) in module Base at error.jl:22 overwritten in module Lumberjack at /home/ubuntu/.julia/v0.5/Lumberjack/src/lumbermill.jl:114.\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: LoadError: LoadError: syntax: extra token \"prob\" after end of expression\nwhile loading /mnt_volume/phd/prototypes/SenseSplittingWord2Vec/src/Query.jl, in expression starting on line 1\nwhile loading /mnt_volume/phd/prototypes/SenseSplittingWord2Vec/src/Training.jl, in expression starting on line 10\nwhile loading In[13], in expression starting on line 2",
     "output_type": "error",
     "traceback": [
      "LoadError: LoadError: LoadError: syntax: extra token \"prob\" after end of expression\nwhile loading /mnt_volume/phd/prototypes/SenseSplittingWord2Vec/src/Query.jl, in expression starting on line 1\nwhile loading /mnt_volume/phd/prototypes/SenseSplittingWord2Vec/src/Training.jl, in expression starting on line 10\nwhile loading In[13], in expression starting on line 2",
      ""
     ]
    }
   ],
   "source": [
    "using WordEmbeddings\n",
    "using Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"models/text8_miniscule.model\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_dir = joinpath(\"data\") #For local run from testing directory\n",
    "\n",
    "test_filename = \"text8_miniscule\"\n",
    "test_file = joinpath(data_dir, test_filename)\n",
    "\n",
    "\n",
    "model_file = \"models/text8_miniscule.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_sense_embedding (generic function with 1 method)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function test_sense_embedding(inputfile)\n",
    "\n",
    "\tembed = WordSenseEmbedding(30, random_inited, huffman_tree, subsampling = 0, iter=2, strength=0.4, force_minibatch_size=100)\n",
    "\t@time train(embed, inputfile)\n",
    "\n",
    "    save(embed, model_file)\n",
    "\tembed\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Profile.clear()  # in case we have any previous profiling data\n",
    "@profile test_sense_embedding(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using ProfileView\n",
    "\n",
    "ProfileView.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using SoftmaxClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition test(Any) in module Main at In[17]:2 overwritten at In[58]:2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "test (generic function with 2 methods)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function test(fn)\n",
    "    embed = restore(model_file)\n",
    "    words = split(open(readstring,test_file,\"r\"))\n",
    "    time_total = 0.0\n",
    "    for ii in 10:length(words)-1000\n",
    "        word = words[ii]\n",
    "        !haskey(embed.codebook,word) && continue \n",
    "        context = [words[ii-5:ii-1]; words[ii+1:ii+5]]\n",
    "        context = filter(w->haskey(embed.codebook,w),context)\n",
    "        tic()\n",
    "        fn(embed,word, context)\n",
    "        time_total+=toq()\n",
    "    end\n",
    "    time_total\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition WSD2(WordEmbeddings.WordSenseEmbedding, AbstractString, AbstractArray{#S<:AbstractString, 1}) in module Main at In[18]:2 overwritten at In[59]:2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WSD2 (generic function with 1 method)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function WSD2{S<:AbstractString}(embed::WordEmbeddings.WordSenseEmbedding, word::AbstractString, context::AbstractVector{S})\n",
    "    function prob_of_context{S<:AbstractString}(embed::GenWordEmbedding, context::AbstractVector{S}, input::Vector{Float32})\n",
    "        total_logprob=0.0 #Work in logprob to avoid underflow, and get more stability\n",
    "        for target_word in context\n",
    "            # discard words not presenting in the classification tree\n",
    "            haskey(embed.codebook, target_word) || continue\n",
    "            node = embed.classification_tree      \n",
    "\n",
    "            word_logprob = 0.0\n",
    "            for code in embed.codebook[target_word]  \n",
    "                word_logprob+=log(predict(node.data, input)[code])\n",
    "                node = node.children[code]\n",
    "            end\n",
    "            total_logprob+=word_logprob\n",
    "        end\n",
    "        exp(total_logprob) #Going back out of the log domain is not required for external logic, but it is nice for clarity\n",
    "    end\n",
    "    \n",
    "    \n",
    "    sense_embeddings = embed.embedding[word]\n",
    "    prob, most_likely_sense_id = findmax([prob_of_context(embed, context, input) for input in sense_embeddings])\n",
    "    return most_likely_sense_id\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.448796 seconds (14.17 M allocations: 758.525 MB, 5.53% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.0366613659999997"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc()\n",
    "@time test(WSD2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition WSD3(WordEmbeddings.WordSenseEmbedding, AbstractString, AbstractArray{#S<:AbstractString, 1}) in module Main at In[19]:2 overwritten at In[61]:2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WSD3 (generic function with 1 method)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@fastmath function WSD3{S<:AbstractString}(embed::WordEmbeddings.WordSenseEmbedding, word::AbstractString, context::AbstractVector{S})\n",
    "    function prob_of_context{S<:AbstractString}(embed::GenWordEmbedding, context::AbstractVector{S}, input::Vector{Float32})\n",
    "        total_prob=0.0 #Work in logprob to avoid underflow, and get more stability\n",
    "        @inbounds for target_word in context\n",
    "            # discard words not presenting in the classification tree\n",
    "            haskey(embed.codebook, target_word) || continue\n",
    "            node = embed.classification_tree      \n",
    "\n",
    "            word_prob = 0.0\n",
    "            @inbounds for code in embed.codebook[target_word]  \n",
    "                word_prob*=predict(node.data, input)[code]\n",
    "                node = node.children[code]\n",
    "            end\n",
    "            total_prob*=word_prob\n",
    "        end\n",
    "        exp(total_prob) #Going back out of the log domain is not required for external logic, but it is nice for clarity\n",
    "    end\n",
    "    \n",
    "    \n",
    "    sense_embeddings = embed.embedding[word]\n",
    "    prob, most_likely_sense_id = findmax([prob_of_context(embed, context, input) for input in sense_embeddings])\n",
    "    return most_likely_sense_id\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.237457 seconds (14.16 M allocations: 758.139 MB, 6.06% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.8264243710000008"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc()\n",
    "@time test(WSD3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using StatsFuns\n",
    "import SoftmaxClassifier.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition softmax5!(AbstractArray{#R<:AbstractFloat, N<:Any}) in module Main at In[22]:2 overwritten at In[64]:2.\n",
      "WARNING: Method definition predict5(SoftmaxClassifier.LinearClassifier, AbstractArray{#F<:AbstractFloat, 1}) in module Main at In[22]:11 overwritten at In[64]:11.\n",
      "WARNING: Method definition WSD5(WordEmbeddings.WordSenseEmbedding, AbstractString, AbstractArray{#S<:AbstractString, 1}) in module Main at In[22]:23 overwritten at In[64]:23.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WSD5 (generic function with 1 method)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@inbounds @inline function softmax5!{R<:AbstractFloat}(t::AbstractArray{R})\n",
    "    u = max(t[1],t[2])\n",
    "    @inbounds t[1] = exp(t[1] - u)\n",
    "    @inbounds t[2] = exp(t[2] - u)\n",
    "    @inbounds u = t[1]+t[2]\n",
    "    @inbounds t[1]/=u\n",
    "    @inbounds t[2]/=u\n",
    "    t\n",
    "end\n",
    "@fastmath function predict5{F<:AbstractFloat}(c::LinearClassifier, x::AbstractVector{F})\n",
    "    t= F[0., 0.]\n",
    "    @inbounds for ii in 1: size(c.weights,1)\n",
    "        @inbounds t[1]+=c.weights[ii,1]*x[ii]\n",
    "        @inbounds t[2]+=c.weights[ii,2]*x[ii]\n",
    "    end\n",
    "    t\n",
    "    return softmax5!(t)\n",
    "end\n",
    "\n",
    "@fastmath function WSD5{S<:AbstractString}(embed::WordEmbeddings.WordSenseEmbedding, word::AbstractString, context::AbstractVector{S})\n",
    "\n",
    "    \n",
    "    function prob_of_context{S<:AbstractString}(embed::GenWordEmbedding, context::AbstractVector{S}, input::Vector{Float32})\n",
    "        total_prob=0.0 #Work in logprob to avoid underflow, and get more stability\n",
    "        @inbounds for target_word in context\n",
    "            # discard words not presenting in the classification tree\n",
    "            haskey(embed.codebook, target_word) || continue\n",
    "            node = embed.classification_tree      \n",
    "\n",
    "            word_prob = 0.0\n",
    "            @inbounds for code in embed.codebook[target_word]  \n",
    "                word_prob*=predict5(node.data, input)[code]\n",
    "                node = node.children[code]\n",
    "            end\n",
    "            total_prob*=word_prob\n",
    "        end\n",
    "        total_prob #Going back out of the log domain is not required for external logic, but it is nice for clarity\n",
    "    end\n",
    "        \n",
    "    sense_embeddings = embed.embedding[word]\n",
    "    prob, most_likely_sense_id = findmax([prob_of_context(embed, context, input) for input in sense_embeddings])\n",
    "    return most_likely_sense_id\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8.032951 seconds (3.72 M allocations: 333.225 MB, 0.92% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.645712660000006"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc()\n",
    "@time test(WSD5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: MethodError: Cannot `convert` an object of type Tuple{Int64,Int64,Int64} to an object of type Array{Int64,1}\nThis may have arisen from a call to the constructor Array{Int64,1}(...),\nsince type constructors fall back to convert methods.\nClosest candidates are:\n  convert{T}(::Type{Array{T,1}}, !Matched::Range{T})\n  convert{T,n}(::Type{Array{T,n}}, !Matched::Array{T,n})\n  convert{T,S,N}(::Type{Array{T,N}}, !Matched::Base.ReshapedArray{S,N,P<:AbstractArray{T,N},MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}}})\n  ...\nwhile loading In[102], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "LoadError: MethodError: Cannot `convert` an object of type Tuple{Int64,Int64,Int64} to an object of type Array{Int64,1}\nThis may have arisen from a call to the constructor Array{Int64,1}(...),\nsince type constructors fall back to convert methods.\nClosest candidates are:\n  convert{T}(::Type{Array{T,1}}, !Matched::Range{T})\n  convert{T,n}(::Type{Array{T,n}}, !Matched::Array{T,n})\n  convert{T,S,N}(::Type{Array{T,N}}, !Matched::Base.ReshapedArray{S,N,P<:AbstractArray{T,N},MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}}})\n  ...\nwhile loading In[102], in expression starting on line 1",
      ""
     ]
    }
   ],
   "source": [
    "convert(Vector{Int64},(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition softmax6!(#R<:AbstractFloat, #R<:AbstractFloat) in module Main at In[93]:2 overwritten at In[96]:2.\n",
      "WARNING: Method definition predict6(SoftmaxClassifier.LinearClassifier, AbstractArray{#F<:AbstractFloat, 1}) in module Main at In[93]:11 overwritten at In[96]:11.\n",
      "WARNING: Method definition WSD6(WordEmbeddings.WordSenseEmbedding, AbstractString, AbstractArray{#S<:AbstractString, 1}) in module Main at In[93]:24 overwritten at In[96]:24.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WSD6 (generic function with 1 method)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@fastmath @inline function softmax2{R<:AbstractFloat}(t1::R,t2::R)\n",
    "    u = max(t1,t2)\n",
    "    t1 = exp(t1 - u)\n",
    "    t2 = exp(t2 - u)\n",
    "    s = t1+t2\n",
    "    t1/=s\n",
    "    t2/=s\n",
    "    t1,t2\n",
    "end\n",
    "@fastmath function predict{F<:AbstractFloat}(c::LinearClassifier{2}, x::AbstractVector{F})\n",
    "    t1=zero(F)\n",
    "    t2=zero(F)\n",
    "    @inbounds for ii in 1: size(c.weights,1)\n",
    "        @inbounds t1+=c.weights[ii,1]*x[ii]\n",
    "        @inbounds t2+=c.weights[ii,2]*x[ii]\n",
    "    end\n",
    "    #return softmax!([t1,t2])\n",
    "    return softmax2(t1,t2)\n",
    "end\n",
    "\n",
    "@fastmath function WSD6{S<:AbstractString}(embed::WordEmbeddings.WordSenseEmbedding, word::AbstractString, context::AbstractVector{S})\n",
    "\n",
    "    \n",
    "    function prob_of_context{S<:AbstractString}(embed::GenWordEmbedding, context::AbstractVector{S}, input::Vector{Float32})\n",
    "        total_prob=0.0 #Work in logprob to avoid underflow, and get more stability\n",
    "        @inbounds for target_word in context\n",
    "            # discard words not presenting in the classification tree\n",
    "            haskey(embed.codebook, target_word) || continue\n",
    "            node = embed.classification_tree      \n",
    "\n",
    "            word_prob = 0.0\n",
    "            @inbounds for code in embed.codebook[target_word]  \n",
    "                word_prob*=predict6(node.data, input)[code]\n",
    "                node = node.children[code]\n",
    "            end\n",
    "            total_prob*=word_prob\n",
    "        end\n",
    "        total_prob #Going back out of the log domain is not required for external logic, but it is nice for clarity\n",
    "    end\n",
    "        \n",
    "    sense_embeddings = embed.embedding[word]\n",
    "    prob, most_likely_sense_id = findmax([prob_of_context(embed, context, input) for input in sense_embeddings])\n",
    "    return most_likely_sense_id\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.726085 seconds (3.62 M allocations: 62.971 MB, 1.19% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.440744877999999"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc()\n",
    "@time test(WSD6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition softmax7!(NTuple{#K<:Any, #R<:AbstractFloat}) in module Main at In[68]:2 overwritten at In[90]:2.\n",
      "WARNING: Method definition predict7(SoftmaxClassifier.LinearClassifier{#K<:Any}, AbstractArray{#F<:AbstractFloat, 1}) in module Main at In[68]:12 overwritten at In[90]:12.\n",
      "WARNING: Method definition prob_of_context(WordEmbeddings.GenWordEmbedding, AbstractArray{#S<:AbstractString, 1}, Array{Float32, 1}) in module Main at In[68]:23 overwritten at In[90]:23.\n",
      "WARNING: Method definition WSD7(WordEmbeddings.WordSenseEmbedding, AbstractString, AbstractArray{#S<:AbstractString, 1}) in module Main at In[68]:39 overwritten at In[90]:39.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WSD7 (generic function with 1 method)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@generated function softmax7!{K,R<:AbstractFloat}(ts::NTuple{K,R})\n",
    "    quote\n",
    "        u = maximum(ts)\n",
    "        @nexprs $K j->t_j = exp(ts[j]-u)\n",
    "        s=0.0\n",
    "        @nexprs $K j->s+=t_j\n",
    "        @nexprs $K j->t_j/=s\n",
    "        @ntuple $K t\n",
    "    end\n",
    "end\n",
    "@generated function predict7{K,F<:AbstractFloat}(c::LinearClassifier{K}, x::AbstractVector{F})\n",
    "    quote\n",
    "        @inbounds begin \n",
    "            @nexprs $K j->t_j = 0.0f0\n",
    "            @simd for ii in 1: size(c.weights,1)\n",
    "                @nexprs $K j->t_j = c.weights[ii,j]*x[j]\n",
    "            end\n",
    "            return softmax!(@ntuple $K t)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "@inbounds function prob_of_context{S<:AbstractString}(embed::GenWordEmbedding, context::AbstractVector{S}, input::Vector{Float32})\n",
    "    total_prob=0.0 #Work in logprob to avoid underflow, and get more stability\n",
    "    for target_word in context\n",
    "        # discard words not presenting in the classification tree\n",
    "        haskey(embed.codebook, target_word) || continue\n",
    "        node = embed.classification_tree      \n",
    "        \n",
    "        \n",
    "        for code in embed.codebook[target_word]  \n",
    "            total_prob*=predict7(node.data, input)[code]\n",
    "            node = node.children[code]\n",
    "        end\n",
    "    end\n",
    "    total_prob #Going back out of the log domain is not required for external logic, but it is nice for clarity\n",
    "end\n",
    "\n",
    "@fastmath function WSD7{S<:AbstractString}(embed::WordEmbeddings.WordSenseEmbedding, word::AbstractString, context::AbstractVector{S})       \n",
    "    sense_embeddings = embed.embedding[word]\n",
    "    prob, most_likely_sense_id = findmax([prob_of_context(embed, context, input) for input in sense_embeddings])\n",
    "    return most_likely_sense_id\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.272414 seconds (10.59 M allocations: 169.394 MB, 1.49% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9885797159999994"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc()\n",
    "@time test(WSD7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WSD8 (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@generated function softmax8!{K,R<:AbstractFloat}(ts::AbstractVector{R}, ::Type{Val{K}})\n",
    "    quote\n",
    "        u = maximum(ts)\n",
    "        @nexprs $K j->t_j = exp(ts[j]-u)\n",
    "        s=0.0\n",
    "        @nexprs $K j->s+=t_j\n",
    "        @nexprs $K j->+t_j\n",
    "        @nexprs $K j->t_j/=s\n",
    "        @ntuple $K t\n",
    "    end\n",
    "end\n",
    "\n",
    "@fastmath function predict8{F<:AbstractFloat}(c::LinearClassifier, x::AbstractVector{F})\n",
    "    t1=zero(F)\n",
    "    t2=zero(F)\n",
    "    x_len = length(x)\n",
    "    @simd for ii in 1:x_len\n",
    "        @inbounds t1+=c.weights[ii,1]*x[ii]\n",
    "        @inbounds t2+=c.weights[ii,2]*x[ii]\n",
    "    end\n",
    "    return softmax8!(t1,t2)\n",
    "end\n",
    "@inbounds function prob_of_context8{S<:AbstractString}(embed::GenWordEmbedding, context::AbstractVector{S}, input::Vector{Float32})\n",
    "    total_prob=0.0 #Work in logprob to avoid underflow, and get more stability\n",
    "    for target_word in context\n",
    "        # discard words not presenting in the classification tree\n",
    "        haskey(embed.codebook, target_word) || continue\n",
    "        node = embed.classification_tree      \n",
    "        \n",
    "        \n",
    "        for code in embed.codebook[target_word]  \n",
    "            total_prob*=predict8(node.data, input)[code]\n",
    "            node = node.children[code]\n",
    "        end\n",
    "    end\n",
    "    total_prob #Going back out of the log domain is not required for external logic, but it is nice for clarity\n",
    "end\n",
    "\n",
    "@fastmath function WSD8{S<:AbstractString}(embed::WordEmbeddings.WordSenseEmbedding, word::AbstractString, context::AbstractVector{S})       \n",
    "    sense_embeddings = embed.embedding[word]\n",
    "    prob, most_likely_sense_id = findmax([prob_of_context8(embed, context, input) for input in sense_embeddings])\n",
    "    return most_likely_sense_id\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: MethodError: no method matching softmax8!(::Float32, ::Float32)\nwhile loading In[29], in expression starting on line 155",
     "output_type": "error",
     "traceback": [
      "LoadError: MethodError: no method matching softmax8!(::Float32, ::Float32)\nwhile loading In[29], in expression starting on line 155",
      ""
     ]
    }
   ],
   "source": [
    "gc()\n",
    "@time test(WSD8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "2 methods for generic function <b>softmax!</b>:<ul><li> softmax!<i>{R<:AbstractFloat,T<:Real}</i>(r::<b>AbstractArray{R,N<:Any}</b>, x::<b>AbstractArray{T,N<:Any}</b>) at <a href=\"https://github.com/JuliaStats/StatsFuns.jl/tree/723edaa163c95fc78c8d0b520150a91139acf434/src/basicfuns.jl#L147\" target=\"_blank\">/home/ubuntu/.julia/v0.5/StatsFuns/src/basicfuns.jl:147</a></li> <li> softmax!<i>{T<:AbstractFloat}</i>(x::<b>AbstractArray{T,N<:Any}</b>) at <a href=\"https://github.com/JuliaStats/StatsFuns.jl/tree/723edaa163c95fc78c8d0b520150a91139acf434/src/basicfuns.jl#L161\" target=\"_blank\">/home/ubuntu/.julia/v0.5/StatsFuns/src/basicfuns.jl:161</a></li> </ul>"
      ],
      "text/plain": [
       "# 2 methods for generic function \"softmax!\":\n",
       "softmax!{R<:AbstractFloat,T<:Real}(r::AbstractArray{R,N<:Any}, x::AbstractArray{T,N<:Any}) at /home/ubuntu/.julia/v0.5/StatsFuns/src/basicfuns.jl:147\n",
       "softmax!{T<:AbstractFloat}(x::AbstractArray{T,N<:Any}) at /home/ubuntu/.julia/v0.5/StatsFuns/src/basicfuns.jl:161"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods(StatsFuns.softmax!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using ProfileView\n",
    "Profile.clear()\n",
    "@profile test(WSD7)\n",
    "ProfileView.view()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using ProfileView\n",
    "Profile.clear()\n",
    "@profile test(WSD6)\n",
    "ProfileView.view()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using ProfileView\n",
    "Profile.clear()\n",
    "@profile test(WSD5)\n",
    "ProfileView.view()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using ProfileView\n",
    "Profile.clear()\n",
    "@profile test(WSD3)\n",
    "ProfileView.view()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Base.Cartesian\n",
    "using StatsFuns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax2! (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@fastmath function softmax2!{R<:AbstractFloat}(t1::R,t2::R)\n",
    "    u = max(t1,t2)\n",
    "    t1 = exp(t1 - u)\n",
    "    t2 = exp(t2 - u)\n",
    "    s = t1+t2\n",
    "    t1/=s\n",
    "    t2/=s\n",
    "    t1,t2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition softmaxK!(NTuple{#K<:Any, #R<:AbstractFloat}) in module Main at In[3]:2 overwritten at In[36]:2.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "softmaxK! (generic function with 1 method)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@generated function softmaxK!{K,R<:AbstractFloat}(ts::NTuple{K,R})\n",
    "    quote\n",
    "        u = maximum(ts)\n",
    "        @nexprs $K j->t_j = exp(ts[j]-u)\n",
    "        s=0.0\n",
    "        @nexprs $K j->s+=t_j\n",
    "        @nexprs $K j->t_j/=s\n",
    "        @ntuple $K t\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: syntax: missing comma or ) in argument list\nwhile loading In[44], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "LoadError: syntax: missing comma or ) in argument list\nwhile loading In[44], in expression starting on line 1",
      ""
     ]
    }
   ],
   "source": [
    "@generated function softmaxKf!{K,R<:AbstractFloat}(ts::NTuple{K,R})\n",
    "    quote\n",
    "        u = maximum(ts)\n",
    "        @nexprs( $K j-> t_j = @fastmath(exp(ts[j]-u))\n",
    "        s=0.0\n",
    "        @nexprs( $K j->( s+=t_j)\n",
    "        @nexprs( $K j->( t_j/=s)\n",
    "        @ntuple $K t\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmaxKAB! (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@generated function softmaxKAB!{K,R<:AbstractFloat}(ts::AbstractVector{R}, ::Type{Val{K}})\n",
    "    quote\n",
    "        u = maximum(ts)\n",
    "        @nexprs $K j->t_j = exp(ts[j]-u)\n",
    "        s=0.0f0\n",
    "        @nexprs $K j->s+=t_j\n",
    "        @nexprs $K j->t_j/=s\n",
    "        @ntuple $K t\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.007916 seconds (2.89 k allocations: 180.205 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition test() in module Main at In[7]:2 overwritten at In[8]:2.\n"
     ]
    }
   ],
   "source": [
    "function test()\n",
    "    for ii in 1:1000\n",
    "        StatsFuns.softmax!([0.25,0.5])\n",
    "    end\n",
    "end\n",
    "gc()\n",
    "@time test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.007242 seconds (1.68 k allocations: 76.063 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition test() in module Main at In[8]:2 overwritten at In[9]:2.\n"
     ]
    }
   ],
   "source": [
    "function test()\n",
    "    for ii in 1:1000\n",
    "        softmax2!(0.25,0.5)\n",
    "    end\n",
    "end\n",
    "gc()\n",
    "@time test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.006289 seconds (1.75 k allocations: 80.685 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition test() in module Main at In[37]:2 overwritten at In[38]:2.\n"
     ]
    }
   ],
   "source": [
    "function test()\n",
    "    for ii in 1:1000\n",
    "        softmaxK!((0.25,0.5))\n",
    "    end\n",
    "end\n",
    "gc()\n",
    "@time test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.008434 seconds (4.99 k allocations: 217.851 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition test() in module Main at In[11]:2 overwritten at In[12]:2.\n"
     ]
    }
   ],
   "source": [
    "function test()\n",
    "    for ii in 1:1000\n",
    "        softmaxKAB!([0.25,0.5],Val{2})\n",
    "    end\n",
    "end\n",
    "gc()\n",
    "@time test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Float32(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a[:]=(4,5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median([1,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0-dev",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
