{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "importfrom (generic function with 3 methods)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function importfrom(moduleinstance::Module, functionname::Symbol, argtypes::Tuple)\n",
    "    meths = methods(moduleinstance.(functionname), argtypes)\n",
    "    importfrom(moduleinstance, functionname, meths)\n",
    "end \n",
    "\n",
    "function importfrom(moduleinstance::Module, functionname::Symbol)\n",
    "    meths = methods(moduleinstance.(functionname))\n",
    "    importfrom(moduleinstance, functionname, meths)\n",
    "end \n",
    "\n",
    "function importfrom(moduleinstance::Module, functionname::Symbol, meths::Base.MethodList)\n",
    "    for mt in meths\n",
    "        paramnames = collect(mt.lambda_template.slotnames[2:end])\n",
    "        paramtypes = collect(mt.sig.parameters[2:end])\n",
    "        paramsig = ((n,t)->Expr(:(::),n,t)).(paramnames, paramtypes)\n",
    "\n",
    "        funcdec = Expr(:(=), \n",
    "                        Expr(:call, functionname, paramsig...),\n",
    "                        Expr(:call, :($moduleinstance.$functionname), paramnames...)\n",
    "        )\n",
    "        eval(funcdec) #Runs at global scope\n",
    "        \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using ProgressMeter\n",
    "using WordNet\n",
    "using AdaGram\n",
    "using AdaGramCompat\n",
    "using SwiftObjectStores\n",
    "using CorpusLoaders\n",
    "using CorpusLoaders.Semcor\n",
    "using WordEmbeddings, SoftmaxClassifier\n",
    "using Utils\n",
    "using Query\n",
    "using Distances\n",
    "using JLD\n",
    "importfrom(CorpusLoaders.Semcor, :sensekey)\n",
    "importfrom(WordNet, :sensekey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all_word_sense_vectors (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_word_sense_vectors = AdaGramCompat.all_word_sense_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordNet.DB"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const WN_PATH=\"data/corpora/WordNet-2.1/\"\n",
    "#WN_PATH3 = \"/usr/share/nltk_data/corpora/wordnet/\"\n",
    "db = DB(WN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#am = AdaGramCompat.AdaGramModel(load_model(\"models/adagram/v1_d100.adagram_model\")...)\n",
    "#am = AdaGramModel(load_model(\"models/adagram/more_senses.adagram_model\")...)\n",
    "#am = open(deserialize,\"models/adagram/more_senses.adagram_model.jsz\", \"r\");\n",
    "am = load(\"models/adagram/more_senses.adagram_model.jld\", \"am\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_am = get_jld(SwiftService(), \"sensemodels/adagram/\", \"semhuff_more_senses.adagram_model.jld\", \"am\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ee = restore(\"models/ss/tokenised_lowercase_WestburyLab.wikicorp.201004_100_i1.jld\");\n",
    "ee = load(\"models/ss/tokenised_lowercase_WestburyLab.wikicorp.201004_100_i1.jld\", \"ee\")\n",
    "\n",
    "#ee = restore(\"models/ss/keep/tokenised_lowercase_WestburyLab.wikicorp.201004_50__m170000000.model\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using CorpusLoaders.Semeval2007t7\n",
    "challenges = load_challenges_semeval2007t7(\"data/corpora/wsd/semeval2007_t7/test/eng-coarse-all-words.xml\");\n",
    "solutions = load_solutions_semeval2007t7(\"data/corpora/wsd/semeval2007_t7/key/dataset21.test.key\");\n",
    "semcor = index_semcor(lazyload_semcor(\"data/corpora/semcor2.1/brown1/tagfiles/\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element SubArray{String,1,Array{String,1},Tuple{Array{Int64,1}},false}:\n",
       " \"fire\"  \n",
       " \"can\"   \n",
       " \"be\"    \n",
       " \"built;\"\n",
       " \"\\\"the\" \n",
       " \"was\"   \n",
       " \"so\"    \n",
       " \"large\" \n",
       " \"you\"   \n",
       " \"could\" "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function window(context, index::Int, window_size::Int=10)\n",
    "    window_lower_bound = max(index - window_size÷2, 1)\n",
    "    window_upper_bound = min(index + window_size÷2, length(context))\n",
    "    view(context, [window_lower_bound:index-1 ; index+1:window_upper_bound])\n",
    "end\n",
    "\n",
    "function window(tagged_sense::TaggedSentence, index::Int, window_size::Int=10)\n",
    "    context = lowercase.(strip_tags(tagged_sense))\n",
    "    window(context, index, window_size)\n",
    "end\n",
    "   \n",
    "function window(context, word::AbstractString, window_size::Int=10)\n",
    "    context = lowercase.(context)\n",
    "    occurances = find(context.==word)\n",
    "    index =  length(occurances) > 0 ? occurances[ceil(Int, end/2)] : 0\n",
    "    window(context, index, window_size)\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "window((drop(semcor,400) |> first |> last |> first)...)\n",
    "\n",
    "window((synsets(db, db[\"fireplace\", 'n']) |> first).gloss |> split, \"fireplace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_all_usages (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"Collect up the usages from a indexed tagged source\"\n",
    "function get_usages(usage_index::SemcorIndex, key)\n",
    "    if haskey(usage_index, key)\n",
    "        [window(lowercase.(strip_tags(sent)), index) for (sent, index) in usage_index[key]]\n",
    "    else\n",
    "        Vector{String}[]\n",
    "    end\n",
    "end\n",
    "\n",
    "function get_usages(synset::Synset, lemma_word::AbstractString)\n",
    "    gloss::Vector{SubString{String}} = lowercase.(punctuation_space_tokenize(synset.gloss))\n",
    "    [window(gloss, lemma_word)]\n",
    "end\n",
    "\n",
    "function get_all_usages(wn::DB, semcor::SemcorIndex, lemma_word, pos)   \n",
    "    lemma = db[pos, lemma_word]\n",
    "    target_synsets::Vector{Synset} = synsets(db, lemma)\n",
    "    \n",
    "    Dict{Synset,AbstractVector{AbstractVector}}((synset => #get_usages(synset)\n",
    "        [get_usages(semcor, sensekey(db, synset, lemma)); get_usages(synset, lemma_word)]\n",
    "                    for synset in target_synsets))  \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "general_wsd (generic function with 2 methods)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_probs(logprobs::Vector) = normal_probs(copy(logprobs))\n",
    "function normal_probs!{F<:AbstractFloat}(logprobs::Vector{F})\n",
    "    ret = logprobs\n",
    "    max_lp = maximum(logprobs)\n",
    "    ret.-=max_lp #Bring closer to zero\n",
    "    map!(exp,ret)\n",
    "    denom = sum(ret)\n",
    "    ret./=denom\n",
    "    ret\n",
    "end\n",
    "\n",
    "function general_wsd(ee, context, wvs, priors=ones(length(wvs)))\n",
    "    lps = Vector{Float64}(length(wvs))\n",
    "    for (ii,(prior, wv)) in enumerate(zip(priors, wvs))\n",
    "        lps[ii] = Query.logprob_of_context(ee, context, wv; skip_oov=true, normalise_over_length=true)\n",
    "        lps[ii] += log(prior)\n",
    "    end\n",
    "    \n",
    "    normal_probs!(lps)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "synthesize_embedding1 (generic function with 2 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function synthesize_embedding1(ee,context::AbstractVector, \n",
    "                               word_or_phrase::AbstractString,\n",
    "                               fallback_word_or_phrase=\"\"::AbstractString)\n",
    "    \n",
    "    words = split(word_or_phrase, \" \")\n",
    "    wvs = vcat((all_word_sense_vectors(ee,w) for w in words)...)\n",
    "    if length(wvs) == 0\n",
    "        fallbacks = split(fallback_word_or_phrase, \" \")\n",
    "        wvs = vcat((all_word_sense_vectors(ee,w) for w in fallbacks)...)\n",
    "        if length(wvs) == 0\n",
    "            throw(KeyError(\" $words, nor $fallbacks have embeddings\"))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    probs = general_wsd(ee, context, wvs)\n",
    "    sum(wvs.*probs)\n",
    "end\n",
    "    \n",
    "\n",
    "#function synthesize_embedding(am::AdaGramModel,context::AbstractVector, word::AbstractString)\n",
    "#    known_context = filter(c->haskey(am.dict.word2id, c), context)\n",
    "#    sum(all_word_sense_vectors(am, word).*disambiguate(am.vm, am.dict, word, known_context, false))\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Note the disambig prob fumctions :\n",
    " - Query.logprob_of_context(am, context, wv; , normalise_over_length=false)\n",
    "- Adagram.disambiguate(am.vm, am.dict, word, known_context, false))\n",
    "Are identical. That is with normalise_over_length false for Logprob-of-context, \n",
    "and use_prior false for Adagram.disambiguate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lexically_informed_embeddings (generic function with 2 methods)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_identical(col) = length(col)==1 || !any(x->x!=col[1],col[2:end])\n",
    "\n",
    "function _lexically_informed_embeddings(wn::DB,ee, word,lemma_word, pos)\n",
    "    indexed_corpus = semcor #TODO: Pass this as a parameter, not as a gloel\n",
    "    target_synset_examples = get_all_usages(wn, indexed_corpus, lemma_word, pos)\n",
    "    target_synsets = collect(keys(target_synset_examples))\n",
    "    lem = db[lemma_word, pos]\n",
    "        \n",
    "    embeddings = Vector{Vector{Float32}}(length(target_synsets))\n",
    "    for (ii,(synset, examples)) in enumerate(target_synset_examples)\n",
    "        context::Vector{SubString{String}} = vcat(examples...)\n",
    "        embeddings[ii] =synthesize_embedding1(ee,context, word, lemma_word)\n",
    "    end\n",
    "        \n",
    "    \n",
    "    if all_identical(embeddings)\n",
    "        #Use MCWS\n",
    "        score, index = findmax(sensecount(db, ss, lem ) for ss in target_synsets)\n",
    "        target_synsets=[target_synsets[index]] # Throw out others\n",
    "        embeddings=[embeddings[1]]\n",
    "    end\n",
    "        \n",
    "    embeddings,target_synsets,lem\n",
    "end\n",
    "\n",
    "lexically_informed_embeddings(wn::DB, ee, lemma_word, pos) = lexically_informed_embeddings(db,ee, lemma_word, lemma_word, pos)\n",
    "        \n",
    "_li_embeddings = Dict{Tuple{DB, Any, String, String, Char}, Tuple{Vector{Vector{Float32}}, Vector{Synset}, Lemma}}()\n",
    "function lexically_informed_embeddings(wn::DB, ee, word,lemma_word, pos)\n",
    "    get!(_li_embeddings, (wn, ee, word, lemma_word, pos)) do\n",
    "        _lexically_informed_embeddings(wn, ee, word, lemma_word, pos)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function supervised_wsd(challenge, ee, wn::DB)\n",
    "    try\n",
    "        embeddings,target_synsets,lem = lexically_informed_embeddings(wn,ee,\n",
    "                                                            challenge.word, \n",
    "                                                            challenge.lemma, challenge.pos)\n",
    "        \n",
    "        priors = [float(sensecount(db, ss, lem)) for ss in target_synsets] #TODO use thing\n",
    "        priors .+= 0\n",
    "        priors .+= sqrt(sum(priors))/length(priors)\n",
    "        priors ./= sum(priors)\n",
    "        probs_of_sense = general_wsd(ee, challenge.context, embeddings, priors)\n",
    "               \n",
    "     \n",
    "        sense_index= indmax(probs_of_sense)        \n",
    "        synset = target_synsets[sense_index]\n",
    "        sk = sensekey(wn, synset, lem)\n",
    "        Nullable(sk)\n",
    "    catch ex\n",
    "        isa(ex, KeyError) || rethrow(ex)\n",
    "        @show ex\n",
    "        Nullable{String}()\n",
    "    end\n",
    "end\n",
    "\n",
    "function most_frequent_sense(challenge, wn::DB)\n",
    "    try\n",
    "        lem = wn[challenge.pos, challenge.lemma]\n",
    "        target_synsets::Vector{Synset} = synsets(db, lem)\n",
    "        \n",
    "        sense_freqs =  Float32[sensecount(db, ss, lem) for ss in target_synsets]\n",
    "        sense_index= indmax(sense_freqs)\n",
    "        synset = target_synsets[sense_index]\n",
    "        \n",
    "        sk = sensekey(wn, synset,  lem)\n",
    "        Nullable(sk)\n",
    "    catch ex\n",
    "        isa(ex, KeyError) || rethrow(ex)\n",
    "        Nullable{String}()\n",
    "    end\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function evalute_on_wsd_challenge(challenges, solutions, method)\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    notattempted = 0\n",
    "    @showprogress for (challenge, ground_solution) in zip(challenges, solutions)\n",
    "        assert(challenge.id == ground_solution.id)\n",
    "        output_sense = method(challenge)\n",
    "        if isnull(output_sense)\n",
    "            notattempted+=1\n",
    "        else\n",
    "            if get(output_sense) ∈ ground_solution.solutions\n",
    "                correct+=1\n",
    "            else\n",
    "                incorrect+=1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    @show notattempted\n",
    "    precision = correct/(correct+incorrect)\n",
    "    recall = correct/(correct+incorrect+notattempted)\n",
    "    f1 = (2*precision*recall) / (precision+recall)\n",
    "    return precision, recall, f1\n",
    "end\n",
    "    \n",
    "    \n",
    "function evalute_on_wsd_challenge(challenges, solutions, ee, wn::DB)\n",
    "    method = challenge -> supervised_wsd(challenge, ee, wn)\n",
    "    evalute_on_wsd_challenge(challenges, solutions, method)\n",
    "end\n",
    "\n",
    "function evalute_on_wsd_challenge_MFS(challenges, solutions, wn::DB)\n",
    "    method = challenge -> most_frequent_sense(challenge, wn)\n",
    "    evalute_on_wsd_challenge(challenges, solutions, method)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, am, db))\n",
    "\n",
    "#println(\"only nouns  : \\t\\t\", \n",
    "#@show nn = evalute_on_wsd_challenge(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), am, db)\n",
    "#println(\"only verbs  : \\t\\t\", \n",
    "#@show vv = evalute_on_wsd_challenge(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), am, db)\n",
    "#println(\"only adjecti: \\t\\t\", \n",
    "#@show aa = evalute_on_wsd_challenge(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), am, db)\n",
    "#println(\"only adverbs: \\t\\t\", \n",
    "#@show rr = evalute_on_wsd_challenge(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), am, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, s_am, db))\n",
    "\n",
    "#println(\"only nouns  : \\t\\t\", \n",
    "#@show nn = evalute_on_wsd_challenge(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), s_am, db)\n",
    "#println(\"only verbs  : \\t\\t\", \n",
    "#@show vv = evalute_on_wsd_challenge(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), s_am, db)\n",
    "#println(\"only adjecti: \\t\\t\", \n",
    "#@show aa = evalute_on_wsd_challenge(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), s_am, db)\n",
    "#println(\"only adverbs: \\t\\t\", \n",
    "#@show rr = evalute_on_wsd_challenge(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), s_am, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"models/adagram/more_senses.adagram_model.jld\n",
    "### with Adagram Disambig weighting (prior used)\n",
    "### with SemCor data \n",
    "overall: \t\t(0.655408489274304,0.6328779197884531,0.6439461883408072)\n",
    "- nn = evalute_on_wsd_challenge(only_of_pos(challenges,'n'),only_of_pos(solutions,'n'),am,db) = (0.7161410018552876,0.6967509025270758,0.706312900274474)\n",
    "- vv = evalute_on_wsd_challenge(only_of_pos(challenges,'v'),only_of_pos(solutions,'v'),am,db) = (0.5106382978723404,0.4873096446700508,0.4987012987012987)\n",
    "- aa = evalute_on_wsd_challenge(only_of_pos(challenges,'a'),only_of_pos(solutions,'a'),am,db) = (0.6619718309859155,0.649171270718232,0.6555090655509065)\n",
    "- rr = evalute_on_wsd_challenge(only_of_pos(challenges,'r'),only_of_pos(solutions,'r'),am,db) = (0.7268041237113402,0.6778846153846154,0.7014925373134329)\n",
    "\n",
    "### with Adagram Disambig weighting (prior used)\n",
    "### with just glosses \n",
    "\n",
    "overall: \t\t(0.6704701049748973,0.6474217717055972,0.658744394618834)\n",
    "- nn = evalute_on_wsd_challenge(only_of_pos(challenges,'n'),only_of_pos(solutions,'n'),am,db) = (0.7133580705009277,0.694043321299639,0.7035681610247027)\n",
    "- vv = evalute_on_wsd_challenge(only_of_pos(challenges,'v'),only_of_pos(solutions,'v'),am,db) = (0.5780141843971631,0.5516074450084603,0.5645021645021645)\n",
    "- aa = evalute_on_wsd_challenge(only_of_pos(challenges,'a'),only_of_pos(solutions,'a'),am,db) = (0.6535211267605634,0.6408839779005525,0.6471408647140865)\n",
    "- rr = evalute_on_wsd_challenge(only_of_pos(challenges,'r'),only_of_pos(solutions,'r'),am,db) = (0.7319587628865979,0.6826923076923077,0.7064676616915424)\n",
    "\n",
    "\n",
    "\n",
    "# \"semhuff_more_senses.adagram_model.jld\n",
    "\n",
    "### with Adagram Disambig weighting (prior used)\n",
    "### with just glosses \n",
    " - overall: \t\t(0.6704701049748973,0.6474217717055972,0.658744394618834)\n",
    " - nn = evalute_on_wsd_challenge(only_of_pos(challenges,'n'),only_of_pos(solutions,'n'),am,db) = (0.7133580705009277,0.694043321299639,0.7035681610247027)\n",
    " - vv = evalute_on_wsd_challenge(only_of_pos(challenges,'v'),only_of_pos(solutions,'v'),am,db) = (0.5780141843971631,0.5516074450084603,0.5645021645021645)\n",
    " - aa = evalute_on_wsd_challenge(only_of_pos(challenges,'a'),only_of_pos(solutions,'a'),am,db) = (0.6535211267605634,0.6408839779005525,0.6471408647140865)\n",
    " - rr = evalute_on_wsd_challenge(only_of_pos(challenges,'r'),only_of_pos(solutions,'r'),am,db) = (0.7319587628865979,0.6826923076923077,0.7064676616915424)\n",
    " \n",
    " ### with Adagram Disambig weighting (prior used)\n",
    " ### With semcore\n",
    " - overall: \t\t(0.6134185303514377,0.5923314235345968,0.6026905829596412)\n",
    " - nn = evalute_on_wsd_challenge(only_of_pos(challenges,'n'),only_of_pos(solutions,'n'),s_am,db) = (0.6252319109461967,0.6083032490974729,0.6166514181152789)\n",
    " - vv = evalute_on_wsd_challenge(only_of_pos(challenges,'v'),only_of_pos(solutions,'v'),s_am,db) = (0.5638297872340425,0.5380710659898477,0.5506493506493505)\n",
    " - aa = evalute_on_wsd_challenge(only_of_pos(challenges,'a'),only_of_pos(solutions,'a'),s_am,db) = (0.6169014084507042,0.6049723756906077,0.610878661087866)\n",
    " - rr = evalute_on_wsd_challenge(only_of_pos(challenges,'r'),only_of_pos(solutions,'r'),s_am,db) = (0.6855670103092784,0.6394230769230769,0.6616915422885572)\n",
    " \n",
    " \n",
    " # Most Frequent Sense\n",
    " \n",
    "  -  overall: \t\t(0.7783164389598942,0.7783164389598942,0.7783164389598942)\n",
    "  - only nouns  : \t\t(0.7653429602888087,0.7653429602888087,0.7653429602888087)\n",
    "  - only verbs  : \t\t(0.7529610829103215,0.7529610829103215,0.7529610829103215)\n",
    "  - only adjecti: \t\t(0.8038674033149171,0.8038674033149171,0.8038674033149171)\n",
    "  - only adverbs: \t\t(0.875,0.875,0.875)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, ee, db))\n",
    "\n",
    "#println(\"only nouns  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), ee, db))\n",
    "#println(\"only verbs  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), ee, db))\n",
    "#println(\"only adjecti: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), ee, db))\n",
    "#println(\"only adverbs: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), ee, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge_MFS(challenges, solutions, db))\n",
    "\n",
    "#println(\"only nouns  : \\t\\t\", evalute_on_wsd_challenge_MFS(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), db))\n",
    "#println(\"only verbs  : \\t\\t\", evalute_on_wsd_challenge_MFS(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), db))\n",
    "#println(\"only adjecti: \\t\\t\", evalute_on_wsd_challenge_MFS(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), db))\n",
    "#println(\"only adverbs: \\t\\t\", evalute_on_wsd_challenge_MFS(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), db))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Zero shot WSI for 1 shot WSD\n",
    "using Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rr = FixedWordSenseEmbedding(ee.dimension, random_inited, huffman_tree; initial_nsenses=50)\n",
    "rr.corpus_size = ee.corpus_size\n",
    "rr.distribution = ee.distribution\n",
    "rr.codebook = ee.codebook\n",
    "rr.classification_tree = ee.classification_tree\n",
    "Training.initialize_embedding(rr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rr.embedding[\"us\"] |> length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, rr, db))\n",
    "\n",
    "println(\"only nouns  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), rr, db))\n",
    "println(\"only verbs  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), rr, db))\n",
    "println(\"only adjecti: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), rr, db))\n",
    "println(\"only adverbs: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), rr, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hh = deepcopy(rr)\n",
    "for word in keys(rr.embedding)\n",
    "    append!(hh.embedding[word], ee.embedding[word])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, hh, db))\n",
    "\n",
    "println(\"only nouns  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), hh, db))\n",
    "println(\"only verbs  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), hh, db))\n",
    "println(\"only adjecti: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), hh, db))\n",
    "println(\"only adverbs: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), hh, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "importfrom (generic function with 1 method)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funcdec = :(sensekey(##324::WordNet.DB,##325::WordNet.Synset,##326::WordNet.Lemma) = (WordNet).(sensekey)(##324,##325,##326))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapping_corpus = load_semcor(\"data/corpora/semcor2.1/brown1/tagfiles/\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agirresAlignment (generic function with 2 methods)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function agirresAlignment(ee, wn::DB, mapping_corpus, hard=true)\n",
    "    maps = Dict{Tuple{String, Char}, Matrix{Float32}}()\n",
    "    freqs = Dict{Tuple{String, Char}, Vector{Int}}()\n",
    "    \n",
    "    function proc_word(word::TaggedWord, sentence::TaggedSentence)\n",
    "    end\n",
    "    \n",
    "    function proc_word(taggedword::SenseAnnotatedWord, sentence::TaggedSentence)\n",
    "        local wn_sensekeys\n",
    "        pos = pennPOStoWordNetPOS(taggedword.pos)\n",
    "        \n",
    "\n",
    "        \n",
    "        try \n",
    "            wn_sensekeys = sensekeys(wn, wn[pos, taggedword.lemma])\n",
    "        catch exception\n",
    "            exception |> typeof <: KeyError || rethrow()\n",
    "            warn(\"Could not find wordnet lemma for $taggedword\")\n",
    "            return\n",
    "        end\n",
    "        \n",
    "        target_wnsn = findfirst(wn_sensekeys .== sensekey(taggedword))\n",
    "        if target_wnsn==0\n",
    "            warn(\"$(sensekey(taggedword)) not in  $(wn_sensekeys).\\n\\nIssue is with $taggedword\")\n",
    "            return\n",
    "        end\n",
    "\n",
    "        wvs = all_word_sense_vectors(ee,taggedword.word)\n",
    "        if length(wvs) == 0\n",
    "            warn(\"No embedding for $(taggedword.word); skipping\")\n",
    "            return\n",
    "        end\n",
    "        \n",
    "        ########\n",
    "        \n",
    "        map = get!(maps, (taggedword.lemma, pos)) do\n",
    "            zeros(length(wvs), length(wn_sensekeys))\n",
    "        end\n",
    "        \n",
    "        freq = get!(freqs, (taggedword.lemma, pos)) do \n",
    "            zeros(Int, length(wn_sensekeys))\n",
    "        end\n",
    "        \n",
    "        @assert(length(freq) == size(map,2) == length(wn_sensekeys),\n",
    "        \"\"\"\n",
    "        $(size(map)) $(length(freq)) $(length(wn_sensekeys))\n",
    "        $taggedword\n",
    "        \"\"\" )\n",
    "        @assert(size(map,1) == length(wvs))\n",
    "        \n",
    "        \n",
    "        context = lowercase.(strip_tags(sentence))\n",
    "        \n",
    "        wv_probs = general_wsd(ee,context, wvs)\n",
    "        @assert !any(isnan.(wv_probs))\n",
    "        freq[target_wnsn] += 1\n",
    "        if hard\n",
    "            map[indmax(wv_probs),target_wnsn] += 1\n",
    "        else\n",
    "            map[:,target_wnsn] += wv_probs\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "    for sentence in mapping_corpus\n",
    "        for word in sentence\n",
    "            proc_word(word, sentence)\n",
    "        end\n",
    "    end\n",
    " \n",
    "    if !(hard)\n",
    "        for ((word,pos), freq) in freqs\n",
    "            map = maps[(word,pos)]\n",
    "            map ./= freq'\n",
    "            map[isnan.(map)] = 0.0 #NaNs are just frequency 0 items\n",
    "            @assert(all((isapprox(x,1.0; atol=10.0^-5)  for x in sum(map, 1))))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    maps, freqs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maps, freqs  = agirresAlignment(am, db, mapping_corpus, false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = Float32[0.855639 0.177298 0.0171722; 0.909973 0.175451 0.0122244; 0.47952 0.0315287 0.00442265; 1.05545 0.265723 0.0104815; 0.884937 0.139869 0.0130272; 1.14186 0.248063 0.0270401; 1.04605 0.176553 0.0137959; 1.21186 0.247378 0.0108512; 1.13308 0.401118 0.0163113; 1.12129 0.216413 0.0197542; 0.999944 0.241073 0.0147291; 0.510784 0.020609 0.00859674; 1.1716 0.265569 0.021097; 1.13275 0.19544 0.0175935; 1.03405 0.208593 0.0139159; 0.849475 0.12992 0.0123045; 1.08369 0.168431 0.0107693; 3.08044 0.665897 0.0573607; 3.10473 0.668272 0.0582956; 3.10973 0.669066 0.058339; 3.10907 0.668374 0.0581404; 3.10823 0.668336 0.0584079; 3.10801 0.669025 0.0580926; 3.11235 0.669242 0.0580193; 3.11006 0.668676 0.0581764; 3.10551 0.669185 0.0581798; 3.10266 0.667512 0.058163; 3.10847 0.668792 0.0581324; 3.10971 0.669737 0.0581876; 3.10907 0.668856 0.0584186]\n",
      "f = [57,12,1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " 57\n",
       " 12\n",
       "  1"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = maps[\"city\",'n']\n",
    "f = freqs[\"city\",'n']\n",
    "@show m\n",
    "@show f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isapprox(1.0,2.0; atol=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a ./ b' = [0.2 -0.4 3.0 NaN; 2.2 -4.4 33.0 NaN; 22.2 -44.4 333.0 NaN; 222.2 -444.4 3333.0 NaN; 2222.2 -4444.4 33333.0 Inf]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5×4 Array{Float64,2}:\n",
       "    0.2     -0.4      3.0    0.0\n",
       "    2.2     -4.4     33.0    0.0\n",
       "   22.2    -44.4    333.0    0.0\n",
       "  222.2   -444.4   3333.0    0.0\n",
       " 2222.2  -4444.4  33333.0  Inf  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1.0 2 3 0 ; 11 22 33 0 ; 111 222 333 0; 1111 2222 3333 0; 11111 22222 33333 5]\n",
    "b = [5, -5, 1, 0]\n",
    "\n",
    "@show a./b'\n",
    "\n",
    "a./=b'\n",
    "a[isnan(a)]=0.0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = Float32[0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 0.0 0.0 0.0; 57.0 12.0 1.0]\n",
      "f = [57,12,1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30×3 Array{Float32,2}:\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 1.0  1.0  1.0"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = copy(m)\n",
    "@show n\n",
    "@show f\n",
    "n./=f'\n",
    "n[isnan.(n)]=0.0\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Bool,2}:\n",
       "  true  false\n",
       " false  false"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isnan.([NaN 1; Inf 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"funds%1:21:00::\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww = CorpusLoaders.Semcor.SenseAnnotatedWord{SubString{String}}(\"NN\",\"funds\",1,\"1:21:00::\",\"funds\")\n",
    "sensekey(ww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ww.pos = \"NN\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"funds\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@show ww.pos \n",
    "ww.lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "methods(sensekey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0-rc0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
