{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using WordNet\n",
    "using AdaGram\n",
    "using AdaGramCompat\n",
    "\n",
    "using CorpusLoaders\n",
    "using WordEmbeddings, SoftmaxClassifier\n",
    "using Utils\n",
    "using Query\n",
    "using Distances\n",
    "using Iterators\n",
    "using LightXML\n",
    "using JLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordNet.DB"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const WN_PATH=\"data/corpora/WordNet-2.1/\"\n",
    "#WN_PATH3 = \"/usr/share/nltk_data/corpora/wordnet/\"\n",
    "db = DB(WN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#am = AdaGramCompat.AdaGramModel(load_model(\"models/adagram/v1_d100.adagram_model\")...)\n",
    "am = AdaGramCompat.AdaGramModel(load_model(\"models/adagram/more_senses.adagram_model\")...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ee = restore(\"models/ss/tokenised_lowercase_WestburyLab.wikicorp.201004_100_i1.jld\");\n",
    "ee = load(\"models/ss/tokenised_lowercase_WestburyLab.wikicorp.201004_100_i1.jld\", \"ee\")\n",
    "\n",
    "#ee = restore(\"models/ss/keep/tokenised_lowercase_WestburyLab.wikicorp.201004_50__m170000000.model\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CorpusLoaders.punctuation_space_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(CorpusLoaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using CorpusLoaders.Semeval2007t7\n",
    "\n",
    "challenges = load_challenges_semeval2007t7(\"data/corpora/wsd/semeval2007_t7/test/eng-coarse-all-words.xml\");\n",
    "solutions = load_solutions_semeval2007t7(\"data/corpora/wsd/semeval2007_t7/key/dataset21.test.key\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "only_of_pos(data, pos) = filter(d->d.pos==pos, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function sense_frequency(ss::Synset, lem::Lemma)\n",
    "    lem_count = lem.tagsense_count\n",
    "    #lem_count+0.1(sum(values(ss.word_counts))-lem_count) #Do not use Synset's other counts. It owrks out worse\n",
    "    lem_count\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "typemax(Int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "const AdaGram_lib2 = AdaGramCompat.AdaGram_lib\n",
    "function Query.logprob_of_context(am::AdaGramModel, context, input::AbstractVector{Float32}; skip_oov=false, normalise_over_length=false)\n",
    "    total_lprob=zero(Float32)\n",
    "    context_length = 0\n",
    "    for context_word in context\n",
    "        skip_oov && !haskey(am.dict.word2id, context_word) && continue\n",
    "        context_length+=1\n",
    "        context_word_id = am.dict.word2id[context_word]\n",
    "        context_word_path = view(am.vm.path, :, context_word_id)\n",
    "        context_word_code = view(am.vm.code, :, context_word_id)\n",
    "\n",
    "        word_lprob = ccall((:skip_gram, AdaGram_lib2), Float32,\n",
    "            (Ptr{Float32}, Ptr{Float32}, Clonglong, Ptr{Int32}, Ptr{Int8},        Clonglong),\n",
    "            input, am.vm.Out,  size(am.vm.In, 1), context_word_path, context_word_code, size(am.vm.code, 1))\n",
    "        total_lprob+=word_lprob\n",
    "    end\n",
    "    if normalise_over_length\n",
    "        total_lprob/=context_length #This is equivlent to taking the context_length-th root in nonlog domain. Which makes sense.\n",
    "    end\n",
    "    total_lprob\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all_word_sense_vectors (generic function with 2 methods)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function all_word_sense_vectors(ee::WordSenseEmbedding, word)\n",
    "    get(ee.embedding, word, Vector{Float32}[])\n",
    "end\n",
    "\n",
    "function all_word_sense_vectors(am::AdaGramCompat.AdaGramModel, word)\n",
    "    if haskey(am.dict.word2id, word)\n",
    "        wsv_mat = word_sense_vectors(am, word)\n",
    "        [view(wsv_mat,:,ii) for ii in 1:size(wsv_mat,2)]\n",
    "    else\n",
    "        Vector{Float32}[]\n",
    "    end\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lexically_informed_embeddings (generic function with 2 methods)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function normal_probs(logprobs::Vector)\n",
    "    ret = copy(logprobs)\n",
    "    max_lp = maximum(logprobs)\n",
    "    ret.-=max_lp #Bring closer to zero\n",
    "    map!(exp,ret)\n",
    "    denom = sum(ret)\n",
    "    ret./=denom\n",
    "    ret\n",
    "end\n",
    "function weighted_average(logprobs, embeddings)\n",
    "    ret = zeros(first(embeddings))\n",
    "    for (weight, embedding) in zip(normal_probs(logprobs), embeddings)\n",
    "        ret.+= weight.*embedding\n",
    "    end\n",
    "    ret\n",
    "end\n",
    "\n",
    "\n",
    "function synthesize_embedding(ee,context, word_or_phrase)\n",
    "    words = split(word_or_phrase, \" \")\n",
    "    wvs = vcat((all_word_sense_vectors(ee,w) for w in words)...)\n",
    "    if length(wvs) == 0\n",
    "            throw(KeyError(\"None of $words have embeddings\"))\n",
    "    end\n",
    "    logprobs = [Query.logprob_of_context(ee, context,wv; skip_oov=true, normalise_over_length=true) for wv in wvs]\n",
    "    weighted_average(logprobs, wvs)\n",
    "end\n",
    "\n",
    "\n",
    "all_identical(col) = length(col)==1 || !any(x->x!=col[1],col[2:end])\n",
    "\n",
    "lexically_informed_embeddings(wn::DB, ee, lemma_word, pos) = lexically_informed_embeddings(db,ee, lemma_word, lemma_word, pos)\n",
    "function lexically_informed_embeddings(wn::DB, ee, word,lemma_word, pos)\n",
    "    \n",
    "    lemma = db[pos, lemma_word]\n",
    "    target_synsets::Vector{Synset} = synsets(db, lemma)\n",
    "         \n",
    "    embeddings = Vector{Vector{Float32}}(length(target_synsets))\n",
    "    for (ii,synset) in enumerate(target_synsets)\n",
    "        context::Vector{SubString{String}} = collect(filter!(w->w!=word, punctuation_space_tokenize(synset.gloss)))\n",
    "        embeddings[ii] = synthesize_embedding(ee,context, word)\n",
    "    end\n",
    "        \n",
    "    \n",
    "    if all_identical(embeddings)\n",
    "        #Use MCWS\n",
    "        score, index = findmax(sense_frequency(ss,lemma) for ss in target_synsets)\n",
    "        target_synsets=[target_synsets[index]] # Throw out others\n",
    "        embeddings=[embeddings[1]]\n",
    "    end\n",
    "        \n",
    "    embeddings,target_synsets,lemma\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "most_frequent_sense (generic function with 1 method)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function supervised_wsd(challenge, ee, wn::DB)\n",
    "    try\n",
    "        embeddings,target_synsets,lemma = lexically_informed_embeddings(wn,ee,\n",
    "                                                            challenge.word, \n",
    "                                                            challenge.lemma, challenge.pos)\n",
    "        #sense_index = Query.WSD(ee, embeddings, challenge.context; skip_oov=true)\n",
    "        logprobs_of_context = [logprob_of_context(ee,challenge.context, wv; skip_oov=true, normalise_over_length=true) \n",
    "                                    for wv in embeddings] \n",
    "      \n",
    "        sense_index= indmax(logprobs_of_context)\n",
    "\n",
    "        \n",
    "        synset = target_synsets[sense_index]\n",
    "        sk = sensekey(wn, synset, lemma)\n",
    "        Nullable(sk)\n",
    "    catch ex\n",
    "        isa(ex, KeyError) || rethrow(ex)\n",
    "        Nullable{String}()\n",
    "    end\n",
    "end\n",
    "\n",
    "function most_frequent_sense(challenge, wn::DB)\n",
    "    try\n",
    "        lemma = wn[challenge.pos, challenge.lemma]\n",
    "        target_synsets::Vector{Synset} = synsets(db, lemma)\n",
    "        \n",
    "        sense_freqs =  Float32[sense_frequency(ss,lemma) for ss in target_synsets]      \n",
    "        sense_index= indmax(sense_freqs)\n",
    "        synset = target_synsets[sense_index]\n",
    "        \n",
    "        sk = sensekey(wn, synset,  lemma)\n",
    "        Nullable(sk)\n",
    "    catch ex\n",
    "        isa(ex, KeyError) || rethrow(ex)\n",
    "        Nullable{String}()\n",
    "    end\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evalute_on_wsd_challenge_MFS (generic function with 1 method)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function evalute_on_wsd_challenge(challenges, solutions, method)\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    notattempted = 0\n",
    "    for (challenge, ground_solution) in zip(challenges, solutions)\n",
    "        assert(challenge.id == ground_solution.id)\n",
    "        output_sense = method(challenge)\n",
    "        if isnull(output_sense)\n",
    "            notattempted+=1\n",
    "        else\n",
    "            if get(output_sense) ∈ ground_solution.solutions\n",
    "                correct+=1\n",
    "            else\n",
    "                incorrect+=1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    precision = correct/(correct+incorrect)\n",
    "    recall = correct/(correct+incorrect+notattempted)\n",
    "    f1 = (2*precision*recall) / (precision+recall)\n",
    "    return precision, recall, f1\n",
    "end\n",
    "    \n",
    "    \n",
    "function evalute_on_wsd_challenge(challenges, solutions, ee, wn::DB)\n",
    "    method = challenge -> supervised_wsd(challenge, ee, wn)\n",
    "    evalute_on_wsd_challenge(challenges, solutions, method)\n",
    "end\n",
    "\n",
    "\n",
    "function evalute_on_wsd_challenge_MFS(challenges, solutions, wn::DB)\n",
    "    method = challenge -> most_frequent_sense(challenge, wn)\n",
    "    evalute_on_wsd_challenge(challenges, solutions, method)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall: \t\t(0.6326710816777041,0.6315557514323491,0.6321129245699162)\n",
      "only nouns  : \t\t(0.6504065040650406,0.6498194945848376,0.6501128668171559)\n",
      "only verbs  : \t\t(0.576271186440678,0.5752961082910322,0.5757832345469941)\n",
      "only adjecti: \t\t(0.6260387811634349,0.6243093922651933,0.6251728907330568)\n",
      "only adverbs: \t\t(0.7101449275362319,0.7067307692307693,0.708433734939759)\n"
     ]
    }
   ],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, am, db))\n",
    "\n",
    "println(\"only nouns  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), am, db))\n",
    "println(\"only verbs  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), am, db))\n",
    "println(\"only adjecti: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), am, db))\n",
    "println(\"only adverbs: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), am, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: ccall sense_counts_callback HierarchicalSoftmaxNode AbstractChannel\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "ccall((symbol, library) or function_pointer, ReturnType, (ArgumentType1, ...), ArgumentValue1, ...)\n",
       "\\end{verbatim}\n",
       "Call function in C-exported shared library, specified by \\texttt{(function name, library)} tuple, where each component is a string or symbol.\n",
       "Note that the argument type tuple must be a literal tuple, and not a tuple-valued variable or expression. Alternatively, \\texttt{ccall} may also be used to call a function pointer, such as one returned by \\texttt{dlsym}.\n",
       "Each \\texttt{ArgumentValue} to the \\texttt{ccall} will be converted to the corresponding \\texttt{ArgumentType}, by automatic insertion of calls to \\texttt{unsafe_convert(ArgumentType, cconvert(ArgumentType, ArgumentValue))}. (See also the documentation for each of these functions for further details.) In most cases, this simply results in a call to \\texttt{convert(ArgumentType, ArgumentValue)}.\n"
      ],
      "text/markdown": [
       "```\n",
       "ccall((symbol, library) or function_pointer, ReturnType, (ArgumentType1, ...), ArgumentValue1, ...)\n",
       "```\n",
       "\n",
       "Call function in C-exported shared library, specified by `(function name, library)` tuple, where each component is a string or symbol.\n",
       "\n",
       "Note that the argument type tuple must be a literal tuple, and not a tuple-valued variable or expression. Alternatively, `ccall` may also be used to call a function pointer, such as one returned by `dlsym`.\n",
       "\n",
       "Each `ArgumentValue` to the `ccall` will be converted to the corresponding `ArgumentType`, by automatic insertion of calls to `unsafe_convert(ArgumentType, cconvert(ArgumentType, ArgumentValue))`. (See also the documentation for each of these functions for further details.) In most cases, this simply results in a call to `convert(ArgumentType, ArgumentValue)`.\n"
      ],
      "text/plain": [
       "```\n",
       "ccall((symbol, library) or function_pointer, ReturnType, (ArgumentType1, ...), ArgumentValue1, ...)\n",
       "```\n",
       "\n",
       "Call function in C-exported shared library, specified by `(function name, library)` tuple, where each component is a string or symbol.\n",
       "\n",
       "Note that the argument type tuple must be a literal tuple, and not a tuple-valued variable or expression. Alternatively, `ccall` may also be used to call a function pointer, such as one returned by `dlsym`.\n",
       "\n",
       "Each `ArgumentValue` to the `ccall` will be converted to the corresponding `ArgumentType`, by automatic insertion of calls to `unsafe_convert(ArgumentType, cconvert(ArgumentType, ArgumentValue))`. (See also the documentation for each of these functions for further details.) In most cases, this simply results in a call to `convert(ArgumentType, ArgumentValue)`.\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?ccall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall: \t\t(0.6486486486486487,0.6346408109299251,0.6415682780129205)\n",
      "only nouns  : \t\t(0.6593001841620626,0.6462093862815884,0.6526891522333637)\n",
      "only verbs  : \t\t(0.5785837651122625,0.5668358714043993,0.5726495726495726)\n",
      "only adjecti: \t\t(0.6762177650429799,0.6519337016574586,0.6638537271448665)\n",
      "only adverbs: \t\t(0.7427184466019418,0.7355769230769231,0.7391304347826086)\n"
     ]
    }
   ],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, ee, db))\n",
    "\n",
    "println(\"only nouns  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), ee, db))\n",
    "println(\"only verbs  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), ee, db))\n",
    "println(\"only adjecti: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), ee, db))\n",
    "println(\"only adverbs: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), ee, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall: \t\t(0.7783164389598942,0.7783164389598942,0.7783164389598942)\n",
      "only nouns  : \t\t(0.7653429602888087,0.7653429602888087,0.7653429602888087)\n",
      "only verbs  : \t\t(0.7529610829103215,0.7529610829103215,0.7529610829103215)\n",
      "only adjecti: \t\t(0.8038674033149171,0.8038674033149171,0.8038674033149171)\n",
      "only adverbs: \t\t(0.875,0.875,0.875)\n"
     ]
    }
   ],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge_MFS(challenges, solutions, db))\n",
    "\n",
    "println(\"only nouns  : \\t\\t\", evalute_on_wsd_challenge_MFS(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), db))\n",
    "println(\"only verbs  : \\t\\t\", evalute_on_wsd_challenge_MFS(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), db))\n",
    "println(\"only adjecti: \\t\\t\", evalute_on_wsd_challenge_MFS(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), db))\n",
    "println(\"only adverbs: \\t\\t\", evalute_on_wsd_challenge_MFS(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), db))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Zero shot WSI for 1 shot WSD\n",
    "using Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rr = FixedWordSenseEmbedding(ee.dimension, random_inited, huffman_tree; initial_nsenses=1)\n",
    "rr.distribution = ee.distribution\n",
    "rr.codebook = ee.codebook\n",
    "rr.classification_tree = ee.classification_tree\n",
    "Training.initialize_embedding(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rr.initial_nsenses=100\n",
    "Training.initialize_embedding(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, rr, db, sense_key_map))\n",
    "\n",
    "println(\"only nouns  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), rr, db, sense_key_map))\n",
    "println(\"only verbs  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), rr, db, sense_key_map))\n",
    "println(\"only adjecti: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), rr, db, sense_key_map))\n",
    "println(\"only adverbs: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), rr, db, sense_key_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hh = deepcopy(rr)\n",
    "for word in keys(rr.embedding)\n",
    "    append!(hh.embedding[word], ee.embedding[word])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, hh, db, sense_key_map))\n",
    "\n",
    "println(\"only nouns  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), hh, db, sense_key_map))\n",
    "println(\"only verbs  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), hh, db, sense_key_map))\n",
    "println(\"only adjecti: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), hh, db, sense_key_map))\n",
    "println(\"only adverbs: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), hh, db, sense_key_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using JLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0-rc0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
