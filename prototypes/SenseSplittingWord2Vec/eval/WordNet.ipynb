{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(\"Utils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using ProgressMeter\n",
    "using SwiftObjectStores\n",
    "using CorpusLoaders\n",
    "using WordNet\n",
    "using AdaGram\n",
    "using AdaGramCompat\n",
    "using WordEmbeddings, SoftmaxClassifier\n",
    "using Utils\n",
    "using Query\n",
    "using Distances\n",
    "using JLD\n",
    "\n",
    "using SenseAlignment\n",
    "\n",
    "importfrom(CorpusLoaders, :sensekey)\n",
    "importfrom(WordNet, :sensekey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordNet.DB"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const WN_PATH=\"data/corpora/WordNet-2.1/\"\n",
    "#WN_PATH3 = \"/usr/share/nltk_data/corpora/wordnet/\"\n",
    "db = DB(WN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#am = AdaGramCompat.AdaGramModel(load_model(\"models/adagram/v1_d100.adagram_model\")...)\n",
    "#am = AdaGramModel(load_model(\"models/adagram/more_senses.adagram_model\")...)\n",
    "#am = open(deserialize,\"models/adagram/more_senses.adagram_model.jsz\", \"r\");\n",
    "am = load(\"models/adagram/more_senses.adagram_model.jld\", \"am\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_am = get_jld(SwiftService(), \"sensemodels\", \"adagram/semhuff_more_senses.adagram_model.jld\", \"am\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ee = restore(\"models/ss/tokenised_lowercase_WestburyLab.wikicorp.201004_100_i1.jld\");\n",
    "ee = load(\"models/ss/tokenised_lowercase_WestburyLab.wikicorp.201004_100_i1.jld\", \"ee\")\n",
    "\n",
    "#ee = restore(\"models/ss/keep/tokenised_lowercase_WestburyLab.wikicorp.201004_50__m170000000.model\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "challenges = load_challenges_semeval2007t7(\"data/corpora/wsd/semeval2007_t7/test/eng-coarse-all-words.xml\");\n",
    "solutions = load_solutions_semeval2007t7(\"data/corpora/wsd/semeval2007_t7/key/dataset21.test.key\");\n",
    "semcor = index_semcor(lazyload_semcor(\"data/corpora/semcor2.1/brown1/tagfiles/\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element SubArray{String,1,Array{String,1},Tuple{Array{Int64,1}},false}:\n",
       " \"fire\"  \n",
       " \"can\"   \n",
       " \"be\"    \n",
       " \"built;\"\n",
       " \"\\\"the\" \n",
       " \"was\"   \n",
       " \"so\"    \n",
       " \"large\" \n",
       " \"you\"   \n",
       " \"could\" "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function window(context, index::Int, window_size::Int=10)\n",
    "    window_lower_bound = max(index - window_size÷2, 1)\n",
    "    window_upper_bound = min(index + window_size÷2, length(context))\n",
    "    view(context, [window_lower_bound:index-1 ; index+1:window_upper_bound])\n",
    "end\n",
    "\n",
    "function window(tagged_sense::TaggedSentence, index::Int, window_size::Int=10)\n",
    "    context = lowercase.(strip_tags(tagged_sense))\n",
    "    window(context, index, window_size)\n",
    "end\n",
    "   \n",
    "function window(context, word::AbstractString, window_size::Int=10)\n",
    "    context = lowercase.(context)\n",
    "    occurances = find(context.==word)\n",
    "    index =  length(occurances) > 0 ? occurances[ceil(Int, end/2)] : 0\n",
    "    window(context, index, window_size)\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "window((drop(semcor,400) |> first |> last |> first)...)\n",
    "\n",
    "window((synsets(db, db[\"fireplace\", 'n']) |> first).gloss |> split, \"fireplace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_all_usages (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"Collect up the usages from a indexed tagged source\"\n",
    "function get_usages(usage_index::SemcorIndex, key)\n",
    "    if haskey(usage_index, key)\n",
    "        [window(lowercase.(strip_tags(sent)), index) for (sent, index) in usage_index[key]]\n",
    "    else\n",
    "        Vector{String}[]\n",
    "    end\n",
    "end\n",
    "\n",
    "function get_usages(synset::Synset, lemma_word::AbstractString)\n",
    "    gloss::Vector{SubString{String}} = lowercase.(punctuation_space_tokenize(synset.gloss))\n",
    "    [window(gloss, lemma_word)]\n",
    "end\n",
    "\n",
    "function get_all_usages(wn::DB, semcor::SemcorIndex, lemma_word, pos)   \n",
    "    lemma = db[pos, lemma_word]\n",
    "    target_synsets::Vector{Synset} = synsets(db, lemma)\n",
    "    \n",
    "    Dict{Synset,AbstractVector{AbstractVector}}((synset => get_usages(synset, lemma_word)\n",
    "        #[get_usages(semcor, sensekey(db, synset, lemma)); get_usages(synset, lemma_word)]\n",
    "                    for synset in target_synsets))  \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lexically_informed_embeddings (generic function with 2 methods)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_identical(col) = length(col)==1 || !any(x->x!=col[1],col[2:end])\n",
    "\n",
    "function _lexically_informed_embeddings(wn::DB,ee, word,lemma_word, pos)\n",
    "    indexed_corpus = semcor #TODO: Pass this as a parameter, not as a gloel\n",
    "    target_synset_examples = get_all_usages(wn, indexed_corpus, lemma_word, pos)\n",
    "    target_synsets = collect(keys(target_synset_examples))\n",
    "    lem = db[lemma_word, pos]\n",
    "        \n",
    "    embeddings = Vector{Vector{Float32}}(length(target_synsets))\n",
    "    for (ii,(synset, examples)) in enumerate(target_synset_examples)\n",
    "        context::Vector{SubString{String}} = vcat(examples...)\n",
    "        embeddings[ii] =synthesize_embedding(ee,context, word, lemma_word)\n",
    "    end\n",
    "        \n",
    "    \n",
    "    if all_identical(embeddings)\n",
    "        #Use MCWS\n",
    "        score, index = findmax(sensecount(db, ss, lem ) for ss in target_synsets)\n",
    "        target_synsets=[target_synsets[index]] # Throw out others\n",
    "        embeddings=[embeddings[1]]\n",
    "    end\n",
    "        \n",
    "    embeddings,target_synsets,lem\n",
    "end\n",
    "\n",
    "lexically_informed_embeddings(wn::DB, ee, lemma_word, pos) = lexically_informed_embeddings(db,ee, lemma_word, lemma_word, pos)\n",
    "        \n",
    "_li_embeddings = Dict{Tuple{DB, Any, String, String, Char}, Tuple{Vector{Vector{Float32}}, Vector{Synset}, Lemma}}()\n",
    "function lexically_informed_embeddings(wn::DB, ee, word,lemma_word, pos)\n",
    "    get!(_li_embeddings, (wn, ee, word, lemma_word, pos)) do\n",
    "        _lexically_informed_embeddings(wn, ee, word, lemma_word, pos)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "most_frequent_sense (generic function with 1 method)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function supervised_wsd(challenge, ee, wn::DB)\n",
    "    try\n",
    "        embeddings,target_synsets,lem = lexically_informed_embeddings(wn,ee,\n",
    "                                                            challenge.word, \n",
    "                                                            challenge.lemma, challenge.pos)\n",
    "        \n",
    "        priors = [float(sensecount(db, ss, lem)) for ss in target_synsets] #TODO use thing\n",
    "        priors .+= 100\n",
    "        #priors .+= sqrt(sum(priors))/length(priors)\n",
    "        priors ./= sum(priors)\n",
    "        probs_of_sense = general_wsd(ee, challenge.context, embeddings, priors)\n",
    "               \n",
    "     \n",
    "        sense_index= indmax(probs_of_sense)        \n",
    "        synset = target_synsets[sense_index]\n",
    "        sk = sensekey(wn, synset, lem)\n",
    "        Nullable(sk)\n",
    "    catch ex\n",
    "        isa(ex, KeyError) || rethrow(ex)\n",
    "        @show ex\n",
    "        Nullable{String}()\n",
    "    end\n",
    "end\n",
    "\n",
    "function most_frequent_sense(challenge, wn::DB)\n",
    "    try\n",
    "        lem = wn[challenge.pos, challenge.lemma]\n",
    "        target_synsets::Vector{Synset} = synsets(db, lem)\n",
    "        \n",
    "        sense_freqs =  Float32[sensecount(db, ss, lem) for ss in target_synsets]\n",
    "        sense_index= indmax(sense_freqs)\n",
    "        synset = target_synsets[sense_index]\n",
    "        \n",
    "        sk = sensekey(wn, synset,  lem)\n",
    "        Nullable(sk)\n",
    "    catch ex\n",
    "        isa(ex, KeyError) || rethrow(ex)\n",
    "        Nullable{String}()\n",
    "    end\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evalute_on_wsd_challenge_MFS (generic function with 1 method)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function evalute_on_wsd_challenge(challenges, solutions, method)\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    notattempted = 0\n",
    "    @showprogress for (challenge, ground_solution) in zip(challenges, solutions)\n",
    "        assert(challenge.id == ground_solution.id)\n",
    "        output_sense = method(challenge)\n",
    "        if isnull(output_sense)\n",
    "            notattempted+=1\n",
    "        else\n",
    "            if get(output_sense) ∈ ground_solution.solutions\n",
    "                correct+=1\n",
    "            else\n",
    "                incorrect+=1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    @show notattempted\n",
    "    precision = correct/(correct+incorrect)\n",
    "    recall = correct/(correct+incorrect+notattempted)\n",
    "    f1 = (2*precision*recall) / (precision+recall)\n",
    "    return precision, recall, f1\n",
    "end\n",
    "    \n",
    "    \n",
    "function evalute_on_wsd_challenge(challenges, solutions, ee, wn::DB)\n",
    "    method = challenge -> supervised_wsd(challenge, ee, wn)\n",
    "    evalute_on_wsd_challenge(challenges, solutions, method)\n",
    "end\n",
    "\n",
    "function evalute_on_wsd_challenge_MFS(challenges, solutions, wn::DB)\n",
    "    method = challenge -> most_frequent_sense(challenge, wn)\n",
    "    evalute_on_wsd_challenge(challenges, solutions, method)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  19%|████████                                 |  ETA: 0:00:08ex = KeyError(\" SubString{String}[\\\"noncompetitively\\\"], nor SubString{String}[\\\"noncompetitively\\\"] have embeddings\")\n",
      "Progress:  23%|██████████                               |  ETA: 0:00:08ex = KeyError(\" SubString{String}[\\\"semiliterate\\\"], nor SubString{String}[\\\"semiliterate\\\"] have embeddings\")\n",
      "Progress: 100%|█████████████████████████████████████████| Time: 0:00:09\n",
      "notattempted = 2\n",
      "overall: \t\t(0.7953242170269078,0.7946231820185103,0.794973544973545)\n"
     ]
    }
   ],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, am, db))\n",
    "\n",
    "#println(\"only nouns  : \\t\\t\", \n",
    "#@show nn = evalute_on_wsd_challenge(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), am, db)\n",
    "#println(\"only verbs  : \\t\\t\", \n",
    "#@show vv = evalute_on_wsd_challenge(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), am, db)\n",
    "#println(\"only adjecti: \\t\\t\", \n",
    "#@show aa = evalute_on_wsd_challenge(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), am, db)\n",
    "#println(\"only adverbs: \\t\\t\", \n",
    "#@show rr = evalute_on_wsd_challenge(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), am, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ex = KeyError(\" SubString{String}[\\\"aftereffects\\\"], nor SubString{String}[\\\"aftereffect\\\"] have embeddings\")\n",
      "Progress:   4%|██                                       |  ETA: 0:00:02ex = KeyError(\" SubString{String}[\\\"predispose\\\"], nor SubString{String}[\\\"predispose\\\"] have embeddings\")\n",
      "Progress:  12%|█████                                    |  ETA: 0:00:04ex = KeyError(\" SubString{String}[\\\"insinuating\\\"], nor SubString{String}[\\\"insinuate\\\"] have embeddings\")\n",
      "Progress:  14%|██████                                   |  ETA: 0:00:04ex = KeyError(\" SubString{String}[\\\"high-minded\\\"], nor SubString{String}[\\\"high-minded\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"scammers\\\"], nor SubString{String}[\\\"scammer\\\"] have embeddings\")\n",
      "Progress:  19%|████████                                 |  ETA: 0:00:03ex = KeyError(\" SubString{String}[\\\"noncompetitively\\\"], nor SubString{String}[\\\"noncompetitively\\\"] have embeddings\")\n",
      "Progress:  23%|█████████                                |  ETA: 0:00:03ex = KeyError(\" SubString{String}[\\\"semiliterate\\\"], nor SubString{String}[\\\"semiliterate\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"scammers\\\"], nor SubString{String}[\\\"scammer\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"scammers\\\"], nor SubString{String}[\\\"scammer\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"tidbits\\\"], nor SubString{String}[\\\"tidbit\\\"] have embeddings\")\n",
      "Progress:  30%|████████████                             |  ETA: 0:00:02ex = KeyError(\" SubString{String}[\\\"nomenklatura\\\"], nor SubString{String}[\\\"nomenklatura\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"halfhearted\\\"], nor SubString{String}[\\\"halfhearted\\\"] have embeddings\")\n",
      "Progress:  35%|██████████████                           |  ETA: 0:00:02ex = KeyError(\" SubString{String}[\\\"scoffed\\\"], nor SubString{String}[\\\"scoff\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"balloonists\\\"], nor SubString{String}[\\\"balloonist\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"chi-chi\\\"], nor SubString{String}[\\\"chichi\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"covetous\\\"], nor SubString{String}[\\\"covetous\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"balloonists\\\"], nor SubString{String}[\\\"balloonist\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"lighter-than-air\\\"], nor SubString{String}[\\\"lighter-than-air\\\"] have embeddings\")\n",
      "Progress:  39%|████████████████                         |  ETA: 0:00:02ex = KeyError(\" SubString{String}[\\\"derring-do\\\"], nor SubString{String}[\\\"derring-do\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"soggy\\\"], nor SubString{String}[\\\"soggy\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"walkie-talkie\\\"], nor SubString{String}[\\\"walkie-talkie\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"amble\\\"], nor SubString{String}[\\\"amble\\\"] have embeddings\")\n",
      "Progress:  44%|██████████████████                       |  ETA: 0:00:02ex = KeyError(\" SubString{String}[\\\"bird's-eye\\\"], nor SubString{String}[\\\"bird's-eye\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"squinted\\\"], nor SubString{String}[\\\"squint\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"balloonists\\\"], nor SubString{String}[\\\"balloonist\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"loafers\\\"], nor SubString{String}[\\\"loafer\\\"] have embeddings\")\n",
      "Progress:  49%|████████████████████                     |  ETA: 0:00:02ex = KeyError(\" SubString{String}[\\\"soggy\\\"], nor SubString{String}[\\\"soggy\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"disassemble\\\"], nor SubString{String}[\\\"disassemble\\\"] have embeddings\")\n",
      "Progress:  52%|██████████████████████                   |  ETA: 0:00:01ex = KeyError(\" SubString{String}[\\\"duffers\\\"], nor SubString{String}[\\\"duffer\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"clambered\\\"], nor SubString{String}[\\\"clamber\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"alfresco\\\"], nor SubString{String}[\\\"alfresco\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"soggy\\\"], nor SubString{String}[\\\"soggy\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"loafers\\\"], nor SubString{String}[\\\"loafer\\\"] have embeddings\")\n",
      "Progress:  60%|█████████████████████████                |  ETA: 0:00:01ex = KeyError(\" SubString{String}[\\\"pasteboard\\\"], nor SubString{String}[\\\"pasteboard\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"plugboards\\\"], nor SubString{String}[\\\"plugboard\\\"] have embeddings\")\n",
      "Progress:  85%|███████████████████████████████████      |  ETA: 0:00:00ex = KeyError(\" SubString{String}[\\\"untidy\\\"], nor SubString{String}[\\\"untidy\\\"] have embeddings\")\n",
      "Progress:  88%|████████████████████████████████████     |  ETA: 0:00:00ex = KeyError(\" SubString{String}[\\\"untidy\\\"], nor SubString{String}[\\\"untidy\\\"] have embeddings\")\n",
      "ex = KeyError(\" SubString{String}[\\\"untidy\\\"], nor SubString{String}[\\\"untidy\\\"] have embeddings\")\n",
      "Progress: 100%|█████████████████████████████████████████| Time: 0:00:03\n",
      "notattempted = 38\n",
      "overall: \t\t(0.7745405647691618,0.7615689731159101,0.768)\n"
     ]
    }
   ],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, ee, db))\n",
    "\n",
    "#println(\"only nouns  : \\t\\t\", \n",
    "#@show nn = evalute_on_wsd_challenge(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), am, db)\n",
    "#println(\"only verbs  : \\t\\t\", \n",
    "#@show vv = evalute_on_wsd_challenge(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), am, db)\n",
    "#println(\"only adjecti: \\t\\t\", \n",
    "#@show aa = evalute_on_wsd_challenge(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), am, db)\n",
    "#println(\"only adverbs: \\t\\t\", \n",
    "#@show rr = evalute_on_wsd_challenge(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), am, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, s_am, db))\n",
    "\n",
    "#println(\"only nouns  : \\t\\t\", \n",
    "#@show nn = evalute_on_wsd_challenge(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), s_am, db)\n",
    "#println(\"only verbs  : \\t\\t\", \n",
    "#@show vv = evalute_on_wsd_challenge(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), s_am, db)\n",
    "#println(\"only adjecti: \\t\\t\", \n",
    "#@show aa = evalute_on_wsd_challenge(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), s_am, db)\n",
    "#println(\"only adverbs: \\t\\t\", \n",
    "#@show rr = evalute_on_wsd_challenge(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), s_am, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"models/adagram/more_senses.adagram_model.jld\n",
    "### with Adagram Disambig weighting (prior used)\n",
    "### with SemCor data \n",
    "overall: \t\t(0.655408489274304,0.6328779197884531,0.6439461883408072)\n",
    "- nn = evalute_on_wsd_challenge(only_of_pos(challenges,'n'),only_of_pos(solutions,'n'),am,db) = (0.7161410018552876,0.6967509025270758,0.706312900274474)\n",
    "- vv = evalute_on_wsd_challenge(only_of_pos(challenges,'v'),only_of_pos(solutions,'v'),am,db) = (0.5106382978723404,0.4873096446700508,0.4987012987012987)\n",
    "- aa = evalute_on_wsd_challenge(only_of_pos(challenges,'a'),only_of_pos(solutions,'a'),am,db) = (0.6619718309859155,0.649171270718232,0.6555090655509065)\n",
    "- rr = evalute_on_wsd_challenge(only_of_pos(challenges,'r'),only_of_pos(solutions,'r'),am,db) = (0.7268041237113402,0.6778846153846154,0.7014925373134329)\n",
    "\n",
    "### with Adagram Disambig weighting (prior used)\n",
    "### with just glosses \n",
    "\n",
    "overall: \t\t(0.6704701049748973,0.6474217717055972,0.658744394618834)\n",
    "- nn = evalute_on_wsd_challenge(only_of_pos(challenges,'n'),only_of_pos(solutions,'n'),am,db) = (0.7133580705009277,0.694043321299639,0.7035681610247027)\n",
    "- vv = evalute_on_wsd_challenge(only_of_pos(challenges,'v'),only_of_pos(solutions,'v'),am,db) = (0.5780141843971631,0.5516074450084603,0.5645021645021645)\n",
    "- aa = evalute_on_wsd_challenge(only_of_pos(challenges,'a'),only_of_pos(solutions,'a'),am,db) = (0.6535211267605634,0.6408839779005525,0.6471408647140865)\n",
    "- rr = evalute_on_wsd_challenge(only_of_pos(challenges,'r'),only_of_pos(solutions,'r'),am,db) = (0.7319587628865979,0.6826923076923077,0.7064676616915424)\n",
    "\n",
    "\n",
    "\n",
    "# \"semhuff_more_senses.adagram_model.jld\n",
    "\n",
    "### with Adagram Disambig weighting (prior used)\n",
    "### with just glosses \n",
    " - overall: \t\t(0.6704701049748973,0.6474217717055972,0.658744394618834)\n",
    " - nn = evalute_on_wsd_challenge(only_of_pos(challenges,'n'),only_of_pos(solutions,'n'),am,db) = (0.7133580705009277,0.694043321299639,0.7035681610247027)\n",
    " - vv = evalute_on_wsd_challenge(only_of_pos(challenges,'v'),only_of_pos(solutions,'v'),am,db) = (0.5780141843971631,0.5516074450084603,0.5645021645021645)\n",
    " - aa = evalute_on_wsd_challenge(only_of_pos(challenges,'a'),only_of_pos(solutions,'a'),am,db) = (0.6535211267605634,0.6408839779005525,0.6471408647140865)\n",
    " - rr = evalute_on_wsd_challenge(only_of_pos(challenges,'r'),only_of_pos(solutions,'r'),am,db) = (0.7319587628865979,0.6826923076923077,0.7064676616915424)\n",
    " \n",
    " ### with Adagram Disambig weighting (prior used)\n",
    " ### With semcore\n",
    " - overall: \t\t(0.6134185303514377,0.5923314235345968,0.6026905829596412)\n",
    " - nn = evalute_on_wsd_challenge(only_of_pos(challenges,'n'),only_of_pos(solutions,'n'),s_am,db) = (0.6252319109461967,0.6083032490974729,0.6166514181152789)\n",
    " - vv = evalute_on_wsd_challenge(only_of_pos(challenges,'v'),only_of_pos(solutions,'v'),s_am,db) = (0.5638297872340425,0.5380710659898477,0.5506493506493505)\n",
    " - aa = evalute_on_wsd_challenge(only_of_pos(challenges,'a'),only_of_pos(solutions,'a'),s_am,db) = (0.6169014084507042,0.6049723756906077,0.610878661087866)\n",
    " - rr = evalute_on_wsd_challenge(only_of_pos(challenges,'r'),only_of_pos(solutions,'r'),s_am,db) = (0.6855670103092784,0.6394230769230769,0.6616915422885572)\n",
    " \n",
    " \n",
    " # Most Frequent Sense\n",
    " \n",
    "  -  overall: \t\t(0.7783164389598942,0.7783164389598942,0.7783164389598942)\n",
    "  - only nouns  : \t\t(0.7653429602888087,0.7653429602888087,0.7653429602888087)\n",
    "  - only verbs  : \t\t(0.7529610829103215,0.7529610829103215,0.7529610829103215)\n",
    "  - only adjecti: \t\t(0.8038674033149171,0.8038674033149171,0.8038674033149171)\n",
    "  - only adverbs: \t\t(0.875,0.875,0.875)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, ee, db))\n",
    "\n",
    "#println(\"only nouns  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), ee, db))\n",
    "#println(\"only verbs  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), ee, db))\n",
    "#println(\"only adjecti: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), ee, db))\n",
    "#println(\"only adverbs: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), ee, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge_MFS(challenges, solutions, db))\n",
    "\n",
    "#println(\"only nouns  : \\t\\t\", evalute_on_wsd_challenge_MFS(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), db))\n",
    "#println(\"only verbs  : \\t\\t\", evalute_on_wsd_challenge_MFS(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), db))\n",
    "#println(\"only adjecti: \\t\\t\", evalute_on_wsd_challenge_MFS(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), db))\n",
    "#println(\"only adverbs: \\t\\t\", evalute_on_wsd_challenge_MFS(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), db))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Zero shot WSI for 1 shot WSD\n",
    "using Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rr = FixedWordSenseEmbedding(ee.dimension, random_inited, huffman_tree; initial_nsenses=50)\n",
    "rr.corpus_size = ee.corpus_size\n",
    "rr.distribution = ee.distribution\n",
    "rr.codebook = ee.codebook\n",
    "rr.classification_tree = ee.classification_tree\n",
    "Training.initialize_embedding(rr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rr.embedding[\"us\"] |> length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, rr, db))\n",
    "\n",
    "println(\"only nouns  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), rr, db))\n",
    "println(\"only verbs  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), rr, db))\n",
    "println(\"only adjecti: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), rr, db))\n",
    "println(\"only adverbs: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), rr, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hh = deepcopy(rr)\n",
    "for word in keys(rr.embedding)\n",
    "    append!(hh.embedding[word], ee.embedding[word])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, hh, db))\n",
    "\n",
    "println(\"only nouns  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), hh, db))\n",
    "println(\"only verbs  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), hh, db))\n",
    "println(\"only adjecti: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), hh, db))\n",
    "println(\"only adverbs: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), hh, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapping_corpus = load_semcor(\"data/corpora/semcor2.1/brown1/tagfiles/\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "identify (generic function with 1 method)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function identify(taggedword::TaggedWord, wn::DB,ee)\n",
    "    pos = pennPOStoWordNetPOS(taggedword.pos)\n",
    "    wn_sensekeys = sensekeys(wn, wn[pos, taggedword.lemma])\n",
    "    nsenses = length(wn_sensekeys)\n",
    "    \n",
    "    target_wnsn = findfirst(wn_sensekeys .== sensekey(taggedword))\n",
    "    target_wnsn==0 && throw(KeyError(\"$(sensekey(taggedword)) not in  $(wn_sensekeys).\\n\\nIssue is with $taggedword\"))\n",
    "\n",
    "    wvs = all_word_sense_vectors(ee,taggedword.word)\n",
    "    length(wvs) == 0 && throw(KeyError(\"No embedding for $(taggedword.word); skipping\"))\n",
    "    \n",
    "    target_wnsn, nsenses, pos, wvs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agirresAlignment (generic function with 2 methods)"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function agirresAlignment(ee, wn::DB, mapping_corpus, hard=true)\n",
    "    maps = Dict{Tuple{String, String, Char}, Matrix{Float32}}()\n",
    "    freqs = Dict{Tuple{String, String, Char}, Vector{Int}}()\n",
    "    \n",
    "    function proc_word(word::TaggedWord, sentence::TaggedSentence)\n",
    "    end\n",
    "    \n",
    "    function proc_word(taggedword::SenseAnnotatedWord, sentence::TaggedSentence)\n",
    "        local target_wnsn, nsenses, pos, wvs\n",
    "        try\n",
    "            target_wnsn, nsenses, pos, wvs = identify(taggedword, wn::DB,ee)\n",
    "        catch err\n",
    "            typeof(err)<:KeyError ||rethrow(ee)\n",
    "            return\n",
    "        end\n",
    "        ########\n",
    "        \n",
    "        map = get!(maps, (taggedword.word, taggedword.lemma, pos)) do\n",
    "            zeros(length(wvs), nsenses)\n",
    "        end\n",
    "        \n",
    "        freq = get!(freqs, (taggedword.word, taggedword.lemma, pos)) do \n",
    "            zeros(Int, nsenses)\n",
    "        end\n",
    "        \n",
    "        \n",
    "        context = lowercase.(strip_tags(sentence))\n",
    "        wv_probs = general_wsd(ee,context, wvs)\n",
    "        @assert(length(wv_probs) == length(wvs))\n",
    "        @assert sum(wv_probs) ≈ 1f0\n",
    "        @assert !any(isnan.(wv_probs))\n",
    "        freq[target_wnsn] += 1\n",
    "        if hard\n",
    "            map[indmax(wv_probs),target_wnsn] += 1\n",
    "        else\n",
    "            @assert(length(map[:,target_wnsn]) == length(wv_probs),\n",
    "            \"$(length(map[:,target_wnsn])) != $(length(wv_probs)) for \\\"$(taggedword.lemma)\\\"\"           \n",
    "            )\n",
    "            map[:,target_wnsn] += wv_probs\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "    @showprogress for sentence in mapping_corpus\n",
    "        for word in sentence\n",
    "            proc_word(word, sentence)\n",
    "        end\n",
    "    end\n",
    " \n",
    "    \n",
    "    @showprogress for ((word, lem, pos), freq) in freqs\n",
    "        mm = maps[(word, lem, pos)]\n",
    "        mm ./= freq'\n",
    "        mm[isnan.(mm)] = 0f0 #NaNs are just frequency 0 items\n",
    "        @assert(all(isapprox.(sum(mm, 1), 1f0; atol=1f-5) \n",
    "                    | isapprox.(sum(mm, 1), 0f0; atol=1f-5)), \n",
    "                    \"($word , $pos) not sum to one, $(sum(mm,1))\")\n",
    "    end\n",
    "\n",
    "    maps, freqs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evalute_on_wsd_challenge (generic function with 3 methods)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mapped_wsd(challenge, ee, wn::DB, maps::Associative{Tuple{String, String, Char}, Matrix{Float32}})\n",
    "    try\n",
    "        lem = wn[challenge.lemma, challenge.pos]\n",
    "        target_synsets = synsets(wn, lem)\n",
    "        \n",
    "        wvs = all_word_sense_vectors(ee, challenge.word)\n",
    "        length(wvs) > 0 || throw(KeyError(\"No embeddings for \"*challenge.word))\n",
    "        \n",
    "        wv_probs = general_wsd(ee,challenge.context, wvs)\n",
    "\n",
    "        \n",
    "        mm = copy(maps[challenge.word, challenge.lemma, challenge.pos])\n",
    "        mm.+=1.0\n",
    "        probs_of_sense = mm'*wv_probs\n",
    "        probs_of_sense./=sum(probs_of_sense)\n",
    "\n",
    "        \n",
    "        sense_index= indmax(probs_of_sense)        \n",
    "        synset = target_synsets[sense_index]\n",
    "        sk = sensekey(wn, synset, lem)\n",
    "        Nullable(sk)\n",
    "    catch ex\n",
    "        isa(ex, KeyError) || rethrow(ex)\n",
    "        most_frequent_sense(challenge, wn) #Fallback on MFS\n",
    "    end\n",
    "end\n",
    "\n",
    "function evalute_on_wsd_challenge(challenges, solutions, ee, wn::DB, maps)\n",
    "    method = challenge -> mapped_wsd(challenge, ee, wn, maps)\n",
    "    evalute_on_wsd_challenge(challenges, solutions, method)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function AdaGramCompat.all_word_sense_vectors(ee::WordSenseEmbedding, word)\n",
    "    get(ee.embedding, word, Vector{Float32}[])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100%|█████████████████████████████████████████| Time: 0:31:10\n",
      "Progress: 100%|█████████████████████████████████████████| Time: 0:00:02\n"
     ]
    }
   ],
   "source": [
    "maps_hard, freqs  = agirresAlignment(am, db, mapping_corpus, true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100%|█████████████████████████████████████████| Time: 0:00:49\n",
      "notattempted = 0\n",
      "overall: \t\t(0.6897311591009255,0.6897311591009255,0.6897311591009255)\n"
     ]
    }
   ],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, am, db, maps_soft))\n",
    "\n",
    "#maps_soft(am) overall: \t\t(0.6897311591009255,0.6897311591009255,0.6897311591009255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "leastSquaresAlignment (generic function with 2 methods)"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function matrix_from_rows{T}(Xs::Vector{Vector{T}})\n",
    "    ret = Matrix{T}(length(Xs), length(first(Xs)))\n",
    "    for (ii,row) in enumerate(Xs)\n",
    "        @inbounds ret[ii,:] = row\n",
    "    end\n",
    "    ret\n",
    "end\n",
    "\n",
    "function bestfact!(x)\n",
    "    if size(x,1)<size(x,2)\n",
    "        svdfact!(x)\n",
    "    elseif size(x,1)>=size(x,2)\n",
    "        qrfact!(x, Val{true})\n",
    "    #else\n",
    "    #    @assert(size(x,1)==size(x,2))\n",
    "    #    lufact!(x)\n",
    "    end\n",
    "end\n",
    "\n",
    "function leastSquaresAlignment(ee, wn::DB, mapping_corpus, hard=true)\n",
    "    vars = Dict{Tuple{String, String, Char}, Tuple{Vector{Vector{Float32}}, Vector{Vector{Float32}}}}()\n",
    "    \n",
    "    function proc_word(word::TaggedWord, sentence::TaggedSentence)\n",
    "    end\n",
    "    \n",
    "    function proc_word(taggedword::SenseAnnotatedWord, sentence::TaggedSentence)\n",
    "        local target_wnsn, nsenses, pos, wvs\n",
    "        try\n",
    "            target_wnsn, nsenses, pos, wvs = identify(taggedword, wn::DB,ee)\n",
    "        catch err\n",
    "            typeof(err)<:KeyError ||rethrow(ee)\n",
    "            return\n",
    "        end\n",
    "        ########\n",
    "        \n",
    "        Xs, Ys = get!(vars, (taggedword.word, taggedword.lemma, pos)) do\n",
    "            Vector{Vector{Float32}}(), Vector{Vector{Float32}}()\n",
    "        end    \n",
    "        \n",
    "        context = lowercase.(strip_tags(sentence))\n",
    "        wv_probs = general_wsd(ee,context, wvs)\n",
    "        @assert(length(wv_probs) == length(wvs))\n",
    "        @assert sum(wv_probs) ≈ 1f0\n",
    "        @assert !any(isnan.(wv_probs))\n",
    "        push!(Xs, wv_probs)\n",
    "        y = zeros(nsenses)\n",
    "        y[target_wnsn] = 1f0\n",
    "        push!(Ys, y)\n",
    "    end\n",
    "    \n",
    "    @showprogress for sentence in mapping_corpus\n",
    "        for word in sentence\n",
    "            proc_word(word, sentence)\n",
    "        end\n",
    "    end\n",
    " \n",
    "    maps = Dict{Tuple{String, String, Char}, Matrix{Float32}}()\n",
    "    \n",
    "    @showprogress for (key, (Xs,Ys)) in vars\n",
    "        X = matrix_from_rows(Xs)\n",
    "        Y = matrix_from_rows(Ys)\n",
    "        try \n",
    "            maps[key] = bestfact!(X)\\Y\n",
    "        catch err\n",
    "            @show X\n",
    "            println(\"-\"^30)\n",
    "            @show Y\n",
    "            rethrow(err)\n",
    "        end    \n",
    "    end\n",
    "\n",
    "    maps\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:   9%|████                                     |  ETA: 0:23:18"
     ]
    }
   ],
   "source": [
    "maps_ls_am = leastSquaresAlignment(am, db, mapping_corpus);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, am, db, maps_ls_am))\n",
    "#am, db, maps_ls_am overall: \t\t(0.753195240193918,0.753195240193918,0.7531952401939179)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30×3 Array{Float32,2}:\n",
       " 0.0333333  0.0249207  0.0333333\n",
       " 0.0333333  0.0316945  0.0333333\n",
       " 0.0333333  0.0217652  0.0333333\n",
       " 0.0333333  0.0187715  0.0333333\n",
       " 0.0333333  0.0274698  0.0333333\n",
       " 0.0333333  0.0310764  0.0333333\n",
       " 0.0333333  0.0208254  0.0333333\n",
       " 0.0333333  0.0256236  0.0333333\n",
       " 0.0333333  0.0362388  0.0333333\n",
       " 0.0333333  0.0363548  0.0333333\n",
       " 0.0333333  0.0362836  0.0333333\n",
       " 0.0333333  0.036307   0.0333333\n",
       " 0.0333333  0.0362278  0.0333333\n",
       " 0.0333333  0.0360908  0.0333333\n",
       " 0.0333333  0.0361495  0.0333333\n",
       " 0.0333333  0.0360063  0.0333333\n",
       " 0.0333333  0.0361296  0.0333333\n",
       " 0.0333333  0.0361088  0.0333333\n",
       " 0.0333333  0.0364005  0.0333333\n",
       " 0.0333333  0.0364899  0.0333333\n",
       " 0.0333333  0.0363374  0.0333333\n",
       " 0.0333333  0.0365099  0.0333333\n",
       " 0.0333333  0.0364396  0.0333333\n",
       " 0.0333333  0.0362374  0.0333333\n",
       " 0.0333333  0.0364378  0.0333333\n",
       " 0.0333333  0.0362699  0.0333333\n",
       " 0.0333333  0.0362849  0.0333333\n",
       " 0.0333333  0.0362905  0.0333333\n",
       " 0.0333333  0.0360832  0.0333333\n",
       " 0.0333333  0.0361746  0.0333333"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm=drop(maps_ls_am,5) |> first |> last\n",
    "mm+=1\n",
    "mm./=sum(mm,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function hybrid_wsd(challenge, ee, wn::DB, maps::Associative{Tuple{String, String, Char}, Matrix{Float32}})\n",
    "    try\n",
    "        lem = wn[challenge.lemma, challenge.pos]\n",
    "        target_synsets = synsets(wn, lem)\n",
    "        \n",
    "        wvs = all_word_sense_vectors(ee, challenge.word)\n",
    "        length(wvs) > 0 || throw(KeyError(\"No embeddings for \"*challenge.word))\n",
    "        \n",
    "        wv_probs = general_wsd(ee,challenge.context, wvs)\n",
    "\n",
    "        \n",
    "        mm = copy(maps[challenge.word, challenge.lemma, challenge.pos])\n",
    "        mm.+=1.0\n",
    "        mm/=sum(mm,1)\n",
    "        probs_of_sense = mm'*wv_probs\n",
    "        probs_of_sense./=sum(probs_of_sense)\n",
    "\n",
    "        \n",
    "        sense_index= indmax(probs_of_sense)        \n",
    "        synset = target_synsets[sense_index]\n",
    "        sk = sensekey(wn, synset, lem)\n",
    "        Nullable(sk)\n",
    "    catch ex\n",
    "        isa(ex, KeyError) || rethrow(ex)\n",
    "        most_frequent_sense(challenge, wn) #Fallback on MFS\n",
    "    end\n",
    "end\n",
    "\n",
    "function evalute_on_wsd_challenge_hybrid(challenges, solutions, ee, wn::DB, maps)\n",
    "    method = challenge -> hybrid_wsd(challenge, ee, wn, maps)\n",
    "    evalute_on_wsd_challenge(challenges, solutions, method)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge_hybrid(challenges, solutions, am, db, maps_ls_am))\n",
    "#am, db, maps_ls_am overall: \t\t(0.753195240193918,0.753195240193918,0.7531952401939179)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0-rc0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
