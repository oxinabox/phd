{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using WordNet\n",
    "using WordEmbeddings, SoftmaxClassifier\n",
    "using Utils\n",
    "using Query\n",
    "using Distances\n",
    "using Iterators\n",
    "using LightXML\n",
    "using JLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordNet.DB"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const WN_PATH=\"data/corpora/WordNet-2.1/\"\n",
    "db = DB(WN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ee = restore(\"models/ss/tokenised_lowercase_WestburyLab.wikicorp.201004_100_i1.jld\");\n",
    "ee = load(\"models/ss/tokenised_lowercase_WestburyLab.wikicorp.201004_100_i1.jld\", \"ee\")\n",
    "\n",
    "#ee = restore(\"models/ss/keep/tokenised_lowercase_WestburyLab.wikicorp.201004_50__m170000000.model\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "punctuation_space_tokenize (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function poormans_tokenize(source)\n",
    "    cleaned = filter(s->!ispunct(s), lowercase(source))\n",
    "    map(string,  split(cleaned))::Vector{SubString{String}}\n",
    "end\n",
    "\n",
    "function punctuation_space_tokenize(source)\n",
    "    preprocced = lowercase(source)\n",
    "    pass1=replace(preprocced,r\"[[:punct:]]*[[:space:]][[:punct:]]*\",\" \")\n",
    "    pass2=replace(pass1,r\"[[:punct:]]*$|^[[:punct:]]*\",\"\")\n",
    "    split(pass2)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sense_keys (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function load_sense_key_map(wn_basedir)\n",
    "    path=joinpath(wn_basedir, \"dict\",\"index.sense\")\n",
    "    sense_key_map = Dict{Tuple{Int64,String},String}()\n",
    "    for line in eachline(path)\n",
    "        full_sense_key, offset_str, sense_num_str, tagcount_str = split(line) \n",
    "        lemma_name = first(split(full_sense_key,'%'))\n",
    "        sense_offset = parse(Int64, offset_str)\n",
    "        index = (sense_offset,lemma_name)\n",
    "        @assert(!haskey(sense_key_map, index))\n",
    "        sense_key_map[index] = full_sense_key\n",
    "    end\n",
    "    sense_key_map\n",
    "end\n",
    "\n",
    "\n",
    "function sense_key(lem::Lemma, ss::Synset,sense_key_map)\n",
    "    sense_key_map[(ss.offset,lem.word)]\n",
    "end\n",
    "function sense_keys(lem::Lemma,sense_key_map)\n",
    "    [sense_key_map[(ss_offset,lem.word)] for ss_offset in lem.synset_offsets ]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sense_key_map = load_sense_key_map(WN_PATH);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{String,1}:\n",
       " \"six%5:00:00:cardinal:00\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sense_keys(db['a',\"six\"],sense_key_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"discover%2:39:03::\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sense_key_map[2134693,\"discover\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "immutable WsdChallenge\n",
    "    id::String\n",
    "    word::String\n",
    "    lemma::String\n",
    "    pos::Char\n",
    "    context::Vector{String}\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "function load_challenges_semeval2007(xml_file=\"data/corpora/wsd/semeval2007_t7/test/eng-coarse-all-words.xml\")\n",
    "    xdoc = parse_file(xml_file)\n",
    "    xroot = root(xdoc)\n",
    "    Task() do\n",
    "        for text_node in child_elements(xroot)\n",
    "            #@show text_node\n",
    "            #text = child_nodes(text_node) |> collect\n",
    "            #println(text)\n",
    "            for sentence_node in child_elements(text_node)\n",
    "                sentence = punctuation_space_tokenize(content(sentence_node))\n",
    "\n",
    "                for lemma_node in child_elements(sentence_node)\n",
    "                    word = content(lemma_node) |> lowercase\n",
    "                    lemma = attribute(lemma_node,\"lemma\")|> lowercase\n",
    "                    pos = first(attribute(lemma_node,\"pos\"))\n",
    "                    id = attribute(lemma_node,\"id\")\n",
    "                    context = filter(w->w!=word, sentence)\n",
    "                    produce(WsdChallenge(id, word, lemma, pos, context))\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "challenges = load_challenges_semeval2007() |> collect;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "immutable WsdSolution\n",
    "    id\n",
    "    lemma\n",
    "    pos\n",
    "    solutions\n",
    "end\n",
    "only_of_pos(data, pos) = filter(d->d.pos==pos, data)\n",
    "\n",
    "function load_solutions(key_file=\"data/corpora/wsd/semeval2007_t7/key/dataset21.test.key\")\n",
    "    map(eachline(key_file)) do line\n",
    "        line_data, comment = split(line,\"!!\")\n",
    "        fields = split(line_data)\n",
    "        doc_id = fields[1]\n",
    "        instance_id = fields[2]\n",
    "        solutions = fields[3:end]\n",
    "        \n",
    "        lemma,pos = match(r\"lemma=(.*)#(.)\", comment).captures\n",
    "        WsdSolution(instance_id, lemma, pos[1], solutions)\n",
    "    end\n",
    "end\n",
    "\n",
    "solutions = load_solutions();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WsdSolution(\"d001.s001.t002\",\"Ill\",'a',SubString{String}[\"ill%3:00:01::\"])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solutions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sense_frequency (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sense_frequency(ss::Synset, lem::Lemma)\n",
    "    lem_count = lem.tagsense_count\n",
    "    #lem_count+0.1(sum(values(ss.word_counts))-lem_count) #Do not use Synset's other counts. It owrks out worse\n",
    "    lem_count\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lexically_informed_embeddings (generic function with 2 methods)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function normal_probs(logprobs::Vector)\n",
    "    ret = copy(logprobs)\n",
    "    max_lp = maximum(logprobs)\n",
    "    ret.-=max_lp #Bring closer to zero\n",
    "    map!(exp,ret)\n",
    "    denom = sum(ret)\n",
    "    ret./=denom\n",
    "    ret\n",
    "end\n",
    "function weighted_average(logprobs, embeddings)\n",
    "    ret = zeros(first(embeddings))\n",
    "    for (weight, embedding) in zip(normal_probs(logprobs), embeddings)\n",
    "        ret.+= weight.*embedding\n",
    "    end\n",
    "    ret\n",
    "end\n",
    "\n",
    "\n",
    "function synthesize_embedding(ee,context, phrase)\n",
    "    words = split(phrase,\" \")\n",
    "    wvs = vcat((get(ee.embedding,w,Vector{Float32}[]) for w in words)...)\n",
    "    if length(wvs) == 0\n",
    "            throw(KeyError(\"None of $words have embeddings\"))\n",
    "    end\n",
    "    logprobs = [Query.logprob_of_context(ee, context,wv; skip_oov=true, normalise_over_length=true) for wv in wvs]\n",
    "    weighted_average(logprobs, wvs)\n",
    "end\n",
    "\n",
    "\n",
    "all_identical(col) = length(col)==1 || !any(x->x!=col[1],col[2:end])\n",
    "\n",
    "lexically_informed_embeddings(db,ee, lemma_word, pos) = lexically_informed_embeddings(db,ee, lemma_word, lemma_word, pos)\n",
    "function lexically_informed_embeddings(db,ee, word,lemma_word, pos)\n",
    "    \n",
    "    lemma = db[pos, lemma_word]\n",
    "    target_synsets::Vector{Synset} = synsets(db, lemma)\n",
    "         \n",
    "    embeddings = Vector{Vector{Float32}}(length(target_synsets))\n",
    "    for (ii,synset) in enumerate(target_synsets)\n",
    "        context::Vector{SubString{String}} = collect(filter!(w->w!=word, punctuation_space_tokenize(synset.gloss)))\n",
    "        embeddings[ii] = synthesize_embedding(ee,context, word)\n",
    "    end\n",
    "        \n",
    "    \n",
    "    if all_identical(embeddings)\n",
    "        #Use MCWS\n",
    "        score, index = findmax(sense_frequency(ss,lemma) for ss in target_synsets)\n",
    "        target_synsets=[target_synsets[index]] # Throw out others\n",
    "        embeddings=[embeddings[1]]\n",
    "    end\n",
    "        \n",
    "    embeddings,target_synsets,lemma\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "most_frequent_sense (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function supervised_wsd(challenge::WsdChallenge, ee::WordSenseEmbedding, wn::DB, sense_key_map)\n",
    "    try\n",
    "        embeddings,target_synsets,lemma = lexically_informed_embeddings(wn,ee,\n",
    "                                                            challenge.word, \n",
    "                                                            challenge.lemma, challenge.pos)\n",
    "        #sense_index = Query.WSD(ee, embeddings, challenge.context; skip_oov=true)\n",
    "        logprobs_of_context = [logprob_of_context(ee,challenge.context, wv; skip_oov=true, normalise_over_length=true) \n",
    "                                    for wv in embeddings] \n",
    "      \n",
    "        sense_index= indmax(logprobs_of_context)\n",
    "\n",
    "        \n",
    "        synset = target_synsets[sense_index]\n",
    "        sk = sense_key(lemma, synset, sense_key_map)\n",
    "        Nullable(sk)\n",
    "    catch ex\n",
    "        isa(ex, KeyError) || rethrow(ex)\n",
    "        Nullable{String}()\n",
    "    end\n",
    "end\n",
    "\n",
    "function most_frequent_sense(challenge, wn::DB, sense_key_map)\n",
    "    try\n",
    "        lemma = wn[challenge.pos, challenge.lemma]\n",
    "        target_synsets::Vector{Synset} = synsets(db, lemma)\n",
    "        \n",
    "        sense_freqs =  Float32[sense_frequency(ss,lemma) for ss in target_synsets]      \n",
    "        sense_index= indmax(sense_freqs)\n",
    "        synset = target_synsets[sense_index]\n",
    "        \n",
    "        sk = sense_key(lemma, synset, sense_key_map)\n",
    "        Nullable(sk)\n",
    "    catch ex\n",
    "        isa(ex, KeyError) || rethrow(ex)\n",
    "        Nullable{String}()\n",
    "    end\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evalute_on_wsd_challenge_MFS (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function evalute_on_wsd_challenge(challenges, solutions, method)\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    notattempted = 0\n",
    "    for (challenge, ground_solution) in zip(challenges, solutions)\n",
    "        assert(challenge.id == ground_solution.id)\n",
    "        output_sense = method(challenge)\n",
    "        if isnull(output_sense)\n",
    "            notattempted+=1\n",
    "        else\n",
    "            if get(output_sense) âˆˆ ground_solution.solutions\n",
    "                correct+=1\n",
    "            else\n",
    "                incorrect+=1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    precision = correct/(correct+incorrect)\n",
    "    recall = correct/(correct+incorrect+notattempted)\n",
    "    f1 = (2*precision*recall) / (precision+recall)\n",
    "    return precision, recall, f1\n",
    "end\n",
    "    \n",
    "    \n",
    "function evalute_on_wsd_challenge(challenges, solutions, ee::WordSenseEmbedding, wn::DB, sense_key_map)\n",
    "    method = challenge -> supervised_wsd(challenge, ee, wn, sense_key_map)\n",
    "    evalute_on_wsd_challenge(challenges, solutions, method)\n",
    "end\n",
    "\n",
    "\n",
    "function evalute_on_wsd_challenge_MFS(challenges, solutions, wn::DB, sense_key_map)\n",
    "    method = challenge -> most_frequent_sense(challenge, wn, sense_key_map)\n",
    "    evalute_on_wsd_challenge(challenges, solutions, method)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall: \t\t(0.6486486486486487,0.6346408109299251,0.6415682780129205)\n",
      "only nouns  : \t\t(0.6593001841620626,0.6462093862815884,0.6526891522333637)\n",
      "only verbs  : \t\t(0.5785837651122625,0.5668358714043993,0.5726495726495726)\n",
      "only adjecti: \t\t(0.6762177650429799,0.6519337016574586,0.6638537271448665)\n",
      "only adverbs: \t\t(0.7427184466019418,0.7355769230769231,0.7391304347826086)\n"
     ]
    }
   ],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, ee, db, sense_key_map))\n",
    "\n",
    "println(\"only nouns  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), ee, db, sense_key_map))\n",
    "println(\"only verbs  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), ee, db, sense_key_map))\n",
    "println(\"only adjecti: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), ee, db, sense_key_map))\n",
    "println(\"only adverbs: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), ee, db, sense_key_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall: \t\t(0.7783164389598942,0.7783164389598942,0.7783164389598942)\n",
      "only nouns  : \t\t(0.7653429602888087,0.7653429602888087,0.7653429602888087)\n",
      "only verbs  : \t\t(0.7529610829103215,0.7529610829103215,0.7529610829103215)\n",
      "only adjecti: \t\t(0.8038674033149171,0.8038674033149171,0.8038674033149171)\n",
      "only adverbs: \t\t(0.875,0.875,0.875)\n"
     ]
    }
   ],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge_MFS(challenges, solutions, db, sense_key_map))\n",
    "\n",
    "println(\"only nouns  : \\t\\t\", evalute_on_wsd_challenge_MFS(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), db, sense_key_map))\n",
    "println(\"only verbs  : \\t\\t\", evalute_on_wsd_challenge_MFS(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), db, sense_key_map))\n",
    "println(\"only adjecti: \\t\\t\", evalute_on_wsd_challenge_MFS(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), db, sense_key_map))\n",
    "println(\"only adverbs: \\t\\t\", evalute_on_wsd_challenge_MFS(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), db, sense_key_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Zero shot WSI for 1 shot WSD\n",
    "using Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rr = FixedWordSenseEmbedding(ee.dimension, random_inited, huffman_tree; initial_nsenses=1)\n",
    "rr.distribution = ee.distribution\n",
    "rr.codebook = ee.codebook\n",
    "rr.classification_tree = ee.classification_tree\n",
    "Training.initialize_embedding(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rr.initial_nsenses=100\n",
    "Training.initialize_embedding(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, rr, db, sense_key_map))\n",
    "\n",
    "println(\"only nouns  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), rr, db, sense_key_map))\n",
    "println(\"only verbs  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), rr, db, sense_key_map))\n",
    "println(\"only adjecti: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), rr, db, sense_key_map))\n",
    "println(\"only adverbs: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), rr, db, sense_key_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hh = deepcopy(rr)\n",
    "for word in keys(rr.embedding)\n",
    "    append!(hh.embedding[word], ee.embedding[word])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println(\"overall: \\t\\t\", evalute_on_wsd_challenge(challenges, solutions, hh, db, sense_key_map))\n",
    "\n",
    "println(\"only nouns  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'n'), only_of_pos(solutions,'n'), hh, db, sense_key_map))\n",
    "println(\"only verbs  : \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'v'), only_of_pos(solutions,'v'), hh, db, sense_key_map))\n",
    "println(\"only adjecti: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'a'), only_of_pos(solutions,'a'), hh, db, sense_key_map))\n",
    "println(\"only adverbs: \\t\\t\", evalute_on_wsd_challenge(only_of_pos(challenges,'r'), only_of_pos(solutions,'r'), hh, db, sense_key_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using JLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0-rc0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
