{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Iterators\n",
    "using DataStructures\n",
    "macro printval(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\" = \", $ee)))\n",
    "end\n",
    "\n",
    "macro pz(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\"\\t\\t\",typeof($ee), \"\\t\", size($ee))))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Pipe\n",
    "push!(LOAD_PATH, \"../word-embeddings2\")\n",
    "using WordEmbeddings\n",
    "we = @pipe load_word2vec_embeddings(\"../../Resources/example_code/word2vec/GoogleNews-vectors-negative300.bin\", 15000) |> WE(_...);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_data (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function load_data(filepath)\n",
    "    lines = open(filepath) do filehandle\n",
    "        map(eachline(filehandle)) do line\n",
    "            fields = split(line)\n",
    "            (fields[1], fields[2:end])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    data = String[]\n",
    "    labels = String[]\n",
    "    \n",
    "    for (hyper, hypos) in lines\n",
    "        if has_word(we,hyper) # Skip ones we don't have \n",
    "            hypos = @pipe hypos |> filter(w->has_word(we,w), _)\n",
    "            append!(data,hypos)\n",
    "            @pipe hyper |> fill(_,length(hypos)) |> append!(labels, _)\n",
    "        end\n",
    "    end \n",
    "    data, labels\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "300x31385 Array{Float64,2}:\n",
       " -0.0399788    0.0189571    0.0359512    …  -0.0287392   -0.0773216 \n",
       " -0.0084166   -0.0127032    0.0252146        0.0443861    0.0428814 \n",
       " -0.108013     0.0163187   -0.0810122       -0.114957    -0.0315702 \n",
       "  0.00257539  -0.0613662    0.132092        -0.0392769    0.0131683 \n",
       " -0.00176442   0.029706     0.032535         0.0044905    0.0167136 \n",
       "  0.0355952    0.0242338    0.0702756    …  -0.157746     0.0877887 \n",
       " -0.0121865   -0.0101137    0.0113059       -0.0788731    0.148565  \n",
       " -0.114325    -0.0836457   -0.0475011       -0.049176    -0.0519979 \n",
       " -0.0264772    0.0512037    0.0257027        0.0365626    0.0985934 \n",
       " -0.00132605   0.00195434   0.0178943        0.00502936   0.1695    \n",
       " -0.110819     0.175109     0.0291188    …  -0.0309745   -0.0709062 \n",
       " -0.0866208   -0.118042     0.00825576      -0.111125    -0.106022  \n",
       "  0.06593     -0.0212046    0.0150474       -0.0130125    0.040349  \n",
       "  ⋮                                      ⋱                          \n",
       " -0.0429597   -0.0478813    0.100859        -0.154553    -0.00882107\n",
       " -0.1522       0.114133     0.0692996        0.0456634    0.112099  \n",
       "  0.149395    -0.0898996    0.044573     …   0.0139704   -0.0379855 \n",
       " -0.046642    -0.0164164   -0.000338059      0.0285795   -0.0129151 \n",
       "  0.00749603  -0.0934174   -0.135346        -0.0625876    0.00595106\n",
       "  0.0422583   -0.0996713    0.0138274        0.0807891   -0.0130839 \n",
       " -0.0112221    0.020716     0.0171622        0.0970746   -0.015363  \n",
       " -0.00583025  -0.0160256   -0.0816629    …   0.0207561   -0.0584132 \n",
       " -0.0248991   -0.104753     0.0637686       -0.0352853    0.10332   \n",
       "  0.0331404   -0.0977169    0.0240759       -0.00375206  -0.00797695\n",
       " -0.0736452    0.011726    -0.0813375       -0.0201973    0.0393361 \n",
       " -0.0540065    0.0353735   -0.03953         -0.0450247   -0.116826  ,\n",
       "\n",
       "300x31385 Array{Float64,2}:\n",
       "  0.00482386    0.0352818     0.0625785   …  -0.0322475    0.0999165 \n",
       " -0.000362343  -0.00887772    0.0130186       0.0596763    0.0318752 \n",
       "  0.0290317    -0.00252013   -0.0185501       0.0248342   -0.0462803 \n",
       "  0.00161533    0.11226      -0.00122224      0.00290737  -0.00720257\n",
       " -0.101965     -0.0989722     0.0299483       0.0309501   -0.0373921 \n",
       "  0.0640821    -0.000366922  -0.0153094   …  -0.0956304    0.0671218 \n",
       "  0.0715171     0.0142043    -0.0195558       0.0893291   -0.0471998 \n",
       "  0.0263763    -0.0893499    -0.0344182      -0.0485565   -0.0185428 \n",
       "  0.0885112     0.00592802    0.0312893       0.0700548    0.059153  \n",
       " -0.0849708     0.0407802     0.104596        0.0195523   -0.0143285 \n",
       " -0.0757656     0.0497152    -0.0274899   …  -0.00261779   0.0116467 \n",
       " -0.0934678    -0.0971394    -0.109065        0.0670895   -0.216997  \n",
       " -0.047442      0.0241703    -0.00173907     -0.103044    -0.0049805 \n",
       "  ⋮                                       ⋱                          \n",
       "  0.016109     -0.0175263     0.10102         0.118611     0.0332544 \n",
       "  0.0453177     0.00270627   -0.0502863       0.037622    -0.00427174\n",
       "  0.0467339    -0.160372     -0.0621315   …  -0.14011      0.0999165 \n",
       " -0.0293857     0.0966812     0.0095544       0.00280312   0.05854   \n",
       "  0.0704549    -0.0435295    -0.0150859       0.0214983    0.0347869 \n",
       "  0.0430164     0.021192     -0.0235787       0.0315061   -0.0242129 \n",
       " -0.00420428   -0.0646069    -0.0337477       0.0689428    0.0536361 \n",
       "  0.0251372     0.0396347     0.0346417   …   0.0137144   -0.0297298 \n",
       "  0.0222163     0.0272632     0.0181031      -0.0307648   -0.0701867 \n",
       " -0.0644362    -0.0189009    -0.0116217      -0.0315061   -0.0496517 \n",
       " -0.031864      0.0529227    -0.0167621       0.00544407   0.0208415 \n",
       " -0.0105771    -0.0943902    -0.021232       -0.00634756  -0.0833659 )"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function load_and_embed(path)\n",
    "    data_str, labels_str = load_data(path)\n",
    "    data = eval_word_embeddings(we,data_str)\n",
    "    labels =  eval_word_embeddings(we,labels_str);\n",
    "    order = randperm(size(data,2));\n",
    "    data[:,order], labels[:,order];\n",
    "end\n",
    "\n",
    "data, labels = load_and_embed(\"HyponymGen/hyponym-generation-noun-train.txt\")\n",
    "data_valid, labels_valid = load_and_embed(\"HyponymGen/hyponym-generation-noun-dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type NN \n",
    "    Ws:: Vector{Matrix{Float64}} \n",
    "    bs:: Vector{Vector{Float64}} \n",
    "end\n",
    "\n",
    "function NN(layer_sizes::Vector{Int}, var=0.01)\n",
    "    Ws = [var*randn(layer_sizes[ii], layer_sizes[ii-1]) for ii in 2:length(layer_sizes)]\n",
    "    bs = [var*randn(layer_sizes[ii]) for ii in 2:length(layer_sizes)]\n",
    "    NN(Ws, bs)\n",
    "end\n",
    "\n",
    "function NN_empty(layer_sizes::Vector{Int})\n",
    "    Ws = [Array(Float64,(layer_sizes[ii], layer_sizes[ii-1])) for ii in 2:length(layer_sizes)]\n",
    "    bs = [Array(Float64,layer_sizes[ii]) for ii in 2:length(layer_sizes)]\n",
    "    NN(Ws, bs)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function feedfoward(nn::NN, xs::AbstractMatrix{Float64})\n",
    "    as = AbstractMatrix{Float64}[ [NaN]' for _ in 1:length(nn.Ws)+1 ] \n",
    "   \n",
    "    as[1] =  xs\n",
    "    for ii in 1:length(nn.Ws)\n",
    "        as[ii+1] = tanh(nn.Ws[ii]*as[ii] .+ nn.bs[ii])\n",
    "    end \n",
    "    as[end], as\n",
    "end\n",
    "\n",
    "function backprop(nn::NN, ys::AbstractMatrix{Float64}, as::Vector{Matrix{Float64}}, loss_grad= as[end]-ys)\n",
    "    function dZ(z)\n",
    "        1.0-z.^2 \n",
    "    end\n",
    "    Δbs = Vector{Float64}[ [NaN] for _ in 1:length(nn.Ws) ] \n",
    "    ΔWs = Matrix{Float64}[ [NaN]' for _ in 1:length(nn.Ws) ] \n",
    "    ŷs = as[end]\n",
    "    δ_above = loss_grad.*dZ(ŷs)\n",
    "    for ii in length(nn.Ws):-1:1\n",
    "        Δbs[ii] = sum(δ_above,2)[:]\n",
    "        ΔWs[ii] = (δ_above * as[ii]')\n",
    "        δ_above = (nn.Ws[ii]'*δ_above) .*dZ(as[ii])\n",
    "    end\n",
    "   \n",
    "    ΔWs,Δbs\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function loss(ŷs, ys)\n",
    "    sum(0.5*(ys-ŷs).^2,2) |> sum\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colnorm(A) = [norm(A[:,ii]) for ii in 1:size(A,2)]\n",
    "coldot(A,B) = [A[:,ii]⋅B[:,ii] for ii in 1:size(A,2)]\n",
    "\n",
    "function cosine_sim(ys,ts)\n",
    "    coldot(ys,ts)./(colnorm(ys).*colnorm(ts))\n",
    "end\n",
    "\n",
    "function loss_cosine(ŷs, ys)\n",
    "    0.5(1.0-cosine_sim(ŷs, ys)).^2 |> mean\n",
    "end\n",
    "\n",
    "function loss_diff_cosine(ys, ts)\n",
    "    df = similar(ys)\n",
    "    for jj in size(df,2)\n",
    "        tjs = ts[:,jj]\n",
    "        yjs = ys[:,jj]\n",
    "        \n",
    "        normprod = norm(tjs)*norm(yjs)\n",
    "        df[:,jj] = tjs./normprod + abs(yjs).*norm(tjs)./(normprod.^3)\n",
    "    end\n",
    "    \n",
    "    df.*(1.0-cosine_sim(ys,ts))'\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09-Jun 16:02:25:INFO:root:Constructing net nym-train on CPUBackend...\n",
      "09-Jun 16:02:25:INFO:root:Topological sorting 4 layers...\n",
      "09-Jun 16:02:25:INFO:root:Setup layers...\n",
      "09-Jun 16:02:25:INFO:root:Network constructed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "************************************************************\n",
       "          NAME: nym-train\n",
       "       BACKEND: CPUBackend\n",
       "  ARCHITECTURE: 4 layers\n",
       "............................................................\n",
       " *** MemoryDataLayer(train-data)\n",
       "    Outputs ---------------------------\n",
       "          data: Blob(300 x 10)\n",
       "         label: Blob(300 x 10)\n",
       "............................................................\n",
       " *** InnerProductLayer(ip1)\n",
       "    Inputs ----------------------------\n",
       "          data: Blob(300 x 10)\n",
       "    Outputs ---------------------------\n",
       "           ip1: Blob(500 x 10)\n",
       "............................................................\n",
       " *** InnerProductLayer(ip2)\n",
       "    Inputs ----------------------------\n",
       "           ip1: Blob(500 x 10)\n",
       "    Outputs ---------------------------\n",
       "           ip2: Blob(300 x 10)\n",
       "............................................................\n",
       " *** SquareLossLayer(loss)\n",
       "    Inputs ----------------------------\n",
       "           ip2: Blob(300 x 10)\n",
       "         label: Blob(300 x 10)\n",
       "************************************************************\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Mocha\n",
    "\n",
    "data_l = MemoryDataLayer(name=\"train-data\",tops=[:data,:label], data=Array[data, labels],batch_size=10)\n",
    "fc1   = InnerProductLayer(name=\"ip1\",output_dim=500,neuron=Neurons.ReLU(),\n",
    "                          bottoms=[:data],tops=[:ip1])\n",
    "fc2   = InnerProductLayer(name=\"ip2\",output_dim=300,neuron=Neurons.ReLU(),\n",
    "                          bottoms=[:ip1], tops=[:ip2])\n",
    "loss_l = SquareLossLayer(name=\"loss\", bottoms=[:ip2,:label])\n",
    "backend = CPUBackend()\n",
    "init(backend)\n",
    "common_layers = [fc1,fc2]\n",
    "net = Net(\"nym-train\", backend, [data_l, common_layers..., loss_l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{CoffeeBreak,1}:\n",
       " CoffeeBreak(TrainingSummary(),100,0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = \"snapshots\"\n",
    "params = SolverParameters(max_iter=10000, regu_coef=0.0005,\n",
    "    mom_policy=MomPolicy.Fixed(0.9),\n",
    "    lr_policy=LRPolicy.Inv(0.01, 0.0001, 0.75),\n",
    "    load_from=exp_dir)\n",
    "solver =  Nesterov(params)\n",
    "\n",
    "setup_coffee_lounge(solver, save_into=\"$exp_dir/statistics.jld\", every_n_iter=1000)\n",
    "\n",
    "# report training progress every 100 iterations\n",
    "add_coffee_break(solver, TrainingSummary(), every_n_iter=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09-Jun 16:02:27:INFO:root:Constructing net nym-valid on CPUBackend...\n",
      "09-Jun 16:02:27:INFO:root:Topological sorting 4 layers...\n",
      "09-Jun 16:02:27:INFO:root:Setup layers...\n",
      "09-Jun 16:02:27:DEBUG:root:InnerProductLayer(ip1): sharing weights and bias\n",
      "09-Jun 16:02:27:DEBUG:root:InnerProductLayer(ip2): sharing weights and bias\n",
      "09-Jun 16:02:27:INFO:root:Network constructed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2-element Array{CoffeeBreak,1}:\n",
       " CoffeeBreak(TrainingSummary(),100,0)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       " CoffeeBreak(ValidationPerformance(************************************************************\n",
       "          NAME: nym-valid\n",
       "       BACKEND: CPUBackend\n",
       "  ARCHITECTURE: 4 layers\n",
       "............................................................\n",
       " *** MemoryDataLayer(validation-data)\n",
       "    Outputs ---------------------------\n",
       "          data: Blob(300 x 1000)\n",
       "         label: Blob(300 x 1000)\n",
       "............................................................\n",
       " *** InnerProductLayer(ip1)\n",
       "    Inputs ----------------------------\n",
       "          data: Blob(300 x 1000)\n",
       "    Outputs ---------------------------\n",
       "           ip1: Blob(500 x 1000)\n",
       "............................................................\n",
       " *** InnerProductLayer(ip2)\n",
       "    Inputs ----------------------------\n",
       "           ip1: Blob(500 x 1000)\n",
       "    Outputs ---------------------------\n",
       "           ip2: Blob(300 x 1000)\n",
       "............................................................\n",
       " *** SquareLossLayer(validation-loss)\n",
       "    Inputs ----------------------------\n",
       "           ip2: Blob(300 x 1000)\n",
       "         label: Blob(300 x 1000)\n",
       "************************************************************\n",
       ",[]),1000,0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_valid_l = MemoryDataLayer(name=\"validation-data\",tops=[:data,:label], data=Array[data_valid, labels_valid],batch_size=1000)\n",
    "accuracy = SquareLossLayer(name=\"validation-loss\",bottoms=[:ip2, :label])\n",
    "valid_net = Net(\"nym-valid\", backend, [data_valid_l, common_layers..., accuracy])\n",
    "add_coffee_break(solver, ValidationPerformance(valid_net), every_n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09-Jun 16:02:33:DEBUG:root:Checking network topology for back-propagation\n",
      "09-Jun 16:02:33:DEBUG:root:Init network nym-train\n",
      "09-Jun 16:02:33:DEBUG:root:Init parameter weight for layer ip1\n",
      "09-Jun 16:02:33:DEBUG:root:Init parameter bias for layer ip1\n",
      "09-Jun 16:02:33:DEBUG:root:Init parameter weight for layer ip2\n",
      "09-Jun 16:02:33:DEBUG:root:Init parameter bias for layer ip2\n",
      "09-Jun 16:02:33:DEBUG:root:Initializing coffee breaks\n",
      "09-Jun 16:02:33:INFO:root:Merging existing coffee lounge statistics in snapshots/statistics.jld\n",
      "09-Jun 16:02:33:DEBUG:root:Init network nym-valid\n",
      "09-Jun 16:02:33:INFO:root:000000 :: TRAIN obj-val = 0.50004484\n",
      "09-Jun 16:02:42:INFO:root:\n",
      "09-Jun 16:02:42:INFO:root:## Performance on Validation Set after 0 iterations\n",
      "09-Jun 16:02:42:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:02:42:INFO:root:  Square-loss (avg over 32000) = 0.5000\n",
      "09-Jun 16:02:42:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:02:42:INFO:root:\n",
      "09-Jun 16:02:42:DEBUG:root:Entering solver loop\n",
      "09-Jun 16:02:44:INFO:root:000100 :: TRAIN obj-val = 0.46581235\n",
      "09-Jun 16:02:45:INFO:root:000200 :: TRAIN obj-val = 0.48912722\n",
      "09-Jun 16:02:46:INFO:root:000300 :: TRAIN obj-val = 0.46722099\n",
      "09-Jun 16:02:47:INFO:root:000400 :: TRAIN obj-val = 0.46571770\n",
      "09-Jun 16:02:49:INFO:root:000500 :: TRAIN obj-val = 0.48038064\n",
      "09-Jun 16:02:50:INFO:root:000600 :: TRAIN obj-val = 0.48120370\n",
      "09-Jun 16:02:51:INFO:root:000700 :: TRAIN obj-val = 0.47919282\n",
      "09-Jun 16:02:53:INFO:root:000800 :: TRAIN obj-val = 0.46486274\n",
      "09-Jun 16:02:54:INFO:root:000900 :: TRAIN obj-val = 0.44947855\n",
      "09-Jun 16:02:55:INFO:root:001000 :: TRAIN obj-val = 0.48729926\n",
      "09-Jun 16:03:04:INFO:root:\n",
      "09-Jun 16:03:04:INFO:root:## Performance on Validation Set after 1000 iterations\n",
      "09-Jun 16:03:04:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:03:04:INFO:root:  Square-loss (avg over 31000) = 0.4909\n",
      "09-Jun 16:03:04:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:03:04:INFO:root:\n",
      "09-Jun 16:03:06:INFO:root:001100 :: TRAIN obj-val = 0.48373766\n",
      "09-Jun 16:03:07:INFO:root:001200 :: TRAIN obj-val = 0.46993290\n",
      "09-Jun 16:03:08:INFO:root:001300 :: TRAIN obj-val = 0.47478033\n",
      "09-Jun 16:03:09:INFO:root:001400 :: TRAIN obj-val = 0.47068040\n",
      "09-Jun 16:03:11:INFO:root:001500 :: TRAIN obj-val = 0.47228616\n",
      "09-Jun 16:03:12:INFO:root:001600 :: TRAIN obj-val = 0.47756996\n",
      "09-Jun 16:03:13:INFO:root:001700 :: TRAIN obj-val = 0.47660535\n",
      "09-Jun 16:03:14:INFO:root:001800 :: TRAIN obj-val = 0.47910536\n",
      "09-Jun 16:03:15:INFO:root:001900 :: TRAIN obj-val = 0.48068813\n",
      "09-Jun 16:03:17:INFO:root:002000 :: TRAIN obj-val = 0.47602383\n",
      "09-Jun 16:03:26:INFO:root:\n",
      "09-Jun 16:03:26:INFO:root:## Performance on Validation Set after 2000 iterations\n",
      "09-Jun 16:03:26:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:03:26:INFO:root:  Square-loss (avg over 32000) = 0.4892\n",
      "09-Jun 16:03:26:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:03:26:INFO:root:\n",
      "09-Jun 16:03:27:INFO:root:002100 :: TRAIN obj-val = 0.45035682\n",
      "09-Jun 16:03:29:INFO:root:002200 :: TRAIN obj-val = 0.48580242\n",
      "09-Jun 16:03:30:INFO:root:002300 :: TRAIN obj-val = 0.47105753\n",
      "09-Jun 16:03:31:INFO:root:002400 :: TRAIN obj-val = 0.45902876\n",
      "09-Jun 16:03:32:INFO:root:002500 :: TRAIN obj-val = 0.47718014\n",
      "09-Jun 16:03:34:INFO:root:002600 :: TRAIN obj-val = 0.47554580\n",
      "09-Jun 16:03:35:INFO:root:002700 :: TRAIN obj-val = 0.44894208\n",
      "09-Jun 16:03:36:INFO:root:002800 :: TRAIN obj-val = 0.47800434\n",
      "09-Jun 16:03:37:INFO:root:002900 :: TRAIN obj-val = 0.48548465\n",
      "09-Jun 16:03:38:INFO:root:003000 :: TRAIN obj-val = 0.48144523\n",
      "09-Jun 16:03:47:INFO:root:\n",
      "09-Jun 16:03:47:INFO:root:## Performance on Validation Set after 3000 iterations\n",
      "09-Jun 16:03:47:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:03:47:INFO:root:  Square-loss (avg over 31000) = 0.4875\n",
      "09-Jun 16:03:47:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:03:47:INFO:root:\n",
      "09-Jun 16:03:48:INFO:root:003100 :: TRAIN obj-val = 0.47487983\n",
      "09-Jun 16:03:50:INFO:root:003200 :: TRAIN obj-val = 0.48217164\n",
      "09-Jun 16:03:51:INFO:root:003300 :: TRAIN obj-val = 0.47480475\n",
      "09-Jun 16:03:52:INFO:root:003400 :: TRAIN obj-val = 0.49414709\n",
      "09-Jun 16:03:53:INFO:root:003500 :: TRAIN obj-val = 0.47612599\n",
      "09-Jun 16:03:54:INFO:root:003600 :: TRAIN obj-val = 0.47646490\n",
      "09-Jun 16:03:55:INFO:root:003700 :: TRAIN obj-val = 0.46759087\n",
      "09-Jun 16:03:57:INFO:root:003800 :: TRAIN obj-val = 0.46871787\n",
      "09-Jun 16:03:58:INFO:root:003900 :: TRAIN obj-val = 0.49182611\n",
      "09-Jun 16:03:59:INFO:root:004000 :: TRAIN obj-val = 0.46380630\n",
      "09-Jun 16:04:08:INFO:root:\n",
      "09-Jun 16:04:08:INFO:root:## Performance on Validation Set after 4000 iterations\n",
      "09-Jun 16:04:08:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:04:08:INFO:root:  Square-loss (avg over 31000) = 0.4873\n",
      "09-Jun 16:04:08:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:04:08:INFO:root:\n",
      "09-Jun 16:04:09:INFO:root:004100 :: TRAIN obj-val = 0.45733177\n",
      "09-Jun 16:04:10:INFO:root:004200 :: TRAIN obj-val = 0.47749024\n",
      "09-Jun 16:04:11:INFO:root:004300 :: TRAIN obj-val = 0.46509420\n",
      "09-Jun 16:04:13:INFO:root:004400 :: TRAIN obj-val = 0.47671702\n",
      "09-Jun 16:04:14:INFO:root:004500 :: TRAIN obj-val = 0.48595651\n",
      "09-Jun 16:04:15:INFO:root:004600 :: TRAIN obj-val = 0.48392976\n",
      "09-Jun 16:04:16:INFO:root:004700 :: TRAIN obj-val = 0.49033612\n",
      "09-Jun 16:04:17:INFO:root:004800 :: TRAIN obj-val = 0.45775817\n",
      "09-Jun 16:04:18:INFO:root:004900 :: TRAIN obj-val = 0.47028927\n",
      "09-Jun 16:04:20:INFO:root:005000 :: TRAIN obj-val = 0.49105664\n",
      "09-Jun 16:04:28:INFO:root:\n",
      "09-Jun 16:04:28:INFO:root:## Performance on Validation Set after 5000 iterations\n",
      "09-Jun 16:04:28:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:04:28:INFO:root:  Square-loss (avg over 32000) = 0.4878\n",
      "09-Jun 16:04:28:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:04:28:INFO:root:\n",
      "09-Jun 16:04:30:INFO:root:005100 :: TRAIN obj-val = 0.46778015\n",
      "09-Jun 16:04:31:INFO:root:005200 :: TRAIN obj-val = 0.47758300\n",
      "09-Jun 16:04:32:INFO:root:005300 :: TRAIN obj-val = 0.46806584\n",
      "09-Jun 16:04:33:INFO:root:005400 :: TRAIN obj-val = 0.44810372\n",
      "09-Jun 16:04:35:INFO:root:005500 :: TRAIN obj-val = 0.46278385\n",
      "09-Jun 16:04:36:INFO:root:005600 :: TRAIN obj-val = 0.49097083\n",
      "09-Jun 16:04:37:INFO:root:005700 :: TRAIN obj-val = 0.48377098\n",
      "09-Jun 16:04:38:INFO:root:005800 :: TRAIN obj-val = 0.45632878\n",
      "09-Jun 16:04:39:INFO:root:005900 :: TRAIN obj-val = 0.47367133\n",
      "09-Jun 16:04:41:INFO:root:006000 :: TRAIN obj-val = 0.47849306\n",
      "09-Jun 16:04:49:INFO:root:\n",
      "09-Jun 16:04:49:INFO:root:## Performance on Validation Set after 6000 iterations\n",
      "09-Jun 16:04:49:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:04:49:INFO:root:  Square-loss (avg over 31000) = 0.4882\n",
      "09-Jun 16:04:49:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:04:49:INFO:root:\n",
      "09-Jun 16:04:51:INFO:root:006100 :: TRAIN obj-val = 0.47274175\n",
      "09-Jun 16:04:52:INFO:root:006200 :: TRAIN obj-val = 0.48628235\n",
      "09-Jun 16:04:53:INFO:root:006300 :: TRAIN obj-val = 0.47081631\n",
      "09-Jun 16:04:54:INFO:root:006400 :: TRAIN obj-val = 0.43652751\n",
      "09-Jun 16:04:55:INFO:root:006500 :: TRAIN obj-val = 0.47326207\n",
      "09-Jun 16:04:56:INFO:root:006600 :: TRAIN obj-val = 0.48304001\n",
      "09-Jun 16:04:58:INFO:root:006700 :: TRAIN obj-val = 0.46789127\n",
      "09-Jun 16:04:59:INFO:root:006800 :: TRAIN obj-val = 0.46121399\n",
      "09-Jun 16:05:00:INFO:root:006900 :: TRAIN obj-val = 0.47296347\n",
      "09-Jun 16:05:01:INFO:root:007000 :: TRAIN obj-val = 0.46830863\n",
      "09-Jun 16:05:10:INFO:root:\n",
      "09-Jun 16:05:10:INFO:root:## Performance on Validation Set after 7000 iterations\n",
      "09-Jun 16:05:10:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:05:10:INFO:root:  Square-loss (avg over 32000) = 0.4872\n",
      "09-Jun 16:05:10:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:05:10:INFO:root:\n",
      "09-Jun 16:05:11:INFO:root:007100 :: TRAIN obj-val = 0.48023856\n",
      "09-Jun 16:05:13:INFO:root:007200 :: TRAIN obj-val = 0.47460454\n",
      "09-Jun 16:05:14:INFO:root:007300 :: TRAIN obj-val = 0.48231879\n",
      "09-Jun 16:05:15:INFO:root:007400 :: TRAIN obj-val = 0.45903750\n",
      "09-Jun 16:05:16:INFO:root:007500 :: TRAIN obj-val = 0.46998023\n",
      "09-Jun 16:05:17:INFO:root:007600 :: TRAIN obj-val = 0.46028074\n",
      "09-Jun 16:05:19:INFO:root:007700 :: TRAIN obj-val = 0.46647345\n",
      "09-Jun 16:05:20:INFO:root:007800 :: TRAIN obj-val = 0.46543533\n",
      "09-Jun 16:05:21:INFO:root:007900 :: TRAIN obj-val = 0.47688294\n",
      "09-Jun 16:05:23:INFO:root:008000 :: TRAIN obj-val = 0.47189718\n",
      "09-Jun 16:05:31:INFO:root:\n",
      "09-Jun 16:05:31:INFO:root:## Performance on Validation Set after 8000 iterations\n",
      "09-Jun 16:05:31:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:05:31:INFO:root:  Square-loss (avg over 31000) = 0.4887\n",
      "09-Jun 16:05:31:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:05:31:INFO:root:\n",
      "09-Jun 16:05:32:INFO:root:008100 :: TRAIN obj-val = 0.47753667\n",
      "09-Jun 16:05:34:INFO:root:008200 :: TRAIN obj-val = 0.48892271\n",
      "09-Jun 16:05:35:INFO:root:008300 :: TRAIN obj-val = 0.47846325\n",
      "09-Jun 16:05:36:INFO:root:008400 :: TRAIN obj-val = 0.46447796\n",
      "09-Jun 16:05:37:INFO:root:008500 :: TRAIN obj-val = 0.47221733\n",
      "09-Jun 16:05:38:INFO:root:008600 :: TRAIN obj-val = 0.47427625\n",
      "09-Jun 16:05:39:INFO:root:008700 :: TRAIN obj-val = 0.48661805\n",
      "09-Jun 16:05:40:INFO:root:008800 :: TRAIN obj-val = 0.48223696\n",
      "09-Jun 16:05:42:INFO:root:008900 :: TRAIN obj-val = 0.46239281\n",
      "09-Jun 16:05:43:INFO:root:009000 :: TRAIN obj-val = 0.46895345\n",
      "09-Jun 16:05:51:INFO:root:\n",
      "09-Jun 16:05:51:INFO:root:## Performance on Validation Set after 9000 iterations\n",
      "09-Jun 16:05:51:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:05:51:INFO:root:  Square-loss (avg over 31000) = 0.4880\n",
      "09-Jun 16:05:51:INFO:root:---------------------------------------------------------\n",
      "09-Jun 16:05:51:INFO:root:\n",
      "09-Jun 16:05:52:INFO:root:009100 :: TRAIN obj-val = 0.47674244\n",
      "09-Jun 16:05:54:INFO:root:009200 :: TRAIN obj-val = 0.47775296\n",
      "09-Jun 16:05:55:INFO:root:009300 :: TRAIN obj-val = 0.44393360\n"
     ]
    }
   ],
   "source": [
    "solve(solver, net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09-Jun 16:02:18:DEBUG:root:Destroying network nym-train\n",
      "09-Jun 16:02:18:DEBUG:root:Destroying network nym-valid\n"
     ]
    }
   ],
   "source": [
    "destroy(net)\n",
    "destroy(valid_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function unpack!(nn::NN, θ::Vector)\n",
    "    endpoint=0\n",
    "    for Wi in 1:length(nn.Ws)\n",
    "        startpoint, endpoint =endpoint+1, endpoint+length(nn.Ws[Wi])\n",
    "#        @printval startpoint\n",
    "#        @printval endpoint\n",
    "        \n",
    "        nn.Ws[Wi][:] = θ[startpoint:endpoint]\n",
    "    end\n",
    "    for bi in 1:length(nn.bs)\n",
    "        startpoint, endpoint =endpoint+1, endpoint+length(nn.bs[bi])\n",
    "#       @printval startpoint\n",
    "#       @printval endpoint\n",
    "        \n",
    "        nn.bs[bi][:] = θ[startpoint:endpoint]\n",
    "    end\n",
    "    nn\n",
    "end\n",
    "\n",
    "function pack(nn::NN)\n",
    "    pack(nn.Ws, nn.bs)\n",
    "end\n",
    "\n",
    "function pack(Ws::Vector{Matrix{Float64}}, bs::Vector{Vector{Float64}})\n",
    "    vcat([W[:] for W in Ws]..., [b[:] for b in bs]...)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importall EmpiricalRisks\n",
    "using ArrayViews\n",
    "\n",
    "type NNPred <: PredictionModel{1,1}\n",
    "    layer_sizes::Vector{Int}\n",
    "end\n",
    "\n",
    "inputlen(pm::NNPred) = pm.layer_sizes[1]\n",
    "inputsize(pm::NNPred) = (inputlen(pm),)\n",
    "outputlen(pm::NNPred) = pm.layer_sizes[end]\n",
    "outputsize(pm::NNPred) = (outputlen(pm),)\n",
    "paramlen(pm::NNPred) =  pm.layer_sizes⋅[0,pm.layer_sizes[1:end-1]] + sum(pm.layer_sizes[2:end])\n",
    "paramsize(pm::NNPred) = (paramlen,)\n",
    "function ninputs(pm::NNPred, x)\n",
    "    @assert(size(x,1)==inputlen(pm))\n",
    "    @assert(ndims(x)<=2)\n",
    "    ndims==2 ? size(x,2) : 1\n",
    "end\n",
    "\n",
    "predict(pm::NNPred, theta::Vector, x::Vector) = predict(pm,theta,x'')\n",
    "function predict(pm::NNPred, theta::Vector, x::Matrix)\n",
    "    nn=unpack!(NN_empty(pm.layer_sizes), theta)\n",
    "    ys,as = feedfoward(nn,x)\n",
    "    ys\n",
    "end\n",
    "\n",
    "\n",
    "function value_and_addgrad!{T<:MultivariateLoss}(rm::SupervisedRiskModel{NNPred, T}, beta, grad, alpha, theta, x, y)\n",
    "    if ndims(x)==1\n",
    "        x=x''\n",
    "        y=y''\n",
    "    end\n",
    "    @pz theta\n",
    "    nn=unpack!(NN_empty(rm.predmodel.layer_sizes), theta)\n",
    "    u,as = feedfoward(nn, x)\n",
    "    @pz y\n",
    "    #u starts as ŷ, and is over written with loss_grad\n",
    "    v=0.0\n",
    "    for i = 1:size(x,2)\n",
    "        u_i = view(u,:,i)\n",
    "        @pz u_i\n",
    "        println(\"***\")\n",
    "        y_i = view(y,:,2)\n",
    "        @pz y_i\n",
    "        v_i, _ = value_and_grad!(loss, u_i, u_i, y_i)\n",
    "        v += v_i\n",
    "    end\n",
    "    \n",
    "    #If things were easy:\n",
    "    #(loss, loss_grad) = value_and_grad(rm.loss, ŷ, y)\n",
    "    \n",
    "    Δs = pack(backprop(nn,y,as,loss_grad)...)\n",
    "    \n",
    "    grad[:].*=beta\n",
    "    grad[:].+=alpha.*Δs\n",
    "    \n",
    "    loss\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using SGDOptim\n",
    "training_seq = minibatch_seq(data, labels,50)\n",
    "\n",
    "\n",
    "pm = NNPred([300,300, 300])\n",
    "rm = riskmodel(pm, SumSqrLoss())\n",
    "\n",
    "nn_init = NN_empty(pm.layer_sizes)\n",
    "nn_init.Ws[1] = eye(300)\n",
    "nn_init.Ws[2] = eye(300)\n",
    "nn_init.bs[1]*=0.0\n",
    "nn_init.bs[2]*=0.0\n",
    "θ=pack(nn_init)\n",
    "θ2=sgd_model = sgd(rm, θ, training_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "value(rm, θ2, data, labels)/size(data,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ŷ=predict(pm, θ2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs = data\n",
    "ys = labels\n",
    "nn_outer = NN([size(xs,1), 1000, size(ys,1)])\n",
    "#nn_outer = NN([size(xs,1), 300, 300, size(ys,1)])\n",
    "#nn_outer.Ws[1]=W1\n",
    "#nn_outer.Ws[2]=W2\n",
    "\n",
    "\n",
    "function loss_and_loss_grad!(θ::Vector, grad::Vector)   \n",
    "    unpack!(nn_outer, θ)\n",
    "    ŷs,as = feedfoward(nn_outer, xs)\n",
    "    \n",
    "    grad[:] = pack(backprop(nn_outer,ys,as)...)\n",
    "    loss(ŷs,ys)\n",
    "end\n",
    "\n",
    "function loss!(θ::Vector)  \n",
    "    error(\"loss! not defined\")\n",
    "end\n",
    "\n",
    "function loss_grad!(θ::Vector, storage::Vector) \n",
    "    error(\"loss_grad not defined\")\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "#---------------------\n",
    "loss_and_loss_grad_cache = Dict{Vector{Float64},(Float64, Vector{Float64})}()\n",
    "loss_and_loss_grad_cache_hits = 0\n",
    "loss_and_loss_grad_cache_misses = 0\n",
    "function cached_loss_and_loss_grad!(θ::Vector, grad::Vector)\n",
    "    global loss_and_loss_grad_cache\n",
    "    global loss_and_loss_grad_cache_hits\n",
    "    global loss_and_loss_grad_cache_misses\n",
    "    if haskey(loss_and_loss_grad_cache,θ)\n",
    "        loss_and_loss_grad_cache_hits+=1\n",
    "        err, grad[:] = loss_and_loss_grad_cache[θ]\n",
    "        err\n",
    "    else\n",
    "        loss_and_loss_grad_cache_misses+=1\n",
    "        err = loss_and_loss_grad!(θ, grad)\n",
    "        loss_and_loss_grad_cache[θ] = (err, grad)\n",
    "        err\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using Optim #https://github.com/JuliaOpt/Optim.jl\n",
    "f=DifferentiableFunction(loss!,loss_grad!,cached_loss_and_loss_grad!)\n",
    "#θ = pack(nn_outer)\n",
    "θ=res.minimum\n",
    "res = optimize(f, θ, method=:l_bfgs, show_trace = true, store_trace = true, iterations = 1000);\n",
    "@printval res.f_calls \n",
    "@printval res.g_calls \n",
    "@printval res.f_minimum\n",
    "@printval res.gr_converged\n",
    "@printval res.iterations\n",
    "@printval res.x_converged \n",
    "\n",
    "@printval res.trace\n",
    "@printval loss_and_loss_grad_cache_hits\n",
    "@printval loss_and_loss_grad_cache_misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using NLopt\n",
    "\n",
    "f_call_count = 0\n",
    "function tracking_loss_and_loss_grad!(θ::Vector, grad::Vector)\n",
    "    global f_call_count\n",
    "    f_call_count+=1\n",
    "    f_val = cached_loss_and_loss_grad!(θ, grad)   \n",
    "    println(f_call_count, '\\t',f_val,'\\t',norm(grad))\n",
    "    f_val\n",
    "end\n",
    "#:LD_MMA, :LD_CCSAQ, :LD_LBFGS, :LD_SLSQP, :LD_VAR2, :LD_VAR1, :LD_TNEWTON_RESTART\n",
    "opt = Opt(:LD_LBFGS, length(pack(nn_outer)))\n",
    "\n",
    "#ftol_abs!(opt,1e-9)\n",
    "maxtime!(opt, 60*60*8)\n",
    "min_objective!(opt, tracking_loss_and_loss_grad!)\n",
    "\n",
    "θ = pack(nn_outer)\n",
    "\n",
    "\n",
    "(optf,optx,ret) = optimize!(opt,θ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unpack!(nn_outer, optx);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subset =  100:120\n",
    "xos = data[:,subset]\n",
    "yos = labels[:,subset]\n",
    "\n",
    "#ŷos,_ = feedfoward(nn_outer,xos);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ŷos=ŷ[:,subset]\n",
    "loss(ŷ,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Distances\n",
    "\n",
    "import WordEmbeddings.show_bests\n",
    "import WordEmbeddings.show_best\n",
    "import WordEmbeddings.neighbour_sims\n",
    "\n",
    "function neighbour_sims(cc::Vector{Float64}, globe::Matrix{Float64}, similarity=cosine_sim)\n",
    "    [similarity(cc, globe[:,ii]) for ii in 1:size(globe,2)]\n",
    "end\n",
    "\n",
    "\n",
    "function show_best(embedder,ĉ::Embedding, nbest=20, similarity=cosine_sim )\n",
    "    candidates=neighbour_sims(ĉ,embedder.L, similarity)   \n",
    "    best_cands = [ (findfirst(candidates,score), score)\n",
    "                    for score in select(candidates,1:nbest, rev=true)[1:nbest]]\n",
    "    vcat([[embedder.indexed_words[ii] round(score,2)] for (ii,score) in best_cands]...)\n",
    "end\n",
    "\n",
    "function show_bests(embedder,ĉs::Embeddings, nbest=20, similarity=cosine_sim)\n",
    "    hcat([show_best(embedder,ĉs[:,ii],nbest, similarity) for ii in 1:size(ĉs,2)]...)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_bests(we, ŷos, 1)[1,1:2:end] #(x,y)->-1.0*Distances.euclidean(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_bests(we, yos, 3)# (x,y)->-1.0*Distances.euclidean(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.9-pre",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
