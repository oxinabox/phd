{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Iterators\n",
    "using DataStructures\n",
    "macro printval(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\" = \", $ee)))\n",
    "end\n",
    "\n",
    "macro pz(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\"\\t\\t\",typeof($ee), \"\\t\", size($ee))))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Pipe\n",
    "push!(LOAD_PATH, \"../word-embeddings2\")\n",
    "using WordEmbeddings\n",
    "we = @pipe load_word2vec_embeddings(\"../../Resources/example_code/word2vec/GoogleNews-vectors-negative300.bin\", 15000) |> WE(_...);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_data (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function load_data(filepath)\n",
    "    lines = open(filepath) do filehandle\n",
    "        map(eachline(filehandle)) do line\n",
    "            fields = split(line)\n",
    "            (fields[1], fields[2:end])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    data = String[]\n",
    "    labels = String[]\n",
    "    \n",
    "    for (hyper, hypos) in lines\n",
    "        if has_word(we,hyper) # Skip ones we don't have \n",
    "            hypos = @pipe hypos |> filter(w->has_word(we,w), _)\n",
    "            append!(data,hypos)\n",
    "            @pipe hyper |> fill(_,length(hypos)) |> append!(labels, _)\n",
    "        end\n",
    "    end \n",
    "    data, labels\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "300x31385 Array{Float64,2}:\n",
       " -0.0581674  -0.0671601    -0.00800945  …  -0.027202     0.0867118 \n",
       "  0.0520085  -0.0491877    -0.0703835      -0.00294688   0.075921  \n",
       "  0.0708274  -0.0209678    -0.0100844      -0.0652847    0.0566517 \n",
       "  0.0144563   0.107835     -0.00813395     -0.00516837   0.060891  \n",
       " -0.146445   -0.0845019    -0.0531196       0.0182253   -0.00674425\n",
       " -0.0448231   0.0253821    -0.0108729   …   0.0725385    0.0196547 \n",
       " -0.076302    0.00409897   -0.0252318       0.0819685    0.0570371 \n",
       " -0.0831452  -0.124861      0.0275558       0.0160492   -0.117928  \n",
       " -0.0605626  -0.00291658    0.0511277      -0.0337304    0.0766918 \n",
       "  0.0136009   0.0532866     0.0634116       0.0544039    0.0581933 \n",
       "  0.0455075  -0.0342107     0.0406697   …  -0.0681862    0.0639741 \n",
       "  0.0210429  -0.038152     -0.0474757       0.0297408   -0.0855557 \n",
       "  0.0684323  -0.0124546     0.00294648      0.0134196   -0.0263989 \n",
       "  ⋮                                     ⋱                          \n",
       "  0.0306234  -0.0577009     0.0190069      -0.0558547   -0.0616618 \n",
       " -0.0540615  -0.0851325     0.0806755      -0.018588    -0.0624325 \n",
       " -0.0804079   0.125492      0.0507957   …  -0.0562174   -0.055881  \n",
       " -0.0121467   0.0703131     0.0208329       0.018044     0.0142593 \n",
       " -0.0313078  -0.0220714    -0.0263938       0.0268393   -0.0342993 \n",
       "  0.0660372   0.0384673    -0.0267258       0.0576681   -0.0578079 \n",
       " -0.0475604  -0.0201796    -0.0401717       0.01886      0.0373824 \n",
       " -0.0718539  -0.000507445  -0.0275558   …   0.0322796    0.0171497 \n",
       " -0.0749334   0.079457      0.0109559      -0.00983804  -0.0747649 \n",
       " -0.0335318  -0.0469805     0.0600916      -0.00261819  -0.0217743 \n",
       "  0.0073137   0.0731509     0.0428277      -0.0801551    0.0158972 \n",
       "  0.0128311   0.0813488     0.0889754       0.0594816    0.00580488,\n",
       "\n",
       "300x31385 Array{Float64,2}:\n",
       "  0.0614284    0.0352818     0.11665     …   0.11665      0.110443  \n",
       " -0.0180419   -0.00887772   -0.0442141      -0.0442141   -0.0237453 \n",
       "  0.0408091   -0.00252013    0.022013        0.022013     0.0314763 \n",
       " -0.0156793    0.11226       0.120413        0.120413     0.0163824 \n",
       " -0.0119205   -0.0989722    -0.0423326      -0.0423326   -0.212051  \n",
       " -0.118561    -0.000366922  -0.0571961   …  -0.0571961    0.0581667 \n",
       " -0.0266333    0.0142043    -0.0160864      -0.0160864    0.10308   \n",
       "  0.00587973  -0.0893499     0.0180619       0.0180619   -0.0199718 \n",
       "  0.0889208    0.00592802    0.0152398       0.0152398    0.0320285 \n",
       "  0.0567031    0.0407802    -0.0174975      -0.0174975    0.0548534 \n",
       " -0.0109003    0.0497152    -0.0549384   …  -0.0549384   -0.0421524 \n",
       "  0.0162162   -0.0971394    -0.0801498      -0.0801498   -0.0566941 \n",
       "  0.0258815    0.0241703    -0.00425678     -0.00425678  -0.0175789 \n",
       "  ⋮                                      ⋱                          \n",
       " -0.00242975  -0.0175263     0.0252114       0.0252114   -0.0155541 \n",
       "  0.0756042    0.00270627    0.0126057       0.0126057   -0.0218125 \n",
       "  0.0220154   -0.160372      0.0434615   …   0.0434615   -0.0758376 \n",
       "  0.0204045    0.0966812     0.109877        0.109877     0.0964536 \n",
       " -0.036943    -0.0435295    -0.130196       -0.130196     0.00354338\n",
       " -0.103956     0.021192     -0.0150516      -0.0150516   -0.0163824 \n",
       "  0.0597101   -0.0646069    -0.0846653      -0.0846653   -0.0684747 \n",
       "  0.0408091    0.0396347     0.0229537   …   0.0229537   -0.00524604\n",
       "  0.0148201    0.0272632     0.0228596       0.0228596    0.0151859 \n",
       "  0.00961161  -0.0189009    -0.0741292      -0.0741292    0.00425666\n",
       " -0.0474674    0.0529227    -0.0381935      -0.0381935   -0.0541171 \n",
       " -0.0265259   -0.0943902    -0.0428971      -0.0428971    0.0163824 )"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function load_and_embed(path)\n",
    "    data_str, labels_str = load_data(path)\n",
    "    data = eval_word_embeddings(we,data_str)\n",
    "    labels =  eval_word_embeddings(we,labels_str);\n",
    "    order = randperm(size(data,2));\n",
    "    data[:,order], labels[:,order];\n",
    "end\n",
    "\n",
    "data, labels = load_and_embed(\"HyponymGen/hyponym-generation-noun-train.txt\")\n",
    "data_valid, labels_valid = load_and_embed(\"HyponymGen/hyponym-generation-noun-dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN_empty (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type NN \n",
    "    Ws:: Vector{Matrix{Float64}} \n",
    "    bs:: Vector{Vector{Float64}} \n",
    "end\n",
    "\n",
    "function NN(layer_sizes::Vector{Int}, var=0.01)\n",
    "    Ws = [var*randn(layer_sizes[ii], layer_sizes[ii-1]) for ii in 2:length(layer_sizes)]\n",
    "    bs = [var*randn(layer_sizes[ii]) for ii in 2:length(layer_sizes)]\n",
    "    NN(Ws, bs)\n",
    "end\n",
    "\n",
    "function NN_empty(layer_sizes::Vector{Int})\n",
    "    Ws = [Array(Float64,(layer_sizes[ii], layer_sizes[ii-1])) for ii in 2:length(layer_sizes)]\n",
    "    bs = [Array(Float64,layer_sizes[ii]) for ii in 2:length(layer_sizes)]\n",
    "    NN(Ws, bs)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function feedfoward(nn::NN, xs::AbstractMatrix{Float64})\n",
    "    as = AbstractMatrix{Float64}[ [NaN]' for _ in 1:length(nn.Ws)+1 ] \n",
    "   \n",
    "    as[1] =  xs\n",
    "    for ii in 1:length(nn.Ws)\n",
    "        as[ii+1] = tanh(nn.Ws[ii]*as[ii] .+ nn.bs[ii])\n",
    "    end \n",
    "    as[end], as\n",
    "end\n",
    "\n",
    "function backprop(nn::NN, ys::AbstractMatrix{Float64}, as::Vector{Matrix{Float64}}, loss_grad= as[end]-ys)\n",
    "    function dZ(z)\n",
    "        1.0-z.^2 \n",
    "    end\n",
    "    Δbs = Vector{Float64}[ [NaN] for _ in 1:length(nn.Ws) ] \n",
    "    ΔWs = Matrix{Float64}[ [NaN]' for _ in 1:length(nn.Ws) ] \n",
    "    ŷs = as[end]\n",
    "    δ_above = loss_grad.*dZ(ŷs)\n",
    "    for ii in length(nn.Ws):-1:1\n",
    "        Δbs[ii] = sum(δ_above,2)[:]\n",
    "        ΔWs[ii] = (δ_above * as[ii]')\n",
    "        δ_above = (nn.Ws[ii]'*δ_above) .*dZ(as[ii])\n",
    "    end\n",
    "   \n",
    "    ΔWs,Δbs\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function loss(ŷs, ys)\n",
    "    sum(0.5*(ys-ŷs).^2,2) |> sum\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colnorm(A) = [norm(A[:,ii]) for ii in 1:size(A,2)]\n",
    "coldot(A,B) = [A[:,ii]⋅B[:,ii] for ii in 1:size(A,2)]\n",
    "\n",
    "function cosine_sim(ys,ts)\n",
    "    coldot(ys,ts)./(colnorm(ys).*colnorm(ts))\n",
    "end\n",
    "\n",
    "function loss_cosine(ŷs, ys)\n",
    "    0.5(1.0-cosine_sim(ŷs, ys)).^2 |> mean\n",
    "end\n",
    "\n",
    "function loss_diff_cosine(ys, ts)\n",
    "    df = similar(ys)\n",
    "    for jj in size(df,2)\n",
    "        tjs = ts[:,jj]\n",
    "        yjs = ys[:,jj]\n",
    "        \n",
    "        normprod = norm(tjs)*norm(yjs)\n",
    "        df[:,jj] = tjs./normprod + abs(yjs).*norm(tjs)./(normprod.^3)\n",
    "    end\n",
    "    \n",
    "    df.*(1.0-cosine_sim(ys,ts))'\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-Jun 10:27:30:INFO:root:Constructing net nym-train on CPUBackend...\n",
      "10-Jun 10:27:30:INFO:root:Topological sorting 4 layers...\n",
      "10-Jun 10:27:30:INFO:root:Setup layers...\n",
      "10-Jun 10:27:30:INFO:root:Network constructed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "************************************************************\n",
       "          NAME: nym-train\n",
       "       BACKEND: CPUBackend\n",
       "  ARCHITECTURE: 4 layers\n",
       "............................................................\n",
       " *** MemoryDataLayer(train-data)\n",
       "    Outputs ---------------------------\n",
       "          data: Blob(300 x 10)\n",
       "         label: Blob(300 x 10)\n",
       "............................................................\n",
       " *** InnerProductLayer(ip1)\n",
       "    Inputs ----------------------------\n",
       "          data: Blob(300 x 10)\n",
       "    Outputs ---------------------------\n",
       "           ip1: Blob(500 x 10)\n",
       "............................................................\n",
       " *** InnerProductLayer(ip2)\n",
       "    Inputs ----------------------------\n",
       "           ip1: Blob(500 x 10)\n",
       "    Outputs ---------------------------\n",
       "           ip2: Blob(300 x 10)\n",
       "............................................................\n",
       " *** SquareLossLayer(loss)\n",
       "    Inputs ----------------------------\n",
       "           ip2: Blob(300 x 10)\n",
       "         label: Blob(300 x 10)\n",
       "************************************************************\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Mocha\n",
    "\n",
    "data_l = MemoryDataLayer(name=\"train-data\",tops=[:data,:label], data=Array[data, labels],batch_size=10)\n",
    "fc1   = InnerProductLayer(name=\"ip1\",output_dim=500,neuron=Neurons.ReLU(),\n",
    "bottoms=[:data],tops=[:ip1], weight_init=GaussianInitializer(0.0,0.0001))\n",
    "fc2   = InnerProductLayer(name=\"ip2\",output_dim=300,neuron=Neurons.ReLU(),\n",
    "                          bottoms=[:ip1], tops=[:ip2],  weight_init=GaussianInitializer(0.0,0.0001))\n",
    "loss_l = SquareLossLayer(name=\"loss\", bottoms=[:ip2,:label])\n",
    "backend = CPUBackend()\n",
    "init(backend)\n",
    "common_layers = [fc1,fc2]\n",
    "net = Net(\"nym-train\", backend, [data_l, common_layers..., loss_l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{CoffeeBreak,1}:\n",
       " CoffeeBreak(TrainingSummary(),100,0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = \"snapshots\"\n",
    "params = SolverParameters(max_iter=10000, regu_coef=0.0005,\n",
    "    mom_policy=MomPolicy.Fixed(0.9),\n",
    "    lr_policy=LRPolicy.Inv(0.01, 0.0001, 0.75),\n",
    "    load_from=exp_dir)\n",
    "solver =  Nesterov(params)\n",
    "\n",
    "setup_coffee_lounge(solver, save_into=\"$exp_dir/statistics.jld\", every_n_iter=1000)\n",
    "\n",
    "# report training progress every 100 iterations\n",
    "add_coffee_break(solver, TrainingSummary(), every_n_iter=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-Jun 10:27:34:INFO:root:Constructing net nym-valid on CPUBackend...\n",
      "10-Jun 10:27:34:INFO:root:Topological sorting 4 layers...\n",
      "10-Jun 10:27:34:INFO:root:Setup layers...\n",
      "10-Jun 10:27:34:DEBUG:root:InnerProductLayer(ip1): sharing weights and bias\n",
      "10-Jun 10:27:34:DEBUG:root:InnerProductLayer(ip2): sharing weights and bias\n",
      "10-Jun 10:27:34:INFO:root:Network constructed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2-element Array{CoffeeBreak,1}:\n",
       " CoffeeBreak(TrainingSummary(),100,0)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       " CoffeeBreak(ValidationPerformance(************************************************************\n",
       "          NAME: nym-valid\n",
       "       BACKEND: CPUBackend\n",
       "  ARCHITECTURE: 4 layers\n",
       "............................................................\n",
       " *** MemoryDataLayer(validation-data)\n",
       "    Outputs ---------------------------\n",
       "          data: Blob(300 x 1000)\n",
       "         label: Blob(300 x 1000)\n",
       "............................................................\n",
       " *** InnerProductLayer(ip1)\n",
       "    Inputs ----------------------------\n",
       "          data: Blob(300 x 1000)\n",
       "    Outputs ---------------------------\n",
       "           ip1: Blob(500 x 1000)\n",
       "............................................................\n",
       " *** InnerProductLayer(ip2)\n",
       "    Inputs ----------------------------\n",
       "           ip1: Blob(500 x 1000)\n",
       "    Outputs ---------------------------\n",
       "           ip2: Blob(300 x 1000)\n",
       "............................................................\n",
       " *** SquareLossLayer(validation-loss)\n",
       "    Inputs ----------------------------\n",
       "           ip2: Blob(300 x 1000)\n",
       "         label: Blob(300 x 1000)\n",
       "************************************************************\n",
       ",[]),1000,0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_valid_l = MemoryDataLayer(name=\"validation-data\",tops=[:data,:label], data=Array[data_valid, labels_valid],batch_size=1000)\n",
    "accuracy = SquareLossLayer(name=\"validation-loss\",bottoms=[:ip2, :label])\n",
    "valid_net = Net(\"nym-valid\", backend, [data_valid_l, common_layers..., accuracy])\n",
    "add_coffee_break(solver, ValidationPerformance(valid_net), every_n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-Jun 10:27:39:DEBUG:root:Checking network topology for back-propagation\n",
      "10-Jun 10:27:39:DEBUG:root:Init network nym-train\n",
      "10-Jun 10:27:39:DEBUG:root:Init parameter weight for layer ip1\n",
      "10-Jun 10:27:39:DEBUG:root:Init parameter bias for layer ip1\n",
      "10-Jun 10:27:39:DEBUG:root:Init parameter weight for layer ip2\n",
      "10-Jun 10:27:39:DEBUG:root:Init parameter bias for layer ip2\n",
      "10-Jun 10:27:39:DEBUG:root:Initializing coffee breaks\n",
      "10-Jun 10:27:39:INFO:root:Merging existing coffee lounge statistics in snapshots/statistics.jld\n",
      "10-Jun 10:27:39:DEBUG:root:Init network nym-valid\n",
      "10-Jun 10:27:39:INFO:root:000000 :: TRAIN obj-val = 0.50000006\n",
      "10-Jun 10:27:49:INFO:root:\n",
      "10-Jun 10:27:49:INFO:root:## Performance on Validation Set after 0 iterations\n",
      "10-Jun 10:27:49:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:27:49:INFO:root:  Square-loss (avg over 32000) = 0.5000\n",
      "10-Jun 10:27:49:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:27:49:INFO:root:\n",
      "10-Jun 10:27:49:DEBUG:root:Entering solver loop\n",
      "10-Jun 10:27:50:INFO:root:000100 :: TRAIN obj-val = 0.48883663\n",
      "10-Jun 10:27:52:INFO:root:000200 :: TRAIN obj-val = 0.46480904\n",
      "10-Jun 10:27:53:INFO:root:000300 :: TRAIN obj-val = 0.47136768\n",
      "10-Jun 10:27:55:INFO:root:000400 :: TRAIN obj-val = 0.47490220\n",
      "10-Jun 10:27:56:INFO:root:000500 :: TRAIN obj-val = 0.46790173\n",
      "10-Jun 10:27:58:INFO:root:000600 :: TRAIN obj-val = 0.47198237\n",
      "10-Jun 10:28:00:INFO:root:000700 :: TRAIN obj-val = 0.46895250\n",
      "10-Jun 10:28:01:INFO:root:000800 :: TRAIN obj-val = 0.48003644\n",
      "10-Jun 10:28:03:INFO:root:000900 :: TRAIN obj-val = 0.47291779\n",
      "10-Jun 10:28:04:INFO:root:001000 :: TRAIN obj-val = 0.46264825\n",
      "10-Jun 10:28:13:INFO:root:\n",
      "10-Jun 10:28:13:INFO:root:## Performance on Validation Set after 1000 iterations\n",
      "10-Jun 10:28:13:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:28:13:INFO:root:  Square-loss (avg over 31000) = 0.4884\n",
      "10-Jun 10:28:13:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:28:13:INFO:root:\n",
      "10-Jun 10:28:14:INFO:root:001100 :: TRAIN obj-val = 0.46010436\n",
      "10-Jun 10:28:15:INFO:root:001200 :: TRAIN obj-val = 0.48519646\n",
      "10-Jun 10:28:17:INFO:root:001300 :: TRAIN obj-val = 0.47630396\n",
      "10-Jun 10:28:18:INFO:root:001400 :: TRAIN obj-val = 0.48101344\n",
      "10-Jun 10:28:19:INFO:root:001500 :: TRAIN obj-val = 0.46491980\n",
      "10-Jun 10:28:20:INFO:root:001600 :: TRAIN obj-val = 0.45920647\n",
      "10-Jun 10:28:22:INFO:root:001700 :: TRAIN obj-val = 0.48326681\n",
      "10-Jun 10:28:23:INFO:root:001800 :: TRAIN obj-val = 0.47238661\n",
      "10-Jun 10:28:24:INFO:root:001900 :: TRAIN obj-val = 0.48211444\n",
      "10-Jun 10:28:25:INFO:root:002000 :: TRAIN obj-val = 0.44525278\n",
      "10-Jun 10:28:35:INFO:root:\n",
      "10-Jun 10:28:35:INFO:root:## Performance on Validation Set after 2000 iterations\n",
      "10-Jun 10:28:35:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:28:35:INFO:root:  Square-loss (avg over 32000) = 0.4899\n",
      "10-Jun 10:28:35:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:28:35:INFO:root:\n",
      "10-Jun 10:28:37:INFO:root:002100 :: TRAIN obj-val = 0.48604226\n",
      "10-Jun 10:28:38:INFO:root:002200 :: TRAIN obj-val = 0.48332629\n",
      "10-Jun 10:28:40:INFO:root:002300 :: TRAIN obj-val = 0.48356120\n",
      "10-Jun 10:28:41:INFO:root:002400 :: TRAIN obj-val = 0.47731913\n",
      "10-Jun 10:28:42:INFO:root:002500 :: TRAIN obj-val = 0.48607225\n",
      "10-Jun 10:28:44:INFO:root:002600 :: TRAIN obj-val = 0.48175597\n",
      "10-Jun 10:28:45:INFO:root:002700 :: TRAIN obj-val = 0.47892705\n",
      "10-Jun 10:28:47:INFO:root:002800 :: TRAIN obj-val = 0.47653180\n",
      "10-Jun 10:28:48:INFO:root:002900 :: TRAIN obj-val = 0.46621383\n",
      "10-Jun 10:28:50:INFO:root:003000 :: TRAIN obj-val = 0.48425078\n",
      "10-Jun 10:29:00:INFO:root:\n",
      "10-Jun 10:29:00:INFO:root:## Performance on Validation Set after 3000 iterations\n",
      "10-Jun 10:29:00:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:29:00:INFO:root:  Square-loss (avg over 31000) = 0.4878\n",
      "10-Jun 10:29:00:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:29:00:INFO:root:\n",
      "10-Jun 10:29:02:INFO:root:003100 :: TRAIN obj-val = 0.46425406\n",
      "10-Jun 10:29:04:INFO:root:003200 :: TRAIN obj-val = 0.47384931\n",
      "10-Jun 10:29:05:INFO:root:003300 :: TRAIN obj-val = 0.47648531\n",
      "10-Jun 10:29:07:INFO:root:003400 :: TRAIN obj-val = 0.48387292\n",
      "10-Jun 10:29:08:INFO:root:003500 :: TRAIN obj-val = 0.47259511\n",
      "10-Jun 10:29:10:INFO:root:003600 :: TRAIN obj-val = 0.47992594\n",
      "10-Jun 10:29:11:INFO:root:003700 :: TRAIN obj-val = 0.45653472\n",
      "10-Jun 10:29:13:INFO:root:003800 :: TRAIN obj-val = 0.47037075\n",
      "10-Jun 10:29:14:INFO:root:003900 :: TRAIN obj-val = 0.49001100\n",
      "10-Jun 10:29:16:INFO:root:004000 :: TRAIN obj-val = 0.47111151\n",
      "10-Jun 10:29:26:INFO:root:\n",
      "10-Jun 10:29:26:INFO:root:## Performance on Validation Set after 4000 iterations\n",
      "10-Jun 10:29:26:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:29:26:INFO:root:  Square-loss (avg over 31000) = 0.4878\n",
      "10-Jun 10:29:26:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:29:26:INFO:root:\n",
      "10-Jun 10:29:27:INFO:root:004100 :: TRAIN obj-val = 0.49154458\n",
      "10-Jun 10:29:29:INFO:root:004200 :: TRAIN obj-val = 0.47157905\n",
      "10-Jun 10:29:30:INFO:root:004300 :: TRAIN obj-val = 0.48663125\n",
      "10-Jun 10:29:32:INFO:root:004400 :: TRAIN obj-val = 0.47814314\n",
      "10-Jun 10:29:34:INFO:root:004500 :: TRAIN obj-val = 0.48404395\n",
      "10-Jun 10:29:35:INFO:root:004600 :: TRAIN obj-val = 0.47586801\n",
      "10-Jun 10:29:37:INFO:root:004700 :: TRAIN obj-val = 0.47927130\n",
      "10-Jun 10:29:39:INFO:root:004800 :: TRAIN obj-val = 0.46036240\n",
      "10-Jun 10:29:40:INFO:root:004900 :: TRAIN obj-val = 0.48743301\n",
      "10-Jun 10:29:41:INFO:root:005000 :: TRAIN obj-val = 0.48185594\n",
      "10-Jun 10:29:52:INFO:root:\n",
      "10-Jun 10:29:52:INFO:root:## Performance on Validation Set after 5000 iterations\n",
      "10-Jun 10:29:52:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:29:52:INFO:root:  Square-loss (avg over 32000) = 0.4888\n",
      "10-Jun 10:29:52:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:29:52:INFO:root:\n",
      "10-Jun 10:29:54:INFO:root:005100 :: TRAIN obj-val = 0.47127597\n",
      "10-Jun 10:29:56:INFO:root:005200 :: TRAIN obj-val = 0.47265567\n",
      "10-Jun 10:29:57:INFO:root:005300 :: TRAIN obj-val = 0.46927035\n",
      "10-Jun 10:29:59:INFO:root:005400 :: TRAIN obj-val = 0.48516035\n",
      "10-Jun 10:30:00:INFO:root:005500 :: TRAIN obj-val = 0.47402762\n",
      "10-Jun 10:30:02:INFO:root:005600 :: TRAIN obj-val = 0.47053494\n",
      "10-Jun 10:30:04:INFO:root:005700 :: TRAIN obj-val = 0.47081749\n",
      "10-Jun 10:30:05:INFO:root:005800 :: TRAIN obj-val = 0.47398741\n",
      "10-Jun 10:30:07:INFO:root:005900 :: TRAIN obj-val = 0.46753355\n",
      "10-Jun 10:30:08:INFO:root:006000 :: TRAIN obj-val = 0.46907804\n",
      "10-Jun 10:30:19:INFO:root:\n",
      "10-Jun 10:30:19:INFO:root:## Performance on Validation Set after 6000 iterations\n",
      "10-Jun 10:30:19:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:30:19:INFO:root:  Square-loss (avg over 31000) = 0.4892\n",
      "10-Jun 10:30:19:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:30:19:INFO:root:\n",
      "10-Jun 10:30:21:INFO:root:006100 :: TRAIN obj-val = 0.46490584\n",
      "10-Jun 10:30:23:INFO:root:006200 :: TRAIN obj-val = 0.47471506\n",
      "10-Jun 10:30:24:INFO:root:006300 :: TRAIN obj-val = 0.48483094\n",
      "10-Jun 10:30:26:INFO:root:006400 :: TRAIN obj-val = 0.48318876\n",
      "10-Jun 10:30:27:INFO:root:006500 :: TRAIN obj-val = 0.47059191\n",
      "10-Jun 10:30:28:INFO:root:006600 :: TRAIN obj-val = 0.46779332\n",
      "10-Jun 10:30:30:INFO:root:006700 :: TRAIN obj-val = 0.48152483\n",
      "10-Jun 10:30:31:INFO:root:006800 :: TRAIN obj-val = 0.48534144\n",
      "10-Jun 10:30:33:INFO:root:006900 :: TRAIN obj-val = 0.46487961\n",
      "10-Jun 10:30:34:INFO:root:007000 :: TRAIN obj-val = 0.47447764\n",
      "10-Jun 10:30:44:INFO:root:\n",
      "10-Jun 10:30:44:INFO:root:## Performance on Validation Set after 7000 iterations\n",
      "10-Jun 10:30:44:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:30:44:INFO:root:  Square-loss (avg over 32000) = 0.4891\n",
      "10-Jun 10:30:44:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:30:44:INFO:root:\n",
      "10-Jun 10:30:46:INFO:root:007100 :: TRAIN obj-val = 0.46836249\n",
      "10-Jun 10:30:47:INFO:root:007200 :: TRAIN obj-val = 0.47883864\n",
      "10-Jun 10:30:49:INFO:root:007300 :: TRAIN obj-val = 0.48284943\n",
      "10-Jun 10:30:50:INFO:root:007400 :: TRAIN obj-val = 0.47894667\n",
      "10-Jun 10:30:52:INFO:root:007500 :: TRAIN obj-val = 0.48049274\n",
      "10-Jun 10:30:53:INFO:root:007600 :: TRAIN obj-val = 0.47499379\n",
      "10-Jun 10:30:55:INFO:root:007700 :: TRAIN obj-val = 0.48514676\n",
      "10-Jun 10:30:56:INFO:root:007800 :: TRAIN obj-val = 0.48400449\n",
      "10-Jun 10:30:57:INFO:root:007900 :: TRAIN obj-val = 0.46851700\n",
      "10-Jun 10:30:59:INFO:root:008000 :: TRAIN obj-val = 0.47146519\n",
      "10-Jun 10:31:09:INFO:root:\n",
      "10-Jun 10:31:09:INFO:root:## Performance on Validation Set after 8000 iterations\n",
      "10-Jun 10:31:09:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:31:09:INFO:root:  Square-loss (avg over 31000) = 0.4868\n",
      "10-Jun 10:31:09:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:31:09:INFO:root:\n",
      "10-Jun 10:31:11:INFO:root:008100 :: TRAIN obj-val = 0.46165080\n",
      "10-Jun 10:31:13:INFO:root:008200 :: TRAIN obj-val = 0.48383683\n",
      "10-Jun 10:31:15:INFO:root:008300 :: TRAIN obj-val = 0.47800907\n",
      "10-Jun 10:31:16:INFO:root:008400 :: TRAIN obj-val = 0.47186569\n",
      "10-Jun 10:31:18:INFO:root:008500 :: TRAIN obj-val = 0.46983205\n",
      "10-Jun 10:31:20:INFO:root:008600 :: TRAIN obj-val = 0.47103093\n",
      "10-Jun 10:31:21:INFO:root:008700 :: TRAIN obj-val = 0.47163420\n",
      "10-Jun 10:31:23:INFO:root:008800 :: TRAIN obj-val = 0.47841891\n",
      "10-Jun 10:31:24:INFO:root:008900 :: TRAIN obj-val = 0.48722962\n",
      "10-Jun 10:31:26:INFO:root:009000 :: TRAIN obj-val = 0.48884831\n",
      "10-Jun 10:31:35:INFO:root:\n",
      "10-Jun 10:31:35:INFO:root:## Performance on Validation Set after 9000 iterations\n",
      "10-Jun 10:31:35:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:31:35:INFO:root:  Square-loss (avg over 31000) = 0.4878\n",
      "10-Jun 10:31:35:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:31:35:INFO:root:\n",
      "10-Jun 10:31:37:INFO:root:009100 :: TRAIN obj-val = 0.47104933\n",
      "10-Jun 10:31:38:INFO:root:009200 :: TRAIN obj-val = 0.47558842\n",
      "10-Jun 10:31:40:INFO:root:009300 :: TRAIN obj-val = 0.49405345\n",
      "10-Jun 10:31:41:INFO:root:009400 :: TRAIN obj-val = 0.47181903\n",
      "10-Jun 10:31:43:INFO:root:009500 :: TRAIN obj-val = 0.46535910\n",
      "10-Jun 10:31:44:INFO:root:009600 :: TRAIN obj-val = 0.48439034\n",
      "10-Jun 10:31:46:INFO:root:009700 :: TRAIN obj-val = 0.46460025\n",
      "10-Jun 10:31:48:INFO:root:009800 :: TRAIN obj-val = 0.48240393\n",
      "10-Jun 10:31:50:INFO:root:009900 :: TRAIN obj-val = 0.47618744\n",
      "10-Jun 10:31:51:INFO:root:010000 :: TRAIN obj-val = 0.46859444\n",
      "10-Jun 10:32:02:INFO:root:\n",
      "10-Jun 10:32:02:INFO:root:## Performance on Validation Set after 10000 iterations\n",
      "10-Jun 10:32:02:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:32:02:INFO:root:  Square-loss (avg over 32000) = 0.4876\n",
      "10-Jun 10:32:02:INFO:root:---------------------------------------------------------\n",
      "10-Jun 10:32:02:INFO:root:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2-element Array{Array{Nothing,1},1}:\n",
       " [nothing,nothing]\n",
       " [nothing,nothing]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve(solver, net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09-Jun 16:02:18:DEBUG:root:Destroying network nym-train\n",
      "09-Jun 16:02:18:DEBUG:root:Destroying network nym-valid\n"
     ]
    }
   ],
   "source": [
    "destroy(net)\n",
    "destroy(valid_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function unpack!(nn::NN, θ::Vector)\n",
    "    endpoint=0\n",
    "    for Wi in 1:length(nn.Ws)\n",
    "        startpoint, endpoint =endpoint+1, endpoint+length(nn.Ws[Wi])\n",
    "#        @printval startpoint\n",
    "#        @printval endpoint\n",
    "        \n",
    "        nn.Ws[Wi][:] = θ[startpoint:endpoint]\n",
    "    end\n",
    "    for bi in 1:length(nn.bs)\n",
    "        startpoint, endpoint =endpoint+1, endpoint+length(nn.bs[bi])\n",
    "#       @printval startpoint\n",
    "#       @printval endpoint\n",
    "        \n",
    "        nn.bs[bi][:] = θ[startpoint:endpoint]\n",
    "    end\n",
    "    nn\n",
    "end\n",
    "\n",
    "function pack(nn::NN)\n",
    "    pack(nn.Ws, nn.bs)\n",
    "end\n",
    "\n",
    "function pack(Ws::Vector{Matrix{Float64}}, bs::Vector{Vector{Float64}})\n",
    "    vcat([W[:] for W in Ws]..., [b[:] for b in bs]...)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importall EmpiricalRisks\n",
    "using ArrayViews\n",
    "\n",
    "type NNPred <: PredictionModel{1,1}\n",
    "    layer_sizes::Vector{Int}\n",
    "end\n",
    "\n",
    "inputlen(pm::NNPred) = pm.layer_sizes[1]\n",
    "inputsize(pm::NNPred) = (inputlen(pm),)\n",
    "outputlen(pm::NNPred) = pm.layer_sizes[end]\n",
    "outputsize(pm::NNPred) = (outputlen(pm),)\n",
    "paramlen(pm::NNPred) =  pm.layer_sizes⋅[0,pm.layer_sizes[1:end-1]] + sum(pm.layer_sizes[2:end])\n",
    "paramsize(pm::NNPred) = (paramlen,)\n",
    "function ninputs(pm::NNPred, x)\n",
    "    @assert(size(x,1)==inputlen(pm))\n",
    "    @assert(ndims(x)<=2)\n",
    "    ndims==2 ? size(x,2) : 1\n",
    "end\n",
    "\n",
    "predict(pm::NNPred, theta::Vector, x::Vector) = predict(pm,theta,x'')\n",
    "function predict(pm::NNPred, theta::Vector, x::Matrix)\n",
    "    nn=unpack!(NN_empty(pm.layer_sizes), theta)\n",
    "    ys,as = feedfoward(nn,x)\n",
    "    ys\n",
    "end\n",
    "\n",
    "\n",
    "function value_and_addgrad!{T<:MultivariateLoss}(rm::SupervisedRiskModel{NNPred, T}, beta, grad, alpha, theta, x, y)\n",
    "    if ndims(x)==1\n",
    "        x=x''\n",
    "        y=y''\n",
    "    end\n",
    "    @pz theta\n",
    "    nn=unpack!(NN_empty(rm.predmodel.layer_sizes), theta)\n",
    "    u,as = feedfoward(nn, x)\n",
    "    @pz y\n",
    "    #u starts as ŷ, and is over written with loss_grad\n",
    "    v=0.0\n",
    "    for i = 1:size(x,2)\n",
    "        u_i = view(u,:,i)\n",
    "        @pz u_i\n",
    "        println(\"***\")\n",
    "        y_i = view(y,:,2)\n",
    "        @pz y_i\n",
    "        v_i, _ = value_and_grad!(loss, u_i, u_i, y_i)\n",
    "        v += v_i\n",
    "    end\n",
    "    \n",
    "    #If things were easy:\n",
    "    #(loss, loss_grad) = value_and_grad(rm.loss, ŷ, y)\n",
    "    \n",
    "    Δs = pack(backprop(nn,y,as,loss_grad)...)\n",
    "    \n",
    "    grad[:].*=beta\n",
    "    grad[:].+=alpha.*Δs\n",
    "    \n",
    "    loss\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using SGDOptim\n",
    "training_seq = minibatch_seq(data, labels,50)\n",
    "\n",
    "\n",
    "pm = NNPred([300,300, 300])\n",
    "rm = riskmodel(pm, SumSqrLoss())\n",
    "\n",
    "nn_init = NN_empty(pm.layer_sizes)\n",
    "nn_init.Ws[1] = eye(300)\n",
    "nn_init.Ws[2] = eye(300)\n",
    "nn_init.bs[1]*=0.0\n",
    "nn_init.bs[2]*=0.0\n",
    "θ=pack(nn_init)\n",
    "θ2=sgd_model = sgd(rm, θ, training_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "value(rm, θ2, data, labels)/size(data,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ŷ=predict(pm, θ2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs = data\n",
    "ys = labels\n",
    "nn_outer = NN([size(xs,1), 1000, size(ys,1)])\n",
    "#nn_outer = NN([size(xs,1), 300, 300, size(ys,1)])\n",
    "#nn_outer.Ws[1]=W1\n",
    "#nn_outer.Ws[2]=W2\n",
    "\n",
    "\n",
    "function loss_and_loss_grad!(θ::Vector, grad::Vector)   \n",
    "    unpack!(nn_outer, θ)\n",
    "    ŷs,as = feedfoward(nn_outer, xs)\n",
    "    \n",
    "    grad[:] = pack(backprop(nn_outer,ys,as)...)\n",
    "    loss(ŷs,ys)\n",
    "end\n",
    "\n",
    "function loss!(θ::Vector)  \n",
    "    error(\"loss! not defined\")\n",
    "end\n",
    "\n",
    "function loss_grad!(θ::Vector, storage::Vector) \n",
    "    error(\"loss_grad not defined\")\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "#---------------------\n",
    "loss_and_loss_grad_cache = Dict{Vector{Float64},(Float64, Vector{Float64})}()\n",
    "loss_and_loss_grad_cache_hits = 0\n",
    "loss_and_loss_grad_cache_misses = 0\n",
    "function cached_loss_and_loss_grad!(θ::Vector, grad::Vector)\n",
    "    global loss_and_loss_grad_cache\n",
    "    global loss_and_loss_grad_cache_hits\n",
    "    global loss_and_loss_grad_cache_misses\n",
    "    if haskey(loss_and_loss_grad_cache,θ)\n",
    "        loss_and_loss_grad_cache_hits+=1\n",
    "        err, grad[:] = loss_and_loss_grad_cache[θ]\n",
    "        err\n",
    "    else\n",
    "        loss_and_loss_grad_cache_misses+=1\n",
    "        err = loss_and_loss_grad!(θ, grad)\n",
    "        loss_and_loss_grad_cache[θ] = (err, grad)\n",
    "        err\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using Optim #https://github.com/JuliaOpt/Optim.jl\n",
    "f=DifferentiableFunction(loss!,loss_grad!,cached_loss_and_loss_grad!)\n",
    "#θ = pack(nn_outer)\n",
    "θ=res.minimum\n",
    "res = optimize(f, θ, method=:l_bfgs, show_trace = true, store_trace = true, iterations = 1000);\n",
    "@printval res.f_calls \n",
    "@printval res.g_calls \n",
    "@printval res.f_minimum\n",
    "@printval res.gr_converged\n",
    "@printval res.iterations\n",
    "@printval res.x_converged \n",
    "\n",
    "@printval res.trace\n",
    "@printval loss_and_loss_grad_cache_hits\n",
    "@printval loss_and_loss_grad_cache_misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using NLopt\n",
    "\n",
    "f_call_count = 0\n",
    "function tracking_loss_and_loss_grad!(θ::Vector, grad::Vector)\n",
    "    global f_call_count\n",
    "    f_call_count+=1\n",
    "    f_val = cached_loss_and_loss_grad!(θ, grad)   \n",
    "    println(f_call_count, '\\t',f_val,'\\t',norm(grad))\n",
    "    f_val\n",
    "end\n",
    "#:LD_MMA, :LD_CCSAQ, :LD_LBFGS, :LD_SLSQP, :LD_VAR2, :LD_VAR1, :LD_TNEWTON_RESTART\n",
    "opt = Opt(:LD_LBFGS, length(pack(nn_outer)))\n",
    "\n",
    "#ftol_abs!(opt,1e-9)\n",
    "maxtime!(opt, 60*60*8)\n",
    "min_objective!(opt, tracking_loss_and_loss_grad!)\n",
    "\n",
    "θ = pack(nn_outer)\n",
    "\n",
    "\n",
    "(optf,optx,ret) = optimize!(opt,θ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unpack!(nn_outer, optx);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subset =  100:120\n",
    "xos = data[:,subset]\n",
    "yos = labels[:,subset]\n",
    "\n",
    "#ŷos,_ = feedfoward(nn_outer,xos);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ŷos=ŷ[:,subset]\n",
    "loss(ŷ,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Distances\n",
    "\n",
    "import WordEmbeddings.show_bests\n",
    "import WordEmbeddings.show_best\n",
    "import WordEmbeddings.neighbour_sims\n",
    "\n",
    "function neighbour_sims(cc::Vector{Float64}, globe::Matrix{Float64}, similarity=cosine_sim)\n",
    "    [similarity(cc, globe[:,ii]) for ii in 1:size(globe,2)]\n",
    "end\n",
    "\n",
    "\n",
    "function show_best(embedder,ĉ::Embedding, nbest=20, similarity=cosine_sim )\n",
    "    candidates=neighbour_sims(ĉ,embedder.L, similarity)   \n",
    "    best_cands = [ (findfirst(candidates,score), score)\n",
    "                    for score in select(candidates,1:nbest, rev=true)[1:nbest]]\n",
    "    vcat([[embedder.indexed_words[ii] round(score,2)] for (ii,score) in best_cands]...)\n",
    "end\n",
    "\n",
    "function show_bests(embedder,ĉs::Embeddings, nbest=20, similarity=cosine_sim)\n",
    "    hcat([show_best(embedder,ĉs[:,ii],nbest, similarity) for ii in 1:size(ĉs,2)]...)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_bests(we, ŷos, 1)[1,1:2:end] #(x,y)->-1.0*Distances.euclidean(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_bests(we, yos, 3)# (x,y)->-1.0*Distances.euclidean(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.9-pre",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
