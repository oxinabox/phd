{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addprocs(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import FunctionalCollections\n",
    "import Iterators\n",
    "import Pipe\n",
    "import Compat\n",
    "import JLD\n",
    "@everywhere using FunctionalCollections\n",
    "@everywhere using Iterators\n",
    "@everywhere using Pipe\n",
    "@everywhere using Compat\n",
    "@everywhere using JLD\n",
    "\n",
    "macro printval(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\" = \", $ee)))\n",
    "end\n",
    "\n",
    "macro pz(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\"\\t\\t\",typeof($ee), \"\\t\", size($ee))))\n",
    "end\n",
    "\n",
    "push!(LOAD_PATH, \".\")\n",
    "push!(LOAD_PATH, \"../util/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#shuffled_indexes = 1:length(ground_sents) |> collect |> shuffle!\n",
    "#nfolds=10\n",
    "#fold_indexes = Vector{Int}[\n",
    "#    shuffled_indexes[(ii-1)*end÷nfolds + 1: ii*end÷nfolds]\n",
    "#    for ii in 1:nfolds]\n",
    "\n",
    "#@save(\"brown_glove_folds.jld\", fold_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@everywhere fold_indexes=load(\"brown_glove_folds.jld\",\"fold_indexes\")\n",
    "\n",
    "@everywhere function fold_split(fold_ii, raw_bow_res)\n",
    "    ground_sents = Vector{ASCIIString}[rset[1] for rset in raw_bow_res]\n",
    "    reconstructed_bows = Vector{ASCIIString}[rset[2] for rset in raw_bow_res]\n",
    "    \n",
    "    test_indexes = fold_indexes[fold_ii]\n",
    "    training_indexes = trues(ground_sents)\n",
    "    training_indexes[test_indexes]=false\n",
    "\n",
    "    test_unordered_sents = reconstructed_bows[fold_indexes[fold_ii]]\n",
    "    test_ground = ground_sents[fold_indexes[fold_ii]]\n",
    "    training_sents = ground_sents[fold_indexes[fold_ii]]\n",
    "    test_unordered_sents,test_ground, training_sents\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import PyCall\n",
    "@everywhere using PyCall\n",
    "#http://www.nltk.org/howto/probability.html\n",
    "@everywhere @pyimport nltk\n",
    "@everywhere @pyimport nltk.probability as nltk_prob\n",
    "\n",
    "@everywhere function train_language_model{T}(train_corpus::Vector{Vector{T}})\n",
    "    function py_collect(xs::PyObject)\n",
    "        xst = []\n",
    "        for x in xs\n",
    "            push!(xst,x)\n",
    "        end\n",
    "        xst\n",
    "    end\n",
    "    function trigram_buffer(sent)\n",
    "        [START_MARKER1, START_MARKER2, sent..., END_MARKER1, END_MARKER2] \n",
    "    end\n",
    "\n",
    "    training_trigrams = vcat([py_collect(nltk.trigrams(trigram_buffer(sent))) for sent in train_corpus]...)\n",
    "    kn_prob_dist = nltk_prob.KneserNeyProbDist(pycall(nltk_prob.FreqDist, PyObject, training_trigrams))\n",
    "    \n",
    "    function trigram_model(given1::S, given2::S, event::S)\n",
    "        kn_prob_dist[:prob]((given1, given2, event))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@everywhere const START_MARKER1 = \"**START1**\"\n",
    "@everywhere const START_MARKER2 = \"**START2**\"\n",
    "@everywhere const END_MARKER1 = \"**END1**\"\n",
    "@everywhere const END_MARKER2 = \"**END2**\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@everywhere typealias S ASCIIString\n",
    "@everywhere typealias State{T} Tuple{T,T}\n",
    "@everywhere typealias OrderOptionsCache Dict{Tuple{State{S}, Vector{S}}, Vector{Tuple{plist{S}, Float64}}}\n",
    "\n",
    "\n",
    "#\"\"\"\n",
    "#returns all possible orderings of the remaining words.\n",
    "#the freewords are also inserted into every possible position, EXCEPT at the end.\n",
    "#Freewords also will not be inserted after other freewords\n",
    "#(For now)\n",
    "#\"\"\"\n",
    "@everywhere function get_all_orders(unordered_words::Vector{S}, languauge_model::Function; beam_width=Inf)\n",
    "    _get_options_cache = OrderOptionsCache()\n",
    "\n",
    "    function transition_prob(cur_state::State{S}, next_word::S)\n",
    "        languauge_model(cur_state[1],cur_state[2], next_word)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    function get_options(state::State{S}, remaining_words)\n",
    "        if length(remaining_words)>0\n",
    "            get!(_get_options_cache, (state, remaining_words)) do\n",
    "                _get_options(state, remaining_words)\n",
    "            end\n",
    "        else\n",
    "            _get_options(state)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    function _get_options(cur_state::State{S})\n",
    "        tp = transition_prob(cur_state, END_MARKER1)\n",
    "        # Given P(END_MARKER2 | curstatep[2]==END_MARKER1) = 1.0\n",
    "        # Do not need to consider P(END_MARKER2 | curstatep[2]==END_MARKER1, curstatep[1])\n",
    "        [(EmptyList{S}(), tp)]\n",
    "    end\n",
    "    \n",
    "    function _get_options(cur_state::State{S}, remaining_words)\n",
    "        function inner()\n",
    "            (@task begin\n",
    "                branches = Tuple{Int, Float64}[(ii,transition_prob(cur_state, next_word)) \n",
    "                                                for (ii, next_word) in enumerate(remaining_words)]\n",
    "                beam_end = min(length(remaining_words), beam_width)\n",
    "                beam = select!(branches, 1:beam_end, by=ip->ip[2], rev=true)\n",
    "                \n",
    "                for (next_word_ii, tp) in beam\n",
    "                    if tp==0.0\n",
    "                        break #There are No good solutions left (as it is sorted best first)\n",
    "                    end\n",
    "                    \n",
    "                    @inbounds word = remaining_words[next_word_ii]\n",
    "                    new_remaining_words = sub(remaining_words,[1:next_word_ii-1; next_word_ii+1:length(remaining_words)])\n",
    "                    @inbounds next_state = (cur_state[2],word)\n",
    "                    tails_and_tailprobs = get_options(next_state, new_remaining_words) #Actually doing a beam depth-first\n",
    "                    \n",
    "                    for (tail, tailprob) in tails_and_tailprobs\n",
    "                        total_prob = tp*tailprob\n",
    "                        if total_prob>0.0\n",
    "                            produce(cons(word, tail), total_prob)\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end)\n",
    "        end\n",
    "        \n",
    "        inner() |> collect\n",
    "    end\n",
    "    \n",
    "    initial_state = (START_MARKER1, START_MARKER2)\n",
    "    get_options(initial_state, unordered_words)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function order(unordered_words::Vector{S}, language_model::Function; beam_width=Inf, best_n=1::Int)\n",
    "    orders_and_probs = get_all_orders(free_words, unordered_words, beam_width)\n",
    "    best_n = min(best_n, length(orders_and_probs))\n",
    "    if best_n==0 #None found\n",
    "        #warn(\"No possible Orderding found. Defaulting to unordered: \".*string(unordered_words))\n",
    "        Tuple[(unordered_words, 0.0)]\n",
    "    elseif best_n==1 #4x as fast as the else\n",
    "        max_prob, max_ii = @pipe orders_and_probs |> map(op->op[2],_) |> findmax\n",
    "        Tuple[orders_and_probs[max_ii]]\n",
    "    else\n",
    "        select!(orders_and_probs, 1:best_n, by=op->-op[2] )\n",
    "    end\n",
    "end\n",
    "\n",
    "function norm_order(unordered_words::Vector{S}, language_model::Function;  kwargs...)\n",
    "    orders_and_probs = order(unordered_words, language_model; kwargs...)\n",
    "    total_prob = @pipe orders_and_probs |> map(op->op[2], _ ) |> sum\n",
    "    total_prob = total_prob == 0.0 ? 1.0 : total_prob\n",
    "    [(join(order, \" \"), prob/total_prob) for (order,prob) in orders_and_probs]\n",
    "end\n",
    "\n",
    "@everywhere function best_order(unordered_words::Vector{S}, language_model::Function;  kwargs...)\n",
    "    orders_and_probs = get_all_orders(unordered_words, language_model; kwargs...)\n",
    "    if length(orders_and_probs)==0\n",
    "        return (unordered_words, 0.0)\n",
    "    end\n",
    "        \n",
    "    total_prob = Pipe.@pipe orders_and_probs |> map(op->op[2], _ ) |> sum\n",
    "    max_prob, max_ii = Pipe.@pipe orders_and_probs |> map(op->op[2],_) |> findmax\n",
    "    order, prob = orders_and_probs[max_ii]\n",
    "    @assert(prob==max_prob)\n",
    "    order, prob/total_prob\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "function test(fold_ii)\n",
    "    test_bows, test_ground_order, train = fold_split(fold_ii,raw_bow_res)\n",
    "    r_language_models =Dict([pid=>remotecall(pid, train_language_model, train) for pid in workers()])\n",
    "    pmap([(\"a\",\"man\", \"did\"),(\"the\",\"woman\", \"did\")], err_stop=true) do abc\n",
    "        lm = fetch(r_language_models[myid()])\n",
    "        lm(abc...)\n",
    "        \n",
    "    end\n",
    "end\n",
    "\n",
    "test(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@everywhere const len_cap = 18\n",
    "@everywhere raw_bow_res = load(\"results/bags/brown_glove300_res.jld\", \"res\")\n",
    "\n",
    "function process_fold(fold_ii)\n",
    "    test_bows, test_ground_order, train = fold_split(fold_ii,raw_bow_res)\n",
    "    \n",
    "    #Avoid \"serialising a pointer\" by getting each process to create their own copy of Language model\n",
    "    #This also means that the language models are independent (as under the hood they are not readonly, readomg them changes them as they have cache)\n",
    "    r_language_models =Dict([pid=>remotecall(pid, train_language_model, train) for pid in workers()])\n",
    "    pmap(test_ground_order, test_bows, err_stop=true) do ground_order, bow\n",
    "        lm = fetch(r_language_models[myid()])\n",
    "        generated_order, prob = if length(ground_order)<=len_cap\n",
    "            best_order(bow, lm; beam_width=5)\n",
    "        else\n",
    "            (bow, NaN)\n",
    "        end\n",
    "        (ground_order, generated_order, prob)\n",
    "    end\n",
    "end\n",
    "\n",
    "jldopen(\"brown_glove300_ordered.jld\", \"w\") do file\n",
    "    for fold_ii in 1:length(fold_indexes)\n",
    "        res = process_fold(fold_ii)\n",
    "        write(file, \"fold_$(fold_ii)\", res)\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@load \"brown_glove300_ordered.jld\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_bows, test_ground_order, train = fold_split(2,raw_bow_res)\n",
    "language_model = train_language_model(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@time ord, prob = best_order(test_bows[4110], language_model; beam_width=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = pmap(test_bow, err_stop=true) do bow\n",
    "    order, prob = best_order(bow, language_model; beam_width=5)\n",
    "    (target_sent, sol, score)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "5^20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@pipe test |> map(x->length(x)==50, _) |> find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@time best_order(test[60], language_model, beam_width=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ground_sents[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_trigrams = [nltk.trigrams(sent)|>collect for sent in train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "map(examples) do unordered_words\n",
    "    order(unordered_words, ASCIIString[])\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "short_cases = Bool[length(ws) <=20 for ws in unordered_output]\n",
    "\n",
    "true_ordered_sents = test_set[short_cases]\n",
    "ordered_sents_and_probs = pmap(unordered_output[short_cases]) do unordered_words\n",
    "    order(unordered_words, zeroed_words)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ordered_sents = map(op->op[1], ordered_sents_and_probs, be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perfect_matches = Bool[]  \n",
    "for ii in 1:length(ordered_sents)\n",
    "    ordered_words = ordered_sents[ii]\n",
    "    actual_words = true_ordered_sents[ii]\n",
    "    \n",
    "    match = ordered_words == actual_words\n",
    "    push!(perfect_matches, match)\n",
    "    #println(\"$ii - $match\")\n",
    "end\n",
    "mean(perfect_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@pyimport nltk\n",
    "@pyimport nltk.translate.bleu_score as nltk_bleu\n",
    "\n",
    "function bleu_score(candidate, reference)\n",
    "    reference = reference |> collect\n",
    "    candidate = candidate |> collect\n",
    "    \n",
    "    if reference==candidate #Perfect Match\n",
    "        1.0\n",
    "    else\n",
    "        weights = [1,1,1,1]/4\n",
    "        nltk_bleu.bleu(Any[reference],candidate, weights)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map(bleu_score, ordered_sents,true_ordered_sents) |> mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bleu_score(true_ordered_sents[3], true_ordered_sents[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_ordered_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ordered_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ordered_sents[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unordered_output[short_cases][eval_cases][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_ordered_sents[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x= [\"A\", \"B\", \"C\", \"D\"]\n",
    "#y= UTF8String[\"A\", \"B\", \"C\", \"D\"]\n",
    "x = true_ordered_sents[10]\n",
    "y=ordered_sents[10] |> collect\n",
    "pycall(nltk_bleu.bleu, PyAny, Any[x], y, Any[0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Any[true_ordered_sents[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk_bleu._modified_precision(Any[reference1, reference2, reference3],candidate1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@pyimport pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdb.runcall(nltk_bleu._modified_precision, [reference1, reference2, reference3],candidate1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I think I have to reimplement BLEU in julia as for some reason it does not play nice with PyCall\n",
    "# Can basically port http://www.nltk.org/_modules/nltk/align/bleu_score.html#bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pycall(nltk_bleu.bleu, Int, candidate1, [reference1], weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@pyimport nltk.util as nltk_util\n",
    "nltk_util.ngrams(candidate1,2) |> py_collections.Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@pyimport collections as py_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@pyimport nltk.util as nltk_util\n",
    "ngs = nltk_util.ngrams(candidate1,2)\n",
    "pycall(py_collections.Counter, PyObject, ngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0-dev",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
