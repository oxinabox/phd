{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Iterators\n",
    "using DataStructures\n",
    "using Pipe\n",
    "using Compat\n",
    "using HDF5\n",
    "\n",
    "macro printval(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\" = \", $ee)))\n",
    "end\n",
    "\n",
    "macro pz(ee)\n",
    "    ee_expr = @sprintf \"%s\" string(ee)\n",
    "    esc(:(println($ee_expr,\"\\t\\t\",typeof($ee), \"\\t\", size($ee))))\n",
    "end\n",
    "\n",
    "push!(LOAD_PATH, \".\")\n",
    "push!(LOAD_PATH, \"../util\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############## ATIS 2 DATA\n",
    "function atis_clean(ss)\n",
    "    @pipe (ss \n",
    "    |> replace(_, r\"\\[.*?\\] ?\",\"\")  #Remove Nonword sounds\n",
    "    |> replace(_, r\"\\<.*?\\> \",\"\")  #Remove Verbal Deltions\n",
    "    |> replace(_, r\"\\*(.*?)\\*\",s\"\\1\") #Remove mispronounciation marks\n",
    "    |> replace(_,r\"\\:|\\-\\s\\.\\s\\-\", \"\") #remove intraword pauses\n",
    "    |> replace(_,r\"\\w+\\- \",\"\") #remove stuttered words\n",
    "    |> replace(_, r\"[!\\.,\\?]\",\"\")        #Remove punctation as it is not used traditionally (see sro spec)\n",
    "    \n",
    "    |> replace(_, r\"\\s+\",' ') #Remove repeated spaces\n",
    "    #|> replace(_, r\"([A-Z])\\s([A-Z])\", s\"\\1\\2\") #Merge len(2) abbrev\n",
    "    |> basic_clean\n",
    "    )\n",
    "end\n",
    "\n",
    "function basic_clean(ss)\n",
    "    @pipe (ss \n",
    "    |> replace(_, r\"\\s+\",' ') #Remove repeated spaces\n",
    "    |> lowercase\n",
    "    |> strip \n",
    "    )\n",
    "end\n",
    "\n",
    "\n",
    "function valid(ss)\n",
    "    typeof(ss) <: ASCIIString && length(ss)>0\n",
    "end\n",
    "\n",
    "\n",
    "path, ext, clean = (\"../../Resources/corpora/atis2_text/\", \".sro\", atis_clean)\n",
    "corpus = @pipe readdir(path)  |> filter!(fn -> splitext(fn)[2]==ext, _) |> map(_) do fn\n",
    "    try open(readall, path*fn) end\n",
    "    end |> filter!(valid,_) |> map(clean,_) |> filter!(valid,_) |> map(s->split(s),_) |> filter(ss->length(ss)>1, _);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "const START_MARKER1 = \"**START1**\"\n",
    "const START_MARKER2 = \"**START2**\"\n",
    "const END_MARKER1 = \"**END1**\"\n",
    "const END_MARKER2 = \"**END2**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############# Doctor's Letters, Brown\n",
    "using PyCall\n",
    "@pyimport nltk.corpus as nltk_corpus\n",
    "#corpus_reader=nltk_corpus.PlaintextCorpusReader(\"../../Resources/corpora/confidential_corpora/Doctors_Letters/files/\",\".*\\.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_reader=nltk_corpus.brown\n",
    "bonus_corpus = Vector{ASCIIString}[[lowercase(word) for word in sent] for sent in (corpus_reader[:sents]()|> collect)]\n",
    "bonus_corpus_vocab = @pipe bonus_corpus |> map(Set,_) |> reduce(union,_)\n",
    "length(bonus_corpus)\n",
    "corpus = @pipe [\"name this 1922 novel about leopold bloom written by james joyce\",\n",
    "    \"ralph waldo emerson dismissed this poet as the jingle man and james russell lowell called him three-fifths genius and two-fifths sheer fudge\",\n",
    "    \"this is the basis of a comedy of manners first performed in 1892\",\n",
    "    \"in a third novel a sailor abandons the patna and meets marlow who in another novel meets kurtz in the congo\",\n",
    "    \"thus she leaves her husband and child for aleksei vronsky but all ends sadly when she leaps in front of a train\",\n",
    "    \"we looked out at the setting sun .\",\n",
    "    \" i went to the kitchen .\",\n",
    "    \"how are you doing ?\"\n",
    "    ] |>map(split,_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_vocab = @pipe corpus |> map(Set,_) |> reduce(union!,_)\n",
    "union!(corpus_vocab, bonus_corpus_vocab)\n",
    "corpus_vocab |> length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############## Books Corpus\n",
    "corpus = Vector{Symbol}[]\n",
    "\n",
    "corpus_files = [\"../../Resources/corpora/\"*fn for fn in [\"Books/books_large_p1.txt\", \"Books/books_large_p2.txt\",]]\n",
    "for fn in corpus_files\n",
    "    open(fn) do corpus_fh\n",
    "        for sent in eachline(corpus_fh)\n",
    "            push!(corpus,[symbol(lowercase(word)) for word in split(sent)])\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "open(\"books_corpus.jsz\",\"w\") do fh\n",
    "    serialize(fh, corpus)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using JLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@save(\"books_corpus.jld\", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_vocab = Set{Symbol}()\n",
    "for sent in corpus \n",
    "    union!(corpus_vocab, sent)\n",
    "end\n",
    "corpus_vocab = Set{ASCIIString}(map(string,corpus_vocab))\n",
    "corpus_vocab |> length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using WordEmbeddings\n",
    "LL, word_indexes, indexed_words =\n",
    "    #load_word2vec_embeddings(\"word_emb_data/GoogleNews-vectors-negative300.bin\", length(corpus_vocab), corpus_vocab);\n",
    "    load_embeddings(\"word_embeddings/glove.6B.300d.txt\", length(corpus_vocab), corpus_vocab);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unknown_words = setdiff(corpus_vocab,indexed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Kind of the opposite of a stop word. This word has little meaning (So zero value), but much structural importance\n",
    "forcewords = [\"and\", \"a\", \"of\", \"to\", \"the\"]\n",
    "markers = [START_MARKER1,START_MARKER2,END_MARKER1,END_MARKER2]\n",
    "added_cols = 0\n",
    "for word in [forcewords..., markers...]\n",
    "    if !(word in indexed_words)\n",
    "        added_cols+=1\n",
    "        push!(indexed_words, word)\n",
    "        word_indexes[word] = length(indexed_words)\n",
    "    end\n",
    "end\n",
    "LL = [LL zeros(size(LL,1),length(added_cols))]\n",
    "    \n",
    "zeroed_words = forcewords[Bool[!haskey(word_indexes,word) for word in forcewords]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "known_vocab = Set(indexed_words)\n",
    "known_corpus = filter(corpus) do sent\n",
    "    for word in sent\n",
    "        if !(string(word) in known_vocab)\n",
    "            return false\n",
    "        end\n",
    "    end\n",
    "    true\n",
    "end;\n",
    "length(known_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "known_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_indexes = rand(length(known_corpus)).>0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "open(\"train_books_corpus.jsz\",\"w\") do fh\n",
    "    serialize(fh, known_corpus[!test_indexes])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sym = Dict{ByteString,Any}()\n",
    "data_sym[\"LL\"]= data[\"LL\"]\n",
    "data_sym[\"indexed_words\"] = map(Symbol, data[\"indexed_words\"])\n",
    "data_sym[\"word_indexes\"] = [Symbol(word)=>index for (word,index) in data[\"word_indexes\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using JLD\n",
    "jldopen(\"results/data/books300d.jld\", \"w\") do file\n",
    "    #write(file, \"corpus\", known_corpus)\n",
    "    \n",
    "    @write file LL\n",
    "    @write file word_indexes\n",
    "    @write file indexed_words\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################LOADING Done, Now processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_corpus = open(\"results/data/books/train_books_corpus.jsz\", \"r\") do fh\n",
    "    deserialize(fh)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "open(\"results/data/books/train_books_corpus.txt\", \"w\") do fh\n",
    "    for sent in train_corpus\n",
    "        write(fh, string(START_MARKER1)*\" \"*string(START_MARKER2)*\" \")\n",
    "        write(fh, join(sent, \" \"))\n",
    "        write(fh, \" \"*string(END_MARKER1)*\" \"*string(END_MARKER2)*\"\\n\")\n",
    "    end    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "#Old Processing (internally done langyage modelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_corpus = known_corpus[1:end÷10]\n",
    "train_corpus = known_corpus[1+end÷10:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using DataStructures\n",
    "\n",
    "function Base.sum(acc::Accumulator)\n",
    "    sum(values(acc.map))\n",
    "end\n",
    "\n",
    "function Base.sum(acc::Dict)\n",
    "    sum(values(acc))\n",
    "end\n",
    "\n",
    "function freq2prob{T,V<:Number}(acc::Union{Accumulator{T,V},Dict{T,V}})\n",
    "    \n",
    "    ret=Dict{T,Float64}()\n",
    "    total = sum(acc)\n",
    "    for (k,v) in acc\n",
    "        ret[k]=v/total\n",
    "    end\n",
    "    ret\n",
    "end\n",
    "\n",
    "function dict2mat(bigrams::Dict, word_indexes::Dict{AbstractString,Int64}, dense=False)\n",
    "    mat  = (dense ? zeros: spzeros)(length(word_indexes),length(word_indexes))\n",
    "    for first in keys(bigrams)\n",
    "        for second in keys(bigrams[first])\n",
    "            mat[word_indexes[second], word_indexes[first]] = bigrams[first][second]\n",
    "        end\n",
    "    end\n",
    "    mat\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "function collect_cooccur_stats(sentences)\n",
    "    unioccur = counter(AbstractString)\n",
    "    bioccur = DefaultDict(()->counter(AbstractString))\n",
    "    \n",
    "    for sent in sentences\n",
    "        for ii in 1:length(sent)\n",
    "            push!(unioccur, sent[ii])\n",
    "            for jj in 1:length(sent)\n",
    "                if ii==jj\n",
    "                    continue\n",
    "                end\n",
    "                push!(bioccur[sent[ii]], sent[jj])    \n",
    "            end\n",
    "        end       \n",
    "    end\n",
    "    \n",
    "    [k=>v.map for (k,v) in bioccur], unioccur.map\n",
    "    \n",
    "end\n",
    "\n",
    "bioccur_freq, unioccur_freq = collect_cooccur_stats(train_corpus)\n",
    "\n",
    "bioccur_mat = dict2mat(bioccur_freq,word_indexes,true)\n",
    "bioccur_mat.+=1.0 # Add one smoothing\n",
    "bioccur_mat./=sum(bioccur_mat)\n",
    "\n",
    "unioccur = freq2prob(unioccur_freq)\n",
    "unioccur_vec = Float64[haskey(unioccur,word) ? unioccur[word] : 0.0 for word in indexed_words]\n",
    "\n",
    "unioccur_vec_smoothed = Float64[word in keys(unioccur_freq) ? unioccur_freq[word] : 0.0 for word in indexed_words]\n",
    "unioccur_vec_smoothed.+=1.0 # Add one smoothing\n",
    "unioccur_vec_smoothed./=sum(unioccur_vec_smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Gadfly\n",
    "using Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_lengths = map(length, train_corpus)\n",
    "plot(x=sent_lengths, Geom.histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_length_dist = fit_mle(Gamma, sent_lengths)\n",
    "plot(x=[round(rand(sent_length_dist)) for _ in 1:length(sent_lengths)], Geom.histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "length_prob = cdf(sent_length_dist, collect(1.5:1.0:50.5)) - cdf(sent_length_dist, collect(0.5:1.0:49.5))\n",
    "length_prob./=sum(length_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using PyCall\n",
    "#http://www.nltk.org/howto/probability.html\n",
    "@pyimport nltk\n",
    "@pyimport nltk.probability as nltk_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "function trigram_buffer(sent)\n",
    "    [START_MARKER1, START_MARKER2, sent..., END_MARKER1, END_MARKER2] \n",
    "end\n",
    "\n",
    "train_trigrams = vcat([collect(nltk.trigrams(trigram_buffer(sent))) for sent in train_corpus]...)\n",
    "#tfd = pycall(nltk_prob.FreqDist, PyObject, all_trigrams)\n",
    "#kn_prob_dist = nltk_prob.KneserNeyProbDist(tfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"atis_data2.jsz\"\n",
    "#filename = \"doctors_letters.jsz\"\n",
    "#filename = \"brown_data.jsz\"\n",
    "\n",
    "open(filename,\"w\") do fh\n",
    "    data = Dict([\n",
    "        (\"trigrams\", train_trigrams),\n",
    "        (\"length_prob\", length_prob),\n",
    "        (\"LL\",LL),\n",
    "        (\"word_indexes\", word_indexes),\n",
    "        (\"indexed_words\", indexed_words),\n",
    "        (\"test_set\", test_corpus),\n",
    "        (\"zeroed_words\", zeroed_words)\n",
    "        ])\n",
    "    serialize(fh, data)    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function likelyhood(sent, bigrams)\n",
    "    words = split(sent)\n",
    "    words = [START_MARKER; words; END_MARKER]\n",
    "    \n",
    "    p=1.0\n",
    "    for ii in 1:length(words)-1\n",
    "        p*=bigrams[words[ii]][words[ii+1]]\n",
    "    end\n",
    "    p\n",
    "end\n",
    "\n",
    "\n",
    "function select_word{S<:AbstractString,V}(unigrams::Dict{S,V})\n",
    "    cutoff = rand()\n",
    "    total = 0.0\n",
    "    for next_word in keys(unigrams)\n",
    "        total+=unigrams[next_word]\n",
    "        if total>=cutoff\n",
    "            return next_word\n",
    "        end\n",
    "    end\n",
    "    assert(False, \"Should never reach here\") \n",
    "end\n",
    "\n",
    "function random_walk(bigrams)\n",
    "    words=[]\n",
    "    cur = START_MARKER\n",
    "    while(cur!=END_MARKER)\n",
    "        cur = select_word(bigrams[cur])\n",
    "        push!(words,cur)\n",
    "    end\n",
    "    words = words[1:end-1]\n",
    "    join(words, \" \")\n",
    "end\n",
    "\n",
    "walk =random_walk(kbigrams) \n",
    "print(walk*\"\\t\")\n",
    "print(likelyhood(walk,kbigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Magic Mass-Sharing Co-occurance PMF, Inspired by Bengio 2003\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Packing\n",
    "using Optim\n",
    "push!(LOAD_PATH, \"../Optimisation\")\n",
    "using AdaDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "addprocs(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@everywhere immutable LU_NN{N<:Number}\n",
    "    C::Matrix{N} #lookup matrix\n",
    "    H::Matrix{N}\n",
    "    d::Vector{N}\n",
    "    U::Matrix{N}\n",
    "    b::Vector{N}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset_indexes_for_len = Dict{Int,Vector{Vector{Bool}}}()\n",
    "\n",
    "\n",
    "function prepare_cases(sentence)\n",
    "    xs = Int[word_indexes[word] for word in sentence]\n",
    "    K = length(xs)\n",
    "    ss ::Vector{Vector{Bool}} = if haskey(subset_indexes_for_len,K)\n",
    "            subset_indexes_for_len[K]\n",
    "        else \n",
    "            subset_indexes_for_len[K] = Vector{Bool}[collect(pr) for pr in  product([(true,false) for k in 1:K]...)][2:end-1]\n",
    "        end\n",
    "    \n",
    "    CT = Tuple{Vector{Int},Vector{Int}}\n",
    "    #vcat([CT[(xs[bb],[y]) for y in xs[!bb]] for bb in ss]...)\n",
    "    CT[(xs[bb],xs[!bb]) for bb in ss]\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_cases = @pipe known_corpus |> map(prepare_cases,_) |> vcat(_...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function LU_NN(dEmb, dHidden, dOut)\n",
    "    LU_NN(\n",
    "        0.01*(randn((dEmb,dOut))), #C\n",
    "        0.01*(randn((dHidden,dEmb))), #H\n",
    "        0.01*(randn(dHidden)), #d\n",
    "        0.01*(randn((dOut,dHidden))), #U\n",
    "        0.01*(randn(dOut))#b\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@everywhere function softmax(xs)\n",
    "    numer = exp(xs)\n",
    "    numer./sum(numer)\n",
    "end\n",
    "\n",
    "\n",
    "@everywhere function σ(xs)\n",
    "    1.0./(1+exp(-xs))\n",
    "end\n",
    "\n",
    "#########################################\n",
    "\n",
    "\n",
    "\n",
    "@everywhere function δ(δ_above, W)\n",
    "    (W'*δ_above)\n",
    "end\n",
    "\n",
    "@everywhere function δ_tanh(δ_above, a)\n",
    "    const dz = 1-a.^2\n",
    "    δ_above.*dz\n",
    "end\n",
    "\n",
    "@everywhere function δ_σ(δ_above, a)\n",
    "    const dz = a .- a.^2\n",
    "    δ_above.*dz\n",
    "end\n",
    "\n",
    "@everywhere function δ_softmax_ce(actual, expected)\n",
    "    actual-expected\n",
    "end\n",
    "\n",
    "@everywhere function δ_output_sq_loss(actual, expected) \n",
    "    -(expected-actual)\n",
    "end\n",
    "\n",
    "#############################################\n",
    "\n",
    "@everywhere function sq_loss(actual, expected)\n",
    "    0.5*sum((expected-actual).^2)\n",
    "end\n",
    "\n",
    "@everywhere function ce_loss(actual, expected)\n",
    "    @assert(all(actual.>0))\n",
    "    -sum(expected.*log(actual))\n",
    "end\n",
    "\n",
    "@everywhere function forward(x,nn::LU_NN)\n",
    "    aa = tanh(nn.H*x+nn.d)\n",
    "    out = softmax(nn.U*aa + nn.b)\n",
    "    out, aa\n",
    "end\n",
    "\n",
    "@everywhere function feedforward_backprop(xx,nn::LU_NN, expected_output)\n",
    "    actual_output, aa = forward(xx,nn)\n",
    "    err = ce_loss(actual_output, expected_output)\n",
    "    #δ_top = δ_tanh(δ_softmax_ce(actual_output, expected_output),actual_output)\n",
    "    δ_top = δ_softmax_ce(actual_output, expected_output)\n",
    "    ΔU  = δ_top*aa'\n",
    "    Δb  = δ_top\n",
    "    δ_hidden = δ_tanh(δ(δ_top, nn.U),aa)\n",
    "    ΔH  = δ_hidden*xx'\n",
    "    Δd  = δ_hidden\n",
    "    δ_bottom = δ(δ_hidden, nn.H)\n",
    "    Δx  = δ_bottom\n",
    "    Δx,ΔH,Δd,ΔU,Δb,err\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@everywhere function mysubarray(xs, id=myid(), nchunks=nworkers())\n",
    "    #THis is broken if workers are not sequentially numbered from 2:nworkers\n",
    "    len = length(xs)\n",
    "    chunk_size = div(len, nchunks+1)\n",
    "    start_index = (id-2)*chunk_size + 1\n",
    "    end_index = start_index+chunk_size-1\n",
    "    print(start_index : end_index)\n",
    "    sub(xs, start_index : end_index)\n",
    "end\n",
    "\n",
    "@everywhere function expand_training_case(given_iis, target_iis, nn)\n",
    "    given_sowe = length(given_iis)>0 ? sum([nn.C[:,g_ii] for g_ii in given_iis]) : zeros(nn.C[:,1])\n",
    "    target = zeros(nn.b) #just while we are testing use a one hot set rep\n",
    "    for t_ii in target_iis\n",
    "        target+=1.0/length(target_iis)\n",
    "    end\n",
    "    given_sowe,target\n",
    "end\n",
    "\n",
    "@everywhere function train_one(given_iis, target_iis, nn)\n",
    "    given_sowe,target = expand_training_case(given_iis, target_iis, nn)\n",
    "        \n",
    "    Δx,ΔH,Δd,ΔU,Δb,err = feedforward_backprop(given_sowe,nn, target)\n",
    "    \n",
    "    ΔC = zeros(nn.C)\n",
    "    for g_ii in given_iis\n",
    "        @inbounds ΔC[:, g_ii]+=Δx./length(given_iis)\n",
    "    end\n",
    "    ΔC,ΔH,Δd,ΔU,Δb,err\n",
    "end\n",
    "\n",
    "\n",
    "function train_all(training_cases,nn)\n",
    "    \n",
    "    function accumulate_training_over(cases,fun)\n",
    "        totals = Any[  \n",
    "            zeros(nn.C),\n",
    "            zeros(nn.H),\n",
    "            zeros(nn.d),\n",
    "            zeros(nn.U),\n",
    "            zeros(nn.b),\n",
    "            0.0]\n",
    "        for case in cases\n",
    "            Δs = fun(case)\n",
    "            for ii in 1:length(totals)\n",
    "                totals[ii]+=Δs[ii]\n",
    "            end\n",
    "        end\n",
    "        totals\n",
    "    end\n",
    "    \n",
    "    function train_remote()\n",
    "        accumulate_training_over(mysubarray(training_cases),\n",
    "                                 gt_iis -> train_one(gt_iis[1], gt_iis[2], nn) )\n",
    "    end\n",
    "    \n",
    "    r_updates = [@spawnat(id, train_remote())  for id in workers()]\n",
    "        \n",
    "    totals = accumulate_training_over(r_updates, fetch)\n",
    "    ([tot./length(training_cases) for tot in totals]...)\n",
    "end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function uncached_loss_and_loss_grad!(θ::Vector, grad::Vector)    \n",
    "    unpack!(θ, nn_outer)\n",
    "    train_results = train_all(training_cases, nn_outer)\n",
    "    Δs = train_results[1:end-1]\n",
    "    err = train_results[end]\n",
    "    pack!(grad, Δs...)\n",
    "    err+0.01*sum(θ.^2)\n",
    "end\n",
    "\n",
    "_loss_and_loss_grad=Dict{Vector{Float64},Tuple{Float64, Vector{Float64}}}()\n",
    "function loss_and_loss_grad!(θ::Vector, grad::Vector)    \n",
    "    if haskey(_loss_and_loss_grad,θ)\n",
    "        err, grad[:]= _loss_and_loss_grad[θ]\n",
    "    else\n",
    "        err = uncached_loss_and_loss_grad!(θ, grad)\n",
    "        _loss_and_loss_grad[θ] = (err, copy(grad))\n",
    "    end\n",
    "    err\n",
    "end\n",
    "\n",
    "function loss!(θ::Vector)  \n",
    "    global nn_outer\n",
    "    unpack!(θ, nn_outer)\n",
    "    total_loss = @parallel (+) for (g_ii, t_ii) in training_cases\n",
    "        x, target = expand_training_case(g_ii, t_ii,nn_outer)\n",
    "        actual, _ = forward(x,nn_outer)\n",
    "        ce_loss(actual, target)    \n",
    "    end\n",
    "    total_loss/length(training_cases)\n",
    "\n",
    "end\n",
    "\n",
    "function loss_grad!(θ::Vector, grad::Vector) \n",
    "    #warn(\"loss_grad not defined\")\n",
    "    loss_and_loss_grad!(θ, grad)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using MultivariateStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dEmb = 9\n",
    "nn_outer = LU_NN(dEmb, 6, length(indexed_words));\n",
    "nn_outer.b[:] = unioccur_vec;\n",
    "\n",
    "nn_outer.C[:] = transform(fit(PCA, LL; maxoutdim=dEmb), LL) |> vec\n",
    "\n",
    "\n",
    "θ=pack(nn_outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss!(θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opt_func = DifferentiableFunction(loss!,loss_grad!,loss_and_loss_grad!)\n",
    "\n",
    "@time res = optimize(opt_func, θ, method=:l_bfgs, show_trace = true, store_trace = true, iterations = 10);\n",
    "#@time res = adadelta(opt_func, θ, show_trace = true, iterations = 500);\n",
    "@printval res.f_calls \n",
    "@printval res.g_calls \n",
    "@printval res.iterations\n",
    "@printval res.f_minimum\n",
    "@printval res.gr_converged\n",
    "@printval res.x_converged                       \n",
    "@printval res.f_converged \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sowe = nn_outer.C[:,word_indexes[\"find\"]]\n",
    "ns,_ = forward(sowe,nn_outer)\n",
    "indexed_words[findmax(ns)[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@printval res.gr_converged\n",
    "@printval res.x_converged                       \n",
    "@printval res.f_converged \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var(WW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "known_corpus[end-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LL[:,word_indexes[\"first\"]] + LL[:,word_indexes[\"class\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn_outer = LU_NN(8, 16, 19);\n",
    "\n",
    "xx=sum([nn_outer.C[:,g_ii] for g_ii in [1,2,3]])\n",
    "target = zeros(nn_outer.b)\n",
    "target[[10,11,12]]=1.0/3\n",
    "\n",
    "\n",
    "using ForwardDiff\n",
    "\n",
    "function f(θ)\n",
    "    x,H,d,U,b = unpack(θ,size(xx),(16,8),16,(19,16),19) \n",
    "    nn =  LU_NN(0.0*H, H,d,U,b)#using 0.0*H as a dummy value for C\n",
    "    \n",
    "    actual, _=forward(x,nn)\n",
    "    sq_loss(actual, target)\n",
    "end\n",
    "\n",
    "function calc_ag(θ)\n",
    "    x,H,d,U,b = unpack(θ,size(xx),(16,8),16,(19,16),19)\n",
    "    nn =  LU_NN(0.0*H, H,d,U,b) #using 0.0*H as a dummy value for C\n",
    "    Δx,ΔH,Δd,ΔU,Δb,err = feedforward_backprop(x,nn, target)\n",
    "end\n",
    "\n",
    "g = ForwardDiff.gradient(f)\n",
    "\n",
    "t=pack(xx,nn_outer.H,nn_outer.d,nn_outer.U,nn_outer.b)\n",
    "a_x, a_H, a_d, a_U, a_b, a_err = calc_ag(t)\n",
    "dg = g(t)  #Commented out so can't be run  \n",
    "d_x, d_H, d_d, d_U, d_b = unpack(dg,size(xx),(16,8),16,(19,16),19)\n",
    "\n",
    "\n",
    "@printval f(t) == a_err\n",
    "@printval findmax(abs(d_x .- a_x))\n",
    "@printval findmax(abs(d_H .- a_H))\n",
    "@printval findmax(abs(d_d .- a_d))\n",
    "@printval findmax(abs(d_U .- a_U))\n",
    "@printval findmax(abs(d_b .- a_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[a_err f(t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0-dev",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
