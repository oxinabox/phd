{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.chdir(\"prepared_corpora/opinosis/\")\n",
    "os.chdir(\"prepared_corpora/msrp/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrases = None\n",
    "with open(\"phrases.txt\", \"r\") as phrases_fh:\n",
    "    phrases = list(phrases_fh)\n",
    "\n",
    "#tokenised_phrases = nltk.tokenize.SpaceTokenizer().tokenize_sents(map(str.lower, phrases))\\\n",
    "tokenised_phrases = nltk.tokenize.TreebankWordTokenizer().tokenize_sents(map(str.lower, phrases))\n",
    "\n",
    "with open(\"tokenized_phrases.txt\",\"w\") as fh:\n",
    "    for phrase in tokenised_phrases:\n",
    "        fh.write(\" \".join(phrase))\n",
    "        fh.write(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def infer_and_save(model,tokenised_phrases, savename):\n",
    "    with open(savename, \"w\") as outvectors_fh:\n",
    "        for tockenised_line in tokenised_phrases:\n",
    "            vector = model.infer_vector(tockenised_line)\n",
    "            outvectors_fh.write(str(vector.tolist())[1:-1])\n",
    "            outvectors_fh.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec.load(\"../../models/wiki_sentence_model_concat_model\")\n",
    "infer_and_save(model, tokenised_phrases,\"outVectors_wiki_sentence_concat_pvdm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1209507"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec.load(\"../../models/wiki_sentence_model_dbow_model\")\n",
    "infer_and_save(model, tokenised_phrases,\"outVectors_wiki_sentence_dbow.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.corpora.dictionary as gensim_bow\n",
    "\n",
    "\n",
    "class BowModel(object):\n",
    "    def __init__(self, tokenised_phrases):\n",
    "        self.bow_dict = gensim_bow.Dictionary(tokenised_phrases)\n",
    "        \n",
    "    def _unsparse(self, bow_kv_list):\n",
    "        ret = np.zeros(len(self.bow_dict))\n",
    "        for key, value in bow_kv_list:\n",
    "            ret[key]=value\n",
    "        return ret\n",
    "    \n",
    "    def infer_vector(self,tokenised_phrase):\n",
    "        bow = self.bow_dict.doc2bow(tokenised_phrase)\n",
    "        return self._unsparse(bow)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bow_model = BowModel(tokenised_phrases)\n",
    "infer_and_save(bow_model, tokenised_phrases,\"outVectors_bow.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Word Embeeding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import io\n",
    "\n",
    "def read_until(until_char,stream):\n",
    "    \"\"\"Consumes up to and including the next occurance of the until_char. Raises EOFError if reached end of file without finign the untilchar\"\"\"\n",
    "    ret = b''\n",
    "    while(True):\n",
    "        read_char=stream.read(1)\n",
    "        if len(read_char)==0:\n",
    "            raise EOFError()\n",
    "        elif read_char==until_char:\n",
    "            return ret\n",
    "        else:\n",
    "            ret+=read_char\n",
    "        \n",
    "        \n",
    "def load_word2vec_embeddings(embedding_file, words_to_keep=set()):\n",
    "    \"\"\"\n",
    "    words_to_keep: if this is a nonempty set then only those words will be kept. if it is empty (or not given) then all words will be kept\n",
    "    \"\"\"\n",
    "    \n",
    "    word_vectors = dict()\n",
    "    \n",
    "    with io.open(embedding_file,\"rb\") as fh:\n",
    "        #return fh\n",
    "        vocab_size, vector_size = [int(s) for s in fh.readline().split()]\n",
    "        for ii in range(vocab_size):\n",
    "            word = read_until(b' ', fh)\n",
    "            word = word.decode(encoding='iso-8859-1')\n",
    "            \n",
    "            #print(word)\n",
    "            encoded_embedding = fh.read(4*vector_size)#4 bytes for Float32\n",
    "            \n",
    "            if len(words_to_keep)==0 or word in words_to_keep:\n",
    "                word_vectors[word] = np.fromstring(encoded_embedding, dtype=np.float32, count=vector_size)    \n",
    "    \n",
    "    return word_vectors\n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_vectors = load_word2vec_embeddings(\"../../../../../Resources/example_code/word2vec/GoogleNews-vectors-negative300.bin\", bow_model.bow_dict.token2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SumOfWordEmbeddingsModel(object):\n",
    "    def __init__(self, tokenised_phrases, word_embeddings, mean=False):\n",
    "        self.word_vectors = word_embeddings\n",
    "        self.mean=mean\n",
    "\n",
    "    \n",
    "    def infer_vector(self,tokenised_phrase):\n",
    "        n_words = 0\n",
    "        sum_of_word_embeddings = np.zeros(300)\n",
    "        for word in tokenised_phrase:\n",
    "            if word in self.word_vectors:\n",
    "                n_words+=1\n",
    "                sum_of_word_embeddings+=self.word_vectors[word]\n",
    "        if self.mean:\n",
    "            return sum_of_word_embeddings/n_words\n",
    "        else:\n",
    "            return sum_of_word_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SumOfWordEmbeddingsModel(tokenised_phrases, word_vectors,mean=True)\n",
    "infer_and_save(model, tokenised_phrases,\"outVectors_mowe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SumOfWordEmbeddingsModel(tokenised_phrases, word_vectors,mean=False)\n",
    "infer_and_save(model, tokenised_phrases,\"outVectors_sowe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " \"'\",\n",
       " \"''\",\n",
       " \"''fewer\",\n",
       " \"''i\",\n",
       " \"''is\",\n",
       " \"'s\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '--',\n",
       " '.',\n",
       " '...',\n",
       " '.20',\n",
       " '.85',\n",
       " '.dji',\n",
       " '.ixic',\n",
       " '.spx',\n",
       " '0.02',\n",
       " '0.09',\n",
       " '0.10',\n",
       " '0.15',\n",
       " '0.17',\n",
       " '0.18',\n",
       " '0.2',\n",
       " '0.20',\n",
       " '0.21',\n",
       " '0.3',\n",
       " '0.34',\n",
       " '0.41',\n",
       " '0.42',\n",
       " '0.48',\n",
       " '0.5',\n",
       " '0.52',\n",
       " '0.6',\n",
       " '0.60',\n",
       " '0.7',\n",
       " '0.71',\n",
       " '0.9',\n",
       " '0.96',\n",
       " '0.98',\n",
       " '1,100',\n",
       " '1,200',\n",
       " '1,366',\n",
       " '1,600,000',\n",
       " '1,912.65',\n",
       " '1,917.67',\n",
       " '1,921.33',\n",
       " '1.02',\n",
       " '1.15',\n",
       " '1.2',\n",
       " '1.25',\n",
       " '1.36',\n",
       " '1.4',\n",
       " '1.47',\n",
       " '1.6',\n",
       " '1.63',\n",
       " '1/2',\n",
       " '10',\n",
       " '10.89',\n",
       " '100',\n",
       " '107.6',\n",
       " '10:33',\n",
       " '11',\n",
       " '11,000',\n",
       " '11,400',\n",
       " '11.14',\n",
       " '11.64',\n",
       " '11.92',\n",
       " '114',\n",
       " '114.3',\n",
       " '117kg',\n",
       " '118',\n",
       " '11th',\n",
       " '12',\n",
       " '12,400',\n",
       " '12-by-18-inch',\n",
       " '12-inch-by-18-inch',\n",
       " '12-month',\n",
       " '12-nation',\n",
       " '120,000',\n",
       " '121.51',\n",
       " '128-bit',\n",
       " '12:01',\n",
       " '13',\n",
       " '130',\n",
       " '14',\n",
       " '14-member',\n",
       " '14-strong',\n",
       " '14.7',\n",
       " '140',\n",
       " '143-year',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '17-year-old',\n",
       " '17-year-olds',\n",
       " '18',\n",
       " '18.06',\n",
       " '18.34',\n",
       " '1975',\n",
       " '1980s',\n",
       " '1989',\n",
       " '1994',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '2,500',\n",
       " '2,800',\n",
       " '2.11',\n",
       " '2.2-to-',\n",
       " '2.3',\n",
       " '2.5',\n",
       " '2.9',\n",
       " '2.9-million',\n",
       " '2.96',\n",
       " '20',\n",
       " '20-ounce',\n",
       " '2000',\n",
       " '2001',\n",
       " '2002',\n",
       " '2003',\n",
       " '2004',\n",
       " '20s',\n",
       " '21',\n",
       " '21-year-old',\n",
       " '21.03',\n",
       " '21.2',\n",
       " '21.51',\n",
       " '21/2-week',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '250',\n",
       " '26',\n",
       " '260',\n",
       " '27',\n",
       " '27-year-old',\n",
       " '27.3',\n",
       " '28',\n",
       " '28.08',\n",
       " '28.2',\n",
       " '3-million',\n",
       " '3.2',\n",
       " '3.28',\n",
       " '3.8',\n",
       " '30',\n",
       " '300',\n",
       " '30th',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '361,400',\n",
       " '38',\n",
       " '385',\n",
       " '39.5',\n",
       " '395,000',\n",
       " '4.3',\n",
       " '4.4',\n",
       " '4.62',\n",
       " '4.7',\n",
       " '4.77',\n",
       " '4.82',\n",
       " '40',\n",
       " '44.32',\n",
       " '45',\n",
       " '469,000',\n",
       " '47-hour',\n",
       " '47.9',\n",
       " '48',\n",
       " '49',\n",
       " '491',\n",
       " '494.20',\n",
       " '496.83',\n",
       " '497.42',\n",
       " '5.1',\n",
       " '5.20',\n",
       " '5.6',\n",
       " '5.8',\n",
       " '50',\n",
       " '500',\n",
       " '52',\n",
       " '56',\n",
       " '6,000',\n",
       " '6.4',\n",
       " '6.62',\n",
       " '6.7',\n",
       " '6.77',\n",
       " '6.9',\n",
       " '60',\n",
       " '60-year-old',\n",
       " '600',\n",
       " '62',\n",
       " '63',\n",
       " '64',\n",
       " '64.64',\n",
       " '65',\n",
       " '65,000',\n",
       " '65-year-old',\n",
       " '66',\n",
       " '67',\n",
       " '68',\n",
       " '69.89',\n",
       " '7.15',\n",
       " '7.2',\n",
       " '7.5',\n",
       " '7.6',\n",
       " '7.8',\n",
       " '70',\n",
       " '70-million',\n",
       " '70.74',\n",
       " '72',\n",
       " '729',\n",
       " '737s',\n",
       " '75',\n",
       " '78.70',\n",
       " '8,829',\n",
       " '8,832.69',\n",
       " '8-ounce',\n",
       " '8.2',\n",
       " '8.79',\n",
       " '8.8',\n",
       " '80',\n",
       " '897,158',\n",
       " '9,000',\n",
       " '9,165',\n",
       " '9,174.35',\n",
       " '9,191.09',\n",
       " '9,240.25',\n",
       " '9,256.49',\n",
       " '9-11',\n",
       " '9/11',\n",
       " '921',\n",
       " '922',\n",
       " '924.42',\n",
       " '9266.51',\n",
       " '927.62',\n",
       " '929.06',\n",
       " '929.62',\n",
       " '931',\n",
       " '931.12',\n",
       " '931.41',\n",
       " '934',\n",
       " '95',\n",
       " '987',\n",
       " '991',\n",
       " '9:33',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '[',\n",
       " ']',\n",
       " '``',\n",
       " 'a',\n",
       " 'aes.n',\n",
       " 'aest',\n",
       " 'agrawal',\n",
       " 'al-faisal',\n",
       " \"al-qa'ida\",\n",
       " 'al-qaeda',\n",
       " 'al-qaida',\n",
       " 'al-sadr',\n",
       " 'albertson',\n",
       " 'albertsons',\n",
       " 'and',\n",
       " 'anhalt',\n",
       " 'anti-discrimination',\n",
       " 'antonin',\n",
       " 'arifin',\n",
       " 'aritonang',\n",
       " 'ashcroft',\n",
       " 'assyrian',\n",
       " 'aung',\n",
       " 'award-winners',\n",
       " 'axelrod',\n",
       " 'baer',\n",
       " 'bankhead',\n",
       " 'baucus',\n",
       " 'baylor',\n",
       " 'behaviour',\n",
       " 'bergonzi',\n",
       " 'best-known',\n",
       " 'blaine',\n",
       " 'blodget',\n",
       " 'blood-alcohol',\n",
       " 'bloomfield',\n",
       " 'blue-chip',\n",
       " 'board-certified',\n",
       " 'body’s',\n",
       " 'born-again',\n",
       " 'box-office',\n",
       " 'breast-cancer',\n",
       " 'bremer',\n",
       " 'brianna',\n",
       " 'brig.',\n",
       " 'brigadier-general',\n",
       " 'brolin',\n",
       " 'bucklew',\n",
       " 'cantwell',\n",
       " 'carson-based',\n",
       " 'casablanca',\n",
       " 'cbo',\n",
       " 'celf',\n",
       " 'centre',\n",
       " 'centres',\n",
       " 'cepsa',\n",
       " 'chi-chi',\n",
       " 'chip-set',\n",
       " 'chip.m.aker',\n",
       " 'clarence',\n",
       " 'co-founded',\n",
       " 'co-founder',\n",
       " 'co-operation',\n",
       " 'co-ordinated',\n",
       " 'co-owned',\n",
       " 'co.',\n",
       " 'co.wag.n',\n",
       " 'coca-cola',\n",
       " 'comstock',\n",
       " 'conny',\n",
       " 'corp.',\n",
       " 'costley',\n",
       " 'court-martial',\n",
       " 'court-martialled',\n",
       " 'coyne',\n",
       " 'cuomo',\n",
       " 'cyberassets',\n",
       " 'd-wash.',\n",
       " 'dallager',\n",
       " 'daschle',\n",
       " 'dennehy',\n",
       " 'depression-era',\n",
       " 'djihad',\n",
       " 'don’t',\n",
       " 'dotson',\n",
       " 'double-whammy',\n",
       " 'dr.',\n",
       " 'drax',\n",
       " 'duisenberg',\n",
       " 'durousseau',\n",
       " 'dynes',\n",
       " 'e-books',\n",
       " 'e-mail',\n",
       " 'e-mails',\n",
       " 'e-newspapers',\n",
       " 'eaug.pa',\n",
       " 'eichhorn',\n",
       " 'eichorn',\n",
       " 'eleonora',\n",
       " 'eleonora67',\n",
       " 'english-turkish',\n",
       " 'english/turkish',\n",
       " 'erbitux',\n",
       " 'ex-girlfriend',\n",
       " 'fact-finding',\n",
       " 'filevault',\n",
       " 'first-degree',\n",
       " 'fischi',\n",
       " 'fla.',\n",
       " 'fog-shrouded',\n",
       " 'fourth-grade',\n",
       " 'franklyn',\n",
       " 'fujimori',\n",
       " 'gbi',\n",
       " 'gen.',\n",
       " 'gilbertson',\n",
       " 'gov.',\n",
       " 'grassley',\n",
       " 'guarini',\n",
       " 'h.',\n",
       " 'half-owned',\n",
       " 'hambali',\n",
       " 'hamdi',\n",
       " 'hamidi',\n",
       " 'harriet',\n",
       " 'hartsfield-jackson',\n",
       " 'hewlett-packard',\n",
       " 'heyer',\n",
       " 'high-grade',\n",
       " 'high-performance',\n",
       " 'high-speed',\n",
       " 'hoffa',\n",
       " 'hollins',\n",
       " 'hortons',\n",
       " 'hubach',\n",
       " 'i.s.d.',\n",
       " 'imclone',\n",
       " 'inc.jcp.n',\n",
       " 'instant-messaging',\n",
       " 'israel’s',\n",
       " 'j.',\n",
       " 'j.c.',\n",
       " 'jan.',\n",
       " 'jcp',\n",
       " 'jcp.n',\n",
       " 'jeffery',\n",
       " 'jolla',\n",
       " 'judgement',\n",
       " 'karanja',\n",
       " 'karas',\n",
       " 'keker',\n",
       " 'kiely',\n",
       " 'kinshasa',\n",
       " 'koufa',\n",
       " 'kroger',\n",
       " 'labour',\n",
       " 'laci',\n",
       " 'lakhani',\n",
       " 'law-enforcement',\n",
       " 'lemmon',\n",
       " 'licence',\n",
       " 'like-minded',\n",
       " 'long-lived',\n",
       " 'long-term',\n",
       " 'lorenzo',\n",
       " 'low-fat',\n",
       " 'magelang',\n",
       " 'maj-gen',\n",
       " 'malvo',\n",
       " 'many-sided',\n",
       " 'mccloskey',\n",
       " 'mccormack',\n",
       " 'montesinos',\n",
       " 'moqtada',\n",
       " 'morgenthau',\n",
       " 'morley',\n",
       " 'mr.',\n",
       " 'mrs.',\n",
       " 'mudd',\n",
       " 'multi-billion',\n",
       " 'multi-year',\n",
       " 'multibillion-dollar',\n",
       " 'multifoods',\n",
       " 'muqtada',\n",
       " 'mustapha',\n",
       " 'najaf',\n",
       " 'nation’s',\n",
       " 'natsemi',\n",
       " 'netanyahu',\n",
       " 'next-generation',\n",
       " 'northfield-based',\n",
       " 'nov.',\n",
       " \"o'connor\",\n",
       " \"o'neill\",\n",
       " 'obetz',\n",
       " 'oct.',\n",
       " 'of',\n",
       " 'one-hour',\n",
       " 'one-third',\n",
       " 'oneills',\n",
       " 'organiser',\n",
       " 'out-of-court',\n",
       " 'palmsource',\n",
       " 'pc-related',\n",
       " 'pco/pcos',\n",
       " 'pcos',\n",
       " 'peoplesoft',\n",
       " 'per-share',\n",
       " 'policy-setting',\n",
       " 'president’s',\n",
       " 'pro-democracy',\n",
       " 'programmes',\n",
       " 'quattrone',\n",
       " 'racicot',\n",
       " 'ralphs',\n",
       " 'reagans',\n",
       " 'rehnquist',\n",
       " 'rep.',\n",
       " 'runner-up',\n",
       " 'rwanda',\n",
       " 'sahel',\n",
       " 'same-sex',\n",
       " 'same-store',\n",
       " 'scalia',\n",
       " 'scalise',\n",
       " 'schering-plough',\n",
       " 'schweer',\n",
       " 'second-story',\n",
       " 'selenski',\n",
       " 'self-incrimination',\n",
       " 'sen.',\n",
       " 'senate-passed',\n",
       " 'sept.',\n",
       " 'sgt.',\n",
       " \"shi'ites\",\n",
       " 'short-',\n",
       " 'short-lived',\n",
       " 'shumaker',\n",
       " 'sobig.f',\n",
       " 'soweto',\n",
       " 'sprayregen',\n",
       " 'stabilisation',\n",
       " 'state-controlled',\n",
       " 'state-run',\n",
       " 'station’s',\n",
       " 'stock-based',\n",
       " 'strayhorn',\n",
       " 'suu',\n",
       " 'tapias',\n",
       " 'tech-laden',\n",
       " 'technology-heavy',\n",
       " 'technology-laced',\n",
       " 'then-president',\n",
       " 'third-quarter',\n",
       " 'to',\n",
       " 'tony-award',\n",
       " 'torrey',\n",
       " 'turkmen',\n",
       " 'turkoman',\n",
       " 'two-term',\n",
       " 'two-year',\n",
       " 'u.n.',\n",
       " 'u.s.',\n",
       " 'u.s.-backed',\n",
       " 'u.s.-chosen',\n",
       " 'v.n',\n",
       " 'va.',\n",
       " 'venango',\n",
       " 'vigilante-style',\n",
       " 'visudyne',\n",
       " 'vladimiro',\n",
       " 'waagner',\n",
       " 'wag.n',\n",
       " 'wal-',\n",
       " 'wal-mart',\n",
       " 'walgreen',\n",
       " 'wash.',\n",
       " 'weaker-than-expected',\n",
       " 'werdegar',\n",
       " 'westfield',\n",
       " 'wheaton',\n",
       " 'wife’s',\n",
       " 'wolfgang',\n",
       " 'worshipper',\n",
       " 'xscale',\n",
       " 'yangon',\n",
       " 'yarralumla',\n",
       " 'year-long',\n",
       " 'zulfiqar',\n",
       " 'zulifquar',\n",
       " '½',\n",
       " '–',\n",
       " '“that'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonfound = set(bow_model.bow_dict.token2id.keys()).difference(model.word_vectors.keys())\n",
    "print(len(nonfound))\n",
    "nonfound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HistogramOfWordEmbeedingsModel(object):\n",
    "    def __init__(self, tokenised_phrases, word_embeddings, n_bins = 10):\n",
    "        self.word_vectors = word_embeddings\n",
    "        min_value = np.row_stack(word_vectors.values()).min()\n",
    "        max_value = np.row_stack(word_vectors.values()).max()\n",
    "        self.bins = np.linspace(min_value,max_value,n_bins)\n",
    "        \n",
    "    \n",
    "    def infer_vector(self,tokenised_phrase):\n",
    "        word_embeddings = np.row_stack([self.word_vectors[word] for word in tokenised_phrase if word in self.word_vectors])\n",
    "        def get_hists(col_slice):\n",
    "            return np.histogram(col_slice, bins=self.bins)[0]\n",
    "            \n",
    "        return np.apply_along_axis(get_hists,0,word_embeddings).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = HistogramOfWordEmbeedingsModel(tokenised_phrases, word_vectors)\n",
    "infer_and_save(model, tokenised_phrases,\"outVectors_howe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tokenised_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
