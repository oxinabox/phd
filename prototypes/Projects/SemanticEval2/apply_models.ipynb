{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"prepared_corpora/opinosis/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phrases = None\n",
    "with open(\"phrases.txt\", \"r\") as phrases_fh:\n",
    "    phrases = list(phrases_fh)\n",
    "\n",
    "#tokenised_phrases = nltk.tokenize.SpaceTokenizer().tokenize_sents(map(str.lower, phrases))\\\n",
    "tokenised_phrases = nltk.tokenize.TreebankWordTokenizer().tokenize_sents(map(str.lower, phrases))\n",
    "\n",
    "with open(\"tokenized_phrases.txt\",\"w\") as fh:\n",
    "    for phrase in tokenised_phrases:\n",
    "        fh.write(\" \".join(phrase))\n",
    "        fh.write(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def infer_and_save(model,tokenised_phrases, savename):\n",
    "    with open(savename, \"w\") as outvectors_fh:\n",
    "        for tockenised_line in tokenised_phrases:\n",
    "            vector = model.infer_vector(tockenised_line)\n",
    "            outvectors_fh.write(str(vector.tolist())[1:-1])\n",
    "            outvectors_fh.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec.load(\"../../models/wiki_sentence_model_concat_model\")\n",
    "infer_and_save(model, tokenised_phrases,\"outVectors_wiki_sentence_concat_pvdm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec.load(\"../../models/wiki_sentence_model_dbow_model\")\n",
    "infer_and_save(model, tokenised_phrases,\"outVectors_wiki_sentence_dbow.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.corpora.dictionary as gensim_bow\n",
    "\n",
    "\n",
    "class BowModel(object):\n",
    "    def __init__(self, tokenised_phrases):\n",
    "        self.bow_dict = gensim_bow.Dictionary(tokenised_phrases)\n",
    "        \n",
    "    def _unsparse(self, bow_kv_list):\n",
    "        ret = np.zeros(len(self.bow_dict))\n",
    "        for key, value in bow_kv_list:\n",
    "            ret[key]=value\n",
    "        return ret\n",
    "    \n",
    "    def infer_vector(self,tokenised_phrase):\n",
    "        bow = self.bow_dict.doc2bow(tokenised_phrase)\n",
    "        return self._unsparse(bow)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bow_model = BowModel(tokenised_phrases)\n",
    "infer_and_save(bow_model, tokenised_phrases,\"outVectors_bow.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Word Embeeding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Loads googles word2vec_embeddings\n",
    "function load_word2vec_embeddings(embedding_file, max_stored_vocab_size = 1000000)\n",
    "    #Note: I know there actually <10^6 words in the vocab, when phrases are exlusded, so lock the vocab size to this to save 70%RAM\n",
    "    #Words are loosely organised by commonness,  AFAICT\n",
    "    fh = open(embedding_file,\"r\")\n",
    "    vocab_size, vector_size = @pipe readline(fh)|> split |> map(int, _)\n",
    "    max_stored_vocab_size = min(max_stored_vocab_size, vocab_size) #if using a small vocab then there is a chance you might be willing ot store more words than it has\n",
    "    \n",
    "    \n",
    "    indexed_words = Array(String,max_stored_vocab_size)\n",
    "    word_indexes = Dict{String,Int64}()\n",
    "    LL = Array(Float32,(vector_size, max_stored_vocab_size))\n",
    "\n",
    "    #Add a Zero vector for the unknown words\n",
    "    LL[:,1]*=0\n",
    "    indexed_words[1]=UNKNOWN_WORD\n",
    "    word_indexes[UNKNOWN_WORD]=1\n",
    "\n",
    "    #Add all from data\n",
    "    index = 2\n",
    "    for _ in 1:vocab_size\n",
    "        word = readuntil(fh,' ') |> strip #Technically this is 'ISO-8859-1' may have to deal with encoding issues\n",
    "        vector = read(fh, Float32,vector_size ) \n",
    "\n",
    "        if !contains(word, \"_\") #If it isn't a phrase\n",
    "            LL[:,index]=vector./norm(vector)\n",
    "            indexed_words[index] = word\n",
    "            word_indexes[word] = index\n",
    "            \n",
    "            index+=1\n",
    "            if index>max_stored_vocab_size\n",
    "                warn(\"Max Vocab size exceeded. More words are available if you want.\")\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    close(fh)\n",
    "    \n",
    "    LL = LL[:,1:index-1] #throw away unused columns\n",
    "    indexed_words = indexed_words[1:index-1] #throw away unused columns\n",
    "    LL, word_indexes, indexed_words\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import io\n",
    "\n",
    "def read_until(until_char,stream):\n",
    "    \"\"\"Consumes up to and including the next occurance of the until_char. Raises EOFError if reached end of file without finign the untilchar\"\"\"\n",
    "    ret = b''\n",
    "    while(True):\n",
    "        read_char=stream.read(1)\n",
    "        if len(read_char)==0:\n",
    "            raise EOFError()\n",
    "        elif read_char==until_char:\n",
    "            return ret\n",
    "        else:\n",
    "            ret+=read_char\n",
    "        \n",
    "        \n",
    "def load_word2vec_embeddings(embedding_file, words_to_keep=set()):\n",
    "    \"\"\"\n",
    "    words_to_keep: if this is a nonempty set then only those words will be kept. if it is empty (or not given) then all words will be kept\n",
    "    \"\"\"\n",
    "    \n",
    "    word_vectors = dict()\n",
    "    \n",
    "    with io.open(embedding_file,\"rb\") as fh:\n",
    "        #return fh\n",
    "        vocab_size, vector_size = [int(s) for s in fh.readline().split()]\n",
    "        for ii in range(vocab_size):\n",
    "            word = read_until(b' ', fh)\n",
    "            word = word.decode(encoding='iso-8859-1')\n",
    "            \n",
    "            #print(word)\n",
    "            encoded_embedding = fh.read(4*vector_size)#4 bytes for Float32\n",
    "            \n",
    "            if len(words_to_keep)==0 or word in words_to_keep:\n",
    "                word_vectors[word] = np.fromstring(encoded_embedding, dtype=np.float32, count=vector_size)    \n",
    "    \n",
    "    return word_vectors\n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_vectors = load_word2vec_embeddings(\"../../../../../Resources/example_code/word2vec/GoogleNews-vectors-negative300.bin\", bow_model.bow_dict.token2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " \"'\",\n",
       " \"'manager\",\n",
       " \"'s\",\n",
       " \"'select\",\n",
       " \"'tasting\",\n",
       " ',',\n",
       " '.',\n",
       " '10',\n",
       " '100',\n",
       " '101',\n",
       " '15',\n",
       " '150',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '30',\n",
       " '39',\n",
       " \"5'10\",\n",
       " '75',\n",
       " '79',\n",
       " '90',\n",
       " '92',\n",
       " '?',\n",
       " 'a',\n",
       " 'acqure',\n",
       " 'and',\n",
       " 'can\\x92t',\n",
       " 'celan',\n",
       " 'courtious',\n",
       " 'e-ink',\n",
       " 'earily',\n",
       " 'favourable',\n",
       " 'fishermans',\n",
       " 'fisherman\\x92s',\n",
       " 'gloucester',\n",
       " 'inmaculate',\n",
       " 'jekyll',\n",
       " 'kensington',\n",
       " 'latei',\n",
       " 'of',\n",
       " 'offree',\n",
       " 'openzone',\n",
       " 'positivesthe',\n",
       " 'relaxment',\n",
       " 'socialise',\n",
       " 'to',\n",
       " 'toyotas',\n",
       " 'travellers',\n",
       " 'unaccomodating',\n",
       " 'visata',\n",
       " 'w7',\n",
       " 'witheach',\n",
       " '~10'}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(bow_model.bow_dict.token2id.keys()).difference(word_vectors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanOfWordEmbeedingsModel(object):\n",
    "    def __init__(self, tokenised_phrases, word_embeddings):\n",
    "        self.word_vectors = word_embeddings\n",
    "\n",
    "    \n",
    "    def infer_vector(self,tokenised_phrase):\n",
    "        n_words = 0\n",
    "        sum_of_word_embeddings = np.zeros(300)\n",
    "        for word in tokenised_phrase:\n",
    "            if word in self.word_vectors:\n",
    "                n_words+=1\n",
    "                sum_of_word_embeddings+=self.word_vectors[word]\n",
    "        return sum_of_word_embeddings/n_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MeanOfWordEmbeedingsModel(tokenised_phrases, word_vectors)\n",
    "infer_and_save(model, tokenised_phrases,\"outVectors_mowe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HistogramOfWordEmbeedingsModel(object):\n",
    "    def __init__(self, tokenised_phrases, word_embeddings, n_bins = 10):\n",
    "        self.word_vectors = word_embeddings\n",
    "        min_value = np.row_stack(word_vectors.values()).min()\n",
    "        max_value = np.row_stack(word_vectors.values()).max()\n",
    "        self.bins = np.linspace(min_value,max_value,n_bins)\n",
    "        \n",
    "    \n",
    "    def infer_vector(self,tokenised_phrase):\n",
    "        word_embeddings = np.row_stack([self.word_vectors[word] for word in tokenised_phrase if word in self.word_vectors])\n",
    "        def get_hists(col_slice):\n",
    "            return np.histogram(col_slice, bins=self.bins)[0]\n",
    "            \n",
    "        return np.apply_along_axis(get_hists,0,word_embeddings).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = HistogramOfWordEmbeedingsModel(tokenised_phrases, word_vectors)\n",
    "infer_and_save(model, tokenised_phrases,\"outVectors_howe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "669"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenised_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
