{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import itertools\n",
    "import codecs\n",
    "from os import path\n",
    "import csv as csv_module\n",
    "from copy import deepcopy\n",
    "\n",
    "from SemanticCorruption import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_MSRP(filename):\n",
    "    with codecs.open(filename,'r', b\"utf-8\" ) as fh:\n",
    "        nlines = 0\n",
    "        for line in fh.readlines():\n",
    "            nlines+=1\n",
    "            if nlines==1:\n",
    "                continue\n",
    "            isparaphrase, id1, id2, str1, str2 = line.split(\"\\t\") #the quality fielld is 1 for phraphrases and 0 for not\n",
    "            if int(isparaphrase)== 1:\n",
    "                yield((int(id1),str1.strip()),(int(id2),str2.strip()))\n",
    "\n",
    "            \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_eval_corpora(base_paraphrases, folder, max_corruption_level=10):\n",
    "    \"\"\"We want to create the baseline corpus as a new line seperated sentences, so that it works well with Sorchers URAE system.\n",
    "    Thus all the metadata is stored in seperate files referencing the line numbers\"\"\"\n",
    "    \n",
    "    global phrase_line_num \n",
    "    phrase_line_num = 0 #line numebrs are always refered to after icrementing them\n",
    "    openned_filehandles = []\n",
    "    try:\n",
    "        phrases_fh = codecs.open(path.join(folder, \"phrases.txt\"),'w', b\"utf-8\" )\n",
    "        openned_filehandles.append(phrases_fh)\n",
    "\n",
    "        def open_csv(filename, *headings):\n",
    "            fh = open(path.join(folder, filename),'w')\n",
    "            openned_filehandles.append(fh)\n",
    "            csv = csv_module.writer(fh)\n",
    "            return csv\n",
    "        \n",
    "\n",
    "        microsoft_ids_csv = open_csv(\"microsoft_ids.txt\", \"phrase_line_number\",\"microsoft_id\")\n",
    "        paraphrases_csv = open_csv(\"paraphrases.txt\", \"phrase_line_num\", \"paraphrase_line_num\")\n",
    "        \n",
    "        def open_series_of_csvs(base_filename, max_level, *headings):\n",
    "            return [open_csv(str(level)+base_filename, *headings) for level in range(1,max_level+1)]\n",
    "        \n",
    "        def open_series_of_corpuption_csvs(base_filename):\n",
    "            return open_series_of_csvs(base_filename, max_corruption_level, \"uncorrupt_phrase_line_num\", \"corrupt_phrase_line_num\")\n",
    "        \n",
    "\n",
    "        noun_random_semantic_corruption_csvs = open_series_of_corpuption_csvs(\"noun_random_semantic_corruptions.txt\")\n",
    "        noun_sym_semantic_corruption_csvs = open_series_of_corpuption_csvs(\"noun_sym_semantic_corruptions.txt\")\n",
    "        verb_sym_semantic_corruption_csvs= open_series_of_corpuption_csvs(\"verb_sym_semantic_corruptions.txt\")\n",
    "        verb_random_semantic_corruption_csvs= open_series_of_corpuption_csvs(\"verb_random_semantic_corruptions.txt\")\n",
    "        verb_anto_semantic_corruption_csvs= open_series_of_corpuption_csvs(\"verb_anto_semantic_corruptions.txt\")\n",
    "        #adj_anto_semantic_corruption_csvs= open_series_of_corpuption_csvs(\"adj_anto_semantic_corruptions.txt\")\n",
    "        #adj_sym_semantic_corruption_csvs= open_series_of_corpuption_csvs(\"adj_sym_semantic_corruptions.txt\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        def write_phrase(phrase):\n",
    "            global phrase_line_num\n",
    "            phrases_fh.write(phrase)\n",
    "            phrases_fh.write(\"\\n\")\n",
    "            phrase_line_num+=1\n",
    "            return phrase_line_num\n",
    "        \n",
    "        ##Recorder Functions\n",
    "        def add_phrase(phrase):\n",
    "            words, tagged_words = tokenize_and_tag(phrase)\n",
    "            phrase_line_num = write_phrase(' '.join(words))\n",
    "            return words, tagged_words, phrase_line_num\n",
    "        \n",
    "        def add_corruptions(words, tagged_words, base_phrase_ln):\n",
    "            short_phrase_indexes = get_phrases_indexes(tagged_words,3)\n",
    "            \n",
    "            def add_corrpution(csvs, corrupting_method, skip_indexes=short_phrase_indexes):\n",
    "                corrupt_phrases = leveled_semantic_corrupt_sentences_from_pretagged(words,\n",
    "                                                                                    tagged_words, \n",
    "                                                                                    corrupting_method,\n",
    "                                                                                    skip_indexes)\n",
    "                \n",
    "                for corrupt_phrase, csv in zip(corrupt_phrases, csvs):\n",
    "                    phrase_line_num = write_phrase(corrupt_phrase)\n",
    "                    csv.writerow([base_phrase_ln, phrase_line_num])\n",
    "            \n",
    "            \n",
    "            \n",
    "            add_corrpution(noun_sym_semantic_corruption_csvs, get_noun_synonyms_of_most_common)\n",
    "            add_corrpution(verb_sym_semantic_corruption_csvs, get_verb_synonyms_of_most_common)\n",
    "            add_corrpution(verb_anto_semantic_corruption_csvs, get_verb_antos_of_most_common)\n",
    "            add_corrpution(noun_random_semantic_corruption_csvs, get_noun_randoms, [])\n",
    "            add_corrpution(verb_random_semantic_corruption_csvs, get_verb_randoms, [])\n",
    "    \n",
    "        for ((m_id1,phrase1),(m_id2,phrase2)) in base_paraphrases:\n",
    "            #Add the phrases, and the corruptions\n",
    "            words1, tagged_words1, ln1 = add_phrase(phrase1)\n",
    "            add_corruptions(words1, tagged_words1, ln1)\n",
    "            \n",
    "            words2, tagged_words2, ln2 = add_phrase(phrase2)\n",
    "            add_corruptions(words2, tagged_words2, ln2)\n",
    "\n",
    "            #add to the record of microsoft ids\n",
    "            microsoft_ids_csv.writerow([ln1,m_id1])\n",
    "            microsoft_ids_csv.writerow([ln2,m_id2])\n",
    "\n",
    "            #add the paraphases, in both directions\n",
    "            paraphrases_csv.writerow([ln1,ln2])\n",
    "            paraphrases_csv.writerow([ln2,ln1])\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "    finally:\n",
    "        for fh in openned_filehandles:\n",
    "            fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corp_gen = itertools.chain(\n",
    "        load_MSRP(\"corpora/MSRP/msr_paraphrase_test.txt\"),\n",
    "        load_MSRP(\"corpora/MSRP/msr_paraphrase_train.txt\")\n",
    ")\n",
    "\n",
    "create_eval_corpora(corp_gen,\"prepared_corpora/msrp_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
