{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import itertools\n",
    "import codecs\n",
    "from os import path\n",
    "import csv\n",
    "from copy import deepcopy\n",
    "\n",
    "from SemanticCorruption import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_MSRP(filename):\n",
    "    with codecs.open(filename,'r', b\"utf-8\" ) as fh:\n",
    "        nlines = 0\n",
    "        for line in fh.readlines():\n",
    "            nlines+=1\n",
    "            if nlines==1:\n",
    "                continue\n",
    "            isparaphrase, id1, id2, str1, str2 = line.split(\"\\t\") #the quality fielld is 1 for phraphrases and 0 for not\n",
    "            if int(isparaphrase)== 1:\n",
    "                yield((int(id1),str1.strip()),(int(id2),str2.strip()))\n",
    "\n",
    "            \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_eval_corpora(base_paraphrases, folder, max_corruption_level=10):\n",
    "    \"\"\"We want to create the baseline corpus as a new line seperated sentences, so that it works well with Sorchers URAE system.\n",
    "    Thus all the metadata is stored in seperate files referencing the line numbers\"\"\"\n",
    "    \n",
    "    global phrase_line_num \n",
    "    phrase_line_num = 0 #line numebrs are always refered to after icrementing them\n",
    "    openned_filehandles = []\n",
    "    try:\n",
    "        phrases_fh = codecs.open(path.join(folder, \"phrases.txt\"),'w', b\"utf-8\" )\n",
    "        openned_filehandles.append(phrases_fh)\n",
    "\n",
    "        microsoft_ids_fh = open(path.join(folder, \"microsoft_ids.txt\"),'w')\n",
    "        openned_filehandles.append(microsoft_ids_fh)\n",
    "        microsoft_ids_csv = csv.writer(microsoft_ids_fh)\n",
    "        microsoft_ids_csv.writerow([\"phrase_line_number\",\"microsoft_id\"])\n",
    "\n",
    "        paraphrases_fh = open(path.join(folder, \"paraphrases.txt\"),\"w\")\n",
    "        openned_filehandles.append(paraphrases_fh)\n",
    "        paraphrases_csv = csv.writer(paraphrases_fh)\n",
    "        paraphrases_csv.writerow([\"phrase_line_num\", \"paraphrase_line_num\"])\n",
    "\n",
    "        #Open all the alway open files\n",
    "        noun_sym_semantic_corruption_csvs=[]\n",
    "        for level in range(1,max_corruption_level+1):\n",
    "            sc_fh = open(path.join(folder, str(level)+\"noun_sym_semantic_corruptions.txt\"),\"w\") \n",
    "            openned_filehandles.append(sc_fh)\n",
    "            sc_csv = csv.writer(sc_fh)                                         \n",
    "            sc_csv.writerow([\"uncorrupt_phrase_line_num\", \"corrupt_phrase_line_num\"])\n",
    "            noun_sym_semantic_corruption_csvs.append(sc_csv)\n",
    "            \n",
    "        verb_anto_semantic_corruption_csvs=[]\n",
    "        for level in range(1,max_corruption_level+1):\n",
    "            sc_fh = open(path.join(folder, str(level)+\"verb_anto_semantic_corruptions.txt\"),\"w\") \n",
    "            openned_filehandles.append(sc_fh)\n",
    "            sc_csv = csv.writer(sc_fh)                                         \n",
    "            sc_csv.writerow([\"uncorrupt_phrase_line_num\", \"corrupt_phrase_line_num\"])\n",
    "            verb_anto_semantic_corruption_csvs.append(sc_csv)\n",
    "\n",
    "        ##Recorder Functions\n",
    "        def add_phrase(phrase):\n",
    "            global phrase_line_num\n",
    "            words, tagged_words = tokenize_and_tag(phrase)\n",
    "            phrases_fh.write(' '.join(words))\n",
    "            phrases_fh.write(\"\\n\")\n",
    "            phrase_line_num+=1\n",
    "            return words, tagged_words, phrase_line_num\n",
    "        \n",
    "        def add_corruptions(words, tagged_words, phrase_ln):\n",
    "            global phrase_line_num\n",
    "            \n",
    "            short_phrase_indexes = get_phrases_indexes(tagged_words,3)\n",
    "            \n",
    "            noun_corrupted_phases = leveled_semantic_corrupt_sentences_from_pretagged(words, tagged_words, get_noun_synonyms_of_most_common_,short_phrase_indexes)\n",
    "            for corruption, noun_sym_level_sc_csv in zip(noun_corrupted_phases, noun_sym_semantic_corruption_csvs):\n",
    "                phrases_fh.write(corruption)\n",
    "                phrases_fh.write(\"\\n\")\n",
    "                phrase_line_num+=1\n",
    "                noun_sym_level_sc_csv.writerow([phrase_ln, phrase_line_num])\n",
    "                \n",
    "            verb_corrupted_phases = leveled_semantic_corrupt_sentences_from_pretagged(words, tagged_words, get_verb_antos_of_most_common_,short_phrase_indexes)\n",
    "            for corruption, verb_anto_level_sc_csv in zip(verb_corrupted_phases, verb_anto_semantic_corruption_csvs):\n",
    "                phrases_fh.write(corruption)\n",
    "                phrases_fh.write(\"\\n\")\n",
    "                phrase_line_num+=1\n",
    "                verb_anto_level_sc_csv.writerow([phrase_ln, phrase_line_num])\n",
    "        \n",
    "    \n",
    "        for ((m_id1,phrase1),(m_id2,phrase2)) in base_paraphrases:\n",
    "            #Add the phrases, and the corruptions\n",
    "            words1, tagged_words1, ln1 = add_phrase(phrase1)\n",
    "            add_corruptions(words1, tagged_words1, ln1)\n",
    "            \n",
    "            words2, tagged_words2, ln2 = add_phrase(phrase2)\n",
    "            add_corruptions(words2, tagged_words2, ln2)\n",
    "\n",
    "            #add to the record of microsoft ids\n",
    "            microsoft_ids_csv.writerow([ln1,m_id1])\n",
    "            microsoft_ids_csv.writerow([ln2,m_id2])\n",
    "\n",
    "            #add the paraphases, in both directions\n",
    "            paraphrases_csv.writerow([ln1,ln2])\n",
    "            paraphrases_csv.writerow([ln2,ln1])\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "    finally:\n",
    "        for fh in openned_filehandles:\n",
    "            fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corp_gen = itertools.chain(\n",
    "        load_MSRP(\"corpora/MSRP/msr_paraphrase_test.txt\"),\n",
    "        load_MSRP(\"corpora/MSRP/msr_paraphrase_train.txt\")\n",
    ")\n",
    "\n",
    "create_eval_corpora(corp_gen,\"prepared_corpora/msrp_ns_va_nophrase_mfcwsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
