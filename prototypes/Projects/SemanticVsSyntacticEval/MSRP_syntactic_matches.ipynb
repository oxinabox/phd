{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"prepared_corpora/msrp_ns_va_nophrase_mfcwsd/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "from __future__ import division\n",
    "\n",
    "import itertools\n",
    "import codecs\n",
    "from os import path\n",
    "import csv\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_MSRP(filename):\n",
    "    with codecs.open(filename,'r', b\"utf-8\" ) as fh:\n",
    "        nlines = 0\n",
    "        for line in fh.readlines():\n",
    "            nlines+=1\n",
    "            if nlines==1:\n",
    "                continue\n",
    "            isparaphrase, id1, id2, str1, str2 = line.split(\"\\t\") #the quality fielld is 1 for phraphrases and 0 for not\n",
    "            yield(int(isparaphrase)==1, (int(id1),str1.strip()),(int(id2),str2.strip()))\n",
    "\n",
    "            \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_eval_corpora(base_paraphrases, folder, max_corruption_level=10):\n",
    "    \"\"\"We want to create the baseline corpus as a new line seperated sentences, so that it works well with Sorchers URAE system.\n",
    "    Thus all the metadata is stored in seperate files referencing the line numbers\"\"\"\n",
    "    \n",
    "    global phrase_line_num \n",
    "    phrase_line_num = 0 #line numebrs are always refered to after icrementing them\n",
    "    openned_filehandles = []\n",
    "    try:\n",
    "        phrases_fh = codecs.open(path.join(folder, \"phrases.txt\"),'w', b\"utf-8\" )\n",
    "        openned_filehandles.append(phrases_fh)\n",
    "\n",
    "        microsoft_ids_fh = open(path.join(folder, \"microsoft_ids.txt\"),'w')\n",
    "        openned_filehandles.append(microsoft_ids_fh)\n",
    "        microsoft_ids_csv = csv.writer(microsoft_ids_fh)\n",
    "        microsoft_ids_csv.writerow([\"phrase_line_number\",\"microsoft_id\"])\n",
    "\n",
    "        paraphrases_fh = open(path.join(folder, \"paraphrases.txt\"),\"w\")\n",
    "        openned_filehandles.append(paraphrases_fh)\n",
    "        paraphrases_csv = csv.writer(paraphrases_fh)\n",
    "        paraphrases_csv.writerow([\"phrase_line_num\", \"paraphrase_line_num\"])\n",
    "\n",
    "        ##Recorder Functions\n",
    "        def add_phrase(phrase):\n",
    "            global phrase_line_num\n",
    "            phrases_fh.write(phrase)\n",
    "            phrases_fh.write(\"\\n\")\n",
    "            phrase_line_num+=1\n",
    "            return phrase_line_num\n",
    "        \n",
    "   \n",
    "        for (isparaphrase,(m_id1,phrase1),(m_id2,phrase2)) in base_paraphrases:\n",
    "            #Add the phrases, and the corruptions\n",
    "            ln1 = add_phrase(phrase1)\n",
    "            ln2 = add_phrase(phrase2)\n",
    "            #add to the record of microsoft ids\n",
    "            microsoft_ids_csv.writerow([ln1,m_id1])\n",
    "            microsoft_ids_csv.writerow([ln2,m_id2])\n",
    "\n",
    "            if (isparaphrase):\n",
    "                #add the paraphases, in both directions\n",
    "                paraphrases_csv.writerow([ln1,ln2])\n",
    "                paraphrases_csv.writerow([ln2,ln1])\n",
    "            \n",
    "    finally:\n",
    "        for fh in openned_filehandles:\n",
    "            fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corp_gen = itertools.chain(\n",
    "        load_MSRP(\"../../base_corpora/MSRP/msr_paraphrase_test.txt\"),\n",
    "        load_MSRP(\"../../base_corpora/MSRP/msr_paraphrase_train.txt\")\n",
    ")\n",
    "\n",
    "create_eval_corpora(corp_gen,\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Here one must go and generate the Parsed versions (and while at it make the embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "No such file or directory: u'/home/wheel/oxinabox/phd/prototypes/Projects/SemanticVsSyntacticEval/prepared_corpora/msrp_ns_va_nophrase_mfcwsd/parsed.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-56566d17ea6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcorpus_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBracketParseCorpusReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"parsed.txt\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mparse_trees\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparsed_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nltk/corpus/reader/api.pyc\u001b[0m in \u001b[0;36mparsed_sents\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_parsed_sent_block\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         return concat([StreamBackedCorpusView(fileid, reader, encoding=enc)\n\u001b[1;32m--> 388\u001b[1;33m                        for fileid, enc in self.abspaths(fileids, True)])\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtagged_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nltk/corpus/reader/api.pyc\u001b[0m in \u001b[0;36mabspaths\u001b[1;34m(self, fileids, include_encoding, include_fileid)\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[0mpaths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minclude_encoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minclude_fileid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nltk/compat.pyc\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/nltk/data.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No such file or directory: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: No such file or directory: u'/home/wheel/oxinabox/phd/prototypes/Projects/SemanticVsSyntacticEval/prepared_corpora/msrp_ns_va_nophrase_mfcwsd/parsed.txt'"
     ]
    }
   ],
   "source": [
    "corpus_reader = nltk.corpus.BracketParseCorpusReader(\".\",[\"parsed.txt\"])\n",
    "parse_trees = np.array(corpus_reader.parsed_sents(), dtype=object).squeeze();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labels_only(tree):\n",
    "    if isinstance(tree, nltk.Tree):\n",
    "        labels = [tree.label()]\n",
    "        \n",
    "        for child in tree:\n",
    "            child_label = labels_only(child)\n",
    "            if child_label:\n",
    "                labels.append(child_label)\n",
    "        return tuple(labels)\n",
    "            \n",
    "    else:\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 10212, 2: 564, 3: 56, 4: 8, 7: 2, 11: 2, 6: 1, 8: 1, 12: 1})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "count = collections.Counter([labels_only(tree) for id,tree in enumerate(parse_trees)])\n",
    "collections.Counter(count.values())\n",
    "#~700 syntactic matches idk how many are already paraphrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "structures = collections.defaultdict(set)\n",
    "\n",
    "for id,tree in enumerate(parse_trees,1):\n",
    "    struct = labels_only(tree) \n",
    "    structures[struct].add(id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "para_ids = np.loadtxt(\"./paraphrases.txt\", delimiter=\",\",skiprows=1, dtype=np.int)\n",
    "para_id_map = {row[0]:row[1] for row in para_ids}\n",
    "\n",
    "structural_matches = []\n",
    "for struct_set in structures.values():\n",
    "    if len(struct_set)>1:\n",
    "        for id in struct_set:\n",
    "            for other_id in struct_set.difference({id}):\n",
    "                if not (id in para_id_map) or para_id_map[id]!=other_id:\n",
    "                    leaves1 = set(parse_trees[id-1].leaves())\n",
    "                    leaves2 = parse_trees[other_id-1].leaves()\n",
    "                    if len(leaves1.intersection(leaves2))/len(leaves1) < 0.70: # must be 50% difference in bag of words \n",
    "                        structural_matches.append((id,other_id))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_phrase_numbers = np.loadtxt(\"microsoft_ids.txt\", delimiter=\",\",skiprows=1, dtype=np.int)[:,0]\\\n",
    "base_phrases = np.asarray(list(open(\"phrases.txt\",'r')))[base_phrase_numbers-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.parse import stanford\n",
    "from nltk.tag.stanford import POSTagger\n",
    "\n",
    "import os\n",
    "os.environ['CLASSPATH'] = '/home/wheel/oxinabox/nltk_data/standford_models/stanford-postagger/stanford-postagger.jar'\n",
    "os.environ['STANFORD_MODELS'] = '/home/wheel/oxinabox/nltk_data/standford_models/stanford-postagger/models/'\n",
    "\n",
    "\n",
    "standford_pos_tagger = POSTagger(\"english-bidirectional-distsim.tagger\")\n",
    "def pos_tag(words):\n",
    "    return standford_pos_tagger.tag(words)[0]\n",
    "    \n",
    "def tokenize_and_tag(sent):\n",
    "    tokens = nltk.tokenize.word_tokenize(sent)\n",
    "    return tokens, pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tags_only(phrase):\n",
    "    return (zip(*tokenize_and_tag(phrase)[1])[1])\n",
    "\n",
    "base_tags2phrases = defaultdict(list)\n",
    "for sent in base_phrases:\n",
    "    base_tags2phrases[tags_only(sent)].append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "masc_tags2phrases = None\n",
    "with open(\"../../masc_tag_dict.pickle\",\"rb\") as fh:\n",
    "    masc_tags2phrases = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!\n"
     ]
    }
   ],
   "source": [
    "masc_tags= frozenset(masc_tags2phrases.keys())\n",
    "overlapping = []\n",
    "for tags_key in base_tags2phrases.keys():\n",
    "    if tags_key in masc_tags:\n",
    "        overlapping.append(tags_key)\n",
    "        print(\"!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overlap = overlapping[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(u'I', u\"'m\", u'not', u'going', u'to', u'allow', u'that', u'question', u'.')}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masc_tags2phrases[overlap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I 'm never going to forget this day .\\n\"]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_tags2phrases[overlap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7800"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
