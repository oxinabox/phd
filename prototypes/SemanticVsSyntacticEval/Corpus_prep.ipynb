{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a prototype of they system for corrupting sentences, for different semantic meaning:\n",
    "\n",
    "My process is:\n",
    "\n",
    "1. Tokenize the sentence (Currently using the NLTK regex tokenizer, it seem sufficient)\n",
    "2. Parts of Speech tag (Currently using the Stanford POS Tagger (via NLTK))\n",
    "3. For each word that is not blacklisted (I have blacklisted \"had\", \"were\", \"have\", \"was\", and \"be\", as they are unusual verbs with strange antoymns, further more the are rather syntatic)\n",
    "        1. Use WordNet to find antonyms of the same POS tag  (So \"race\" (Noun, as in a competition) has no antonyms, but \"race\" (Verb, to race the train) has \"linger\" as an antonym.\n",
    "        2. Unstem: WordNet stemming/lemmaisation (of the antonym) removes Tense, Plurality, comparativeness, and superlativeness, so I make use of the POS tag of the original to work those out, then restore them using the Pattern library's tools for this (http://jmlr.csail.mit.edu/papers/volume13/desmedt12a/desmedt12a.pdf)\n",
    "        3. I remove any suggested antonyms that are short phrases (eg Wordnet suggests that \"take_away\" is an antonym of \"add\", however as adding a work word change the structure of the sentence.)\n",
    "        4. select one randomly if there are multiple.\n",
    "4. I incrementally subsitute one addional antynym until I have run out of words with antonys, saving the sentence at each step. (What I'm currentl;y callign each sentence of different corruption level)\n",
    "5. I repair the indefinite articles ('an' vs 'a')\n",
    "\n",
    "I can check the final sentence by sending it through the POS tagger and seeing if I get the same tags.\n",
    "This is not perfect. Its not bad though.\n",
    "It got a lot better when I changed to using the Stanford POS tagger, as it was more able to tag and retag correctly and thus was most consistent.\n",
    "\n",
    "The whole method is very heurustic.\n",
    "It's not too bad but makes a lot of unnatural sounding sentences.\n",
    "And will sometimes choose the wrong antonym.\n",
    "\n",
    "For example stating with the sentence:\n",
    " - It's not too bad but makes a lot of unnatural sounding sentences.\n",
    " - It 's not too unregretful but makes a lot of unnatural sounding sentences.\n",
    " - It 's not too unregretful but makes a lot of unnatural devoicing sentences.\n",
    " - It 's not too unregretful but makes a lot of unaffected devoicing sentences.\n",
    " - It 's not too unregretful but makes a lot of unaffected devoicing acquittals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from SemanticCorruption import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pattern.en as en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"It 's not too bad but makes a lot of unnatural sounding acquittals .\",\n",
       " u\"It 's not too unregretful but makes a lot of unnatural sounding acquittals .\",\n",
       " u\"It 's not too unregretful but makes a lot of unnatural devoicing acquittals .\",\n",
       " u\"It 's not too unregretful but makes a lot of unaffected devoicing acquittals .\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"It's not too bad but makes a lot of unnatural sounding sentences.\"\n",
    "list(leveled_semantic_corrupt_sentences(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'a Washington County man may have the countys first nonhuman case of West Nile virus , the health department said Friday',\n",
       " u'an Washington County woman may have the countys first nonhuman case of West Nile virus , the health department said Friday',\n",
       " u'an Washington County woman may have the countys first nonhuman case of West Nile virus , the illness department said Friday']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'A Washington County man may have the countys first human case of West Nile virus, the health department said Friday'\n",
    "list(leveled_semantic_corrupt_sentences(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'It was an dusty , dry , hot night , and the flys were buzzing .',\n",
       " u'It was an dusty , sweet , hot night , and the flys were buzzing .',\n",
       " u'It was an dusty , sweet , cold night , and the flys were buzzing .']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"It was a dusty, dry, hot day, and the flys were buzzing.\"\n",
    "list(leveled_semantic_corrupt_sentences(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed on: (most) RBS  != JJS (least)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'The article is the least uncommon determiner ( DT ) in English .'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_corruption(\"The article is the most common determiner (DT) in English.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'We may have an answer'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_corruption(\"We may have a question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed on: (expected) VBN  != JJ (unexpected)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'Is staying an even number of verbs , adverbs , adjectives and nouns to their synonyms unexpected to produce an semantically close acquittal ?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent=\"Is changing an odd number of verbs, adverbs, adjectives and nouns to their antonyms expected to produce a semantically distant sentence?\"\n",
    "checked_corruption(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Both gerunds and infinitives can be misused as the subject or the complement of an acquittal .'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_corruption(\"Both gerunds and infinitives can be used as the subject or the complement of a sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Shares of Xoma ascended 16 percent in late trade , while shares of Genentech , an much less company with several products on the market , were downwardly 2 percent .'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"Shares of Xoma fell 16 percent in early trade, while shares of Genentech, a much larger company with several products on the market, were up 2 percent.\"\n",
    "checked_corruption(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Six months ago , the IMF and Argentina missed an bare-minimum $ 6.8-billion debt rollover deal that expires in August .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_corruption(\"Six months ago, the IMF and Argentina struck a bare-minimum $6.8-billion debt rollover deal that expires in August.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"He plans to have dinner with troops at Kosovo 's U.S. civilian headquarters , Camp Bondsteel .\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_corruption(\"He plans to have dinner with troops at Kosovo's U.S. military headquarters, Camp Bondsteel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed on: (stationed) VBD  != VBN (stationed)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'After that , he plans to have dinner at Camp Bondsteel with U.S. troops stationed here .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_corruption(\"After that, he plans to have dinner at Camp Bondsteel with U.S. troops stationed there.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'He subtracted that prosecutors will seek the birth reward .'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_corruption(\"He added that prosecutors will seek the death penalty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Who is this woman ?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_corruption(\"Who is this man?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Who is that woman ?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_corruption(\"Who is that man?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'This good thief stole that car !'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_corruption(\"This evil thief stole that car!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'The motorist is unangry , so the pedestrian is unintelligibly scared'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_corruption(\"The motorist is angry, so the pedestrian is understandably scared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The hero was brave , he defeated the dragon .'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_corruption(\"The hero was brave, he defeated the dragon.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed on: (cowardly) JJ  != VB (brave)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'The villian was brave , he was slain by the dragon .'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_corruption(\"The villian was cowardly, he was slain by the dragon.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"War and Peace opens in the Russian city of St. Petersburg in 1805 , as Napoleon 's conquest of eastern Europe is just ending to stir fearlessnesses in Russia .\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_corruption(\"War and Peace opens in the Russian city of St. Petersburg in 1805, as Napoleon's conquest of western Europe is just beginning to stir fears in Russia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'linger']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(get_all_antonyms(\"race\", wn.VERB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
